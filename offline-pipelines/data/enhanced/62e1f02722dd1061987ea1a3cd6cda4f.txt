[Skip to content](https://github.com/unslothai/unsloth/#start-of-content)
## Navigation Menu
Toggle navigation
[ ](https://github.com/)
[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth%2F)
  * Product 
    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)
    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)
    * [ Actions Automate any workflow  ](https://github.com/features/actions)
    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)
    * [ Issues Plan and track work  ](https://github.com/features/issues)
    * [ Code Review Manage code changes  ](https://github.com/features/code-review)
    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)
    * [ Code Search Find more, search less  ](https://github.com/features/code-search)
Explore
    * [ All features ](https://github.com/features)
    * [ Documentation ](https://docs.github.com)
    * [ GitHub Skills ](https://skills.github.com)
    * [ Blog ](https://github.blog)
  * Solutions 
By company size
    * [ Enterprises ](https://github.com/enterprise)
    * [ Small and medium teams ](https://github.com/team)
    * [ Startups ](https://github.com/enterprise/startups)
    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)
By use case
    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)
    * [ DevOps ](https://github.com/solutions/use-case/devops)
    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)
    * [ View all use cases ](https://github.com/solutions/use-case)
By industry
    * [ Healthcare ](https://github.com/solutions/industry/healthcare)
    * [ Financial services ](https://github.com/solutions/industry/financial-services)
    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)
    * [ Government ](https://github.com/solutions/industry/government)
    * [ View all industries ](https://github.com/solutions/industry)
[ View all solutions ](https://github.com/solutions)
  * Resources 
Topics
    * [ AI ](https://github.com/resources/articles/ai)
    * [ DevOps ](https://github.com/resources/articles/devops)
    * [ Security ](https://github.com/resources/articles/security)
    * [ Software Development ](https://github.com/resources/articles/software-development)
    * [ View all ](https://github.com/resources/articles)
Explore
    * [ Learning Pathways ](https://resources.github.com/learn/pathways)
    * [ Events & Webinars ](https://resources.github.com)
    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)
    * [ Customer Stories ](https://github.com/customer-stories)
    * [ Partners ](https://partner.github.com)
    * [ Executive Insights ](https://github.com/solutions/executive-insights)
  * Open Source 
    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)
    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)
Repositories
    * [ Topics ](https://github.com/topics)
    * [ Trending ](https://github.com/trending)
    * [ Collections ](https://github.com/collections)
  * Enterprise 
    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)
Available add-ons
    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)
    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)
    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)
  * [Pricing](https://github.com/pricing)


Search or jump to...
# Search code, repositories, users, issues, pull requests...
Search 
Clear
[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)
#  Provide feedback 
We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
Cancel  Submit feedback 
#  Saved searches 
## Use saved searches to filter your results more quickly
Name
Query
To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). 
Cancel  Create saved search 
[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth%2F)
[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=unslothai%2Funsloth) Reseting focus
You signed in with another tab or window. [Reload](https://github.com/unslothai/unsloth/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/unslothai/unsloth/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/unslothai/unsloth/) to refresh your session. Dismiss alert
{{ message }}
[ unslothai ](https://github.com/unslothai) / **[unsloth](https://github.com/unslothai/unsloth) ** Public
  * [ Notifications ](https://github.com/login?return_to=%2Funslothai%2Funsloth) You must be signed in to change notification settings
  * [ Fork 2.8k ](https://github.com/login?return_to=%2Funslothai%2Funsloth)
  * [ Star  36.2k ](https://github.com/login?return_to=%2Funslothai%2Funsloth)


Finetune Llama 3.3, DeepSeek-R1, Gemma 3 & Reasoning LLMs 2x faster with 70% less memory! 🦥 
[unsloth.ai](https://unsloth.ai "https://unsloth.ai")
### License
[ Apache-2.0 license ](https://github.com/unslothai/unsloth/blob/main/LICENSE)
[ 36.2k stars ](https://github.com/unslothai/unsloth/stargazers) [ 2.8k forks ](https://github.com/unslothai/unsloth/forks) [ Branches ](https://github.com/unslothai/unsloth/branches) [ Tags ](https://github.com/unslothai/unsloth/tags) [ Activity ](https://github.com/unslothai/unsloth/activity)
[ Star  ](https://github.com/login?return_to=%2Funslothai%2Funsloth)
[ Notifications ](https://github.com/login?return_to=%2Funslothai%2Funsloth) You must be signed in to change notification settings
  * [ Code ](https://github.com/unslothai/unsloth)
  * [ Issues 924 ](https://github.com/unslothai/unsloth/issues)
  * [ Pull requests 65 ](https://github.com/unslothai/unsloth/pulls)
  * [ Discussions ](https://github.com/unslothai/unsloth/discussions)
  * [ Actions ](https://github.com/unslothai/unsloth/actions)
  * [ Wiki ](https://github.com/unslothai/unsloth/wiki)
  * [ Security ](https://github.com/unslothai/unsloth/security)
  * [ Insights ](https://github.com/unslothai/unsloth/pulse)


Additional navigation options
  * [ Code  ](https://github.com/unslothai/unsloth)
  * [ Issues  ](https://github.com/unslothai/unsloth/issues)
  * [ Pull requests  ](https://github.com/unslothai/unsloth/pulls)
  * [ Discussions  ](https://github.com/unslothai/unsloth/discussions)
  * [ Actions  ](https://github.com/unslothai/unsloth/actions)
  * [ Wiki  ](https://github.com/unslothai/unsloth/wiki)
  * [ Security  ](https://github.com/unslothai/unsloth/security)
  * [ Insights  ](https://github.com/unslothai/unsloth/pulse)


# unslothai/unsloth
main
[Branches](https://github.com/unslothai/unsloth/branches)[Tags](https://github.com/unslothai/unsloth/tags)
[](https://github.com/unslothai/unsloth/branches)[](https://github.com/unslothai/unsloth/tags)
Go to file
Code
## Folders and files
Name| Name| Last commit message| Last commit date  
---|---|---|---  
## Latest commit
## History
[1,157 Commits](https://github.com/unslothai/unsloth/commits/main/)[](https://github.com/unslothai/unsloth/commits/main/)  
[.github](https://github.com/unslothai/unsloth/tree/main/.github ".github")| [.github](https://github.com/unslothai/unsloth/tree/main/.github ".github")  
[images](https://github.com/unslothai/unsloth/tree/main/images "images")| [images](https://github.com/unslothai/unsloth/tree/main/images "images")  
[tests](https://github.com/unslothai/unsloth/tree/main/tests "tests")| [tests](https://github.com/unslothai/unsloth/tree/main/tests "tests")  
[unsloth](https://github.com/unslothai/unsloth/tree/main/unsloth "unsloth")| [unsloth](https://github.com/unslothai/unsloth/tree/main/unsloth "unsloth")  
[CONTRIBUTING.md](https://github.com/unslothai/unsloth/blob/main/CONTRIBUTING.md "CONTRIBUTING.md")| [CONTRIBUTING.md](https://github.com/unslothai/unsloth/blob/main/CONTRIBUTING.md "CONTRIBUTING.md")  
[LICENSE](https://github.com/unslothai/unsloth/blob/main/LICENSE "LICENSE")| [LICENSE](https://github.com/unslothai/unsloth/blob/main/LICENSE "LICENSE")  
[README.md](https://github.com/unslothai/unsloth/blob/main/README.md "README.md")| [README.md](https://github.com/unslothai/unsloth/blob/main/README.md "README.md")  
[pyproject.toml](https://github.com/unslothai/unsloth/blob/main/pyproject.toml "pyproject.toml")| [pyproject.toml](https://github.com/unslothai/unsloth/blob/main/pyproject.toml "pyproject.toml")  
[unsloth-cli.py](https://github.com/unslothai/unsloth/blob/main/unsloth-cli.py "unsloth-cli.py")| [unsloth-cli.py](https://github.com/unslothai/unsloth/blob/main/unsloth-cli.py "unsloth-cli.py")  
View all files  
## Repository files navigation
  * [README](https://github.com/unslothai/unsloth/)
  * [Apache-2.0 license](https://github.com/unslothai/unsloth/)


[ ![unsloth logo](https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png) ](https://unsloth.ai)
[![](https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_\(8B\)-Alpaca.ipynb) [![](https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png)](https://discord.com/invite/unsloth) [![](https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png)](https://docs.unsloth.ai)
### Finetune Llama 3.3, Gemma 3, Phi-4, Qwen 2.5 & Mistral 2x faster with 80% less VRAM!
[](https://github.com/unslothai/unsloth/#finetune-llama-33-gemma-3-phi-4-qwen-25--mistral-2x-faster-with-80-less-vram)
[![](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)
## ✨ Finetune for Free
[](https://github.com/unslothai/unsloth/#-finetune-for-free)
Notebooks are beginner friendly. Read our [guide](https://docs.unsloth.ai/get-started/fine-tuning-guide). Add your dataset, click "Run All", and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.
Unsloth supports | Free Notebooks | Performance | Memory use  
---|---|---|---  
**GRPO (R1 reasoning)** | [▶️ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_\(8B\)-GRPO.ipynb) | 2x faster | 80% less  
**Gemma 3 (4B)** | [▶️ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_\(4B\).ipynb) | 1.6x faster | 60% less  
**Llama 3.2 (3B)** | [▶️ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_\(1B_and_3B\)-Conversational.ipynb) | 2x faster | 70% less  
**Phi-4 (14B)** | [▶️ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb) | 2x faster | 70% less  
**Llama 3.2 Vision (11B)** | [▶️ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_\(11B\)-Vision.ipynb) | 2x faster | 50% less  
**Llama 3.1 (8B)** | [▶️ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_\(8B\)-Alpaca.ipynb) | 2x faster | 70% less  
**Qwen 2.5 (7B)** | [▶️ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_\(7B\)-Alpaca.ipynb) | 2x faster | 70% less  
**Mistral v0.3 (7B)** | [▶️ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_\(7B\)-Conversational.ipynb) | 2.2x faster | 75% less  
**Ollama** | [▶️ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_\(8B\)-Ollama.ipynb) | 1.9x faster | 60% less  
**DPO Zephyr** | [▶️ Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_\(7B\)-DPO.ipynb) | 1.9x faster | 50% less  
  * See [all our notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) and [all our models](https://docs.unsloth.ai/get-started/all-our-models)
  * **Kaggle Notebooks** for [Llama 3.2 Kaggle notebook](https://www.kaggle.com/danielhanchen/kaggle-llama-3-2-1b-3b-unsloth-notebook), [Llama 3.1 (8B)](https://www.kaggle.com/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook), [Phi-4 (14B)](https://www.kaggle.com/code/danielhanchen/phi-4-finetuning-unsloth-notebook), [Mistral (7B)](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
  * See detailed documentation for Unsloth [here](https://docs.unsloth.ai/).


## ⚡ Quickstart
[](https://github.com/unslothai/unsloth/#-quickstart)
  * **Install with pip (recommended)** for Linux devices:


```
pip install unsloth

```

For Windows install instructions, see [here](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation).
## 🦥 Unsloth.ai News
[](https://github.com/unslothai/unsloth/#-unslothai-news)
  * 📣 NEW! [**EVERYTHING** is now supported](https://unsloth.ai/blog/gemma3#everything) incuding: FFT, ALL models (Mixtral, MOE, Cohere, Mamba) and all training algorithms (KTO, DoRA) etc. MultiGPU support coming very soon. To enable full-finetuning, set `full_finetuning = True` and for 8-bit finetuning, set `load_in_8bit = True`
  * 📣 NEW! **Gemma 3** by Google: [Read Blog](https://unsloth.ai/blog/gemma3). We [uploaded GGUFs, 4-bit models](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa).
  * 📣 NEW! Introducing Long-context [Reasoning (GRPO)](https://unsloth.ai/blog/grpo) in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!
  * 📣 NEW! [DeepSeek-R1](https://unsloth.ai/blog/deepseek-r1) - the most powerful open reasoning models with Llama & Qwen distillations. Run or fine-tune them now [with our guide](https://unsloth.ai/blog/deepseek-r1). All model uploads: [here](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5).
  * 📣 NEW! [Phi-4](https://unsloth.ai/blog/phi4) by Microsoft: We also [fixed bugs](https://unsloth.ai/blog/phi4) in Phi-4 and [uploaded GGUFs, 4-bit](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa).
  * 📣 NEW! [Llama 3.3 (70B)](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f), Meta's latest model is supported.
  * 📣 Introducing Unsloth [Dynamic 4-bit Quantization](https://unsloth.ai/blog/dynamic-4bit)! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using <10% more VRAM than BnB 4-bit. See our collection on [Hugging Face here.](https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7)
  * 📣 [Vision models](https://unsloth.ai/blog/vision) now supported! [Llama 3.2 Vision (11B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_\(11B\)-Vision.ipynb), [Qwen 2.5 VL (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_\(7B\)-Vision.ipynb) and [Pixtral (12B) 2409](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_\(12B\)-Vision.ipynb)

Click for more news
  * 📣 NEW! We worked with Apple to add [Cut Cross Entropy](https://arxiv.org/abs/2411.09009). Unsloth now supports 89K context for Meta's Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.
  * 📣 We found and helped fix a [gradient accumulation bug](https://unsloth.ai/blog/gradient)! Please update Unsloth and transformers.
  * 📣 Try out [Chat interface](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb)!
  * 📣 NEW! Qwen-2.5 including [Coder](https://unsloth.ai/blog/qwen-coder) models are now supported with bugfixes. 14b fits in a Colab GPU! [Qwen 2.5 conversational notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_\(14B\)-Conversational.ipynb)
  * 📣 NEW! [Mistral Small 22b notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_\(22B\)-Alpaca.ipynb) finetuning fits in under 16GB of VRAM!
  * 📣 NEW! `pip install unsloth` now works! Head over to [pypi](https://pypi.org/project/unsloth/) to check it out! This allows non git pull installs. Use `pip install unsloth[colab-new]` for non dependency installs.
  * 📣 NEW! Continued Pretraining [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_\(7B\)-CPT.ipynb) for other languages like Korean!
  * 📣 [2x faster inference](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_\(8B\)-Inference.ipynb) added for all our models
  * 📣 We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support [4x longer context windows](https://unsloth.ai/blog/long-context)!


## 🔗 Links and Resources
[](https://github.com/unslothai/unsloth/#-links-and-resources)
Type | Links  
---|---  
📚 **Documentation & Wiki** | [Read Our Docs](https://docs.unsloth.ai)  
[![](https://camo.githubusercontent.com/2d318d2de87051f80a781377bcf094af5e9a9648beb1f93cb3471cf4d5da0af0/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f362f36662f4c6f676f5f6f665f547769747465722e737667)](https://camo.githubusercontent.com/2d318d2de87051f80a781377bcf094af5e9a9648beb1f93cb3471cf4d5da0af0/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f362f36662f4c6f676f5f6f665f547769747465722e737667) **Twitter (aka X)** | [Follow us on X](https://twitter.com/unslothai)  
💾 **Installation** | [Pip install](https://docs.unsloth.ai/get-started/installing-+-updating)  
🔮 **Our Models** | [Unsloth Releases](https://docs.unsloth.ai/get-started/all-our-models)  
✍️ **Blog** | [Read our Blogs](https://unsloth.ai/blog)  
[![](https://camo.githubusercontent.com/366c9dc2ccd72d171a31e5af3cdfb0b42ce61a6f22a20d3e53609ad9ccb47655/68747470733a2f2f726564646974696e632e636f6d2f68732d66732f68756266732f526564646974253230496e632f4272616e642f5265646469745f4c6f676f2e706e67)](https://camo.githubusercontent.com/366c9dc2ccd72d171a31e5af3cdfb0b42ce61a6f22a20d3e53609ad9ccb47655/68747470733a2f2f726564646974696e632e636f6d2f68732d66732f68756266732f526564646974253230496e632f4272616e642f5265646469745f4c6f676f2e706e67) **Reddit** | [Join our Reddit page](https://reddit.com/r/unsloth)  
## ⭐ Key Features
[](https://github.com/unslothai/unsloth/#-key-features)
  * Supports **full-finetuning** , pretraining, 4b-bit, 16-bit and **8-bit** training
  * All kernels written in [OpenAI's Triton](https://openai.com/index/triton/) language. **Manual backprop engine**.
  * **0% loss in accuracy** - no approximation methods - all exact.
  * No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.
  * Works on **Linux** and **Windows**
  * Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
  * If you trained a model with 🦥Unsloth, you can use this cool sticker! [![](https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png)](https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png)


## 💾 Install Unsloth
[](https://github.com/unslothai/unsloth/#-install-unsloth)
You can also see our documentation for more detailed installation and updating instructions [here](https://docs.unsloth.ai/get-started/installing-+-updating).
### Pip Installation
[](https://github.com/unslothai/unsloth/#pip-installation)
**Install with pip (recommended) for Linux devices:**
```
pip install unsloth

```

See [here](https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation) for advanced pip install instructions.
### Windows Installation
[](https://github.com/unslothai/unsloth/#windows-installation)
Warning
Python 3.13 does not support Unsloth. Use 3.12, 3.11 or 3.10
  1. **Install NVIDIA Video Driver:** You should install the latest version of your GPUs driver. Download drivers here: [NVIDIA GPU Drive](https://www.nvidia.com/Download/index.aspx).
  2. **Install Visual Studio C++:** You will need Visual Studio, with C++ installed. By default, C++ is not installed with [Visual Studio](https://visualstudio.microsoft.com/vs/community/), so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see [here](https://docs.unsloth.ai/get-started/installing-+-updating).
  3. **Install CUDA Toolkit:** Follow the instructions to install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive).
  4. **Install PyTorch:** You will need the correct version of PyTorch that is compatibile with your CUDA drivers, so make sure to select them carefully. [Install PyTorch](https://pytorch.org/get-started/locally/).
  5. **Install Unsloth:**


```
pip install unsloth
```

#### Notes
[](https://github.com/unslothai/unsloth/#notes)
To run Unsloth directly on Windows:
  * Install Triton from this Windows fork and follow the instructions [here](https://github.com/woct0rdho/triton-windows) (be aware that the Windows fork requires PyTorch >= 2.4 and CUDA 12)
  * In the SFTTrainer, set `dataset_num_proc=1` to avoid a crashing issue:


```
trainer = SFTTrainer(
  dataset_num_proc=1,
  ...
)
```

#### Advanced/Troubleshooting
[](https://github.com/unslothai/unsloth/#advancedtroubleshooting)
For **advanced installation instructions** or if you see weird errors during installations:
  1. Install `torch` and `triton`. Go to <https://pytorch.org> to install it. For example `pip install torch torchvision torchaudio triton`
  2. Confirm if CUDA is installated correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.
  3. Install `xformers` manually. You can try installing `vllm` and seeing if `vllm` succeeds. Check if `xformers` succeeded with `python -m xformers.info` Go to <https://github.com/facebookresearch/xformers>. Another option is to install `flash-attn` for Ampere GPUs.
  4. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful.
  5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`


### Conda Installation (Optional)
[](https://github.com/unslothai/unsloth/#conda-installation-optional)
`⚠️Only use Conda if you have it. If not, use Pip`. Select either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.
```
conda create --name unsloth_env \
  python=3.11 \
  pytorch-cuda=12.1 \
  pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
  -y
conda activate unsloth_env
pip install unsloth
```

If you're looking to install Conda in a Linux environment, [read here](https://docs.anaconda.com/miniconda/), or run the below 🔽
```
mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh
```

### Advanced Pip Installation
[](https://github.com/unslothai/unsloth/#advanced-pip-installation)
`⚠️Do **NOT** use this if you have Conda.` Pip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5` and CUDA versions.
For other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.
For example, if you have `torch 2.4` and `CUDA 12.1`, use:
```
pip install --upgrade pip
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
```

Another example, if you have `torch 2.5` and `CUDA 12.4`, use:
```
pip install --upgrade pip
pip install "unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git"
```

And other examples:
```
pip install "unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
```

Or, run the below in a terminal to get the **optimal** pip installation command:
```
wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
```

Or, run the below manually in a Python REPL:
```
try: import torch
except: raise ImportError('Install torch via `pip install torch`')
from packaging.version import Version as V
v = V(torch.__version__)
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] >= 8
if cuda != "12.1" and cuda != "11.8" and cuda != "12.4": raise RuntimeError(f"CUDA = {cuda} not supported!")
if  v <= V('2.1.0'): raise RuntimeError(f"Torch = {v} too old!")
elif v <= V('2.1.1'): x = 'cu{}{}-torch211'
elif v <= V('2.1.2'): x = 'cu{}{}-torch212'
elif v < V('2.3.0'): x = 'cu{}{}-torch220'
elif v < V('2.4.0'): x = 'cu{}{}-torch230'
elif v < V('2.5.0'): x = 'cu{}{}-torch240'
elif v < V('2.6.0'): x = 'cu{}{}-torch250'
else: raise RuntimeError(f"Torch = {v} too new!")
x = x.format(cuda.replace(".", ""), "-ampere" if is_ampere else "")
print(f'pip install --upgrade pip && pip install "unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git"')
```

## 📜 Documentation
[](https://github.com/unslothai/unsloth/#-documentation)
  * Go to our official [Documentation](https://docs.unsloth.ai) for saving to GGUF, checkpointing, evaluation and more!
  * We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!
  * We're in 🤗Hugging Face's official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
  * If you want to download models from the ModelScope community, please use an environment variable: `UNSLOTH_USE_MODELSCOPE=1`, and install the modelscope library by: `pip install modelscope -U`.


> unsloth_cli.py also supports `UNSLOTH_USE_MODELSCOPE=1` to download models and datasets. please remember to use the model and dataset id in the ModelScope community.
```
from unsloth import FastLanguageModel 
import torch
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!
# Get LAION dataset
url = "https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl"
dataset = load_dataset("json", data_files = {"train" : url}, split = "train")
# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
  "unsloth/Meta-Llama-3.1-8B-bnb-4bit",   # Llama-3.1 2x faster
  "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
  "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
  "unsloth/Meta-Llama-3.1-405B-bnb-4bit",  # 4bit for 405b!
  "unsloth/Mistral-Small-Instruct-2409",   # Mistral 22b 2x faster!
  "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
  "unsloth/Phi-3.5-mini-instruct",      # Phi-3.5 2x faster!
  "unsloth/Phi-3-medium-4k-instruct",
  "unsloth/gemma-2-9b-bnb-4bit",
  "unsloth/gemma-2-27b-bnb-4bit",      # Gemma 2x faster!
  "unsloth/Llama-3.2-1B-bnb-4bit",      # NEW! Llama 3.2 models
  "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
  "unsloth/Llama-3.2-3B-bnb-4bit",
  "unsloth/Llama-3.2-3B-Instruct-bnb-4bit",
  "unsloth/Llama-3.3-70B-Instruct-bnb-4bit" # NEW! Llama 3.3 70B!
] # More models at https://huggingface.co/unsloth
model, tokenizer = FastModel.from_pretrained(
  model_name = "unsloth/gemma-3-4B-it",
  max_seq_length = 2048, # Choose any for long context!
  load_in_4bit = True, # 4 bit quantization to reduce memory
  load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
  full_finetuning = False, # [NEW!] We have full finetuning now!
  # token = "hf_...", # use one if using gated models
)
# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
  model,
  r = 16,
  target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
           "gate_proj", "up_proj", "down_proj",],
  lora_alpha = 16,
  lora_dropout = 0, # Supports any, but = 0 is optimized
  bias = "none",  # Supports any, but = "none" is optimized
  # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
  use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
  random_state = 3407,
  max_seq_length = max_seq_length,
  use_rslora = False, # We support rank stabilized LoRA
  loftq_config = None, # And LoftQ
)
trainer = SFTTrainer(
  model = model,
  train_dataset = dataset,
  tokenizer = tokenizer,
  args = SFTConfig(
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    per_device_train_batch_size = 2,
    gradient_accumulation_steps = 4,
    warmup_steps = 10,
    max_steps = 60,
    logging_steps = 1,
    output_dir = "outputs",
    optim = "adamw_8bit",
    seed = 3407,
  ),
)
trainer.train()
# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like
# (1) Saving to GGUF / merging to 16bit for vLLM
# (2) Continued training from a saved LoRA adapter
# (3) Adding an evaluation loop / OOMs
# (4) Customized chat templates
```

## 💡 Reinforcement Learning
[](https://github.com/unslothai/unsloth/#-reinforcement-learning)
RL including DPO, GRPO, PPO, Reward Modelling, Online DPO all work with Unsloth. We're in 🤗Hugging Face's official docs! We're on the [GRPO docs](https://huggingface.co/learn/nlp-course/en/chapter12/6) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)! List of RL notebooks:
  * ORPO notebook: [Link](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_\(8B\)-ORPO.ipynb)
  * DPO Zephyr notebook: [Link](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_\(7B\)-DPO.ipynb)
  * KTO notebook: [Link](https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing)
  * SimPO notebook: [Link](https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing)

Click for DPO code
```
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0" # Optional set GPU device ID
from unsloth import FastLanguageModel
import torch
from trl import DPOTrainer, DPOConfig
max_seq_length = 2048
model, tokenizer = FastLanguageModel.from_pretrained(
  model_name = "unsloth/zephyr-sft-bnb-4bit",
  max_seq_length = max_seq_length,
  load_in_4bit = True,
)
# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
  model,
  r = 64,
  target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
           "gate_proj", "up_proj", "down_proj",],
  lora_alpha = 64,
  lora_dropout = 0, # Supports any, but = 0 is optimized
  bias = "none",  # Supports any, but = "none" is optimized
  # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
  use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
  random_state = 3407,
  max_seq_length = max_seq_length,
)
dpo_trainer = DPOTrainer(
  model = model,
  ref_model = None,
  train_dataset = YOUR_DATASET_HERE,
  # eval_dataset = YOUR_DATASET_HERE,
  tokenizer = tokenizer,
  args = DPOConfig(
    per_device_train_batch_size = 4,
    gradient_accumulation_steps = 8,
    warmup_ratio = 0.1,
    num_train_epochs = 3,
    logging_steps = 1,
    optim = "adamw_8bit",
    seed = 42,
    output_dir = "outputs",
    max_length = 1024,
    max_prompt_length = 512,
    beta = 0.1,
  ),
)
dpo_trainer.train()
```

## 🥇 Performance Benchmarking
[](https://github.com/unslothai/unsloth/#-performance-benchmarking)
  * For our most detailed benchmarks, read our [Llama 3.3 Blog](https://unsloth.ai/blog/llama3-3).
  * Benchmarking of Unsloth was also conducted by [🤗Hugging Face](https://huggingface.co/blog/unsloth-trl).


We tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):
Model | VRAM | 🦥 Unsloth speed | 🦥 VRAM reduction | 🦥 Longer context | 😊 Hugging Face + FA2  
---|---|---|---|---|---  
Llama 3.3 (70B) | 80GB | 2x | >75% | 13x longer | 1x  
Llama 3.1 (8B) | 80GB | 2x | >70% | 12x longer | 1x  
### Context length benchmarks
[](https://github.com/unslothai/unsloth/#context-length-benchmarks)
#### Llama 3.1 (8B) max. context length
[](https://github.com/unslothai/unsloth/#llama-31-8b-max-context-length)
We tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.
GPU VRAM | 🦥Unsloth context length | Hugging Face + FA2  
---|---|---  
8 GB | 2,972 | OOM  
12 GB | 21,848 | 932  
16 GB | 40,724 | 2,551  
24 GB | 78,475 | 5,789  
40 GB | 153,977 | 12,264  
48 GB | 191,728 | 15,502  
80 GB | 342,733 | 28,454  
#### Llama 3.3 (70B) max. context length
[](https://github.com/unslothai/unsloth/#llama-33-70b-max-context-length)
We tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.
GPU VRAM | 🦥Unsloth context length | Hugging Face + FA2  
---|---|---  
48 GB | 12,106 | OOM  
80 GB | 89,389 | 6,916  
[![](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)
### Citation
[](https://github.com/unslothai/unsloth/#citation)
You can cite the Unsloth repo as follows:
```
@software{unsloth,
 author = {Daniel Han, Michael Han and Unsloth team},
 title = {Unsloth},
 url = {http://github.com/unslothai/unsloth},
 year = {2023}
}
```

### Thank You to
[](https://github.com/unslothai/unsloth/#thank-you-to)
  * Hugging Face's [TRL library](https://github.com/huggingface/trl) which serves as the basis foundation for Unsloth
  * [Erik](https://github.com/erikwijmans) for his help adding [Apple's ML Cross Entropy](https://github.com/apple/ml-cross-entropy) in Unsloth
  * [HuyNguyen-hust](https://github.com/HuyNguyen-hust) for making [RoPE Embeddings 28% faster](https://github.com/unslothai/unsloth/pull/238)
  * [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support
  * [152334H](https://github.com/152334H) for experimental DPO support


## About
Finetune Llama 3.3, DeepSeek-R1, Gemma 3 & Reasoning LLMs 2x faster with 70% less memory! 🦥 
[unsloth.ai](https://unsloth.ai "https://unsloth.ai")
### Topics
[ llama ](https://github.com/topics/llama "Topic: llama") [ lora ](https://github.com/topics/lora "Topic: lora") [ gemma ](https://github.com/topics/gemma "Topic: gemma") [ mistral ](https://github.com/topics/mistral "Topic: mistral") [ fine-tuning ](https://github.com/topics/fine-tuning "Topic: fine-tuning") [ finetuning ](https://github.com/topics/finetuning "Topic: finetuning") [ llm ](https://github.com/topics/llm "Topic: llm") [ llms ](https://github.com/topics/llms "Topic: llms") [ qlora ](https://github.com/topics/qlora "Topic: qlora") [ deepseek ](https://github.com/topics/deepseek "Topic: deepseek") [ unsloth ](https://github.com/topics/unsloth "Topic: unsloth") [ llama3 ](https://github.com/topics/llama3 "Topic: llama3") [ phi3 ](https://github.com/topics/phi3 "Topic: phi3") [ gemma2 ](https://github.com/topics/gemma2 "Topic: gemma2") [ deepseek-r1 ](https://github.com/topics/deepseek-r1 "Topic: deepseek-r1") [ gemma3 ](https://github.com/topics/gemma3 "Topic: gemma3")
### Resources
[ Readme ](https://github.com/unslothai/unsloth/#readme-ov-file)
### License
[ Apache-2.0 license ](https://github.com/unslothai/unsloth/#Apache-2.0-1-ov-file)
[ Activity](https://github.com/unslothai/unsloth/activity)
[ Custom properties](https://github.com/unslothai/unsloth/custom-properties)
### Stars
[ **36.2k** stars](https://github.com/unslothai/unsloth/stargazers)
### Watchers
[ **213** watching](https://github.com/unslothai/unsloth/watchers)
### Forks
[ **2.8k** forks](https://github.com/unslothai/unsloth/forks)
[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth&report=unslothai+%28user%29)
##  [Releases 12](https://github.com/unslothai/unsloth/releases)
[ Gemma 3 + FFT Support Latest  Mar 14, 2025 ](https://github.com/unslothai/unsloth/releases/tag/2025-03)
[+ 11 releases](https://github.com/unslothai/unsloth/releases)
## Sponsor this project
  * ![ko_fi](https://github.githubassets.com/assets/ko_fi-53a60c17e75c.svg) [ko-fi.com/**unsloth**](https://ko-fi.com/unsloth)


##  [Packages 0](https://github.com/orgs/unslothai/packages?repo_name=unsloth)
No packages published 
##  [Contributors 71](https://github.com/unslothai/unsloth/graphs/contributors)
  * [ ![@danielhanchen](https://avatars.githubusercontent.com/u/23090290?s=64&v=4) ](https://github.com/danielhanchen)
  * [ ![@shimmyshimmer](https://avatars.githubusercontent.com/u/107991372?s=64&v=4) ](https://github.com/shimmyshimmer)
  * [ ![@Erland366](https://avatars.githubusercontent.com/u/68678137?s=64&v=4) ](https://github.com/Erland366)
  * [ ![@Datta0](https://avatars.githubusercontent.com/u/39181234?s=64&v=4) ](https://github.com/Datta0)
  * [ ![@NinoRisteski](https://avatars.githubusercontent.com/u/95188570?s=64&v=4) ](https://github.com/NinoRisteski)
  * [ ![@xyangk](https://avatars.githubusercontent.com/u/9495054?s=64&v=4) ](https://github.com/xyangk)
  * [ ![@KareemMusleh](https://avatars.githubusercontent.com/u/81531392?s=64&v=4) ](https://github.com/KareemMusleh)
  * [ ![@sebdg](https://avatars.githubusercontent.com/u/1187529?s=64&v=4) ](https://github.com/sebdg)
  * [ ![@bet0x](https://avatars.githubusercontent.com/u/778862?s=64&v=4) ](https://github.com/bet0x)
  * [ ![@neph1](https://avatars.githubusercontent.com/u/7988802?s=64&v=4) ](https://github.com/neph1)
  * [ ![@t-vi](https://avatars.githubusercontent.com/u/20787943?s=64&v=4) ](https://github.com/t-vi)
  * [ ![@Oseltamivir](https://avatars.githubusercontent.com/u/58582368?s=64&v=4) ](https://github.com/Oseltamivir)
  * [ ![@chrehall68](https://avatars.githubusercontent.com/u/60240707?s=64&v=4) ](https://github.com/chrehall68)
  * [ ![@mahiatlinux](https://avatars.githubusercontent.com/u/110882203?s=64&v=4) ](https://github.com/mahiatlinux)


[+ 57 contributors](https://github.com/unslothai/unsloth/graphs/contributors)
## Languages
  * [ Python 100.0% ](https://github.com/unslothai/unsloth/search?l=python)


## Footer
[ ](https://github.com "GitHub") © 2025 GitHub, Inc. 
### Footer navigation
  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
  * [Security](https://github.com/security)
  * [Status](https://www.githubstatus.com/)
  * [Docs](https://docs.github.com/)
  * [Contact](https://support.github.com?tags=dotcom-footer)
  * Manage cookies 
  * Do not share my personal information 


You can’t perform that action at this time. 
