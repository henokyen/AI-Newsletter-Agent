{
    "id": "0a0fae7bc1e010453aaaf02e0b7a9bf5",
    "metadata": {
        "id": "0a0fae7bc1e010453aaaf02e0b7a9bf5",
        "url": "https://www.philschmid.de/sagemaker-llama3/",
        "title": "Deploy Llama 3 on Amazon SageMaker",
        "properties": {
            "description": "In this blog post you will learn how to deploy Llama 3 70B to Amazon SageMaker.",
            "keywords": null,
            "author": "Philipp Schmid",
            "og:title": "Deploy Llama 3 on Amazon SageMaker",
            "og:description": "In this blog post you will learn how to deploy Llama 3 70B to Amazon SageMaker.",
            "og:url": "https://www.philschmid.de/sagemaker-llama3",
            "og:image": "https://www.philschmid.de/static/blog/sagemaker-llama3/thumbnail.jpg",
            "og:image:width": "1200",
            "og:image:height": "630",
            "og:image:alt": "Deploy Llama 3 on Amazon SageMaker",
            "og:type": "article",
            "twitter:card": "summary_large_image",
            "twitter:title": "Deploy Llama 3 on Amazon SageMaker",
            "twitter:description": "In this blog post you will learn how to deploy Llama 3 70B to Amazon SageMaker.",
            "twitter:image": "https://www.philschmid.de/static/blog/sagemaker-llama3/thumbnail.jpg"
        }
    },
    "parent_metadata": {
        "id": "ddacddcab9916b1fae2cbeefc1e377b2",
        "url": "https://www.notion.so/SageMaker-ddacddcab9916b1fae2cbeefc1e377b2",
        "title": "SageMaker",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[![logo](https://www.philschmid.de/_next/image?url=%2Fstatic%2Flogo.png&w=48&q=75)Philschmid](https://www.philschmid.de/)\nSearch`âŒ˜k`\n[Blog](https://www.philschmid.de/)[Projects](https://www.philschmid.de/projects)[Newsletter](https://www.philschmid.de/cloud-attention)[About Me](https://www.philschmid.de/philipp-schmid)Toggle Menu\n# Deploy Llama 3 on Amazon SageMaker\nApril 18, 20249 minute read[View Code](https://github.com/philschmid/llm-sagemaker-sample/blob/main/notebooks/deploy-llama3.ipynb)\nEarlier today Meta released [Llama 3](https://huggingface.co/blog/llama3), the next iteration of the open-access Llama family. Llama 3 comes in two sizes: [8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) for efficient deployment and development on consumer-size GPU, and [70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-instruct) for large-scale AI native applications. Both come in base and instruction-tuned variants. In addition to the 4 models, a new version of Llama Guard was fine-tuned on Llama 3 8B and is released as Llama Guard 2 (safety fine-tune).\nIn this blog you will learn how to deploy [meta-llama/Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) model to Amazon SageMaker. We are going to use the Hugging Face LLM DLC is a purpose-built Inference Container to easily deploy LLMs in a secure and managed environment. The DLC is powered by [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) a scalelable, optimized solution for deploying and serving Large Language Models (LLMs). The Blog post also includes Hardware requirements for the different model sizes.\nIn the blog will cover how to:\n  1. [Setup development environment](https://www.philschmid.de/sagemaker-llama3/#1-setup-development-environment)\n  2. [Hardware requirements](https://www.philschmid.de/sagemaker-llama3/#2-hardware-requirements)\n  3. [Deploy Llama 3 70b to Amazon SageMaker](https://www.philschmid.de/sagemaker-llama3/#3-deploy-llama-3-to-amazon-sagemaker)\n  4. [Run inference and chat with the model](https://www.philschmid.de/sagemaker-llama3/#4-run-inference-and-chat-with-the-model)\n  5. [Benchmark llama 3 70B with llmperf](https://www.philschmid.de/sagemaker-llama3/#5-benchmark-llama-3-70b)\n  6. [Clean up](https://www.philschmid.de/sagemaker-llama3/#6-clean-up)\n\n\nLets get started!\n## [](https://www.philschmid.de/sagemaker-llama3/#1-setup-development-environment)1. Setup development environment\nWe are going to use the `sagemaker` python SDK to deploy Mixtral to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed.\n```\n!pip install \"sagemaker>=2.216.0\" --upgrade --quiet\n```\n\nIf you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n```\nimport sagemaker\nimport boto3\nsess = sagemaker.Session()\n# sagemaker session bucket -> used for uploading data, models and logs\n# sagemaker will automatically create this bucket if it not exists\nsagemaker_session_bucket=None\nif sagemaker_session_bucket is None and sess is not None:\n  # set to default bucket if a bucket name is not given\n  sagemaker_session_bucket = sess.default_bucket()\ntry:\n  role = sagemaker.get_execution_role()\nexcept ValueError:\n  iam = boto3.client('iam')\n  role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\nsess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\nprint(f\"sagemaker role arn: {role}\")\nprint(f\"sagemaker session region: {sess.boto_region_name}\")\n\n```\n\nCompared to deploying regular Hugging Face models we first need to retrieve the container uri and provide it to our `HuggingFaceModel` model class with a `image_uri` pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use the `get_huggingface_llm_image_uri` method provided by the `sagemaker` SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specified `backend`, `session`, `region`, and `version`. You can find the available versions [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)\n_Note: At the time of writing this blog post the latest version of the Hugging Face LLM DLC is not yet available via the`get_huggingface_llm_image_uri` method. We are going to use the raw container uri instead._\n```\n# COMMENT IN WHEN PR (https://github.com/aws/sagemaker-python-sdk/pull/4314) IS MERGED\n# from sagemaker.huggingface import get_huggingface_llm_image_uri\n# # retrieve the llm image uri\n# llm_image = get_huggingface_llm_image_uri(\n#  \"huggingface\",\n#  version=\"2.0.0\"\n# )\nllm_image = f\"763104351884.dkr.ecr.{sess.boto_region_name}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\"\n# print ecr image uri\nprint(f\"llm image uri: {llm_image}\")\n```\n\nllm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\n## [](https://www.philschmid.de/sagemaker-llama3/#2-hardware-requirements)2. Hardware requirements\nLlama 3 comes in 2 different sizes - 8B & 70B parameters. The hardware requirements will vary based on the model size deployed to SageMaker. Below is a set up minimum requirements for each model size we tested.\nModel| Instance Type| Quantization| # of GPUs per replica  \n---|---|---|---  \n[Llama 8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)| `(ml.)g5.2xlarge`| `-`| 1  \n[Llama 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)| `(ml.)g5.12xlarge`| `gptq / awq`| 8  \n[Llama 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)| `(ml.)g5.48xlarge`| `-`| 8  \n[Llama 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)| `(ml.)p4d.24xlarge`| `-`| 8  \n_Note: We haven't tested GPTQ or AWQ models yet._\n## [](https://www.philschmid.de/sagemaker-llama3/#3-deploy-llama-3-to-amazon-sagemaker)3. Deploy Llama 3 to Amazon SageMaker\nTo deploy [Llama 3 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) to Amazon SageMaker we create a `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type` etc. We will use a `p4d.24xlarge` instance type, which has 8 NVIDIA A100 GPUs and 320GB of GPU memory. Llama 3 70B instruct is a fine-tuned model for conversational AI this allows us to enable the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api) from TGI to interact with llama using the common OpenAI format of `messages`.\n```\n{\n \"messages\": [\n  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n  { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n ]\n}\n```\n\n_Note: Llama 3 is a gated model, please visit the[Model Card](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) and accept the license terms and acceptable use policy before submitting this form._\n```\nimport json\nfrom sagemaker.huggingface import HuggingFaceModel\n# sagemaker config\ninstance_type = \"ml.p4d.24xlarge\"\nhealth_check_timeout = 900\n# Define Model and Endpoint configuration parameter\nconfig = {\n 'HF_MODEL_ID': \"meta-llama/Meta-Llama-3-70B-Instruct\", # model_id from hf.co/models\n 'SM_NUM_GPUS': \"8\", # Number of GPU used per replica\n 'MAX_INPUT_LENGTH': \"2048\", # Max length of input text\n 'MAX_TOTAL_TOKENS': \"4096\", # Max length of the generation (including input text)\n 'MAX_BATCH_TOTAL_TOKENS': \"8192\", # Limits the number of tokens that can be processed in parallel during the generation\n 'MESSAGES_API_ENABLED': \"true\", # Enable the messages API\n 'HUGGING_FACE_HUB_TOKEN': \"<REPLACE WITH YOUR TOKEN>\"\n}\n# check if token is set\nassert config['HUGGING_FACE_HUB_TOKEN'] != \"<REPLACE WITH YOUR TOKEN>\", \"Please set your Hugging Face Hub token\"\n# create HuggingFaceModel with the image uri\nllm_model = HuggingFaceModel(\n role=role,\n image_uri=llm_image,\n env=config\n)\n\n```\n\nAfter we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.p4d.24xlarge` instance type. TGI will automatically distribute and shard the model across all GPUs.\n```\n# Deploy model to an endpoint\n# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\nllm = llm_model.deploy(\n initial_instance_count=1,\n instance_type=instance_type,\n container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n)\n\n```\n\nSageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes.\n## [](https://www.philschmid.de/sagemaker-llama3/#4-run-inference-and-chat-with-the-model)4. Run inference and chat with the model\nAfter our endpoint is deployed we can run inference on it. We will use the `predict` method from the `predictor` to run inference on our endpoint. We can inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. You can find supported parameters in the [here](https://huggingface.co/docs/text-generation-inference/messages_api).\nThe Messages API allows us to interact with the model in a conversational way. We can define the role of the message and the content. The role can be either `system`,`assistant` or `user`. The `system` role is used to provide context to the model and the `user` role is used to ask questions or provide input to the model.\n```\n{\n \"messages\": [\n  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n  { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n ]\n}\n```\n\n```\n# Prompt to generate\nmessages=[\n  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n  { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n ]\n# Generation arguments\nparameters = {\n  \"model\": \"meta-llama/Meta-Llama-3-70B-Instruct\", # placholder, needed\n  \"top_p\": 0.6,\n  \"temperature\": 0.9,\n  \"max_tokens\": 512,\n  \"stop\": [\"<|eot_id|>\"],\n}\n```\n\nOkay lets test it.\n```\nchat = llm.predict({\"messages\" :messages, **parameters})\nprint(chat[\"choices\"][0][\"message\"][\"content\"].strip())\n```\n\n## [](https://www.philschmid.de/sagemaker-llama3/#5-benchmark-llama-3-70b-with-llmperf)5. Benchmark llama 3 70B with llmperf\nWe successfully deployed Llama 3 70B to Amazon SageMaker and tested it. Now we want to benchmark the model to see how it performs. We will use a [llmperf](https://github.com/philschmid/llmperf) fork with support for `sagemaker`.\nFirst lets install the `llmperf` package.\n```\n!git clone https://github.com/philschmid/llmperf.git\n!pip install -e llmperf/\n```\n\nNow we can run the benchmark with the following command. We are going to benchmark using `25` concurrent users and max `500` requests. The benchmark will measure `first-time-to-token`, `latency (ms/token)` and `throughput (tokens/s)` full details can be found in the `results` folder\n_ðŸš¨ImportantðŸš¨: This benchmark was initiatied from Europe, while the endpoint runs in us-east-1. This has significant impact on the`first-time-to-token` metric, since it includes the network communication. If you want to measure the `first-time-to-token` correctly, you need to run the benchmark on the same host or your production region._\n```\n# tell llmperf that we are using the messages api\n!MESSAGES_API=true python llmperf/token_benchmark_ray.py \\\n--model {llm.endpoint_name} \\\n--llm-api \"sagemaker\" \\\n--max-num-completed-requests 500 \\\n--timeout 600 \\\n--num-concurrent-requests 25 \\\n--results-dir \"results\"\n```\n\nLets parse the results and display them nicely.\n```\nimport glob\nimport json\n# Reads the summary.json file and prints the results\nwith open(glob.glob(f'results/*summary.json')[0], 'r') as file:\n  data = json.load(file)\nprint(\"Concurrent requests: 25\")\nprint(f\"Avg. Input token length: {data['mean_input_tokens']}\")\nprint(f\"Avg. Output token length: {data['mean_output_tokens']}\")\nprint(f\"Avg. First-Time-To-Token: {data['results_ttft_s_mean']*1000:.2f}ms\")\nprint(f\"Avg. Thorughput: {data['results_mean_output_throughput_token_per_s']:.2f} tokens/sec\")\nprint(f\"Avg. Latency: {data['results_inter_token_latency_s_mean']*1000:.2f}ms/token\")\n#  Concurrent requests: 25\n#  Avg. Input token length: 550\n#  Avg. Output token length: 150\n#  Avg. First-Time-To-Token: 1301.28ms\n#  Avg. Thorughput: 1116.25 tokens/sec\n#  Avg. Latency: 9.45ms/token\n```\n\nThats it! We successfully deployed, tested and benchmarked Llama 3 70B on Amazon SageMaker. The benchmark is not a full representation of the model performance, but it gives you a first good indication. If you plan to use the model in production, we recommend to run a longer and closer to your production benchmark, modify the number of replicas see ([Scale LLM Inference on Amazon SageMaker with Multi-Replica Endpoints](https://www.philschmid.de/sagemaker-multi-replicahttps://www.philschmid.de/sagemaker-multi-replica)) and most importantly test the model with your own data.\n## [](https://www.philschmid.de/sagemaker-llama3/#6-clean-up)6. Clean up\nTo clean up, we can delete the model and endpoint.\n```\nllm.delete_model()\nllm.delete_endpoint()\n```\n\nThanks for reading! If you have any questions, feel free to contact me on [Twitter](https://twitter.com/_philschmid) or [LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).\n[Philipp Schmid Â© 2025](https://www.philschmid.de/philipp-schmid)[Imprint](https://www.philschmid.de/imprint)[RSS Feed](https://www.philschmid.de/rss)\ntheme\nMail[Twitter](https://twitter.com/_philschmid)[LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/)[GitHub](https://github.com/philschmid)\n",
    "content_quality_score": 1.0,
    "summary": null,
    "child_urls": [
        "https://www.philschmid.de/",
        "https://www.philschmid.de/projects",
        "https://www.philschmid.de/cloud-attention",
        "https://www.philschmid.de/philipp-schmid",
        "https://www.philschmid.de/sagemaker-llama3/#1-setup-development-environment",
        "https://www.philschmid.de/sagemaker-llama3/#2-hardware-requirements",
        "https://www.philschmid.de/sagemaker-llama3/#3-deploy-llama-3-to-amazon-sagemaker",
        "https://www.philschmid.de/sagemaker-llama3/#4-run-inference-and-chat-with-the-model",
        "https://www.philschmid.de/sagemaker-llama3/#5-benchmark-llama-3-70b",
        "https://www.philschmid.de/sagemaker-llama3/#6-clean-up",
        "https://www.philschmid.de/sagemaker-llama3/#5-benchmark-llama-3-70b-with-llmperf",
        "https://www.philschmid.de/sagemaker-multi-replicahttps://www.philschmid.de/sagemaker-multi-replica",
        "https://www.philschmid.de/imprint",
        "https://www.philschmid.de/rss",
        "https://github.com/philschmid/llm-sagemaker-sample/blob/main/notebooks/deploy-llama3.ipynb",
        "https://huggingface.co/blog/llama3",
        "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",
        "https://huggingface.co/meta-llama/Meta-Llama-3-70B-instruct",
        "https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct",
        "https://github.com/huggingface/text-generation-inference",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html",
        "https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers",
        "https://huggingface.co/docs/text-generation-inference/messages_api",
        "https://github.com/philschmid/llmperf",
        "https://twitter.com/_philschmid",
        "https://www.linkedin.com/in/philipp-schmid-a6a2bb196/",
        "mailto:schmidphilipp1995@gmail.com",
        "https://github.com/philschmid"
    ]
}