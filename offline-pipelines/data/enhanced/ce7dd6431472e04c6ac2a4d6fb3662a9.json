{
    "id": "ce7dd6431472e04c6ac2a4d6fb3662a9",
    "metadata": {
        "id": "ce7dd6431472e04c6ac2a4d6fb3662a9",
        "url": "https://huggingface.co/docs/transformers/en/pipeline_webserver#using-pipelines-for-a-webserver/",
        "title": "Web server inference",
        "properties": {
            "description": "We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.",
            "keywords": null,
            "author": null,
            "og:title": "Web server inference",
            "og:type": "website",
            "og:url": "https://huggingface.co/docs/transformers/en/pipeline_webserver",
            "og:image": "https://huggingface.co/front/thumbnails/docs/transformers.png",
            "twitter:card": "summary_large_image",
            "twitter:site": "@huggingface",
            "twitter:image": "https://huggingface.co/front/thumbnails/docs/transformers.png"
        }
    },
    "parent_metadata": {
        "id": "1e501176ca66fef3edfc0b6cb5c9cd54",
        "url": "https://www.notion.so/HuggingFace-Ecosystem-1e501176ca66fef3edfc0b6cb5c9cd54",
        "title": "HuggingFace Ecosystem",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\nTransformers documentation\nWeb server inference\n# Transformers\nüè° View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersDistilabelEvaluateGradioHubHub Python LibraryHugging Face Generative AI Services (HUGS)Huggingface.jsInference API (serverless)Inference Endpoints (dedicated)LeaderboardsLightevalOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jssmolagentstimm\nSearch documentation\n`‚åòK`\nmainv4.50.0v4.49.0v4.48.2v4.47.1v4.46.3v4.45.2v4.44.2v4.43.4v4.42.4v4.41.2v4.40.2v4.39.3v4.38.2v4.37.2v4.36.1v4.35.2v4.34.1v4.33.3v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-builder-html ARDEENESFRHIITJAKOPTTETRZH [ 142,161](https://github.com/huggingface/transformers)\nGet started\n[Transformers ](https://huggingface.co/docs/transformers/en/index)[Installation ](https://huggingface.co/docs/transformers/en/installation)[Quickstart ](https://huggingface.co/docs/transformers/en/quicktour)\nBase classes\nInference\nPipeline API\n[Pipeline ](https://huggingface.co/docs/transformers/en/pipeline_tutorial)[Machine learning apps ](https://huggingface.co/docs/transformers/en/pipeline_gradio)[Web server inference ](https://huggingface.co/docs/transformers/en/pipeline_webserver)[Adding a new pipeline ](https://huggingface.co/docs/transformers/en/add_new_pipeline)\nLLMs\nChat with models\nOptimization\n[Agents ](https://huggingface.co/docs/transformers/en/agents)[Tools ](https://huggingface.co/docs/transformers/en/tools)\nTraining\nQuantization\nExport to production\nResources\nContribute\nAPI\n![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\nJoin the Hugging Face community\nand get access to the augmented documentation experience \nCollaborate on models, datasets and Spaces \nFaster examples with accelerated inference \nSwitch between documentation themes \n[Sign Up](https://huggingface.co/join)\nto get started\n# [](https://huggingface.co/docs/transformers/en/pipeline_webserver#web-server-inference) Web server inference\nA web server is a system that waits for requests and serves them as they come in. This means you can use [Pipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.Pipeline) as an inference engine on a web server, since you can use an iterator (similar to how you would [iterate over a dataset](https://huggingface.co/docs/transformers/en/pipeline_tutorial#large-datasets)) to handle each incoming request.\nDesigning a web server with [Pipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.Pipeline) is unique though because they‚Äôre fundamentally different. Web servers are multiplexed (multithreaded, async, etc.) to handle multiple requests concurrently. [Pipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.Pipeline) and its underlying model on the other hand are not designed for parallelism because they take a lot of memory. It‚Äôs best to give a [Pipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.Pipeline) all the available resources when they‚Äôre running or for a compute intensive job.\nThis guide shows how to work around this difference by using a web server to handle the lighter load of receiving and sending requests, and having a single thread to handle the heavier load of running [Pipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.Pipeline).\n## [](https://huggingface.co/docs/transformers/en/pipeline_webserver#create-a-server) Create a server\n[Starlette](https://www.starlette.io/) is a lightweight framework for building web servers. You can use any other framework you‚Äôd like, but you may have to make some changes to the code below.\nBefore you begin, make sure Starlette and [uvicorn](http://www.uvicorn.org/) are installed.\nCopied\n```\n!pip install starlette uvicorn\n```\n\nNow you can create a simple web server in a `server.py` file. The key is to only load the model **once** to prevent unnecessary copies of it from consuming memory.\nCreate a pipeline to fill in the masked token, `[MASK]`.\nCopied\n```\nfrom starlette.applications import Starlette\nfrom starlette.responses import JSONResponse\nfrom starlette.routing import Route\nfrom transformers import pipeline\nimport asyncio\nasync def homepage(request):\n  payload = await request.body()\n  string = payload.decode(\"utf-8\")\n  response_q = asyncio.Queue()\n  await request.app.model_queue.put((string, response_q))\n  output = await response_q.get()\n  return JSONResponse(output)\nasync def server_loop(q):\n  pipeline = pipeline(task=\"fill-mask\",model=\"google-bert/bert-base-uncased\")\n  while True:\n    (string, response_q) = await q.get()\n    out = pipeline(string)\n    await response_q.put(out)\napp = Starlette(\n  routes=[\n    Route(\"/\", homepage, methods=[\"POST\"]),\n  ],\n)\n@app.on_event(\"startup\")\nasync def startup_event():\n  q = asyncio.Queue()\n  app.model_queue = q\n  asyncio.create_task(server_loop(q))\n```\n\nStart the server with the following command.\nCopied\n```\nuvicorn server:app\n```\n\nQuery the server with a POST request.\nCopied\n```\ncurl -X POST -d \"Paris is the [MASK] of France.\" http://localhost:8000/\n[{'score': 0.9969332218170166,\n 'token': 3007,\n 'token_str': 'capital',\n 'sequence': 'paris is the capital of france.'},\n {'score': 0.0005914849461987615,\n 'token': 2540,\n 'token_str': 'heart',\n 'sequence': 'paris is the heart of france.'},\n {'score': 0.00043787318281829357,\n 'token': 2415,\n 'token_str': 'center',\n 'sequence': 'paris is the center of france.'},\n {'score': 0.0003378340043127537,\n 'token': 2803,\n 'token_str': 'centre',\n 'sequence': 'paris is the centre of france.'},\n {'score': 0.00026995912776328623,\n 'token': 2103,\n 'token_str': 'city',\n 'sequence': 'paris is the city of france.'}]\n```\n\n## [](https://huggingface.co/docs/transformers/en/pipeline_webserver#queuing-requests) Queuing requests\nThe server‚Äôs queuing mechanism can be used for some interesting applications such as dynamic batching. Dynamic batching accumulates several requests first before processing them with [Pipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.Pipeline).\nThe example below is written in pseudocode for readability rather than performance, in particular, you‚Äôll notice that:\n  1. There is no batch size limit.\n  2. The timeout is reset on every queue fetch, so you could end up waiting much longer than the `timeout` value before processing a request. This would also delay the first inference request by that amount of time. The web server always waits 1ms even if the queue is empty, which is inefficient, because that time can be used to start inference. It could make sense though if batching is essential to your use case.\nIt would be better to have a single 1ms deadline, instead of resetting it on every fetch.\n\n\nCopied\n```\n(string, rq) = await q.get()\nstrings = []\nqueues = []\nwhile True:\n  try:\n    (string, rq) = await asyncio.wait_for(q.get(), timeout=0.001)\n  except asyncio.exceptions.TimeoutError:\n    break\n  strings.append(string)\n  queues.append(rq)\nstrings\nouts = pipeline(strings, batch_size=len(strings))\nfor rq, out in zip(queues, outs):\n  await rq.put(out)\n```\n\n## [](https://huggingface.co/docs/transformers/en/pipeline_webserver#error-checking) Error checking\nThere are many things that can go wrong in production. You could run out-of-memory, out of space, fail to load a model, have an incorrect model configuration, have an incorrect query, and so much more.\nAdding `try...except` statements is helpful for returning these errors to the user for debugging. Keep in mind this could be a security risk if you shouldn‚Äôt be revealing certain information.\n## [](https://huggingface.co/docs/transformers/en/pipeline_webserver#circuit-breaking) Circuit breaking\nTry to return a 503 or 504 error when the server is overloaded instead of forcing a user to wait indefinitely.\nIt is relatively simple to implement these error types since it‚Äôs only a single queue. Take a look at the queue size to determine when to start returning errors before your server fails under load.\n## [](https://huggingface.co/docs/transformers/en/pipeline_webserver#block-the-main-thread) Block the main thread\nPyTorch is not async aware, so computation will block the main thread from running.\nFor this reason, it‚Äôs better to run PyTorch on its own separate thread or process. When inference of a single request is especially long (more than 1s), it‚Äôs even more important because it means every query during inference must wait 1s before even receiving an error.\n## [](https://huggingface.co/docs/transformers/en/pipeline_webserver#dynamic-batching) Dynamic batching\nDynamic batching can be very effective when used in the correct setting, but it‚Äôs not necessary when you‚Äôre only passing 1 request at a time (see [batch inference](https://huggingface.co/docs/transformers/en/pipeline_tutorial#batch-inference) for more details).\n[< > Update on GitHub](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_webserver.md)\n[‚ÜêMachine learning apps](https://huggingface.co/docs/transformers/en/pipeline_gradio) [Adding a new pipeline‚Üí](https://huggingface.co/docs/transformers/en/add_new_pipeline)\n[Web server inference](https://huggingface.co/docs/transformers/en/pipeline_webserver#web-server-inference) [Create a server](https://huggingface.co/docs/transformers/en/pipeline_webserver#create-a-server) [Queuing requests](https://huggingface.co/docs/transformers/en/pipeline_webserver#queuing-requests) [Error checking](https://huggingface.co/docs/transformers/en/pipeline_webserver#error-checking) [Circuit breaking](https://huggingface.co/docs/transformers/en/pipeline_webserver#circuit-breaking) [Block the main thread](https://huggingface.co/docs/transformers/en/pipeline_webserver#block-the-main-thread) [Dynamic batching](https://huggingface.co/docs/transformers/en/pipeline_webserver#dynamic-batching)\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://huggingface.co/",
        "https://huggingface.co/models",
        "https://huggingface.co/datasets",
        "https://huggingface.co/spaces",
        "https://huggingface.co/posts",
        "https://huggingface.co/docs",
        "https://huggingface.co/enterprise",
        "https://huggingface.co/pricing",
        "https://huggingface.co/login",
        "https://huggingface.co/join",
        "https://huggingface.co/docs/transformers/en/index",
        "https://huggingface.co/docs/transformers/en/installation",
        "https://huggingface.co/docs/transformers/en/quicktour",
        "https://huggingface.co/docs/transformers/en/pipeline_tutorial",
        "https://huggingface.co/docs/transformers/en/pipeline_gradio",
        "https://huggingface.co/docs/transformers/en/pipeline_webserver",
        "https://huggingface.co/docs/transformers/en/add_new_pipeline",
        "https://huggingface.co/docs/transformers/en/agents",
        "https://huggingface.co/docs/transformers/en/tools",
        "https://huggingface.co/docs/transformers/en/pipeline_webserver#web-server-inference",
        "https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.Pipeline",
        "https://huggingface.co/docs/transformers/en/pipeline_tutorial#large-datasets",
        "https://huggingface.co/docs/transformers/en/pipeline_webserver#create-a-server",
        "https://huggingface.co/docs/transformers/en/pipeline_webserver#queuing-requests",
        "https://huggingface.co/docs/transformers/en/pipeline_webserver#error-checking",
        "https://huggingface.co/docs/transformers/en/pipeline_webserver#circuit-breaking",
        "https://huggingface.co/docs/transformers/en/pipeline_webserver#block-the-main-thread",
        "https://huggingface.co/docs/transformers/en/pipeline_webserver#dynamic-batching",
        "https://huggingface.co/docs/transformers/en/pipeline_tutorial#batch-inference",
        "https://github.com/huggingface/transformers",
        "https://www.starlette.io/",
        "http://www.uvicorn.org/",
        "https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_webserver.md"
    ]
}