{
    "id": "43eeb5208be98fbfab54a47f340e96a7",
    "metadata": {
        "id": "43eeb5208be98fbfab54a47f340e96a7",
        "url": "https://www.linkedin.com/posts/ashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW?utm_source=share&utm_medium=member_desktop/",
        "title": "#llms #datascience #artificialintelligence | Ashish Patel ğŸ‡®ğŸ‡³ | 36 comments",
        "properties": {
            "description": "Training a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it.\n\nAlright, every ML engineer has been there. Youâ€™re sitting in a meeting, andâ€¦ | 36 comments on LinkedIn",
            "keywords": null,
            "author": null,
            "og:description": "Training a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it.\n\nAlright, every ML engineer has been there. Youâ€™re sitting in a meeting, andâ€¦ | 36 comments on LinkedIn",
            "og:title": "#llms #datascience #artificialintelligence | Ashish Patel ğŸ‡®ğŸ‡³ | 36 comments",
            "og:type": "article",
            "og:url": "https://www.linkedin.com/posts/ashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW",
            "og:image": "https://media.licdn.com/dms/image/v2/D4D22AQFIquTRXUN6LA/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1730268143172?e=2147483647&v=beta&t=qbQlChbqMGVretqdi39SvEeU6lwSgtb28oZd_Ia5kb8",
            "twitter:description": "Training a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it.\n\nAlright, every ML engineer has been there. Youâ€™re sitting in a meeting, andâ€¦ | 36 comments on LinkedIn",
            "twitter:title": "#llms #datascience #artificialintelligence | Ashish Patel ğŸ‡®ğŸ‡³ | 36 comments",
            "twitter:card": "summary_large_image",
            "twitter:image": "https://media.licdn.com/dms/image/v2/D4D22AQFIquTRXUN6LA/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1730268143172?e=2147483647&v=beta&t=qbQlChbqMGVretqdi39SvEeU6lwSgtb28oZd_Ia5kb8",
            "twitter:site": "@linkedin"
        }
    },
    "parent_metadata": {
        "id": "bfdfd6e98efeb74c06b6fa5a9a7addaa",
        "url": "https://www.notion.so/Training-Specs-bfdfd6e98efeb74c06b6fa5a9a7addaa",
        "title": "Training Specs",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "Agree & Join LinkedIn \nBy clicking Continue to join or sign in, you agree to LinkedInâ€™s [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy). \n[ Skip to main content ](https://www.linkedin.com/posts/ashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW?utm_source=share&utm_medium=member_desktop/#main-content) [ LinkedIn ](https://www.linkedin.com/?trk=public_post_nav-header-logo)\n  * [ Articles  ](https://www.linkedin.com/pulse/topics/home/?trk=public_post_guest_nav_menu_articles)\n  * [ People  ](https://www.linkedin.com/pub/dir/+/+?trk=public_post_guest_nav_menu_people)\n  * [ Learning  ](https://www.linkedin.com/learning/search?trk=public_post_guest_nav_menu_learning)\n  * [ Jobs  ](https://www.linkedin.com/jobs/search?trk=public_post_guest_nav_menu_jobs)\n  * [ Games  ](https://www.linkedin.com/games?trk=public_post_guest_nav_menu_games)\n\n\n[ Join now ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_nav-header-join) [ Sign in ](https://www.linkedin.com/login?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&fromSignIn=true&trk=public_post_nav-header-signin)\n#  Ashish Patel ğŸ‡®ğŸ‡³â€™s Post\n[ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_feed-actor-image)\n[ Ashish Patel ğŸ‡®ğŸ‡³ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_feed-actor-name)\nSr AWS AI ML Solution Architect at IBM | Generative AI Expert Strategist | Author Hands-on Time Series Analytics with Python | IBM Quantum ML Certified | 12+ Years in AI | IIMA | 100k+Followers | 6x LinkedIn Top Voice | \n5mo \n  * [ Report this post ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\n\nTraining a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it. Alright, every ML engineer has been there. Youâ€™re sitting in a meeting, and someone drops the classic, \"Soâ€¦ how long will it take to train this model?\" At first, I had no idea how to answer it, and when I tried finding answers, most articles threw a ton of jargon without giving me the actual numbers I needed. Frustrating, right? I decided to dig into it myself and figured out how to do a rough back-of-the-napkin calculation that actually works. Letâ€™s break down the key stuff. ğ—§ğ—µğ—² ğ— ğ—®ğ˜ğ—µ: ([https://lnkd.in/dWvgWvXM](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post-text)) â–¸ Itâ€™s all about FLOPs (floating point operations) and GPU power. Basically, you calculate how many FLOPs your model and data require, then divide it by how much power your GPU setup can handle. â–¸ For example, the LLaMA 3.1 model has 405 billion parameters and was trained on 15.6 trillion tokens. In plain English: that means it needed 3.8 Ã— 10Â²âµ FLOPs to train (yep, that's an insane number). â–¸ To train this beast, they used 16,000 H100 GPUs, each working at about 400 teraflops per second. But hereâ€™s the catchâ€”not all GPUs run at full speed. In fact, in this case, the GPUs were running at 38% efficiency due to various network and memory bottlenecks. So, how long did it take to train? Letâ€™s calculate it: 3.8 Ã— 10Â²âµ FLOPs Ã· 6.4 Ã— 10Â¹â¸ FLOPs per second = 61 days But, What About the Cost? ğŸ’¸ This part always gets me. It took 26 million GPU hours to train LLaMA 3.1. With each GPU costing about $2/hour, the total came out to $52 million! Thatâ€™s not a typo. I know, itâ€™s wild. ğ—¦ğ—¼, ğ—›ğ—¼ğ˜„ ğ——ğ—¼ ğ—¬ğ—¼ğ˜‚ ğ—˜ğ˜€ğ˜ğ—¶ğ—ºğ—®ğ˜ğ—² ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—¢ğ˜„ğ—» ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—§ğ—¶ğ—ºğ—²? âœ¦ Total FLOPs â€“ Calculate the size of your model (parameters) and multiply it by the size of your dataset (tokens). The formulaâ€™s in the images I shared. âœ¦ GPU FLOPs â€“ Find out how many FLOPs your GPUs can handle (and be honest about that efficiency drop!). âœ¦ Do the division â€“ FLOPs needed Ã· GPU power = training time. The cool part? Once you know how to calculate this, you stop guessing and start making solid predictions, avoiding those \"ummm, not sure\" moments with the team. Lessons from LLaMA 3 â‡‰ If training a 405B parameter model takes 16,000 GPUs and 61 days, scaling up or down from there gets pretty straightforward. But be warned: donâ€™t just trust theoretical max numbers for your hardware. Use the real-world throughput (MFU) youâ€™re actually getting, or youâ€™ll end up way off. â‡‰ This method isnâ€™t flawless, but itâ€™s miles better than guessing. When youâ€™re dropping millions on GPU hours, you definitely want more than a ballpark guess. Would love to hear your thoughts if you've run similar calculations or hit similar roadblocks! Letâ€™s get a conversation going. ğŸ‘‡ [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post-text)\n[ 446  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_social-actions-reactions) [ 36 Comments ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_social-actions-comments)\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment-cta)\nShare \n  * Copy\n  * LinkedIn\n  * Facebook\n  * Twitter\n\n\n[ ](https://at.linkedin.com/in/stefan-komornyik?trk=public_post_comment_actor-image)\n[ Stefan Komornyik ](https://at.linkedin.com/in/stefan-komornyik?trk=public_post_comment_actor-name)\nğŸš€ It is my mission to make time series data valuable ğŸš€ delivering scalable, Cloud-first solutions with on-prem flexibility for the energy sector, with 30+ years of experience in 100+ companies.\n5mo \n  * [ Report this comment ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\nThank you-Super interesting! Did you also think about considering the energy consumption? âš¡ï¸\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 5 Reactions ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 6 Reactions \n[ ](https://ch.linkedin.com/in/isabella-kosch-zh?trk=public_post_comment_actor-image)\n[ Isabella Kosch ](https://ch.linkedin.com/in/isabella-kosch-zh?trk=public_post_comment_actor-name) 5mo \n  * [ Report this comment ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\nThank you! I just learned something. Really interesting.\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 2 Reactions ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 3 Reactions \n[ ](https://it.linkedin.com/in/danielebaranzini?trk=public_post_comment_actor-image)\n[ Daniele Baranzini ](https://it.linkedin.com/in/danielebaranzini?trk=public_post_comment_actor-name)\nHAIKAI\n5mo \n  * [ Report this comment ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\ntotally agree with frends here about energy and consumption issues...but I focus on this other subtle but serious aspect: this AI trajectory is sustainable (and doable) only by FAANG and incoming emerging big AI tech monopolies...this is opposite to AI democratization ...opposite to open source at effort level at least....this instead is towards AI concentration in the hands of a few people in FB Google AWS, NVidia and a few others..... what is partially given out is leaked weights of various (a lot) minor versions of LLM or Multimondal alike ...if you (AI citizen or business company) want to excell on such AI systems you HAVE TO BUY THE API OR THE CLOUD by AWS, IBM, ORACLE GOOGLE etc.... Do not even think to use 1/10 of the effort in the picture above.... the LLM economy is shaping the researchers in AI towards such systems.... problem is not AI ....problem is the economy scale that the FAANG are trying (succesfully) to bring forward : ))) Daniele\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 6 Reactions ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 7 Reactions \n[ ](https://www.linkedin.com/in/visinha?trk=public_post_comment_actor-image)\n[ Vishal Sinha ](https://www.linkedin.com/in/visinha?trk=public_post_comment_actor-name) 4mo \n  * [ Report this comment ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\nTY for sharing these details. From where did you get 38% GPU utilization data? It looks pretty high.\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) 1 Reaction \n[ ](https://in.linkedin.com/in/balaji-kartheek-b6a378205?trk=public_post_comment_actor-image)\n[ Balaji Kartheek ](https://in.linkedin.com/in/balaji-kartheek-b6a378205?trk=public_post_comment_actor-name)\nAssociate QA Engineer @Avaamo.ai | Kaggle Expert | ex-ML Intern @Intel | AI Enthusiastic\n5mo \n  * [ Report this comment ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\nInsightful ğŸ¤¯ğŸ‘\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) 1 Reaction \n[ ](https://in.linkedin.com/in/dontaraju-pavan-srinivas-59b93419?trk=public_post_comment_actor-image)\n[ Dontaraju Pavan Srinivas ](https://in.linkedin.com/in/dontaraju-pavan-srinivas-59b93419?trk=public_post_comment_actor-name)\nChief Digital Officer from ISB, Agile Transformation, Scrum Master, SAFe Agilist, Lotus notes Consultant, Salesforce Consultant,DevOps Consultant, SharePoint/Power BI and Digital Transformation- ISB Alumini\n4mo \n  * [ Report this comment ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\nUseful tips\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) 1 Reaction \n[ ](https://ca.linkedin.com/in/faranakhdr?trk=public_post_comment_actor-image)\n[ Faranak Heidari ](https://ca.linkedin.com/in/faranakhdr?trk=public_post_comment_actor-name)\nGenAI/ML @ IBM, Neuroscience, Tech Enthusiast, Women Techmakers Ambassador Google\n5mo \n  * [ Report this comment ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\nMind blowing! Thank you really informative now I have a better reference\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 1 Reaction ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 2 Reactions \n[ ](https://pk.linkedin.com/in/muhammad-jawad-amin?trk=public_post_comment_actor-image)\n[ Muhammad Jawad Amin ](https://pk.linkedin.com/in/muhammad-jawad-amin?trk=public_post_comment_actor-name)\nTrainee Data Scientist @NextBridge | Ravian ğŸ“ | Medalist ğŸ¥‡| Specializing in Computer Vision & Generative AI | Building innovative solutions with Tensorflow, FastAPI, Playwright, Linux, Docker\n5mo \n  * [ Report this comment ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\nInteresting ğŸ¤”\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) [ 1 Reaction ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions) 2 Reactions \n[ ](https://in.linkedin.com/in/nishant-singh-yadav-b72b33177?trk=public_post_comment_actor-image)\n[ Nishant Singh Yadav ](https://in.linkedin.com/in/nishant-singh-yadav-b72b33177?trk=public_post_comment_actor-name)\nSenior Data Scientist @ Sigmoid | IIT Dhanbad'21\n4mo \n  * [ Report this comment ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\nVery informative! ğŸŒŸ \n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) 1 Reaction \n[ ](https://www.linkedin.com/in/bukolaola?trk=public_post_comment_actor-image)\n[ Lanre Ogunkunle ](https://www.linkedin.com/in/bukolaola?trk=public_post_comment_actor-name) 4mo \n  * [ Report this comment ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\n\nVery informative\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like) [ Reply  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply) 1 Reaction \n[ See more comments ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_see-more-comments)\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_feed-cta-banner-cta)\n##  More Relevant Posts \n  * [](https://www.linkedin.com/posts/ganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr)\n[ ](https://in.linkedin.com/in/ganeshjagadeesan?trk=public_post_feed-actor-image)\n[ Ganesh Jagadeesan ](https://in.linkedin.com/in/ganeshjagadeesan?trk=public_post_feed-actor-name)\nEnterprise Data Science Specialist @Mastech Digital | NLP | NER | Deep Learning | Gen AI | MLops \n5mo \n    * [ Report this post ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\nThis post is a goldmine! ğŸ’¡ Accurately calculating training time for LLMs has always been tricky, and itâ€™s awesome to see a practical breakdown like this. ğŸ”¢ FLOPs and GPU throughput arenâ€™t usually discussed in this level of detailâ€”itâ€™s refreshing to see the real-world math behind training something as massive as LLaMA 3.1. ğŸ§  The efficiency drop you highlighted is such a critical point. Many overlook how network and memory bottlenecks impact real-world GPU performance, and that 38% efficiency makes a big difference in cost and timelines. âš™ï¸ğŸ’¸ Also, $52 million in GPU hours is mind-blowing! ğŸ˜² It really emphasizes how planning and optimization are essential when training models at this scale. Knowing the true throughput youâ€™re getting (instead of trusting theoretical max speeds) is key to avoiding costly surprises. ğŸ—ï¸ğŸ“‰ I love the step-by-step approach for estimating your own training timeâ€”super helpful for anyone navigating those awkward â€œhow long will this take?â€ moments. ğŸ˜‚ Thanks for sharing the formulas too, theyâ€™ll be a lifesaver for future projects. Looking forward to hearing more about others' experiences with training large modelsâ€”these insights will definitely make our jobs a bit easier! ğŸ™Œ [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post-text) [#MachineLearning](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmachinelearning&trk=public_post-text) [#GPU](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpu&trk=public_post-text) [#ModelTraining](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmodeltraining&trk=public_post-text) [#AIInsights](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faiinsights&trk=public_post-text) [#DeepLearning](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdeeplearning&trk=public_post-text) [#AIOptimization](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faioptimization&trk=public_post-text)\n[ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-image)\n[ Ashish Patel ğŸ‡®ğŸ‡³ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-name)\nSr AWS AI ML Solution Architect at IBM | Generative AI Expert Strategist | Author Hands-on Time Series Analytics with Python | IBM Quantum ML Certified | 12+ Years in AI | IIMA | 100k+Followers | 6x LinkedIn Top Voice | \n5mo \nTraining a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it. Alright, every ML engineer has been there. Youâ€™re sitting in a meeting, and someone drops the classic, \"Soâ€¦ how long will it take to train this model?\" At first, I had no idea how to answer it, and when I tried finding answers, most articles threw a ton of jargon without giving me the actual numbers I needed. Frustrating, right? I decided to dig into it myself and figured out how to do a rough back-of-the-napkin calculation that actually works. Letâ€™s break down the key stuff. ğ—§ğ—µğ—² ğ— ğ—®ğ˜ğ—µ: ([https://lnkd.in/dWvgWvXM](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post_reshare-text)) â–¸ Itâ€™s all about FLOPs (floating point operations) and GPU power. Basically, you calculate how many FLOPs your model and data require, then divide it by how much power your GPU setup can handle. â–¸ For example, the LLaMA 3.1 model has 405 billion parameters and was trained on 15.6 trillion tokens. In plain English: that means it needed 3.8 Ã— 10Â²âµ FLOPs to train (yep, that's an insane number). â–¸ To train this beast, they used 16,000 H100 GPUs, each working at about 400 teraflops per second. But hereâ€™s the catchâ€”not all GPUs run at full speed. In fact, in this case, the GPUs were running at 38% efficiency due to various network and memory bottlenecks. So, how long did it take to train? Letâ€™s calculate it: 3.8 Ã— 10Â²âµ FLOPs Ã· 6.4 Ã— 10Â¹â¸ FLOPs per second = 61 days But, What About the Cost? ğŸ’¸ This part always gets me. It took 26 million GPU hours to train LLaMA 3.1. With each GPU costing about $2/hour, the total came out to $52 million! Thatâ€™s not a typo. I know, itâ€™s wild. ğ—¦ğ—¼, ğ—›ğ—¼ğ˜„ ğ——ğ—¼ ğ—¬ğ—¼ğ˜‚ ğ—˜ğ˜€ğ˜ğ—¶ğ—ºğ—®ğ˜ğ—² ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—¢ğ˜„ğ—» ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—§ğ—¶ğ—ºğ—²? âœ¦ Total FLOPs â€“ Calculate the size of your model (parameters) and multiply it by the size of your dataset (tokens). The formulaâ€™s in the images I shared. âœ¦ GPU FLOPs â€“ Find out how many FLOPs your GPUs can handle (and be honest about that efficiency drop!). âœ¦ Do the division â€“ FLOPs needed Ã· GPU power = training time. The cool part? Once you know how to calculate this, you stop guessing and start making solid predictions, avoiding those \"ummm, not sure\" moments with the team. Lessons from LLaMA 3 â‡‰ If training a 405B parameter model takes 16,000 GPUs and 61 days, scaling up or down from there gets pretty straightforward. But be warned: donâ€™t just trust theoretical max numbers for your hardware. Use the real-world throughput (MFU) youâ€™re actually getting, or youâ€™ll end up way off. â‡‰ This method isnâ€™t flawless, but itâ€™s miles better than guessing. When youâ€™re dropping millions on GPU hours, you definitely want more than a ballpark guess. Would love to hear your thoughts if you've run similar calculations or hit similar roadblocks! Letâ€™s get a conversation going. ğŸ‘‡ [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post_reshare-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post_reshare-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post_reshare-text)\n[ 8  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_social-actions-reactions)\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_comment-cta)\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_feed-cta-banner-cta)\n  * [](https://www.linkedin.com/posts/dylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V)\n[ ](https://sg.linkedin.com/in/dylan-poh?trk=public_post_feed-actor-image)\n[ Dylan Poh ](https://sg.linkedin.com/in/dylan-poh?trk=public_post_feed-actor-name)\nAI Engineer at AI Singapore \n4mo \n    * [ Report this post ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\nThe post sheds light on the staggering costs and resources needed to train a 405 billion-parameter model: tens of thousands of GPUs, millions of dollars, and massive energy consumption. This scale pushes us to ask tough questions â€“ are we moving toward genuine problem-solving, or just chasing size? Can we make AI more accessible and sustainable, or are we building barriers with resource-heavy giants? As we push these boundaries, balancing innovation with responsibility becomes more critical than ever.\n[ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-image)\n[ Ashish Patel ğŸ‡®ğŸ‡³ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-name)\nSr AWS AI ML Solution Architect at IBM | Generative AI Expert Strategist | Author Hands-on Time Series Analytics with Python | IBM Quantum ML Certified | 12+ Years in AI | IIMA | 100k+Followers | 6x LinkedIn Top Voice | \n5mo \nTraining a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it. Alright, every ML engineer has been there. Youâ€™re sitting in a meeting, and someone drops the classic, \"Soâ€¦ how long will it take to train this model?\" At first, I had no idea how to answer it, and when I tried finding answers, most articles threw a ton of jargon without giving me the actual numbers I needed. Frustrating, right? I decided to dig into it myself and figured out how to do a rough back-of-the-napkin calculation that actually works. Letâ€™s break down the key stuff. ğ—§ğ—µğ—² ğ— ğ—®ğ˜ğ—µ: ([https://lnkd.in/dWvgWvXM](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post_reshare-text)) â–¸ Itâ€™s all about FLOPs (floating point operations) and GPU power. Basically, you calculate how many FLOPs your model and data require, then divide it by how much power your GPU setup can handle. â–¸ For example, the LLaMA 3.1 model has 405 billion parameters and was trained on 15.6 trillion tokens. In plain English: that means it needed 3.8 Ã— 10Â²âµ FLOPs to train (yep, that's an insane number). â–¸ To train this beast, they used 16,000 H100 GPUs, each working at about 400 teraflops per second. But hereâ€™s the catchâ€”not all GPUs run at full speed. In fact, in this case, the GPUs were running at 38% efficiency due to various network and memory bottlenecks. So, how long did it take to train? Letâ€™s calculate it: 3.8 Ã— 10Â²âµ FLOPs Ã· 6.4 Ã— 10Â¹â¸ FLOPs per second = 61 days But, What About the Cost? ğŸ’¸ This part always gets me. It took 26 million GPU hours to train LLaMA 3.1. With each GPU costing about $2/hour, the total came out to $52 million! Thatâ€™s not a typo. I know, itâ€™s wild. ğ—¦ğ—¼, ğ—›ğ—¼ğ˜„ ğ——ğ—¼ ğ—¬ğ—¼ğ˜‚ ğ—˜ğ˜€ğ˜ğ—¶ğ—ºğ—®ğ˜ğ—² ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—¢ğ˜„ğ—» ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—§ğ—¶ğ—ºğ—²? âœ¦ Total FLOPs â€“ Calculate the size of your model (parameters) and multiply it by the size of your dataset (tokens). The formulaâ€™s in the images I shared. âœ¦ GPU FLOPs â€“ Find out how many FLOPs your GPUs can handle (and be honest about that efficiency drop!). âœ¦ Do the division â€“ FLOPs needed Ã· GPU power = training time. The cool part? Once you know how to calculate this, you stop guessing and start making solid predictions, avoiding those \"ummm, not sure\" moments with the team. Lessons from LLaMA 3 â‡‰ If training a 405B parameter model takes 16,000 GPUs and 61 days, scaling up or down from there gets pretty straightforward. But be warned: donâ€™t just trust theoretical max numbers for your hardware. Use the real-world throughput (MFU) youâ€™re actually getting, or youâ€™ll end up way off. â‡‰ This method isnâ€™t flawless, but itâ€™s miles better than guessing. When youâ€™re dropping millions on GPU hours, you definitely want more than a ballpark guess. Would love to hear your thoughts if you've run similar calculations or hit similar roadblocks! Letâ€™s get a conversation going. ğŸ‘‡ [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post_reshare-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post_reshare-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post_reshare-text)\n[ 6  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_social-actions-reactions)\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_comment-cta)\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_feed-cta-banner-cta)\n  * [](https://www.linkedin.com/posts/daemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3)\n[ ](https://ca.linkedin.com/in/daemonbehr?trk=public_post_feed-actor-image)\n[ Daemon B. ](https://ca.linkedin.com/in/daemonbehr?trk=public_post_feed-actor-name)\nField CTO - Americas - Nutanix | NCX #50 - Enterprise AI Strategic Advisor \n4mo \n    * [ Report this post ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\nThe truth is that most organizations will not train their own models, and if they do, it will not be a 405B parameter model either. Meta's 405B model is not really meant for inferencing anyway. It serves as a \"teaching model\" that can distill its knowledge to smaller, more efficient models. This process, known as model distillation, allows for the creation of high-performing smaller models with synthetic training data that are more feasible for deployment. Then the small model can actually outperform many larger models. The GPU resources for distillation are a very small fraction of what would be required for training. The other alternatives for using your own data are RAG approaches, which are the most common and least GPU intensive. The questions will always be around what is the desired outcome? What are the steps to get there? What is involved in maintaining it? And what does the cost / benefit / constraint Venn diagram look like? Nutanix provides the easiest on-ramp for organizations to leverage their own data with Enterprise AI, using models that are best suited for your use case. They can be LLMs, or SLMs, distilled, or with RAG, or both. Link here: [https://lnkd.in/ej6g_b5E](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fej6g_b5E&urlhash=4gHz&trk=public_post-text) [#GenAI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgenai&trk=public_post-text) [#AIonNutanix](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faionnutanix&trk=public_post-text) [#Nutanix](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fnutanix&trk=public_post-text)\n[ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-image)\n[ Ashish Patel ğŸ‡®ğŸ‡³ ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-name)\nSr AWS AI ML Solution Architect at IBM | Generative AI Expert Strategist | Author Hands-on Time Series Analytics with Python | IBM Quantum ML Certified | 12+ Years in AI | IIMA | 100k+Followers | 6x LinkedIn Top Voice | \n5mo \nTraining a 405B LLM took 16,000 GPUs and 61 daysâ€”hereâ€™s the real math behind it. Alright, every ML engineer has been there. Youâ€™re sitting in a meeting, and someone drops the classic, \"Soâ€¦ how long will it take to train this model?\" At first, I had no idea how to answer it, and when I tried finding answers, most articles threw a ton of jargon without giving me the actual numbers I needed. Frustrating, right? I decided to dig into it myself and figured out how to do a rough back-of-the-napkin calculation that actually works. Letâ€™s break down the key stuff. ğ—§ğ—µğ—² ğ— ğ—®ğ˜ğ—µ: ([https://lnkd.in/dWvgWvXM](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post_reshare-text)) â–¸ Itâ€™s all about FLOPs (floating point operations) and GPU power. Basically, you calculate how many FLOPs your model and data require, then divide it by how much power your GPU setup can handle. â–¸ For example, the LLaMA 3.1 model has 405 billion parameters and was trained on 15.6 trillion tokens. In plain English: that means it needed 3.8 Ã— 10Â²âµ FLOPs to train (yep, that's an insane number). â–¸ To train this beast, they used 16,000 H100 GPUs, each working at about 400 teraflops per second. But hereâ€™s the catchâ€”not all GPUs run at full speed. In fact, in this case, the GPUs were running at 38% efficiency due to various network and memory bottlenecks. So, how long did it take to train? Letâ€™s calculate it: 3.8 Ã— 10Â²âµ FLOPs Ã· 6.4 Ã— 10Â¹â¸ FLOPs per second = 61 days But, What About the Cost? ğŸ’¸ This part always gets me. It took 26 million GPU hours to train LLaMA 3.1. With each GPU costing about $2/hour, the total came out to $52 million! Thatâ€™s not a typo. I know, itâ€™s wild. ğ—¦ğ—¼, ğ—›ğ—¼ğ˜„ ğ——ğ—¼ ğ—¬ğ—¼ğ˜‚ ğ—˜ğ˜€ğ˜ğ—¶ğ—ºğ—®ğ˜ğ—² ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—¢ğ˜„ğ—» ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—§ğ—¶ğ—ºğ—²? âœ¦ Total FLOPs â€“ Calculate the size of your model (parameters) and multiply it by the size of your dataset (tokens). The formulaâ€™s in the images I shared. âœ¦ GPU FLOPs â€“ Find out how many FLOPs your GPUs can handle (and be honest about that efficiency drop!). âœ¦ Do the division â€“ FLOPs needed Ã· GPU power = training time. The cool part? Once you know how to calculate this, you stop guessing and start making solid predictions, avoiding those \"ummm, not sure\" moments with the team. Lessons from LLaMA 3 â‡‰ If training a 405B parameter model takes 16,000 GPUs and 61 days, scaling up or down from there gets pretty straightforward. But be warned: donâ€™t just trust theoretical max numbers for your hardware. Use the real-world throughput (MFU) youâ€™re actually getting, or youâ€™ll end up way off. â‡‰ This method isnâ€™t flawless, but itâ€™s miles better than guessing. When youâ€™re dropping millions on GPU hours, you definitely want more than a ballpark guess. Would love to hear your thoughts if you've run similar calculations or hit similar roadblocks! Letâ€™s get a conversation going. ğŸ‘‡ [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post_reshare-text) [#DataScience](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post_reshare-text) [#ArtificialIntelligence](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post_reshare-text)\n[ 9  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_social-actions-reactions) [ 1 Comment ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_social-actions-comments)\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_comment-cta)\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_feed-cta-banner-cta)\n  * [](https://www.linkedin.com/posts/siddhantsadangi_how-to-optimize-gpu-usage-during-model-training-activity-7186737795497963520-4rWE)\n[ ](https://fr.linkedin.com/in/siddhantsadangi?trk=public_post_feed-actor-image)\n[ Siddhant Sadangi ](https://fr.linkedin.com/in/siddhantsadangi?trk=public_post_feed-actor-name)\nğŸ¥‘ Developer Relations Engineer @neptune.ai | ğŸˆ Creator & Community Moderator @Streamlit | ğŸ‘¨ğŸ’»Ex - Data Scientist @ Reuters, Deloitte \n11mo \n    * [ Report this post ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fsiddhantsadangi_how-to-optimize-gpu-usage-during-model-training-activity-7186737795497963520-4rWE&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\nğŸš€ Maximizing GPU Efficiency ğŸ® Discover how to get the most out of your GPUs during model training with [neptune.ai](https://www.linkedin.com/company/neptuneai?trk=public_post-text)! ğŸŒŸ GPU Metrics: Learn about key GPU performance indicators like utilization, memory usage, and power consumption. ğŸ” Optimization Techniques: Explore strategies such as mixed-precision training and optimizing data transfer to boost GPU usage. âš™ï¸ Resource Monitoring: Find out how tools like Neptune can help you monitor and improve resource utilization. ğŸ“Š Expert Insights: Gain valuable insights from [Mirza Mujtaba](https://it.linkedin.com/in/mirzamujtabahussain?trk=public_post-text) and [Kilian Kluge](https://de.linkedin.com/in/kilian-kluge/en?trk=public_post-text) on optimizing GPU settings for cost-effective operations. ğŸ’¡ Maximize your machine learning potential by reading the full article here: [https://lnkd.in/e5jh7npm](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fe5jh7npm&urlhash=hS27&trk=public_post-text) ğŸ“ˆ [#MachineLearning](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmachinelearning&trk=public_post-text) [#MLOps](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmlops&trk=public_post-text) [#GPUOptimization](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpuoptimization&trk=public_post-text) [#DeepLearning](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdeeplearning&trk=public_post-text) [#NeptuneAI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fneptuneai&trk=public_post-text) [#TechTips](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Ftechtips&trk=public_post-text) Letâ€™s discuss! How do you ensure your GPUs are running at peak performance? Share your tips below! ğŸ‘‡\n## [ How to Optimize GPU Usage During Model Training  neptune.ai  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fneptune%2Eai%2Fblog%2Foptimizing-gpu-usage-during-model-training-with-neptune&urlhash=YIcz&trk=public_post_feed-article-content)\n[ 4  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fsiddhantsadangi_how-to-optimize-gpu-usage-during-model-training-activity-7186737795497963520-4rWE&trk=public_post_social-actions-reactions)\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fsiddhantsadangi_how-to-optimize-gpu-usage-during-model-training-activity-7186737795497963520-4rWE&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fsiddhantsadangi_how-to-optimize-gpu-usage-during-model-training-activity-7186737795497963520-4rWE&trk=public_post_comment-cta)\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fsiddhantsadangi_how-to-optimize-gpu-usage-during-model-training-activity-7186737795497963520-4rWE&trk=public_post_feed-cta-banner-cta)\n  * [](https://www.linkedin.com/posts/avi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-)\n[ ](https://in.linkedin.com/in/avi-chawla?trk=public_post_feed-actor-image)\n[ Avi Chawla ](https://in.linkedin.com/in/avi-chawla?trk=public_post_feed-actor-name)\nCo-founder DailyDoseofDS | IIT Varanasi | ex-AI Engineer MastercardAI | Newsletter (130k+) \n8mo \n    * [ Report this post ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\nHow to synchronize GPUs with multi-GPU training ğŸ§©? A significant run-time bottleneck of multi-GPU training is observed during model synchronization. Consider data parallelism, which: - Replicates the model across all GPUs. - Divides the available data into smaller batches, and each batch is processed by a separate GPU. - Computes the gradients on each GPU and then communicates them to every other GPU. Since every GPU processes a different chunk of the data, the gradients will also be different across devices. Thus, before updating the model parameters on each GPU device, local gradients must be communicated to all other devices. I covered two strategies for intermediate-sized models in yesterday's issue of the [Daily Dose of Data Science](https://in.linkedin.com/company/daily-dose-of-ds?trk=public_post-text) newsletter: [https://lnkd.in/g56-7HsZ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fg56-7HsZ&urlhash=JNnU&trk=public_post-text). 1) All-reduce: - One way is to transfer gradient from every GPU to every other GPU (naive approach). - Another way is to transfer gradients to one GPU, average them, and communicate back to all GPUs (better but not fully optimal and scalable). 2) Ring reduce: - While the total transfers are the same as the second procedure above, ring-reduce is much more scalable and efficient. - It does this with a ring formulation, then segments the local gradients on each device and transfers a segment to the next GPU. We covered it in detail here: [https://lnkd.in/g56-7HsZ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fg56-7HsZ&urlhash=JNnU&trk=public_post-text). ğŸ‘‰ Over to you: Can you optimize ring-reduce even further?\n[ 107  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_social-actions-reactions)\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_comment-cta)\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_feed-cta-banner-cta)\n  * [](https://www.linkedin.com/posts/mazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7)\n[ ](https://ca.linkedin.com/in/mazadgole?trk=public_post_feed-actor-image)\n[ Mehdi Azad ](https://ca.linkedin.com/in/mazadgole?trk=public_post_feed-actor-name)\nMachine Learning | Robotics | Educator \n9mo  Edited \n    * [ Report this post ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\nI'm sure that when you want to work with LLMs, these questions come to mind: 1- How many GPUs are required to store and fine-tune LLMs? (Given an NVIDIA V100 GPU with 32GB memory) 2- How long did it take to pre-train LLMs? 3- How can compute-efficient methods, such as quantization or LoRA, reduce the GPU requirements ? To answer the questions, let's do a simple math: \"Storage\": 1 parameter = 4 bytes. A model with 1B parameters requires 4 GB of memory. With 1 GPU (32 GB), I can perform inference using a model with up to 8 billion parameters. (I can do better with quantization method.) For GPT-3 with 175B parameters, you need ~22 GPUs for just storing the weights. \"Fine-Tuning\": Considering overheads (gradients and optimizer states), fine-tuning GPT-3 requires approximately x6 times more memory than just storing the weights, around 132 GPUs. So I need ~100 more GPUs for full fine tuning. Good news: with LoRA you can to the fine tuning in just 1 GPU!!! \"Pre-training\": What about pre-training GPT-3 on a vast amount of data? How many GPUs do I need, and how long does it take? The answer is ~1000 GPUs running for 30 days. Can I do better?? Yes. You can see my simple calculations and interesting results in this blog post. [#LLMs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post-text) [#GPUs](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpus&trk=public_post-text) [#Quantization](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fquantization&trk=public_post-text) [#LoRA](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Flora&trk=public_post-text) [#MachineLearning](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmachinelearning&trk=public_post-text)\n## [ Back-of-the-envelope calculation for GPU requirements of LLMs  mabbasiazad.github.io  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fmabbasiazad%2Egithub%2Eio%2Fportfolio%2Fposts%2Fllms_gpu%2F&urlhash=AWz1&trk=public_post_feed-article-content)\n[ 9  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_social-actions-reactions)\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_comment-cta)\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_feed-cta-banner-cta)\n  * [](https://www.linkedin.com/posts/martechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z)\n[ ](https://www.linkedin.com/company/martechrichard?trk=public_post_feed-actor-image)\n[ MarTechRichard ](https://www.linkedin.com/company/martechrichard?trk=public_post_feed-actor-name)\n14 followers \n6mo  Edited \n    * [ Report this post ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\nğŸš€ Exciting times in AI! As GPU demand soars, letâ€™s not overlook the power of CPUs! ğŸ¤–ğŸ’» ğŸ”‘ Key insights from our latest article: 1ï¸âƒ£ CPUs can still excel in ML when GPUs are scarce. 2ï¸âƒ£ Performance optimization with Intel Xeon & PyTorch can boost efficiency. 3ï¸âƒ£ Smart data preprocessing minimizes CPU load. ğŸ’¡ Businesses stand to gain: - Cost-effective solutions for smaller projects ğŸ’° - Scalability without GPU constraints ğŸ“ˆ - Enhanced development productivity â²ï¸ How are YOU optimizing CPU performance in your ML tasks? Letâ€™s discuss! ğŸ’¬ ğŸ”— Read more here: [Training AI Models on CPU - Towards Data Science]([https://lnkd.in/eB5w7N8U](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FeB5w7N8U&urlhash=0hjW&trk=public_post-text)) ğŸ“ For deeper insights, contact us via WhatsApp: [[https://lnkd.in/e9sTptsu](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fe9sTptsu&urlhash=QLXV&trk=public_post-text)) or connect on LinkedIn! ğŸ”— \n## [ Exploring CPU Training for AI Models Amid GPU Resource Limitations  towardsdatascience.com  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Ftowardsdatascience%2Ecom%2Ftraining-ai-models-on-cpu-3903adc9f388&urlhash=gBHg&trk=public_post_feed-article-content)\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_comment-cta)\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_feed-cta-banner-cta)\n  * [](https://www.linkedin.com/posts/benjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH)\n[ ](https://uk.linkedin.com/in/benjamin-j-todd?trk=public_post_feed-actor-image)\n[ Benjamin Todd ](https://uk.linkedin.com/in/benjamin-j-todd?trk=public_post_feed-actor-name)\nFounder of 80,000 Hours | author | Trying to understand AGI and what to do about it \n10mo \n    * [ Report this post ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\nIf someone has a bunch of GPUs, how much AI inference can they do with them? The approach I most often see non-experts take is to look up FLOP/s for the GPUs and then divide that by the FLOP per forward pass. E.g. The A100 is listed at 312 teraflop per second on its spec sheet (FP16 tensor), and a forward pass of GPT-4 requires 5.6e11 FLOP per forward pass, which would be 560 per second. But this turns out to be way too high. Even if spec sheet efficiency could be achieved (it can't), the model parameters also need to pass through the GPU's memory. The A100 only has 2000 GB/s of memory bandwidth â€“ only enough for 4 forward passes! But that turns out to be way too low. In reality, multiple GPUs are parallelised and forward passes are processed in batches. This means real world efficiency is somewhere between these upper and lower bounds. Semianalysis estimates for GPT-4 on A100s, it's around 10x the lower bound and 10% of the upper. In the full post, I extend this to more advanced chips, and speculate about it might change in the future: [https://lnkd.in/e5DNbZDK](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fe5DNbZDK&urlhash=8KaZ&trk=public_post-text)\n## [ How much AI inference can we do?  benjamintodd.substack.com  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fbenjamintodd%2Esubstack%2Ecom%2Fp%2Fhow-much-ai-inference-can-we-do&urlhash=QrZ1&trk=public_post_feed-article-content)\n[ 6  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_social-actions-reactions)\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_comment-cta)\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_feed-cta-banner-cta)\n  * [](https://www.linkedin.com/posts/marek-bar%C3%A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT)\n[ ](https://at.linkedin.com/in/marek-bar%C3%A1k-31977a55?trk=public_post_feed-actor-image)\n[ Marek BarÃ¡k ](https://at.linkedin.com/in/marek-bar%C3%A1k-31977a55?trk=public_post_feed-actor-name)\nData Alchemist & code:Breaker \n3mo \n    * [ Report this post ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\nAgain an resource that can be ignored, especially if you are in GPU programming, the skill of 2025+. [#GPU](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpu&trk=public_post-text) [#NVIDIA](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fnvidia&trk=public_post-text)\n[ ](https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_reshare_feed-actor-image)\n[ Jay Shah ](https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_reshare_feed-actor-name)\nResearch Scientist at Colfax International \n3mo \nI'm happy to share the final part of our three part series on writing optimized GEMM kernels for NVIDIA GPUs using CUTLASS library abstractions. This last installment explains the Stream-K algorithm for scheduling work over threadblocks and how it surmounts the problem of wave quantization. To explain the problem, consider a naive \"data-parallel\" approach to GEMM that partitions the output matrix as a grid of tiles and assigns tiles to threadblocks one-to-one. A discrete number of waves will be launched to process the computation, quantized according to the total number of streaming multiprocessors (SMs). If the number of tiles is then not evenly divisible by the number of SMs, the last tail wave will process a reduced number of work tiles while still taking the time of a full wave, severely degrading overall TFLOPS performance. Roughly speaking, Stream-K addresses this issue by introducing parallelism along the inner K-dimension and enabling threadblocks to \"fractionally\" process output tiles, rebalancing the distribution of total work across the SMs. Our tutorial also describes CUTLASS's tile scheduler abstraction, which we leverage to effectively implement scheduling strategies like Stream-K. This separates scheduling optimizations that work at the grid level from those that target the inner load and compute loops, like the pipelining and warp specialization strategies we discussed in part 2 of this series. We both implement a simplified version of the Stream-K tile scheduler for a custom GEMM kernel and discuss the inner workings of CUTLASS's more advanced version. Work done in collaboration with my wonderful colleagues at Colfax Research! Happy holidays! [https://lnkd.in/gZkRqYeN](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgZkRqYeN&urlhash=DQnO&trk=public_post_reshare-text)\n## [ CUTLASS Tutorial: Persistent Kernels and Stream-K  https://research.colfax-intl.com  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fresearch%2Ecolfax-intl%2Ecom%2Fcutlass-tutorial-persistent-kernels-and-stream-k%2F&urlhash=tx4R&trk=public_post_reshare_feed-article-content)\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_comment-cta)\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_feed-cta-banner-cta)\n  * [](https://www.linkedin.com/posts/jay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm)\n[ ](https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_feed-actor-image)\n[ Jay Shah ](https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_feed-actor-name)\nResearch Scientist at Colfax International \n3mo \n    * [ Report this post ](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\nI'm happy to share the final part of our three part series on writing optimized GEMM kernels for NVIDIA GPUs using CUTLASS library abstractions. This last installment explains the Stream-K algorithm for scheduling work over threadblocks and how it surmounts the problem of wave quantization. To explain the problem, consider a naive \"data-parallel\" approach to GEMM that partitions the output matrix as a grid of tiles and assigns tiles to threadblocks one-to-one. A discrete number of waves will be launched to process the computation, quantized according to the total number of streaming multiprocessors (SMs). If the number of tiles is then not evenly divisible by the number of SMs, the last tail wave will process a reduced number of work tiles while still taking the time of a full wave, severely degrading overall TFLOPS performance. Roughly speaking, Stream-K addresses this issue by introducing parallelism along the inner K-dimension and enabling threadblocks to \"fractionally\" process output tiles, rebalancing the distribution of total work across the SMs. Our tutorial also describes CUTLASS's tile scheduler abstraction, which we leverage to effectively implement scheduling strategies like Stream-K. This separates scheduling optimizations that work at the grid level from those that target the inner load and compute loops, like the pipelining and warp specialization strategies we discussed in part 2 of this series. We both implement a simplified version of the Stream-K tile scheduler for a custom GEMM kernel and discuss the inner workings of CUTLASS's more advanced version. Work done in collaboration with my wonderful colleagues at Colfax Research! Happy holidays! [https://lnkd.in/gZkRqYeN](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgZkRqYeN&urlhash=DQnO&trk=public_post-text)\n## [ CUTLASS Tutorial: Persistent Kernels and Stream-K  https://research.colfax-intl.com  ](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fresearch%2Ecolfax-intl%2Ecom%2Fcutlass-tutorial-persistent-kernels-and-stream-k%2F&urlhash=tx4R&trk=public_post_feed-article-content)\n[ 572  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_social-actions-reactions) [ 5 Comments ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_social-actions-comments)\n[ Like  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_like-cta) [ Comment  ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_comment-cta)\nShare \n    * Copy\n    * LinkedIn\n    * Facebook\n    * Twitter\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_feed-cta-banner-cta)\n\n\n![](https://media.licdn.com/dms/image/v2/D4D16AQGKG5t7j7VjNw/profile-displaybackgroundimage-shrink_200_800/profile-displaybackgroundimage-shrink_200_800/0/1677477533507?e=2147483647&v=beta&t=BEOsmY67kMP7Ai1g8vbKMu1EV3nNFMPPnIGGO4aHJJU)\n97,006 followers \n  * [ 3000+ Posts ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Fashishpatel2604%2Frecent-activity%2F&trk=public_post_follow-posts)\n  * [ 33 Articles ](https://www.linkedin.com/today/author/ashishpatel2604?trk=public_post_follow-articles)\n\n\n[ View Profile ](https://in.linkedin.com/in/ashishpatel2604?trk=public_post_follow-view-profile) [ Connect ](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7257270599444512769&trk=public_post_follow)\n##  More from this author \n  * ### [ Why Writing Clean Python Code is Still Non-Negotiable at MAANG â€“ And How You Can Master It  Ashish Patel ğŸ‡®ğŸ‡³  2w  ](https://www.linkedin.com/pulse/why-writing-clean-python-code-still-non-negotiable-maang-patel--uikxf?trk=public_post)\n  * ### [ Memory in LLMs Cuts Training Time by 30%â€”And Hereâ€™s What That Means for AI Agent Development  Ashish Patel ğŸ‡®ğŸ‡³  2mo  ](https://www.linkedin.com/pulse/memory-llms-cuts-training-time-30and-heres-what-means-patel--hcqaf?trk=public_post)\n  * ### [ Over 62% of AI Teams Struggle with Model Deployment â€” PyTorchâ€™s New Features Solve This, Saving Millions on Development  Ashish Patel ğŸ‡®ğŸ‡³  4mo  ](https://www.linkedin.com/pulse/over-62-ai-teams-struggle-model-deployment-pytorchs-new-patel--nizrf?trk=public_post)\n\n\n##  Explore topics \n  * [ Sales ](https://www.linkedin.com/pulse/topics/sales-s5/)\n  * [ Marketing ](https://www.linkedin.com/pulse/topics/marketing-s2461/)\n  * [ IT Services ](https://www.linkedin.com/pulse/topics/it-services-s57547/)\n  * [ Business Administration ](https://www.linkedin.com/pulse/topics/business-administration-s50111/)\n  * [ HR Management ](https://www.linkedin.com/pulse/topics/hr-management-s50359/)\n  * [ Engineering ](https://www.linkedin.com/pulse/topics/engineering-s166/)\n  * [ Soft Skills ](https://www.linkedin.com/pulse/topics/soft-skills-s2976/)\n  * [ See All ](https://www.linkedin.com/pulse/topics/home/)\n\n\n  * LinkedIn Â© 2025\n  * [ About ](https://about.linkedin.com?trk=d_public_post_footer-about)\n  * [ Accessibility ](https://www.linkedin.com/accessibility?trk=d_public_post_footer-accessibility)\n  * [ User Agreement ](https://www.linkedin.com/legal/user-agreement?trk=d_public_post_footer-user-agreement)\n  * [ Privacy Policy ](https://www.linkedin.com/legal/privacy-policy?trk=d_public_post_footer-privacy-policy)\n  * [ Your California Privacy Choices ](https://www.linkedin.com/legal/california-privacy-disclosure?trk=d_public_post_footer-california-privacy-rights-act)\n  * [ Cookie Policy ](https://www.linkedin.com/legal/cookie-policy?trk=d_public_post_footer-cookie-policy)\n  * [ Copyright Policy ](https://www.linkedin.com/legal/copyright-policy?trk=d_public_post_footer-copyright-policy)\n  * [ Brand Policy ](https://brand.linkedin.com/policies?trk=d_public_post_footer-brand-policy)\n  * [ Guest Controls ](https://www.linkedin.com/psettings/guest-controls?trk=d_public_post_footer-guest-controls)\n  * [ Community Guidelines ](https://www.linkedin.com/legal/professional-community-policies?trk=d_public_post_footer-community-guide)\n  *     * Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Arabic) \n    * à¦¬à¦¾à¦‚à¦²à¦¾ (Bangla) \n    * ÄŒeÅ¡tina (Czech) \n    * Dansk (Danish) \n    * Deutsch (German) \n    * Î•Î»Î»Î·Î½Î¹ÎºÎ¬ (Greek) \n    * **English (English)**\n    * EspaÃ±ol (Spanish) \n    * ÙØ§Ø±Ø³ÛŒ (Persian) \n    * Suomi (Finnish) \n    * FranÃ§ais (French) \n    * à¤¹à¤¿à¤‚à¤¦à¥€ (Hindi) \n    * Magyar (Hungarian) \n    * Bahasa Indonesia (Indonesian) \n    * Italiano (Italian) \n    * ×¢×‘×¨×™×ª (Hebrew) \n    * æ—¥æœ¬èª (Japanese) \n    * í•œêµ­ì–´ (Korean) \n    * à¤®à¤°à¤¾à¤ à¥€ (Marathi) \n    * Bahasa Malaysia (Malay) \n    * Nederlands (Dutch) \n    * Norsk (Norwegian) \n    * à¨ªà©°à¨œà¨¾à¨¬à©€ (Punjabi) \n    * Polski (Polish) \n    * PortuguÃªs (Portuguese) \n    * RomÃ¢nÄƒ (Romanian) \n    * Ğ ÑƒÑÑĞºĞ¸Ğ¹ (Russian) \n    * Svenska (Swedish) \n    * à°¤à±†à°²à±à°—à± (Telugu) \n    * à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ (Thai) \n    * Tagalog (Tagalog) \n    * TÃ¼rkÃ§e (Turkish) \n    * Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ° (Ukrainian) \n    * Tiáº¿ng Viá»‡t (Vietnamese) \n    * ç®€ä½“ä¸­æ–‡ (Chinese (Simplified)) \n    * æ­£é«”ä¸­æ–‡ (Chinese (Traditional)) \nLanguage \n\n\n##  Sign in to view more content \nCreate your free account or sign in to continue your search \nSign in \n##  Welcome back \nEmail or phone \nPassword \nShow\n[Forgot password?](https://www.linkedin.com/uas/request-password-reset?trk=public_post_contextual-sign-in-modal_sign-in-modal_forgot_password) Sign in \nor \nBy clicking Continue to join or sign in, you agree to LinkedInâ€™s [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_cookie-policy). \nNew to LinkedIn? [Join now](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_contextual-sign-in-modal_sign-in-modal_join-link)\nor \nNew to LinkedIn? [Join now](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_contextual-sign-in-modal_join-link)\nBy clicking Continue to join or sign in, you agree to LinkedInâ€™s [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy). \n",
    "content_quality_score": 0.8,
    "summary": null,
    "child_urls": [
        "https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement",
        "https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy",
        "https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy",
        "https://www.linkedin.com/posts/ashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW?utm_source=share&utm_medium=member_desktop/#main-content",
        "https://www.linkedin.com/?trk=public_post_nav-header-logo",
        "https://www.linkedin.com/pulse/topics/home/?trk=public_post_guest_nav_menu_articles",
        "https://www.linkedin.com/pub/dir/+/+?trk=public_post_guest_nav_menu_people",
        "https://www.linkedin.com/learning/search?trk=public_post_guest_nav_menu_learning",
        "https://www.linkedin.com/jobs/search?trk=public_post_guest_nav_menu_jobs",
        "https://www.linkedin.com/games?trk=public_post_guest_nav_menu_games",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_nav-header-join",
        "https://www.linkedin.com/login?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&fromSignIn=true&trk=public_post_nav-header-signin",
        "https://in.linkedin.com/in/ashishpatel2604?trk=public_post_feed-actor-image",
        "https://in.linkedin.com/in/ashishpatel2604?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_social-actions-comments",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment-cta",
        "https://at.linkedin.com/in/stefan-komornyik?trk=public_post_comment_actor-image",
        "https://at.linkedin.com/in/stefan-komornyik?trk=public_post_comment_actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_like",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reply",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_comment_reactions",
        "https://ch.linkedin.com/in/isabella-kosch-zh?trk=public_post_comment_actor-image",
        "https://ch.linkedin.com/in/isabella-kosch-zh?trk=public_post_comment_actor-name",
        "https://it.linkedin.com/in/danielebaranzini?trk=public_post_comment_actor-image",
        "https://it.linkedin.com/in/danielebaranzini?trk=public_post_comment_actor-name",
        "https://www.linkedin.com/in/visinha?trk=public_post_comment_actor-image",
        "https://www.linkedin.com/in/visinha?trk=public_post_comment_actor-name",
        "https://in.linkedin.com/in/balaji-kartheek-b6a378205?trk=public_post_comment_actor-image",
        "https://in.linkedin.com/in/balaji-kartheek-b6a378205?trk=public_post_comment_actor-name",
        "https://in.linkedin.com/in/dontaraju-pavan-srinivas-59b93419?trk=public_post_comment_actor-image",
        "https://in.linkedin.com/in/dontaraju-pavan-srinivas-59b93419?trk=public_post_comment_actor-name",
        "https://ca.linkedin.com/in/faranakhdr?trk=public_post_comment_actor-image",
        "https://ca.linkedin.com/in/faranakhdr?trk=public_post_comment_actor-name",
        "https://pk.linkedin.com/in/muhammad-jawad-amin?trk=public_post_comment_actor-image",
        "https://pk.linkedin.com/in/muhammad-jawad-amin?trk=public_post_comment_actor-name",
        "https://in.linkedin.com/in/nishant-singh-yadav-b72b33177?trk=public_post_comment_actor-image",
        "https://in.linkedin.com/in/nishant-singh-yadav-b72b33177?trk=public_post_comment_actor-name",
        "https://www.linkedin.com/in/bukolaola?trk=public_post_comment_actor-image",
        "https://www.linkedin.com/in/bukolaola?trk=public_post_comment_actor-name",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_see-more-comments",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/ganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr",
        "https://in.linkedin.com/in/ganeshjagadeesan?trk=public_post_feed-actor-image",
        "https://in.linkedin.com/in/ganeshjagadeesan?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmachinelearning&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpu&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmodeltraining&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faiinsights&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdeeplearning&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faioptimization&trk=public_post-text",
        "https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-image",
        "https://in.linkedin.com/in/ashishpatel2604?trk=public_post_reshare_feed-actor-name",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FdWvgWvXM&urlhash=Tvqa&trk=public_post_reshare-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllms&trk=public_post_reshare-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdatascience&trk=public_post_reshare-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fartificialintelligence&trk=public_post_reshare-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fganeshjagadeesan_llms-artificialintelligence-datascience-activity-7257283675287953409-NxTr&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/dylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V",
        "https://sg.linkedin.com/in/dylan-poh?trk=public_post_feed-actor-image",
        "https://sg.linkedin.com/in/dylan-poh?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdylan-poh_the-post-sheds-light-on-the-staggering-costs-activity-7258089394434170880-Y08V&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/daemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3",
        "https://ca.linkedin.com/in/daemonbehr?trk=public_post_feed-actor-image",
        "https://ca.linkedin.com/in/daemonbehr?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fej6g_b5E&urlhash=4gHz&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgenai&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Faionnutanix&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fnutanix&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_social-actions-comments",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fdaemonbehr_genai-aionnutanix-nutanix-activity-7259202801782681600-LAt3&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/siddhantsadangi_how-to-optimize-gpu-usage-during-model-training-activity-7186737795497963520-4rWE",
        "https://fr.linkedin.com/in/siddhantsadangi?trk=public_post_feed-actor-image",
        "https://fr.linkedin.com/in/siddhantsadangi?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fsiddhantsadangi_how-to-optimize-gpu-usage-during-model-training-activity-7186737795497963520-4rWE&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/company/neptuneai?trk=public_post-text",
        "https://it.linkedin.com/in/mirzamujtabahussain?trk=public_post-text",
        "https://de.linkedin.com/in/kilian-kluge/en?trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fe5jh7npm&urlhash=hS27&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmlops&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpuoptimization&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fneptuneai&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Ftechtips&trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fneptune%2Eai%2Fblog%2Foptimizing-gpu-usage-during-model-training-with-neptune&urlhash=YIcz&trk=public_post_feed-article-content",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fsiddhantsadangi_how-to-optimize-gpu-usage-during-model-training-activity-7186737795497963520-4rWE&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fsiddhantsadangi_how-to-optimize-gpu-usage-during-model-training-activity-7186737795497963520-4rWE&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fsiddhantsadangi_how-to-optimize-gpu-usage-during-model-training-activity-7186737795497963520-4rWE&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fsiddhantsadangi_how-to-optimize-gpu-usage-during-model-training-activity-7186737795497963520-4rWE&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/avi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-",
        "https://in.linkedin.com/in/avi-chawla?trk=public_post_feed-actor-image",
        "https://in.linkedin.com/in/avi-chawla?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://in.linkedin.com/company/daily-dose-of-ds?trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fg56-7HsZ&urlhash=JNnU&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Favi-chawla_how-to-synchronize-gpus-with-multi-gpu-training-activity-7222547319672463360-uXQ-&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/mazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7",
        "https://ca.linkedin.com/in/mazadgole?trk=public_post_feed-actor-image",
        "https://ca.linkedin.com/in/mazadgole?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpus&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fquantization&trk=public_post-text",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Flora&trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fmabbasiazad%2Egithub%2Eio%2Fportfolio%2Fposts%2Fllms_gpu%2F&urlhash=AWz1&trk=public_post_feed-article-content",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmazadgole_back-of-the-envelope-calculation-for-gpu-activity-7209639861891317761-VmX7&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/martechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z",
        "https://www.linkedin.com/company/martechrichard?trk=public_post_feed-actor-image",
        "https://www.linkedin.com/company/martechrichard?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FeB5w7N8U&urlhash=0hjW&trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fe9sTptsu&urlhash=QLXV&trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Ftowardsdatascience%2Ecom%2Ftraining-ai-models-on-cpu-3903adc9f388&urlhash=gBHg&trk=public_post_feed-article-content",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmartechrichard_training-ai-models-on-cpu-activity-7236981179684466688-zQ9z&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/benjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH",
        "https://uk.linkedin.com/in/benjamin-j-todd?trk=public_post_feed-actor-image",
        "https://uk.linkedin.com/in/benjamin-j-todd?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fe5DNbZDK&urlhash=8KaZ&trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fbenjamintodd%2Esubstack%2Ecom%2Fp%2Fhow-much-ai-inference-can-we-do&urlhash=QrZ1&trk=public_post_feed-article-content",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fbenjamin-j-todd_how-much-ai-inference-can-we-do-activity-7195268236697915393-SFAH&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/marek-bar%C3%A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT",
        "https://at.linkedin.com/in/marek-bar%C3%A1k-31977a55?trk=public_post_feed-actor-image",
        "https://at.linkedin.com/in/marek-bar%C3%A1k-31977a55?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fnvidia&trk=public_post-text",
        "https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_reshare_feed-actor-image",
        "https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_reshare_feed-actor-name",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgZkRqYeN&urlhash=DQnO&trk=public_post_reshare-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fresearch%2Ecolfax-intl%2Ecom%2Fcutlass-tutorial-persistent-kernels-and-stream-k%2F&urlhash=tx4R&trk=public_post_reshare_feed-article-content",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fmarek-bar%25C3%25A1k-31977a55_cutlass-tutorial-persistent-kernels-and-activity-7276513592210821120-tuTT&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/posts/jay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm",
        "https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_feed-actor-image",
        "https://www.linkedin.com/in/jay-shah-335689167?trk=public_post_feed-actor-name",
        "https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgZkRqYeN&urlhash=DQnO&trk=public_post-text",
        "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fresearch%2Ecolfax-intl%2Ecom%2Fcutlass-tutorial-persistent-kernels-and-stream-k%2F&urlhash=tx4R&trk=public_post_feed-article-content",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_social-actions-reactions",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_social-actions-comments",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_like-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_comment-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fjay-shah-335689167_cutlass-tutorial-persistent-kernels-and-activity-7276341827874562048-Fxzm&trk=public_post_feed-cta-banner-cta",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Fashishpatel2604%2Frecent-activity%2F&trk=public_post_follow-posts",
        "https://www.linkedin.com/today/author/ashishpatel2604?trk=public_post_follow-articles",
        "https://in.linkedin.com/in/ashishpatel2604?trk=public_post_follow-view-profile",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7257270599444512769&trk=public_post_follow",
        "https://www.linkedin.com/pulse/why-writing-clean-python-code-still-non-negotiable-maang-patel--uikxf?trk=public_post",
        "https://www.linkedin.com/pulse/memory-llms-cuts-training-time-30and-heres-what-means-patel--hcqaf?trk=public_post",
        "https://www.linkedin.com/pulse/over-62-ai-teams-struggle-model-deployment-pytorchs-new-patel--nizrf?trk=public_post",
        "https://www.linkedin.com/pulse/topics/sales-s5/",
        "https://www.linkedin.com/pulse/topics/marketing-s2461/",
        "https://www.linkedin.com/pulse/topics/it-services-s57547/",
        "https://www.linkedin.com/pulse/topics/business-administration-s50111/",
        "https://www.linkedin.com/pulse/topics/hr-management-s50359/",
        "https://www.linkedin.com/pulse/topics/engineering-s166/",
        "https://www.linkedin.com/pulse/topics/soft-skills-s2976/",
        "https://www.linkedin.com/pulse/topics/home/",
        "https://about.linkedin.com?trk=d_public_post_footer-about",
        "https://www.linkedin.com/accessibility?trk=d_public_post_footer-accessibility",
        "https://www.linkedin.com/legal/user-agreement?trk=d_public_post_footer-user-agreement",
        "https://www.linkedin.com/legal/privacy-policy?trk=d_public_post_footer-privacy-policy",
        "https://www.linkedin.com/legal/california-privacy-disclosure?trk=d_public_post_footer-california-privacy-rights-act",
        "https://www.linkedin.com/legal/cookie-policy?trk=d_public_post_footer-cookie-policy",
        "https://www.linkedin.com/legal/copyright-policy?trk=d_public_post_footer-copyright-policy",
        "https://brand.linkedin.com/policies?trk=d_public_post_footer-brand-policy",
        "https://www.linkedin.com/psettings/guest-controls?trk=d_public_post_footer-guest-controls",
        "https://www.linkedin.com/legal/professional-community-policies?trk=d_public_post_footer-community-guide",
        "https://www.linkedin.com/uas/request-password-reset?trk=public_post_contextual-sign-in-modal_sign-in-modal_forgot_password",
        "https://www.linkedin.com/legal/user-agreement?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_user-agreement",
        "https://www.linkedin.com/legal/privacy-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_privacy-policy",
        "https://www.linkedin.com/legal/cookie-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_cookie-policy",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_contextual-sign-in-modal_sign-in-modal_join-link",
        "https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fashishpatel2604_llms-datascience-artificialintelligence-activity-7257270599444512769-EaiW&trk=public_post_contextual-sign-in-modal_join-link"
    ]
}