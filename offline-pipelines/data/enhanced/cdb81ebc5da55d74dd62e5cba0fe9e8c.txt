[Skip to content](https://github.com/vllm-project/vllm/#start-of-content)
## Navigation Menu
Toggle navigation
[ ](https://github.com/)
[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2F)
  * Product 
    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)
    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)
    * [ Actions Automate any workflow  ](https://github.com/features/actions)
    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)
    * [ Issues Plan and track work  ](https://github.com/features/issues)
    * [ Code Review Manage code changes  ](https://github.com/features/code-review)
    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)
    * [ Code Search Find more, search less  ](https://github.com/features/code-search)
Explore
    * [ All features ](https://github.com/features)
    * [ Documentation ](https://docs.github.com)
    * [ GitHub Skills ](https://skills.github.com)
    * [ Blog ](https://github.blog)
  * Solutions 
By company size
    * [ Enterprises ](https://github.com/enterprise)
    * [ Small and medium teams ](https://github.com/team)
    * [ Startups ](https://github.com/enterprise/startups)
    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)
By use case
    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)
    * [ DevOps ](https://github.com/solutions/use-case/devops)
    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)
    * [ View all use cases ](https://github.com/solutions/use-case)
By industry
    * [ Healthcare ](https://github.com/solutions/industry/healthcare)
    * [ Financial services ](https://github.com/solutions/industry/financial-services)
    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)
    * [ Government ](https://github.com/solutions/industry/government)
    * [ View all industries ](https://github.com/solutions/industry)
[ View all solutions ](https://github.com/solutions)
  * Resources 
Topics
    * [ AI ](https://github.com/resources/articles/ai)
    * [ DevOps ](https://github.com/resources/articles/devops)
    * [ Security ](https://github.com/resources/articles/security)
    * [ Software Development ](https://github.com/resources/articles/software-development)
    * [ View all ](https://github.com/resources/articles)
Explore
    * [ Learning Pathways ](https://resources.github.com/learn/pathways)
    * [ Events & Webinars ](https://resources.github.com)
    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)
    * [ Customer Stories ](https://github.com/customer-stories)
    * [ Partners ](https://partner.github.com)
    * [ Executive Insights ](https://github.com/solutions/executive-insights)
  * Open Source 
    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)
    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)
Repositories
    * [ Topics ](https://github.com/topics)
    * [ Trending ](https://github.com/trending)
    * [ Collections ](https://github.com/collections)
  * Enterprise 
    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)
Available add-ons
    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)
    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)
    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)
  * [Pricing](https://github.com/pricing)


Search or jump to...
# Search code, repositories, users, issues, pull requests...
Search 
Clear
[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)
#  Provide feedback 
We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
Cancel  Submit feedback 
#  Saved searches 
## Use saved searches to filter your results more quickly
Name
Query
To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). 
Cancel  Create saved search 
[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2F)
[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=vllm-project%2Fvllm) Reseting focus
You signed in with another tab or window. [Reload](https://github.com/vllm-project/vllm/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/vllm-project/vllm/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/vllm-project/vllm/) to refresh your session. Dismiss alert
{{ message }}
[ vllm-project ](https://github.com/vllm-project) / **[vllm](https://github.com/vllm-project/vllm) ** Public
  * [ Notifications ](https://github.com/login?return_to=%2Fvllm-project%2Fvllm) You must be signed in to change notification settings
  * [ Fork 6.5k ](https://github.com/login?return_to=%2Fvllm-project%2Fvllm)
  * [ Star  43.1k ](https://github.com/login?return_to=%2Fvllm-project%2Fvllm)


A high-throughput and memory-efficient inference and serving engine for LLMs 
[docs.vllm.ai](https://docs.vllm.ai "https://docs.vllm.ai")
### License
[ Apache-2.0 license ](https://github.com/vllm-project/vllm/blob/main/LICENSE)
[ 43.1k stars ](https://github.com/vllm-project/vllm/stargazers) [ 6.5k forks ](https://github.com/vllm-project/vllm/forks) [ Branches ](https://github.com/vllm-project/vllm/branches) [ Tags ](https://github.com/vllm-project/vllm/tags) [ Activity ](https://github.com/vllm-project/vllm/activity)
[ Star  ](https://github.com/login?return_to=%2Fvllm-project%2Fvllm)
[ Notifications ](https://github.com/login?return_to=%2Fvllm-project%2Fvllm) You must be signed in to change notification settings
  * [ Code ](https://github.com/vllm-project/vllm)
  * [ Issues 1.5k ](https://github.com/vllm-project/vllm/issues)
  * [ Pull requests 538 ](https://github.com/vllm-project/vllm/pulls)
  * [ Discussions ](https://github.com/vllm-project/vllm/discussions)
  * [ Actions ](https://github.com/vllm-project/vllm/actions)
  * [ Projects 7 ](https://github.com/vllm-project/vllm/projects)
  * [ Security ](https://github.com/vllm-project/vllm/security)
  * [ Insights ](https://github.com/vllm-project/vllm/pulse)


Additional navigation options
  * [ Code  ](https://github.com/vllm-project/vllm)
  * [ Issues  ](https://github.com/vllm-project/vllm/issues)
  * [ Pull requests  ](https://github.com/vllm-project/vllm/pulls)
  * [ Discussions  ](https://github.com/vllm-project/vllm/discussions)
  * [ Actions  ](https://github.com/vllm-project/vllm/actions)
  * [ Projects  ](https://github.com/vllm-project/vllm/projects)
  * [ Security  ](https://github.com/vllm-project/vllm/security)
  * [ Insights  ](https://github.com/vllm-project/vllm/pulse)


# vllm-project/vllm
main
[Branches](https://github.com/vllm-project/vllm/branches)[Tags](https://github.com/vllm-project/vllm/tags)
[](https://github.com/vllm-project/vllm/branches)[](https://github.com/vllm-project/vllm/tags)
Go to file
Code
## Folders and files
Name| Name| Last commit message| Last commit date  
---|---|---|---  
## Latest commit
## History
[5,512 Commits](https://github.com/vllm-project/vllm/commits/main/)[](https://github.com/vllm-project/vllm/commits/main/)  
[.buildkite](https://github.com/vllm-project/vllm/tree/main/.buildkite ".buildkite")| [.buildkite](https://github.com/vllm-project/vllm/tree/main/.buildkite ".buildkite")  
[.github](https://github.com/vllm-project/vllm/tree/main/.github ".github")| [.github](https://github.com/vllm-project/vllm/tree/main/.github ".github")  
[benchmarks](https://github.com/vllm-project/vllm/tree/main/benchmarks "benchmarks")| [benchmarks](https://github.com/vllm-project/vllm/tree/main/benchmarks "benchmarks")  
[cmake](https://github.com/vllm-project/vllm/tree/main/cmake "cmake")| [cmake](https://github.com/vllm-project/vllm/tree/main/cmake "cmake")  
[csrc](https://github.com/vllm-project/vllm/tree/main/csrc "csrc")| [csrc](https://github.com/vllm-project/vllm/tree/main/csrc "csrc")  
[docs](https://github.com/vllm-project/vllm/tree/main/docs "docs")| [docs](https://github.com/vllm-project/vllm/tree/main/docs "docs")  
[examples](https://github.com/vllm-project/vllm/tree/main/examples "examples")| [examples](https://github.com/vllm-project/vllm/tree/main/examples "examples")  
[requirements](https://github.com/vllm-project/vllm/tree/main/requirements "requirements")| [requirements](https://github.com/vllm-project/vllm/tree/main/requirements "requirements")  
[tests](https://github.com/vllm-project/vllm/tree/main/tests "tests")| [tests](https://github.com/vllm-project/vllm/tree/main/tests "tests")  
[tools](https://github.com/vllm-project/vllm/tree/main/tools "tools")| [tools](https://github.com/vllm-project/vllm/tree/main/tools "tools")  
[vllm](https://github.com/vllm-project/vllm/tree/main/vllm "vllm")| [vllm](https://github.com/vllm-project/vllm/tree/main/vllm "vllm")  
[.clang-format](https://github.com/vllm-project/vllm/blob/main/.clang-format ".clang-format")| [.clang-format](https://github.com/vllm-project/vllm/blob/main/.clang-format ".clang-format")  
[.dockerignore](https://github.com/vllm-project/vllm/blob/main/.dockerignore ".dockerignore")| [.dockerignore](https://github.com/vllm-project/vllm/blob/main/.dockerignore ".dockerignore")  
[.gitignore](https://github.com/vllm-project/vllm/blob/main/.gitignore ".gitignore")| [.gitignore](https://github.com/vllm-project/vllm/blob/main/.gitignore ".gitignore")  
[.pre-commit-config.yaml](https://github.com/vllm-project/vllm/blob/main/.pre-commit-config.yaml ".pre-commit-config.yaml")| [.pre-commit-config.yaml](https://github.com/vllm-project/vllm/blob/main/.pre-commit-config.yaml ".pre-commit-config.yaml")  
[.readthedocs.yaml](https://github.com/vllm-project/vllm/blob/main/.readthedocs.yaml ".readthedocs.yaml")| [.readthedocs.yaml](https://github.com/vllm-project/vllm/blob/main/.readthedocs.yaml ".readthedocs.yaml")  
[.shellcheckrc](https://github.com/vllm-project/vllm/blob/main/.shellcheckrc ".shellcheckrc")| [.shellcheckrc](https://github.com/vllm-project/vllm/blob/main/.shellcheckrc ".shellcheckrc")  
[.yapfignore](https://github.com/vllm-project/vllm/blob/main/.yapfignore ".yapfignore")| [.yapfignore](https://github.com/vllm-project/vllm/blob/main/.yapfignore ".yapfignore")  
[CMakeLists.txt](https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt "CMakeLists.txt")| [CMakeLists.txt](https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt "CMakeLists.txt")  
[CODE_OF_CONDUCT.md](https://github.com/vllm-project/vllm/blob/main/CODE_OF_CONDUCT.md "CODE_OF_CONDUCT.md")| [CODE_OF_CONDUCT.md](https://github.com/vllm-project/vllm/blob/main/CODE_OF_CONDUCT.md "CODE_OF_CONDUCT.md")  
[CONTRIBUTING.md](https://github.com/vllm-project/vllm/blob/main/CONTRIBUTING.md "CONTRIBUTING.md")| [CONTRIBUTING.md](https://github.com/vllm-project/vllm/blob/main/CONTRIBUTING.md "CONTRIBUTING.md")  
[DCO](https://github.com/vllm-project/vllm/blob/main/DCO "DCO")| [DCO](https://github.com/vllm-project/vllm/blob/main/DCO "DCO")  
[Dockerfile](https://github.com/vllm-project/vllm/blob/main/Dockerfile "Dockerfile")| [Dockerfile](https://github.com/vllm-project/vllm/blob/main/Dockerfile "Dockerfile")  
[Dockerfile.arm](https://github.com/vllm-project/vllm/blob/main/Dockerfile.arm "Dockerfile.arm")| [Dockerfile.arm](https://github.com/vllm-project/vllm/blob/main/Dockerfile.arm "Dockerfile.arm")  
[Dockerfile.cpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.cpu "Dockerfile.cpu")| [Dockerfile.cpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.cpu "Dockerfile.cpu")  
[Dockerfile.hpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.hpu "Dockerfile.hpu")| [Dockerfile.hpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.hpu "Dockerfile.hpu")  
[Dockerfile.neuron](https://github.com/vllm-project/vllm/blob/main/Dockerfile.neuron "Dockerfile.neuron")| [Dockerfile.neuron](https://github.com/vllm-project/vllm/blob/main/Dockerfile.neuron "Dockerfile.neuron")  
[Dockerfile.ppc64le](https://github.com/vllm-project/vllm/blob/main/Dockerfile.ppc64le "Dockerfile.ppc64le")| [Dockerfile.ppc64le](https://github.com/vllm-project/vllm/blob/main/Dockerfile.ppc64le "Dockerfile.ppc64le")  
[Dockerfile.rocm](https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm "Dockerfile.rocm")| [Dockerfile.rocm](https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm "Dockerfile.rocm")  
[Dockerfile.rocm_base](https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm_base "Dockerfile.rocm_base")| [Dockerfile.rocm_base](https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm_base "Dockerfile.rocm_base")  
[Dockerfile.s390x](https://github.com/vllm-project/vllm/blob/main/Dockerfile.s390x "Dockerfile.s390x")| [Dockerfile.s390x](https://github.com/vllm-project/vllm/blob/main/Dockerfile.s390x "Dockerfile.s390x")  
[Dockerfile.tpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.tpu "Dockerfile.tpu")| [Dockerfile.tpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.tpu "Dockerfile.tpu")  
[Dockerfile.xpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.xpu "Dockerfile.xpu")| [Dockerfile.xpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.xpu "Dockerfile.xpu")  
[LICENSE](https://github.com/vllm-project/vllm/blob/main/LICENSE "LICENSE")| [LICENSE](https://github.com/vllm-project/vllm/blob/main/LICENSE "LICENSE")  
[MANIFEST.in](https://github.com/vllm-project/vllm/blob/main/MANIFEST.in "MANIFEST.in")| [MANIFEST.in](https://github.com/vllm-project/vllm/blob/main/MANIFEST.in "MANIFEST.in")  
[README.md](https://github.com/vllm-project/vllm/blob/main/README.md "README.md")| [README.md](https://github.com/vllm-project/vllm/blob/main/README.md "README.md")  
[RELEASE.md](https://github.com/vllm-project/vllm/blob/main/RELEASE.md "RELEASE.md")| [RELEASE.md](https://github.com/vllm-project/vllm/blob/main/RELEASE.md "RELEASE.md")  
[SECURITY.md](https://github.com/vllm-project/vllm/blob/main/SECURITY.md "SECURITY.md")| [SECURITY.md](https://github.com/vllm-project/vllm/blob/main/SECURITY.md "SECURITY.md")  
[collect_env.py](https://github.com/vllm-project/vllm/blob/main/collect_env.py "collect_env.py")| [collect_env.py](https://github.com/vllm-project/vllm/blob/main/collect_env.py "collect_env.py")  
[find_cuda_init.py](https://github.com/vllm-project/vllm/blob/main/find_cuda_init.py "find_cuda_init.py")| [find_cuda_init.py](https://github.com/vllm-project/vllm/blob/main/find_cuda_init.py "find_cuda_init.py")  
[format.sh](https://github.com/vllm-project/vllm/blob/main/format.sh "format.sh")| [format.sh](https://github.com/vllm-project/vllm/blob/main/format.sh "format.sh")  
[pyproject.toml](https://github.com/vllm-project/vllm/blob/main/pyproject.toml "pyproject.toml")| [pyproject.toml](https://github.com/vllm-project/vllm/blob/main/pyproject.toml "pyproject.toml")  
[python_only_dev.py](https://github.com/vllm-project/vllm/blob/main/python_only_dev.py "python_only_dev.py")| [python_only_dev.py](https://github.com/vllm-project/vllm/blob/main/python_only_dev.py "python_only_dev.py")  
[setup.py](https://github.com/vllm-project/vllm/blob/main/setup.py "setup.py")| [setup.py](https://github.com/vllm-project/vllm/blob/main/setup.py "setup.py")  
[use_existing_torch.py](https://github.com/vllm-project/vllm/blob/main/use_existing_torch.py "use_existing_torch.py")| [use_existing_torch.py](https://github.com/vllm-project/vllm/blob/main/use_existing_torch.py "use_existing_torch.py")  
View all files  
## Repository files navigation
  * [README](https://github.com/vllm-project/vllm/)
  * [Code of conduct](https://github.com/vllm-project/vllm/)
  * [Apache-2.0 license](https://github.com/vllm-project/vllm/)
  * [Security](https://github.com/vllm-project/vllm/)


![vLLM](https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png)
###  Easy, fast, and cheap LLM serving for everyone 
[](https://github.com/vllm-project/vllm/#easy-fast-and-cheap-llm-serving-for-everyone)
| [**Documentation**](https://docs.vllm.ai) | [**Blog**](https://vllm.ai) | [**Paper**](https://arxiv.org/abs/2309.06180) | [**Twitter/X**](https://x.com/vllm_project) | [**User Forum**](https://discuss.vllm.ai) | [**Developer Slack**](https://slack.vllm.ai) | 
[2025/03] We are collaborating with Ollama to host an [Inference Night](https://lu.ma/vllm-ollama) at Y Combinator in San Francisco on Thursday, March 27, at 6 PM. Discuss all things inference local or data center!
[2025/04] We're hosting our first-ever _vLLM Asia Developer Day_ in Singapore on _April 3rd_! This is a full-day event (9 AM - 9 PM SGT) in partnership with SGInnovate, AMD, and Embedded LLM. Meet the vLLM team and learn about LLM inference for RL, MI300X, and more! [Register Now](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)
_Latest News_ 🔥
  * [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).
  * [2025/03] We hosted [the East Coast vLLM Meetup](https://lu.ma/7mu4k4xx)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0).
  * [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.
  * [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).
  * [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).
  * [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!

Previous News
  * [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).
  * [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!
  * [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!
  * [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).
  * [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).
  * [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).
  * [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).
  * [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).
  * [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).
  * [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).
  * [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.
  * [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).


## About
[](https://github.com/vllm-project/vllm/#about)
vLLM is a fast and easy-to-use library for LLM inference and serving.
Originally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.
vLLM is fast with:
  * State-of-the-art serving throughput
  * Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)
  * Continuous batching of incoming requests
  * Fast model execution with CUDA/HIP graph
  * Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), INT4, INT8, and FP8.
  * Optimized CUDA kernels, including integration with FlashAttention and FlashInfer.
  * Speculative decoding
  * Chunked prefill


**Performance benchmark** : We include a performance benchmark at the end of [our blog post](https://blog.vllm.ai/2024/09/05/perf-update.html). It compares the performance of vLLM against other LLM serving engines ([TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [SGLang](https://github.com/sgl-project/sglang) and [LMDeploy](https://github.com/InternLM/lmdeploy)). The implementation is under [nightly-benchmarks folder](https://github.com/vllm-project/vllm/blob/main/.buildkite/nightly-benchmarks) and you can [reproduce](https://github.com/vllm-project/vllm/issues/8176) this benchmark using our one-click runnable script.
vLLM is flexible and easy to use with:
  * Seamless integration with popular Hugging Face models
  * High-throughput serving with various decoding algorithms, including _parallel sampling_ , _beam search_ , and more
  * Tensor parallelism and pipeline parallelism support for distributed inference
  * Streaming outputs
  * OpenAI-compatible API server
  * Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron.
  * Prefix caching support
  * Multi-lora support


vLLM seamlessly supports most popular open-source models on HuggingFace, including:
  * Transformer-like LLMs (e.g., Llama)
  * Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)
  * Embedding Models (e.g. E5-Mistral)
  * Multi-modal LLMs (e.g., LLaVA)


Find the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).
## Getting Started
[](https://github.com/vllm-project/vllm/#getting-started)
Install vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):
```
pip install vllm
```

Visit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.
  * [Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)
  * [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
  * [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)


## Contributing
[](https://github.com/vllm-project/vllm/#contributing)
We welcome and value any contributions and collaborations. Please check out [CONTRIBUTING.md](https://github.com/vllm-project/vllm/blob/main/CONTRIBUTING.md) for how to get involved.
## Sponsors
[](https://github.com/vllm-project/vllm/#sponsors)
vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!
Cash Donations:
  * a16z
  * Dropbox
  * Sequoia Capital
  * Skywork AI
  * ZhenFund


Compute Resources:
  * AMD
  * Anyscale
  * AWS
  * Crusoe Cloud
  * Databricks
  * DeepInfra
  * Google Cloud
  * Lambda Lab
  * Nebius
  * Novita AI
  * NVIDIA
  * Replicate
  * Roblox
  * RunPod
  * Trainy
  * UC Berkeley
  * UC San Diego


Slack Sponsor: Anyscale
We also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.
## Citation
[](https://github.com/vllm-project/vllm/#citation)
If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):
```
@inproceedings{kwon2023efficient,
 title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
 author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
 booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
 year={2023}
}
```

## Contact Us
[](https://github.com/vllm-project/vllm/#contact-us)
  * For technical questions and feature requests, please use GitHub [Issues](https://github.com/vllm-project/vllm/issues) or [Discussions](https://github.com/vllm-project/vllm/discussions)
  * For discussing with fellow users, please use the [vLLM Forum](https://discuss.vllm.ai)
  * coordinating contributions and development, please use [Slack](https://slack.vllm.ai)
  * For security disclosures, please use GitHub's [Security Advisories](https://github.com/vllm-project/vllm/security/advisories) feature
  * For collaborations and partnerships, please contact us at vllm-questions@lists.berkeley.edu


## Media Kit
[](https://github.com/vllm-project/vllm/#media-kit)
  * If you wish to use vLLM's logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit).


## About
A high-throughput and memory-efficient inference and serving engine for LLMs 
[docs.vllm.ai](https://docs.vllm.ai "https://docs.vllm.ai")
### Topics
[ amd ](https://github.com/topics/amd "Topic: amd") [ cuda ](https://github.com/topics/cuda "Topic: cuda") [ inference ](https://github.com/topics/inference "Topic: inference") [ pytorch ](https://github.com/topics/pytorch "Topic: pytorch") [ transformer ](https://github.com/topics/transformer "Topic: transformer") [ llama ](https://github.com/topics/llama "Topic: llama") [ gpt ](https://github.com/topics/gpt "Topic: gpt") [ rocm ](https://github.com/topics/rocm "Topic: rocm") [ model-serving ](https://github.com/topics/model-serving "Topic: model-serving") [ tpu ](https://github.com/topics/tpu "Topic: tpu") [ hpu ](https://github.com/topics/hpu "Topic: hpu") [ mlops ](https://github.com/topics/mlops "Topic: mlops") [ xpu ](https://github.com/topics/xpu "Topic: xpu") [ llm ](https://github.com/topics/llm "Topic: llm") [ inferentia ](https://github.com/topics/inferentia "Topic: inferentia") [ llmops ](https://github.com/topics/llmops "Topic: llmops") [ llm-serving ](https://github.com/topics/llm-serving "Topic: llm-serving") [ qwen ](https://github.com/topics/qwen "Topic: qwen") [ deepseek ](https://github.com/topics/deepseek "Topic: deepseek") [ trainium ](https://github.com/topics/trainium "Topic: trainium")
### Resources
[ Readme ](https://github.com/vllm-project/vllm/#readme-ov-file)
### License
[ Apache-2.0 license ](https://github.com/vllm-project/vllm/#Apache-2.0-1-ov-file)
### Code of conduct
[ Code of conduct ](https://github.com/vllm-project/vllm/#coc-ov-file)
### Security policy
[ Security policy ](https://github.com/vllm-project/vllm/#security-ov-file)
[ Activity](https://github.com/vllm-project/vllm/activity)
[ Custom properties](https://github.com/vllm-project/vllm/custom-properties)
### Stars
[ **43.1k** stars](https://github.com/vllm-project/vllm/stargazers)
### Watchers
[ **356** watching](https://github.com/vllm-project/vllm/watchers)
### Forks
[ **6.5k** forks](https://github.com/vllm-project/vllm/forks)
[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm&report=vllm-project+%28user%29)
##  [Releases 55](https://github.com/vllm-project/vllm/releases)
[ v0.8.2 Latest  Mar 23, 2025 ](https://github.com/vllm-project/vllm/releases/tag/v0.8.2)
[+ 54 releases](https://github.com/vllm-project/vllm/releases)
## Sponsor this project
  * ![open_collective](https://github.githubassets.com/assets/open_collective-0a706523753d.svg) [opencollective.com/**vllm**](https://opencollective.com/vllm)


[Learn more about GitHub Sponsors](https://github.com/sponsors)
##  [Contributors 963](https://github.com/vllm-project/vllm/graphs/contributors)
  * [ ![@WoosukKwon](https://avatars.githubusercontent.com/u/46394894?s=64&v=4) ](https://github.com/WoosukKwon)
  * [ ![@youkaichao](https://avatars.githubusercontent.com/u/23236638?s=64&v=4) ](https://github.com/youkaichao)
  * [ ![@DarkLight1337](https://avatars.githubusercontent.com/u/44970335?s=64&v=4) ](https://github.com/DarkLight1337)
  * [ ![@mgoin](https://avatars.githubusercontent.com/u/3195154?s=64&v=4) ](https://github.com/mgoin)
  * [ ![@ywang96](https://avatars.githubusercontent.com/u/136131678?s=64&v=4) ](https://github.com/ywang96)
  * [ ![@simon-mo](https://avatars.githubusercontent.com/u/21118851?s=64&v=4) ](https://github.com/simon-mo)
  * [ ![@Isotr0py](https://avatars.githubusercontent.com/u/41363108?s=64&v=4) ](https://github.com/Isotr0py)
  * [ ![@njhill](https://avatars.githubusercontent.com/u/16958488?s=64&v=4) ](https://github.com/njhill)
  * [ ![@zhuohan123](https://avatars.githubusercontent.com/u/17310766?s=64&v=4) ](https://github.com/zhuohan123)
  * [ ![@robertgshaw2-redhat](https://avatars.githubusercontent.com/u/114415538?s=64&v=4) ](https://github.com/robertgshaw2-redhat)
  * [ ![@tlrmchlsmth](https://avatars.githubusercontent.com/u/1236979?s=64&v=4) ](https://github.com/tlrmchlsmth)
  * [ ![@jeejeelee](https://avatars.githubusercontent.com/u/19733142?s=64&v=4) ](https://github.com/jeejeelee)
  * [ ![@russellb](https://avatars.githubusercontent.com/u/309258?s=64&v=4) ](https://github.com/russellb)
  * [ ![@comaniac](https://avatars.githubusercontent.com/u/8262694?s=64&v=4) ](https://github.com/comaniac)


[+ 949 contributors](https://github.com/vllm-project/vllm/graphs/contributors)
## Languages
  * [ Python 84.5% ](https://github.com/vllm-project/vllm/search?l=python)
  * [ Cuda 10.4% ](https://github.com/vllm-project/vllm/search?l=cuda)
  * [ C++ 3.4% ](https://github.com/vllm-project/vllm/search?l=c%2B%2B)
  * [ C 0.6% ](https://github.com/vllm-project/vllm/search?l=c)
  * [ Shell 0.6% ](https://github.com/vllm-project/vllm/search?l=shell)
  * [ CMake 0.4% ](https://github.com/vllm-project/vllm/search?l=cmake)
  * [ Dockerfile 0.1% ](https://github.com/vllm-project/vllm/search?l=dockerfile)


## Footer
[ ](https://github.com "GitHub") © 2025 GitHub, Inc. 
### Footer navigation
  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
  * [Security](https://github.com/security)
  * [Status](https://www.githubstatus.com/)
  * [Docs](https://docs.github.com/)
  * [Contact](https://support.github.com?tags=dotcom-footer)
  * Manage cookies 
  * Do not share my personal information 


You can’t perform that action at this time. 
