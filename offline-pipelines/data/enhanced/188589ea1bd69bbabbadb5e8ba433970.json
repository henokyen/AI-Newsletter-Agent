{
    "id": "188589ea1bd69bbabbadb5e8ba433970",
    "metadata": {
        "id": "188589ea1bd69bbabbadb5e8ba433970",
        "url": "https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada/",
        "title": "Fine-tune Llama 3 with ORPO | Towards Data Science",
        "properties": {
            "description": null,
            "keywords": null,
            "author": "Maxime Labonne",
            "og:locale": "en_US",
            "og:type": "article",
            "og:title": "Fine-tune Llama 3 with ORPO | Towards Data Science",
            "og:description": "A cheaper and faster unified fine-tuning technique",
            "og:url": "https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada/",
            "og:site_name": "Towards Data Science",
            "og:image": "https://towardsdatascience.com/wp-content/uploads/2024/04/0DSTQkcyX56nl4qYu.png",
            "og:image:width": "1792",
            "og:image:height": "1024",
            "og:image:type": "image/png",
            "twitter:card": "summary_large_image",
            "twitter:creator": "@TDataScience",
            "twitter:site": "@TDataScience",
            "twitter:label1": "Written by",
            "twitter:data1": "Maxime Labonne",
            "twitter:label2": "Est. reading time",
            "twitter:data2": "9 minutes"
        }
    },
    "parent_metadata": {
        "id": "b2bc94cfbeb5dbd53b7a5cf3fe27dbde",
        "url": "https://www.notion.so/Training-Fine-tuning-LLMs-b2bc94cfbeb5dbd53b7a5cf3fe27dbde",
        "title": "Training & Fine-tuning LLMs",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Skip to content](https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada/#wp--skip-link--target)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nThe world‚Äôs leading publication for data science, AI, and ML professionals.\nSign in\nSign out\n[Contributor Portal](https://contributor.insightmediagroup.io/)\n  * [Latest](https://towardsdatascience.com/latest/)\n  * [Editor‚Äôs Picks](https://towardsdatascience.com/tag/editors-pick/)\n  * [Deep Dives](https://towardsdatascience.com/tag/deep-dives/)\n  * [Contribute](https://towardsdatascience.com/questions-96667b06af5/)\n  * [Newsletter](https://newsletter.towardsdatascience.com/subscription-to-the-newsletter)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\n\n\nToggle Mobile Navigation\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [X](https://x.com/TDataScience)\n\n\nToggle Search\nSearch\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\n# Fine-tune Llama 3 with ORPO\nA cheaper and faster unified fine-tuning technique \n[Maxime Labonne](https://towardsdatascience.com/author/mlabonne/)\nApr 19, 2024\n8 min read\nShare \n![Image generated with DALL-E 3 by author](https://towardsdatascience.com/wp-content/uploads/2024/04/0DSTQkcyX56nl4qYu.png)Image generated with DALL-E 3 by author\nORPO is a **new exciting fine-tuning technique** that combines the traditional supervised fine-tuning and preference alignment stages into a single process. This reduces the computational resources and time required for training. Moreover, empirical results demonstrate that ORPO outperforms other alignment methods on various model sizes and benchmarks.\nIn this article, we will fine-tune the new Llama 3 8B model using ORPO with the TRL library. The code is available on [Google Colab](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing) and in the [LLM Course](https://github.com/mlabonne/llm-course) on GitHub.\n## ‚öñÔ∏è ORPO\nInstruction tuning and preference alignment are essential techniques for adapting [Large Language Models](https://towardsdatascience.com/tag/large-language-models/ \"Large Language Models\") (LLMs) to specific tasks. Traditionally, this involves a multi-stage process: 1/ **Supervised Fine-Tuning** (SFT) on instructions to adapt the model to the target domain, followed by 2/ **preference alignment methods** like Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO) to increase the likelihood of generating preferred responses over rejected ones.\n![Image by author](https://towardsdatascience.com/wp-content/uploads/2024/04/0LlRjrJYf7rWtVxGj.png)Image by author\nHowever, researchers have identified a limitation in this approach. While SFT effectively adapts the model to the desired domain, it inadvertently **increases the probability of generating undesirable answers** alongside preferred ones. This is why the preference alignment stage is necessary to widen the gap between the likelihoods of preferred and rejected outputs.\n![Note how the probability of rejected responses increases during supervised fine-tuning \\(image from the ORPO paper\\).](https://towardsdatascience.com/wp-content/uploads/2024/04/0rfs4IexRUX7T6-5y.png)Note how the probability of rejected responses increases during supervised fine-tuning (image from the ORPO paper).\nIntroduced by [Hong and Lee (2024)](https://arxiv.org/abs/2403.07691), ORPO offers an elegant solution to this problem by combining instruction tuning and preference alignment into a single, monolithic training process. ORPO modifies the standard language modeling objective, combining the negative log-likelihood loss with an odds ratio (OR) term. This OR loss weakly penalizes rejected responses while strongly rewarding preferred ones, allowing the model to simultaneously learn the target task and align with human preferences.\n![](https://towardsdatascience.com/wp-content/uploads/2024/04/1r3V1OdKtcWJJKS6cGT8chQ.png)\nORPO has been implemented in the major fine-tuning libraries, like [TRL](https://github.com/huggingface/trl), [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl), and [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory). In the next section, we will see how to use with TRL.\n## üíª Fine-tuning Llama 3 with ORPO\n[Llama 3](https://github.com/meta-llama/llama3/tree/main) is the latest family of LLMs developed by Meta. The models were trained on an extensive dataset of **15 trillion tokens** (compared to 2T tokens for Llama 2). Two model sizes have been released: a 70 billion parameter model and a smaller 8 billion parameter model. The 70B model has already demonstrated impressive performance, scoring 82 on the MMLU benchmark and 81.7 on the HumanEval benchmark.\nLlama 3 models also increased the context length up to 8,192 tokens (4,096 tokens for Llama 2), and potentially scale up to 32k with RoPE. Additionally, the models use a new tokenizer with a 128K-token vocabulary, reducing the number of tokens required to encode text by 15%. This vocabulary also explains the bump from 7B to 8B parameters.\n![Samples from ORPO-DPO-mix-40k \\(image by author\\).](https://towardsdatascience.com/wp-content/uploads/2024/04/0G8pGN8e3ppGj0TCa.png)_Samples from ORPO-DPO-mix-40k (image by author)._\nORPO requires a preference dataset, including a prompt, a chosen answer, and a rejected answer. In this example, we will use `[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)`, a combination of the following high-quality DPO datasets:\n  * `[argilla/distilabel-capybara-dpo-7k-binarized](https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized)`: highly scored chosen answers >=5 (2,882 samples)\n  * `[argilla/distilabel-intel-orca-dpo-pairs](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs)`: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\n  * `[argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)`: highly scored chosen answers >=5 (22,799 samples)\n  * `[argilla/distilabel-math-preference-dpo](https://huggingface.co/datasets/argilla/distilabel-math-preference-dpo)`: highly scored chosen answers >=9 (2,181 samples)\n  * `[unalignment/toxic-dpo-v0.2](https://huggingface.co/datasets/unalignment/toxic-dpo-v0.2)` (541 samples)\n  * `[M4-ai/prm_dpo_pairs_cleaned](https://huggingface.co/datasets/M4-ai/prm_dpo_pairs_cleaned)` (7,958 samples)\n  * `[jondurbin/truthy-dpo-v0.1](https://huggingface.co/datasets/jondurbin/truthy-dpo-v0.1)` (1,016 samples)\n\n\nThanks to [argilla](https://huggingface.co/argilla), [unalignment](https://huggingface.co/unalignment), [M4-ai](https://huggingface.co/M4-ai), and [jondurbin](https://huggingface.co/jondurbin) for providing the source datasets.\nAs per usual, let‚Äôs start by installing the required libraries:\n```\npip install -U transformers datasets accelerate peft trl bitsandbytes wandb\n```\n\nOnce it‚Äôs installed, we can import the necessary libraries and log in to W&B (optional):\n```\nimport gc\nimport os\nimport torch\nimport wandb\nfrom datasets import load_dataset\nfrom google.colab import userdata\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (\n  AutoModelForCausalLM,\n  AutoTokenizer,\n  BitsAndBytesConfig,\n  TrainingArguments,\n  pipeline,\n)\nfrom trl import ORPOConfig, ORPOTrainer, setup_chat_format\nwb_token = userdata.get('wandb')\nwandb.login(key=wb_token)\n```\n\nIf you have a recent GPU, you should also be able to use the [Flash Attention library](https://github.com/Dao-AILab/flash-attention) to replace the default eager attention implementation with a more efficient one.\n```\nif torch.cuda.get_device_capability()[0] >= 8:\n  !pip install -qqq flash-attn\n  attn_implementation = \"flash_attention_2\"\n  torch_dtype = torch.bfloat16\nelse:\n  attn_implementation = \"eager\"\n  torch_dtype = torch.float16\n```\n\nIn the following, we will load the Llama 3 8B model in 4-bit precision thanks to [bitsandbytes](https://github.com/TimDettmers/bitsandbytes). We then set the LoRA configuration using [PEFT](https://github.com/huggingface/peft) for QLoRA. I‚Äôm also using the convenient `setup_chat_format()` function to modify the model and tokenizer for [ChatML](https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use) support. It automatically applies this chat template, adds special tokens, and resizes the model‚Äôs embedding layer to match the new vocabulary size.\nNote that you need to submit a request to access [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) and be logged in to your Hugging Face account. Alternatively, you can load ungated copies of the model, like [NousResearch/Meta-Llama-3-8B](https://huggingface.co/NousResearch/Meta-Llama-3-8B).\n```\n# Model\nbase_model = \"meta-llama/Meta-Llama-3-8B\"\nnew_model = \"OrpoLlama-3-8B\"\n# QLoRA config\nbnb_config = BitsAndBytesConfig(\n  load_in_4bit=True,\n  bnb_4bit_quant_type=\"nf4\",\n  bnb_4bit_compute_dtype=torch_dtype,\n  bnb_4bit_use_double_quant=True,\n)\n# LoRA config\npeft_config = LoraConfig(\n  r=16,\n  lora_alpha=32,\n  lora_dropout=0.05,\n  bias=\"none\",\n  task_type=\"CAUSAL_LM\",\n  target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n  base_model,\n  quantization_config=bnb_config,\n  device_map=\"auto\",\n  attn_implementation=attn_implementation\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = prepare_model_for_kbit_training(model)\n```\n\nNow that the model is ready for training, we can take care of the dataset. We load `[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)` and use the `apply_chat_template()` function to convert the \"chosen\" and \"rejected\" columns into the ChatML format. Note that I‚Äôm only using 1,000 samples and not the entire dataset, as it would take too long to run.\n```\ndataset_name = \"mlabonne/orpo-dpo-mix-40k\"\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=42).select(range(1000))\ndef format_chat_template(row):\n  row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n  row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n  return row\ndataset = dataset.map(\n  format_chat_template,\n  num_proc= os.cpu_count(),\n)\ndataset = dataset.train_test_split(test_size=0.01)\n```\n\nFirst, we need to set a few hyperparameters:\n  * `learning_rate`: ORPO uses very low learning rates compared to traditional SFT or even DPO. This value of 8e-6 comes from the original paper, and roughly corresponds to an SFT learning rate of 1e-5 and a DPO learning rate of 5e-6. I would recommend increasing it around 1e-6 for a real fine-tune.\n  * `beta`: It is the $lambda$ parameter in the paper, with a default value of 0.1. An appendix from the original paper shows how it‚Äôs been selected with an ablation study.\n  * Other parameters, like `max_length` and batch size are set to use as much VRAM as available (~20 GB in this configuration). Ideally, we would train the model for 3-5 epochs, but we‚Äôll stick to 1 here.\n\n\nFinally, we can train the model using the ORPOTrainer, which acts as a wrapper.\n```\norpo_args = ORPOConfig(\n  learning_rate=8e-6,\n  beta=0.1,\n  lr_scheduler_type=\"linear\",\n  max_length=1024,\n  max_prompt_length=512,\n  per_device_train_batch_size=2,\n  per_device_eval_batch_size=2,\n  gradient_accumulation_steps=4,\n  optim=\"paged_adamw_8bit\",\n  num_train_epochs=1,\n  evaluation_strategy=\"steps\",\n  eval_steps=0.2,\n  logging_steps=1,\n  warmup_steps=10,\n  report_to=\"wandb\",\n  output_dir=\"./results/\",\n)\ntrainer = ORPOTrainer(\n  model=model,\n  args=orpo_args,\n  train_dataset=dataset[\"train\"],\n  eval_dataset=dataset[\"test\"],\n  peft_config=peft_config,\n  tokenizer=tokenizer,\n)\ntrainer.train()\ntrainer.save_model(new_model)\n```\n\nTraining the model on these 1,000 samples took about 2 hours on an L4 GPU. Let‚Äôs check the W&B plots:\n![](https://towardsdatascience.com/wp-content/uploads/2024/04/0HDi6G4O5z9rpjeEG.png)\nWhile the loss goes down, the difference between the chosen and rejects answers is not clear: the average margin and accuracy are only slightly above zero and 0.5, respectively.\nIn the original paper, the authors trained models on the `[Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)` dataset (161k samples) for 10 epochs, which is a lot longer than our quick run. They also experimented with Llama 3 and kindly [shared their logs](https://huggingface.co/orpo-explorers/hf-llama3-8b-orpo-v0.0/tensorboard) with me (thanks [Jiwoo Hong](https://twitter.com/jiwoohong98)).\nTo end this tutorial, let‚Äôs merge the QLoRA adapter with the base model and push it to the Hugging Face Hub.\n```\n# Flush memory\ndel trainer, model\ngc.collect()\ntorch.cuda.empty_cache()\n# Reload tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(\n  base_model,\n  low_cpu_mem_usage=True,\n  return_dict=True,\n  torch_dtype=torch.float16,\n  device_map=\"auto\",\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n# Merge adapter with base model\nmodel = PeftModel.from_pretrained(model, new_model)\nmodel = model.merge_and_unload()\nmodel.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)\n```\n\nCongrats, we finished this quick fine-tune of Llama 3: [mlabonne/OrpoLlama-3‚Äì8B](https://huggingface.co/mlabonne/OrpoLlama-3-8B). You can play with it using this [Hugging Face Space](https://huggingface.co/spaces/mlabonne/OrpoLlama-3-8B) (here‚Äôs a [notebook](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC?usp=sharing) to make your own). Although the model is undertrained, as highlighted by the W&B curves, I ran some evaluations on Nous‚Äô benchmark suite using [LLM AutoEval](https://github.com/mlabonne/llm-autoeval).\n![](https://towardsdatascience.com/wp-content/uploads/2024/04/1XLNStboeDllWwCD-XyCTXw.png)\nOur ORPO fine-tune is actually pretty decent and improves the base model‚Äôs performance on every benchmark. This is encouraging and likely means that a fine-tune on the entire 40k samples would yield great results.\nThis is an exciting time for the open-source community, with more and more high-quality open-weight models being released. The gap between closed-source and open-weight models is slowly closing, and fine-tuning is an essential tool to get the best performance for your use cases.\n![Image by author](https://towardsdatascience.com/wp-content/uploads/2024/04/16MeN5SXi4yrnNyf2O_-5zQ.png)Image by author\n## Conclusion\nIn this article, we introduced the ORPO algorithm and explained how it unifies the SFT and preference alignment stages into a single process. Then, we used TRL to fine-tune a Llama 3 8B model on a custom preference dataset. The final model shows encouraging results and highlights ORPO‚Äôs potential as a new fine-tuning paradigm.\nI hope it was useful, and I recommend running the [Colab notebook](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing) to fine-tune your own Llama 3 models. In future articles, we will see how to create high-quality datasets ‚Äì a point that is often overlooked. If you liked this article, please follow me on [Hugging Face](https://huggingface.co/mlabonne/) and Twitter [@maximelabonne](https://twitter.com/maximelabonne).\n## References\n  * J. Hong, N. Lee, and J. Thorne, [ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691). 2024.\n  * L. von Werra et al., TRL: Transformer Reinforcement Learning. GitHub, 2020. [Online]. Available: <https://github.com/huggingface/trl>\n  * Bartolome, A., Martin, G., & Vila, D. (2023). Notus. In GitHub Repository. GitHub. <https://github.com/argilla-io/notus>\n  * AI at Meta, [Introducing Meta Llama 3](https://ai.meta.com/blog/meta-llama-3/), 2024.\n\n\nWritten By\nMaxime Labonne\n[See all from Maxime Labonne](https://towardsdatascience.com/author/mlabonne/)\nTopics:\n[Artificial Intelligence](https://towardsdatascience.com/tag/artificial-intelligence/), [Editors Pick](https://towardsdatascience.com/tag/editors-pick/), [Hands On Tutorials](https://towardsdatascience.com/tag/hands-on-tutorials/), [Large Language Models](https://towardsdatascience.com/tag/large-language-models/), [Machine Learning](https://towardsdatascience.com/tag/machine-learning/)\nShare this article:\n  * [ Share on Facebook  ](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-llama-3-with-orpo-56cfab2f9ada%2F&title=Fine-tune%20Llama%203%20with%20ORPO)\n  * [ Share on LinkedIn  ](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-llama-3-with-orpo-56cfab2f9ada%2F&title=Fine-tune%20Llama%203%20with%20ORPO)\n  * [ Share on X  ](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-llama-3-with-orpo-56cfab2f9ada%2F&text=Fine-tune%20Llama%203%20with%20ORPO)\n\n\n## Related Articles\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/0c09RmbCCpfjAbSMq.png)\n## [Implementing Convolutional Neural Networks in TensorFlow](https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nStep-by-step code guide to building a Convolutional Neural Network \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nAugust 20, 2024\n6 min read\n  * ## [What Do Large Language Models ‚ÄúUnderstand‚Äù?](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA deep dive on the meaning of understanding and how it applies to LLMs \n[Tarik Dzekman](https://towardsdatascience.com/author/tarikdzekman/)\nAugust 21, 2024\n31 min read\n  * ![Photo by Krista Mangulsone on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/0GyVVTbgotH-DhGPH-scaled.jpg)\n## [How to Forecast Hierarchical Time Series](https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA beginner‚Äôs guide to forecast reconciliation \n[Dr. Robert K√ºbler](https://towardsdatascience.com/author/dr-robert-kuebler/)\nAugust 20, 2024\n13 min read\n  * ![Photo by davisuko on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/1bAABgtZtAIG5YW1oEjW3pA-scaled.jpeg)\n## [Hands-on Time Series Anomaly Detection using Autoencoders, with Python](https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nHere‚Äôs how to use Autoencoders to detect signals with anomalies in a few lines of‚Ä¶ \n[Piero Paialunga](https://towardsdatascience.com/author/piero-paialunga/)\nAugust 21, 2024\n12 min read\n  * ![Image from Canva.](https://towardsdatascience.com/wp-content/uploads/2024/08/1UAA9jQVdqMXnwzYiz8Q53Q.png)\n## [3 AI Use Cases (That Are Not a Chatbot)](https://towardsdatascience.com/3-ai-use-cases-that-are-not-a-chatbot-f4f328a2707a/)\n[ Machine Learning ](https://towardsdatascience.com/category/artificial-intelligence/machine-learning/)\nFeature engineering, structuring unstructured data, and lead scoring \n[Shaw Talebi](https://towardsdatascience.com/author/shawhin/)\nAugust 21, 2024\n7 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2023/02/1VEUgT5T4absnTqBMOEuNig.png)\n## [Back To Basics, Part Uno: Linear Regression and Cost Function](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nAn illustrated guide on essential machine learning concepts \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nFebruary 3, 2023\n6 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/1kM8tfYcdaoccB1HX71YDig.png)\n## [Must-Know in Statistics: The Bivariate Normal Projection Explained](https://towardsdatascience.com/must-know-in-statistics-the-bivariate-normal-projection-explained-ace7b2f70b5b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nDerivation and practical examples of this powerful concept \n[Luigi Battistoni](https://towardsdatascience.com/author/lu-battistoni/)\nAugust 14, 2024\n7 min read\n  * ![Photo by Alex Geerts on Unsplash](https://towardsdatascience.com/wp-content/uploads/2020/11/0BF38u2sw4WQdaMLS-scaled.jpg)\n## [Our Columns](https://towardsdatascience.com/our-columns-53501f74c86d/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nColumns on TDS are carefully curated collections of posts on a particular idea or category‚Ä¶ \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nNovember 14, 2020\n4 min read\n  * ![Image created by authors with GPT-4o](https://towardsdatascience.com/wp-content/uploads/2024/08/1vilI3Q4nlwqsAQLq3TOzSA.jpg)\n## [Optimizing Marketing Campaigns with Budgeted Multi-Armed Bandits](https://towardsdatascience.com/optimizing-marketing-campaigns-with-budgeted-multi-armed-bandits-a65fccd61878/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nWith demos, our new solution, and a video \n[Vadim Arzamasov](https://towardsdatascience.com/author/vadim-arzamasov/)\nAugust 16, 2024\n10 min read\n\n\n  * [YouTube](https://www.youtube.com/c/TowardsDataScience)\n  * [X](https://x.com/TDataScience)\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [Threads](https://www.threads.net/@towardsdatascience)\n  * [Bluesky](https://bsky.app/profile/towardsdatascience.com)\n\n\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nYour home for data science and Al. The world‚Äôs leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\n¬©  Insight Media Group, LLC 2025 \n  * [About](https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/)\n  * [Privacy Policy](https://towardsdatascience.com/privacy-policy/)\n  * [Terms of Use](https://towardsdatascience.com/website-terms-of-use/)\n\n\n[Towards Data Science is now independent!](https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/)\nCookies Settings\n## Sign up to our newsletter\n![Company Logo](https://cdn.cookielaw.org/logos/static/ot_company_logo.png)\n## Privacy Preference Center\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer. [More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nAllow All\n###  Manage Consent Preferences\n#### Functional Cookies\nFunctional Cookies Active\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\n#### Strictly Necessary Cookies\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n#### Performance Cookies\nPerformance Cookies Active\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n#### Targeting Cookies\nTargeting Cookies Active\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nBack Button\n### Cookie List\nSearch Icon\nFilter Icon\nClear\ncheckbox label label\nApply Cancel\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nReject All Confirm My Choices\n[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-consent/)\nSome areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada/#wp--skip-link--target",
        "https://towardsdatascience.com/",
        "https://towardsdatascience.com/latest/",
        "https://towardsdatascience.com/tag/editors-pick/",
        "https://towardsdatascience.com/tag/deep-dives/",
        "https://towardsdatascience.com/questions-96667b06af5/",
        "https://newsletter.towardsdatascience.com/subscription-to-the-newsletter",
        "https://towardsdatascience.com/category/artificial-intelligence/",
        "https://towardsdatascience.com/author/mlabonne/",
        "https://towardsdatascience.com/tag/large-language-models/",
        "https://towardsdatascience.com/tag/artificial-intelligence/",
        "https://towardsdatascience.com/tag/hands-on-tutorials/",
        "https://towardsdatascience.com/tag/machine-learning/",
        "https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/",
        "https://towardsdatascience.com/author/shreya-rao/",
        "https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/",
        "https://towardsdatascience.com/author/tarikdzekman/",
        "https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/",
        "https://towardsdatascience.com/author/dr-robert-kuebler/",
        "https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/",
        "https://towardsdatascience.com/category/data-science/",
        "https://towardsdatascience.com/author/piero-paialunga/",
        "https://towardsdatascience.com/3-ai-use-cases-that-are-not-a-chatbot-f4f328a2707a/",
        "https://towardsdatascience.com/category/artificial-intelligence/machine-learning/",
        "https://towardsdatascience.com/author/shawhin/",
        "https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/",
        "https://towardsdatascience.com/must-know-in-statistics-the-bivariate-normal-projection-explained-ace7b2f70b5b/",
        "https://towardsdatascience.com/author/lu-battistoni/",
        "https://towardsdatascience.com/our-columns-53501f74c86d/",
        "https://towardsdatascience.com/author/towardsdatascience/",
        "https://towardsdatascience.com/optimizing-marketing-campaigns-with-budgeted-multi-armed-bandits-a65fccd61878/",
        "https://towardsdatascience.com/author/vadim-arzamasov/",
        "https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/",
        "https://towardsdatascience.com/privacy-policy/",
        "https://towardsdatascience.com/website-terms-of-use/",
        "https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/",
        "https://contributor.insightmediagroup.io/",
        "https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca",
        "https://x.com/TDataScience",
        "https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing",
        "https://github.com/mlabonne/llm-course",
        "https://arxiv.org/abs/2403.07691",
        "https://github.com/huggingface/trl",
        "https://github.com/OpenAccess-AI-Collective/axolotl",
        "https://github.com/hiyouga/LLaMA-Factory",
        "https://github.com/meta-llama/llama3/tree/main",
        "https://huggingface.co/argilla",
        "https://huggingface.co/unalignment",
        "https://huggingface.co/M4-ai",
        "https://huggingface.co/jondurbin",
        "https://github.com/Dao-AILab/flash-attention",
        "https://github.com/TimDettmers/bitsandbytes",
        "https://github.com/huggingface/peft",
        "https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use",
        "https://huggingface.co/meta-llama/Meta-Llama-3-8B",
        "https://huggingface.co/NousResearch/Meta-Llama-3-8B",
        "https://huggingface.co/orpo-explorers/hf-llama3-8b-orpo-v0.0/tensorboard",
        "https://twitter.com/jiwoohong98",
        "https://huggingface.co/mlabonne/OrpoLlama-3-8B",
        "https://huggingface.co/spaces/mlabonne/OrpoLlama-3-8B",
        "https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC?usp=sharing",
        "https://github.com/mlabonne/llm-autoeval",
        "https://huggingface.co/mlabonne/",
        "https://twitter.com/maximelabonne",
        "https://github.com/argilla-io/notus",
        "https://ai.meta.com/blog/meta-llama-3/",
        "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-llama-3-with-orpo-56cfab2f9ada%2F&title=Fine-tune%20Llama%203%20with%20ORPO",
        "https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-llama-3-with-orpo-56cfab2f9ada%2F&title=Fine-tune%20Llama%203%20with%20ORPO",
        "https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-llama-3-with-orpo-56cfab2f9ada%2F&text=Fine-tune%20Llama%203%20with%20ORPO",
        "https://www.youtube.com/c/TowardsDataScience",
        "https://www.threads.net/@towardsdatascience",
        "https://bsky.app/profile/towardsdatascience.com",
        "https://cookiepedia.co.uk/giving-consent-to-cookies",
        "https://www.onetrust.com/products/cookie-consent/"
    ]
}