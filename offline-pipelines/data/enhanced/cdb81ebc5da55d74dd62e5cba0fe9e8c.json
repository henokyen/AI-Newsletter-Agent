{
    "id": "cdb81ebc5da55d74dd62e5cba0fe9e8c",
    "metadata": {
        "id": "cdb81ebc5da55d74dd62e5cba0fe9e8c",
        "url": "https://github.com/vllm-project/vllm/",
        "title": "GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs",
        "properties": {
            "description": "A high-throughput and memory-efficient inference and serving engine for LLMs - vllm-project/vllm",
            "keywords": null,
            "author": null,
            "og:image": "https://opengraph.githubassets.com/d402221131ab8db3d1856615d19663f4510172d61319ac6d64d1ea732e9247ae/vllm-project/vllm",
            "og:image:alt": "A high-throughput and memory-efficient inference and serving engine for LLMs - vllm-project/vllm",
            "og:image:width": "1200",
            "og:image:height": "600",
            "og:site_name": "GitHub",
            "og:type": "object",
            "og:title": "GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs",
            "og:url": "https://github.com/vllm-project/vllm",
            "og:description": "A high-throughput and memory-efficient inference and serving engine for LLMs - vllm-project/vllm",
            "twitter:image": "https://opengraph.githubassets.com/d402221131ab8db3d1856615d19663f4510172d61319ac6d64d1ea732e9247ae/vllm-project/vllm",
            "twitter:site": "@github",
            "twitter:card": "summary_large_image",
            "twitter:title": "GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs",
            "twitter:description": "A high-throughput and memory-efficient inference and serving engine for LLMs - vllm-project/vllm"
        }
    },
    "parent_metadata": {
        "id": "42cef7a98942f7d575a8c4ff558ffa1d",
        "url": "https://www.notion.so/Inference-Engines-42cef7a98942f7d575a8c4ff558ffa1d",
        "title": "Inference Engines",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Skip to content](https://github.com/vllm-project/vllm/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=vllm-project%2Fvllm) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/vllm-project/vllm/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/vllm-project/vllm/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/vllm-project/vllm/) to refresh your session. Dismiss alert\n{{ message }}\n[ vllm-project ](https://github.com/vllm-project) / **[vllm](https://github.com/vllm-project/vllm) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Fvllm-project%2Fvllm) You must be signed in to change notification settings\n  * [ Fork 6.5k ](https://github.com/login?return_to=%2Fvllm-project%2Fvllm)\n  * [ Star  43.1k ](https://github.com/login?return_to=%2Fvllm-project%2Fvllm)\n\n\nA high-throughput and memory-efficient inference and serving engine for LLMs \n[docs.vllm.ai](https://docs.vllm.ai \"https://docs.vllm.ai\")\n### License\n[ Apache-2.0 license ](https://github.com/vllm-project/vllm/blob/main/LICENSE)\n[ 43.1k stars ](https://github.com/vllm-project/vllm/stargazers) [ 6.5k forks ](https://github.com/vllm-project/vllm/forks) [ Branches ](https://github.com/vllm-project/vllm/branches) [ Tags ](https://github.com/vllm-project/vllm/tags) [ Activity ](https://github.com/vllm-project/vllm/activity)\n[ Star  ](https://github.com/login?return_to=%2Fvllm-project%2Fvllm)\n[ Notifications ](https://github.com/login?return_to=%2Fvllm-project%2Fvllm) You must be signed in to change notification settings\n  * [ Code ](https://github.com/vllm-project/vllm)\n  * [ Issues 1.5k ](https://github.com/vllm-project/vllm/issues)\n  * [ Pull requests 538 ](https://github.com/vllm-project/vllm/pulls)\n  * [ Discussions ](https://github.com/vllm-project/vllm/discussions)\n  * [ Actions ](https://github.com/vllm-project/vllm/actions)\n  * [ Projects 7 ](https://github.com/vllm-project/vllm/projects)\n  * [ Security ](https://github.com/vllm-project/vllm/security)\n  * [ Insights ](https://github.com/vllm-project/vllm/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/vllm-project/vllm)\n  * [ Issues  ](https://github.com/vllm-project/vllm/issues)\n  * [ Pull requests  ](https://github.com/vllm-project/vllm/pulls)\n  * [ Discussions  ](https://github.com/vllm-project/vllm/discussions)\n  * [ Actions  ](https://github.com/vllm-project/vllm/actions)\n  * [ Projects  ](https://github.com/vllm-project/vllm/projects)\n  * [ Security  ](https://github.com/vllm-project/vllm/security)\n  * [ Insights  ](https://github.com/vllm-project/vllm/pulse)\n\n\n# vllm-project/vllm\nmain\n[Branches](https://github.com/vllm-project/vllm/branches)[Tags](https://github.com/vllm-project/vllm/tags)\n[](https://github.com/vllm-project/vllm/branches)[](https://github.com/vllm-project/vllm/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[5,512 Commits](https://github.com/vllm-project/vllm/commits/main/)[](https://github.com/vllm-project/vllm/commits/main/)  \n[.buildkite](https://github.com/vllm-project/vllm/tree/main/.buildkite \".buildkite\")| [.buildkite](https://github.com/vllm-project/vllm/tree/main/.buildkite \".buildkite\")  \n[.github](https://github.com/vllm-project/vllm/tree/main/.github \".github\")| [.github](https://github.com/vllm-project/vllm/tree/main/.github \".github\")  \n[benchmarks](https://github.com/vllm-project/vllm/tree/main/benchmarks \"benchmarks\")| [benchmarks](https://github.com/vllm-project/vllm/tree/main/benchmarks \"benchmarks\")  \n[cmake](https://github.com/vllm-project/vllm/tree/main/cmake \"cmake\")| [cmake](https://github.com/vllm-project/vllm/tree/main/cmake \"cmake\")  \n[csrc](https://github.com/vllm-project/vllm/tree/main/csrc \"csrc\")| [csrc](https://github.com/vllm-project/vllm/tree/main/csrc \"csrc\")  \n[docs](https://github.com/vllm-project/vllm/tree/main/docs \"docs\")| [docs](https://github.com/vllm-project/vllm/tree/main/docs \"docs\")  \n[examples](https://github.com/vllm-project/vllm/tree/main/examples \"examples\")| [examples](https://github.com/vllm-project/vllm/tree/main/examples \"examples\")  \n[requirements](https://github.com/vllm-project/vllm/tree/main/requirements \"requirements\")| [requirements](https://github.com/vllm-project/vllm/tree/main/requirements \"requirements\")  \n[tests](https://github.com/vllm-project/vllm/tree/main/tests \"tests\")| [tests](https://github.com/vllm-project/vllm/tree/main/tests \"tests\")  \n[tools](https://github.com/vllm-project/vllm/tree/main/tools \"tools\")| [tools](https://github.com/vllm-project/vllm/tree/main/tools \"tools\")  \n[vllm](https://github.com/vllm-project/vllm/tree/main/vllm \"vllm\")| [vllm](https://github.com/vllm-project/vllm/tree/main/vllm \"vllm\")  \n[.clang-format](https://github.com/vllm-project/vllm/blob/main/.clang-format \".clang-format\")| [.clang-format](https://github.com/vllm-project/vllm/blob/main/.clang-format \".clang-format\")  \n[.dockerignore](https://github.com/vllm-project/vllm/blob/main/.dockerignore \".dockerignore\")| [.dockerignore](https://github.com/vllm-project/vllm/blob/main/.dockerignore \".dockerignore\")  \n[.gitignore](https://github.com/vllm-project/vllm/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/vllm-project/vllm/blob/main/.gitignore \".gitignore\")  \n[.pre-commit-config.yaml](https://github.com/vllm-project/vllm/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/vllm-project/vllm/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")  \n[.readthedocs.yaml](https://github.com/vllm-project/vllm/blob/main/.readthedocs.yaml \".readthedocs.yaml\")| [.readthedocs.yaml](https://github.com/vllm-project/vllm/blob/main/.readthedocs.yaml \".readthedocs.yaml\")  \n[.shellcheckrc](https://github.com/vllm-project/vllm/blob/main/.shellcheckrc \".shellcheckrc\")| [.shellcheckrc](https://github.com/vllm-project/vllm/blob/main/.shellcheckrc \".shellcheckrc\")  \n[.yapfignore](https://github.com/vllm-project/vllm/blob/main/.yapfignore \".yapfignore\")| [.yapfignore](https://github.com/vllm-project/vllm/blob/main/.yapfignore \".yapfignore\")  \n[CMakeLists.txt](https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt \"CMakeLists.txt\")| [CMakeLists.txt](https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt \"CMakeLists.txt\")  \n[CODE_OF_CONDUCT.md](https://github.com/vllm-project/vllm/blob/main/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| [CODE_OF_CONDUCT.md](https://github.com/vllm-project/vllm/blob/main/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")  \n[CONTRIBUTING.md](https://github.com/vllm-project/vllm/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/vllm-project/vllm/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")  \n[DCO](https://github.com/vllm-project/vllm/blob/main/DCO \"DCO\")| [DCO](https://github.com/vllm-project/vllm/blob/main/DCO \"DCO\")  \n[Dockerfile](https://github.com/vllm-project/vllm/blob/main/Dockerfile \"Dockerfile\")| [Dockerfile](https://github.com/vllm-project/vllm/blob/main/Dockerfile \"Dockerfile\")  \n[Dockerfile.arm](https://github.com/vllm-project/vllm/blob/main/Dockerfile.arm \"Dockerfile.arm\")| [Dockerfile.arm](https://github.com/vllm-project/vllm/blob/main/Dockerfile.arm \"Dockerfile.arm\")  \n[Dockerfile.cpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.cpu \"Dockerfile.cpu\")| [Dockerfile.cpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.cpu \"Dockerfile.cpu\")  \n[Dockerfile.hpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.hpu \"Dockerfile.hpu\")| [Dockerfile.hpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.hpu \"Dockerfile.hpu\")  \n[Dockerfile.neuron](https://github.com/vllm-project/vllm/blob/main/Dockerfile.neuron \"Dockerfile.neuron\")| [Dockerfile.neuron](https://github.com/vllm-project/vllm/blob/main/Dockerfile.neuron \"Dockerfile.neuron\")  \n[Dockerfile.ppc64le](https://github.com/vllm-project/vllm/blob/main/Dockerfile.ppc64le \"Dockerfile.ppc64le\")| [Dockerfile.ppc64le](https://github.com/vllm-project/vllm/blob/main/Dockerfile.ppc64le \"Dockerfile.ppc64le\")  \n[Dockerfile.rocm](https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm \"Dockerfile.rocm\")| [Dockerfile.rocm](https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm \"Dockerfile.rocm\")  \n[Dockerfile.rocm_base](https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm_base \"Dockerfile.rocm_base\")| [Dockerfile.rocm_base](https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm_base \"Dockerfile.rocm_base\")  \n[Dockerfile.s390x](https://github.com/vllm-project/vllm/blob/main/Dockerfile.s390x \"Dockerfile.s390x\")| [Dockerfile.s390x](https://github.com/vllm-project/vllm/blob/main/Dockerfile.s390x \"Dockerfile.s390x\")  \n[Dockerfile.tpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.tpu \"Dockerfile.tpu\")| [Dockerfile.tpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.tpu \"Dockerfile.tpu\")  \n[Dockerfile.xpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.xpu \"Dockerfile.xpu\")| [Dockerfile.xpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.xpu \"Dockerfile.xpu\")  \n[LICENSE](https://github.com/vllm-project/vllm/blob/main/LICENSE \"LICENSE\")| [LICENSE](https://github.com/vllm-project/vllm/blob/main/LICENSE \"LICENSE\")  \n[MANIFEST.in](https://github.com/vllm-project/vllm/blob/main/MANIFEST.in \"MANIFEST.in\")| [MANIFEST.in](https://github.com/vllm-project/vllm/blob/main/MANIFEST.in \"MANIFEST.in\")  \n[README.md](https://github.com/vllm-project/vllm/blob/main/README.md \"README.md\")| [README.md](https://github.com/vllm-project/vllm/blob/main/README.md \"README.md\")  \n[RELEASE.md](https://github.com/vllm-project/vllm/blob/main/RELEASE.md \"RELEASE.md\")| [RELEASE.md](https://github.com/vllm-project/vllm/blob/main/RELEASE.md \"RELEASE.md\")  \n[SECURITY.md](https://github.com/vllm-project/vllm/blob/main/SECURITY.md \"SECURITY.md\")| [SECURITY.md](https://github.com/vllm-project/vllm/blob/main/SECURITY.md \"SECURITY.md\")  \n[collect_env.py](https://github.com/vllm-project/vllm/blob/main/collect_env.py \"collect_env.py\")| [collect_env.py](https://github.com/vllm-project/vllm/blob/main/collect_env.py \"collect_env.py\")  \n[find_cuda_init.py](https://github.com/vllm-project/vllm/blob/main/find_cuda_init.py \"find_cuda_init.py\")| [find_cuda_init.py](https://github.com/vllm-project/vllm/blob/main/find_cuda_init.py \"find_cuda_init.py\")  \n[format.sh](https://github.com/vllm-project/vllm/blob/main/format.sh \"format.sh\")| [format.sh](https://github.com/vllm-project/vllm/blob/main/format.sh \"format.sh\")  \n[pyproject.toml](https://github.com/vllm-project/vllm/blob/main/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/vllm-project/vllm/blob/main/pyproject.toml \"pyproject.toml\")  \n[python_only_dev.py](https://github.com/vllm-project/vllm/blob/main/python_only_dev.py \"python_only_dev.py\")| [python_only_dev.py](https://github.com/vllm-project/vllm/blob/main/python_only_dev.py \"python_only_dev.py\")  \n[setup.py](https://github.com/vllm-project/vllm/blob/main/setup.py \"setup.py\")| [setup.py](https://github.com/vllm-project/vllm/blob/main/setup.py \"setup.py\")  \n[use_existing_torch.py](https://github.com/vllm-project/vllm/blob/main/use_existing_torch.py \"use_existing_torch.py\")| [use_existing_torch.py](https://github.com/vllm-project/vllm/blob/main/use_existing_torch.py \"use_existing_torch.py\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/vllm-project/vllm/)\n  * [Code of conduct](https://github.com/vllm-project/vllm/)\n  * [Apache-2.0 license](https://github.com/vllm-project/vllm/)\n  * [Security](https://github.com/vllm-project/vllm/)\n\n\n![vLLM](https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png)\n###  Easy, fast, and cheap LLM serving for everyone \n[](https://github.com/vllm-project/vllm/#easy-fast-and-cheap-llm-serving-for-everyone)\n| [**Documentation**](https://docs.vllm.ai) | [**Blog**](https://vllm.ai) | [**Paper**](https://arxiv.org/abs/2309.06180) | [**Twitter/X**](https://x.com/vllm_project) | [**User Forum**](https://discuss.vllm.ai) | [**Developer Slack**](https://slack.vllm.ai) | \n[2025/03] We are collaborating with Ollama to host an [Inference Night](https://lu.ma/vllm-ollama) at Y Combinator in San Francisco on Thursday, March 27, at 6 PM. Discuss all things inference local or data center!\n[2025/04] We're hosting our first-ever _vLLM Asia Developer Day_ in Singapore on _April 3rd_! This is a full-day event (9 AM - 9 PM SGT) in partnership with SGInnovate, AMD, and Embedded LLM. Meet the vLLM team and learn about LLM inference for RL, MI300X, and more! [Register Now](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)\n_Latest News_ 🔥\n  * [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).\n  * [2025/03] We hosted [the East Coast vLLM Meetup](https://lu.ma/7mu4k4xx)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0).\n  * [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.\n  * [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).\n  * [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).\n  * [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!\n\nPrevious News\n  * [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).\n  * [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!\n  * [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!\n  * [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).\n  * [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).\n  * [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).\n  * [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).\n  * [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).\n  * [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).\n  * [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).\n  * [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.\n  * [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).\n\n\n## About\n[](https://github.com/vllm-project/vllm/#about)\nvLLM is a fast and easy-to-use library for LLM inference and serving.\nOriginally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.\nvLLM is fast with:\n  * State-of-the-art serving throughput\n  * Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)\n  * Continuous batching of incoming requests\n  * Fast model execution with CUDA/HIP graph\n  * Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), INT4, INT8, and FP8.\n  * Optimized CUDA kernels, including integration with FlashAttention and FlashInfer.\n  * Speculative decoding\n  * Chunked prefill\n\n\n**Performance benchmark** : We include a performance benchmark at the end of [our blog post](https://blog.vllm.ai/2024/09/05/perf-update.html). It compares the performance of vLLM against other LLM serving engines ([TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [SGLang](https://github.com/sgl-project/sglang) and [LMDeploy](https://github.com/InternLM/lmdeploy)). The implementation is under [nightly-benchmarks folder](https://github.com/vllm-project/vllm/blob/main/.buildkite/nightly-benchmarks) and you can [reproduce](https://github.com/vllm-project/vllm/issues/8176) this benchmark using our one-click runnable script.\nvLLM is flexible and easy to use with:\n  * Seamless integration with popular Hugging Face models\n  * High-throughput serving with various decoding algorithms, including _parallel sampling_ , _beam search_ , and more\n  * Tensor parallelism and pipeline parallelism support for distributed inference\n  * Streaming outputs\n  * OpenAI-compatible API server\n  * Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron.\n  * Prefix caching support\n  * Multi-lora support\n\n\nvLLM seamlessly supports most popular open-source models on HuggingFace, including:\n  * Transformer-like LLMs (e.g., Llama)\n  * Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)\n  * Embedding Models (e.g. E5-Mistral)\n  * Multi-modal LLMs (e.g., LLaVA)\n\n\nFind the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).\n## Getting Started\n[](https://github.com/vllm-project/vllm/#getting-started)\nInstall vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):\n```\npip install vllm\n```\n\nVisit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.\n  * [Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)\n  * [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)\n  * [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)\n\n\n## Contributing\n[](https://github.com/vllm-project/vllm/#contributing)\nWe welcome and value any contributions and collaborations. Please check out [CONTRIBUTING.md](https://github.com/vllm-project/vllm/blob/main/CONTRIBUTING.md) for how to get involved.\n## Sponsors\n[](https://github.com/vllm-project/vllm/#sponsors)\nvLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!\nCash Donations:\n  * a16z\n  * Dropbox\n  * Sequoia Capital\n  * Skywork AI\n  * ZhenFund\n\n\nCompute Resources:\n  * AMD\n  * Anyscale\n  * AWS\n  * Crusoe Cloud\n  * Databricks\n  * DeepInfra\n  * Google Cloud\n  * Lambda Lab\n  * Nebius\n  * Novita AI\n  * NVIDIA\n  * Replicate\n  * Roblox\n  * RunPod\n  * Trainy\n  * UC Berkeley\n  * UC San Diego\n\n\nSlack Sponsor: Anyscale\nWe also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.\n## Citation\n[](https://github.com/vllm-project/vllm/#citation)\nIf you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):\n```\n@inproceedings{kwon2023efficient,\n title={Efficient Memory Management for Large Language Model Serving with PagedAttention},\n author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},\n booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},\n year={2023}\n}\n```\n\n## Contact Us\n[](https://github.com/vllm-project/vllm/#contact-us)\n  * For technical questions and feature requests, please use GitHub [Issues](https://github.com/vllm-project/vllm/issues) or [Discussions](https://github.com/vllm-project/vllm/discussions)\n  * For discussing with fellow users, please use the [vLLM Forum](https://discuss.vllm.ai)\n  * coordinating contributions and development, please use [Slack](https://slack.vllm.ai)\n  * For security disclosures, please use GitHub's [Security Advisories](https://github.com/vllm-project/vllm/security/advisories) feature\n  * For collaborations and partnerships, please contact us at vllm-questions@lists.berkeley.edu\n\n\n## Media Kit\n[](https://github.com/vllm-project/vllm/#media-kit)\n  * If you wish to use vLLM's logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit).\n\n\n## About\nA high-throughput and memory-efficient inference and serving engine for LLMs \n[docs.vllm.ai](https://docs.vllm.ai \"https://docs.vllm.ai\")\n### Topics\n[ amd ](https://github.com/topics/amd \"Topic: amd\") [ cuda ](https://github.com/topics/cuda \"Topic: cuda\") [ inference ](https://github.com/topics/inference \"Topic: inference\") [ pytorch ](https://github.com/topics/pytorch \"Topic: pytorch\") [ transformer ](https://github.com/topics/transformer \"Topic: transformer\") [ llama ](https://github.com/topics/llama \"Topic: llama\") [ gpt ](https://github.com/topics/gpt \"Topic: gpt\") [ rocm ](https://github.com/topics/rocm \"Topic: rocm\") [ model-serving ](https://github.com/topics/model-serving \"Topic: model-serving\") [ tpu ](https://github.com/topics/tpu \"Topic: tpu\") [ hpu ](https://github.com/topics/hpu \"Topic: hpu\") [ mlops ](https://github.com/topics/mlops \"Topic: mlops\") [ xpu ](https://github.com/topics/xpu \"Topic: xpu\") [ llm ](https://github.com/topics/llm \"Topic: llm\") [ inferentia ](https://github.com/topics/inferentia \"Topic: inferentia\") [ llmops ](https://github.com/topics/llmops \"Topic: llmops\") [ llm-serving ](https://github.com/topics/llm-serving \"Topic: llm-serving\") [ qwen ](https://github.com/topics/qwen \"Topic: qwen\") [ deepseek ](https://github.com/topics/deepseek \"Topic: deepseek\") [ trainium ](https://github.com/topics/trainium \"Topic: trainium\")\n### Resources\n[ Readme ](https://github.com/vllm-project/vllm/#readme-ov-file)\n### License\n[ Apache-2.0 license ](https://github.com/vllm-project/vllm/#Apache-2.0-1-ov-file)\n### Code of conduct\n[ Code of conduct ](https://github.com/vllm-project/vllm/#coc-ov-file)\n### Security policy\n[ Security policy ](https://github.com/vllm-project/vllm/#security-ov-file)\n[ Activity](https://github.com/vllm-project/vllm/activity)\n[ Custom properties](https://github.com/vllm-project/vllm/custom-properties)\n### Stars\n[ **43.1k** stars](https://github.com/vllm-project/vllm/stargazers)\n### Watchers\n[ **356** watching](https://github.com/vllm-project/vllm/watchers)\n### Forks\n[ **6.5k** forks](https://github.com/vllm-project/vllm/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm&report=vllm-project+%28user%29)\n##  [Releases 55](https://github.com/vllm-project/vllm/releases)\n[ v0.8.2 Latest  Mar 23, 2025 ](https://github.com/vllm-project/vllm/releases/tag/v0.8.2)\n[+ 54 releases](https://github.com/vllm-project/vllm/releases)\n## Sponsor this project\n  * ![open_collective](https://github.githubassets.com/assets/open_collective-0a706523753d.svg) [opencollective.com/**vllm**](https://opencollective.com/vllm)\n\n\n[Learn more about GitHub Sponsors](https://github.com/sponsors)\n##  [Contributors 963](https://github.com/vllm-project/vllm/graphs/contributors)\n  * [ ![@WoosukKwon](https://avatars.githubusercontent.com/u/46394894?s=64&v=4) ](https://github.com/WoosukKwon)\n  * [ ![@youkaichao](https://avatars.githubusercontent.com/u/23236638?s=64&v=4) ](https://github.com/youkaichao)\n  * [ ![@DarkLight1337](https://avatars.githubusercontent.com/u/44970335?s=64&v=4) ](https://github.com/DarkLight1337)\n  * [ ![@mgoin](https://avatars.githubusercontent.com/u/3195154?s=64&v=4) ](https://github.com/mgoin)\n  * [ ![@ywang96](https://avatars.githubusercontent.com/u/136131678?s=64&v=4) ](https://github.com/ywang96)\n  * [ ![@simon-mo](https://avatars.githubusercontent.com/u/21118851?s=64&v=4) ](https://github.com/simon-mo)\n  * [ ![@Isotr0py](https://avatars.githubusercontent.com/u/41363108?s=64&v=4) ](https://github.com/Isotr0py)\n  * [ ![@njhill](https://avatars.githubusercontent.com/u/16958488?s=64&v=4) ](https://github.com/njhill)\n  * [ ![@zhuohan123](https://avatars.githubusercontent.com/u/17310766?s=64&v=4) ](https://github.com/zhuohan123)\n  * [ ![@robertgshaw2-redhat](https://avatars.githubusercontent.com/u/114415538?s=64&v=4) ](https://github.com/robertgshaw2-redhat)\n  * [ ![@tlrmchlsmth](https://avatars.githubusercontent.com/u/1236979?s=64&v=4) ](https://github.com/tlrmchlsmth)\n  * [ ![@jeejeelee](https://avatars.githubusercontent.com/u/19733142?s=64&v=4) ](https://github.com/jeejeelee)\n  * [ ![@russellb](https://avatars.githubusercontent.com/u/309258?s=64&v=4) ](https://github.com/russellb)\n  * [ ![@comaniac](https://avatars.githubusercontent.com/u/8262694?s=64&v=4) ](https://github.com/comaniac)\n\n\n[+ 949 contributors](https://github.com/vllm-project/vllm/graphs/contributors)\n## Languages\n  * [ Python 84.5% ](https://github.com/vllm-project/vllm/search?l=python)\n  * [ Cuda 10.4% ](https://github.com/vllm-project/vllm/search?l=cuda)\n  * [ C++ 3.4% ](https://github.com/vllm-project/vllm/search?l=c%2B%2B)\n  * [ C 0.6% ](https://github.com/vllm-project/vllm/search?l=c)\n  * [ Shell 0.6% ](https://github.com/vllm-project/vllm/search?l=shell)\n  * [ CMake 0.4% ](https://github.com/vllm-project/vllm/search?l=cmake)\n  * [ Dockerfile 0.1% ](https://github.com/vllm-project/vllm/search?l=dockerfile)\n\n\n## Footer\n[ ](https://github.com \"GitHub\") © 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can’t perform that action at this time. \n",
    "content_quality_score": null,
    "summary": null,
    "child_urls": [
        "https://github.com/vllm-project/vllm/#start-of-content",
        "https://github.com/",
        "https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm%2F",
        "https://github.com/features/copilot",
        "https://github.com/features/security",
        "https://github.com/features/actions",
        "https://github.com/features/codespaces",
        "https://github.com/features/issues",
        "https://github.com/features/code-review",
        "https://github.com/features/discussions",
        "https://github.com/features/code-search",
        "https://github.com/features",
        "https://docs.github.com",
        "https://skills.github.com",
        "https://github.com/enterprise",
        "https://github.com/team",
        "https://github.com/enterprise/startups",
        "https://github.com/solutions/industry/nonprofits",
        "https://github.com/solutions/use-case/devsecops",
        "https://github.com/solutions/use-case/devops",
        "https://github.com/solutions/use-case/ci-cd",
        "https://github.com/solutions/use-case",
        "https://github.com/solutions/industry/healthcare",
        "https://github.com/solutions/industry/financial-services",
        "https://github.com/solutions/industry/manufacturing",
        "https://github.com/solutions/industry/government",
        "https://github.com/solutions/industry",
        "https://github.com/solutions",
        "https://github.com/resources/articles/ai",
        "https://github.com/resources/articles/devops",
        "https://github.com/resources/articles/security",
        "https://github.com/resources/articles/software-development",
        "https://github.com/resources/articles",
        "https://resources.github.com/learn/pathways",
        "https://resources.github.com",
        "https://github.com/resources/whitepapers",
        "https://github.com/customer-stories",
        "https://partner.github.com",
        "https://github.com/solutions/executive-insights",
        "https://github.com/sponsors",
        "https://github.com/readme",
        "https://github.com/topics",
        "https://github.com/trending",
        "https://github.com/collections",
        "https://github.com/enterprise/advanced-security",
        "https://github.com/features/copilot/copilot-business",
        "https://github.com/premium-support",
        "https://github.com/pricing",
        "https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax",
        "https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=vllm-project%2Fvllm",
        "https://github.com/vllm-project",
        "https://github.com/vllm-project/vllm",
        "https://github.com/login?return_to=%2Fvllm-project%2Fvllm",
        "https://github.com/vllm-project/vllm/blob/main/LICENSE",
        "https://github.com/vllm-project/vllm/stargazers",
        "https://github.com/vllm-project/vllm/forks",
        "https://github.com/vllm-project/vllm/branches",
        "https://github.com/vllm-project/vllm/tags",
        "https://github.com/vllm-project/vllm/activity",
        "https://github.com/vllm-project/vllm/issues",
        "https://github.com/vllm-project/vllm/pulls",
        "https://github.com/vllm-project/vllm/discussions",
        "https://github.com/vllm-project/vllm/actions",
        "https://github.com/vllm-project/vllm/projects",
        "https://github.com/vllm-project/vllm/security",
        "https://github.com/vllm-project/vllm/pulse",
        "https://github.com/vllm-project/vllm/commits/main/",
        "https://github.com/vllm-project/vllm/tree/main/.buildkite",
        "https://github.com/vllm-project/vllm/tree/main/.github",
        "https://github.com/vllm-project/vllm/tree/main/benchmarks",
        "https://github.com/vllm-project/vllm/tree/main/cmake",
        "https://github.com/vllm-project/vllm/tree/main/csrc",
        "https://github.com/vllm-project/vllm/tree/main/docs",
        "https://github.com/vllm-project/vllm/tree/main/examples",
        "https://github.com/vllm-project/vllm/tree/main/requirements",
        "https://github.com/vllm-project/vllm/tree/main/tests",
        "https://github.com/vllm-project/vllm/tree/main/tools",
        "https://github.com/vllm-project/vllm/tree/main/vllm",
        "https://github.com/vllm-project/vllm/blob/main/.clang-format",
        "https://github.com/vllm-project/vllm/blob/main/.dockerignore",
        "https://github.com/vllm-project/vllm/blob/main/.gitignore",
        "https://github.com/vllm-project/vllm/blob/main/.pre-commit-config.yaml",
        "https://github.com/vllm-project/vllm/blob/main/.readthedocs.yaml",
        "https://github.com/vllm-project/vllm/blob/main/.shellcheckrc",
        "https://github.com/vllm-project/vllm/blob/main/.yapfignore",
        "https://github.com/vllm-project/vllm/blob/main/CMakeLists.txt",
        "https://github.com/vllm-project/vllm/blob/main/CODE_OF_CONDUCT.md",
        "https://github.com/vllm-project/vllm/blob/main/CONTRIBUTING.md",
        "https://github.com/vllm-project/vllm/blob/main/DCO",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.arm",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.cpu",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.hpu",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.neuron",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.ppc64le",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm_base",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.s390x",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.tpu",
        "https://github.com/vllm-project/vllm/blob/main/Dockerfile.xpu",
        "https://github.com/vllm-project/vllm/blob/main/MANIFEST.in",
        "https://github.com/vllm-project/vllm/blob/main/README.md",
        "https://github.com/vllm-project/vllm/blob/main/RELEASE.md",
        "https://github.com/vllm-project/vllm/blob/main/SECURITY.md",
        "https://github.com/vllm-project/vllm/blob/main/collect_env.py",
        "https://github.com/vllm-project/vllm/blob/main/find_cuda_init.py",
        "https://github.com/vllm-project/vllm/blob/main/format.sh",
        "https://github.com/vllm-project/vllm/blob/main/pyproject.toml",
        "https://github.com/vllm-project/vllm/blob/main/python_only_dev.py",
        "https://github.com/vllm-project/vllm/blob/main/setup.py",
        "https://github.com/vllm-project/vllm/blob/main/use_existing_torch.py",
        "https://github.com/vllm-project/vllm/",
        "https://github.com/vllm-project/vllm/#easy-fast-and-cheap-llm-serving-for-everyone",
        "https://github.com/vllm-project/vllm/#about",
        "https://github.com/NVIDIA/TensorRT-LLM",
        "https://github.com/sgl-project/sglang",
        "https://github.com/InternLM/lmdeploy",
        "https://github.com/vllm-project/vllm/blob/main/.buildkite/nightly-benchmarks",
        "https://github.com/vllm-project/vllm/issues/8176",
        "https://github.com/vllm-project/vllm/#getting-started",
        "https://github.com/vllm-project/vllm/#contributing",
        "https://github.com/vllm-project/vllm/#sponsors",
        "https://github.com/vllm-project/vllm/#citation",
        "https://github.com/vllm-project/vllm/#contact-us",
        "https://github.com/vllm-project/vllm/security/advisories",
        "https://github.com/vllm-project/vllm/#media-kit",
        "https://github.com/vllm-project/media-kit",
        "https://github.com/topics/amd",
        "https://github.com/topics/cuda",
        "https://github.com/topics/inference",
        "https://github.com/topics/pytorch",
        "https://github.com/topics/transformer",
        "https://github.com/topics/llama",
        "https://github.com/topics/gpt",
        "https://github.com/topics/rocm",
        "https://github.com/topics/model-serving",
        "https://github.com/topics/tpu",
        "https://github.com/topics/hpu",
        "https://github.com/topics/mlops",
        "https://github.com/topics/xpu",
        "https://github.com/topics/llm",
        "https://github.com/topics/inferentia",
        "https://github.com/topics/llmops",
        "https://github.com/topics/llm-serving",
        "https://github.com/topics/qwen",
        "https://github.com/topics/deepseek",
        "https://github.com/topics/trainium",
        "https://github.com/vllm-project/vllm/#readme-ov-file",
        "https://github.com/vllm-project/vllm/#Apache-2.0-1-ov-file",
        "https://github.com/vllm-project/vllm/#coc-ov-file",
        "https://github.com/vllm-project/vllm/#security-ov-file",
        "https://github.com/vllm-project/vllm/custom-properties",
        "https://github.com/vllm-project/vllm/watchers",
        "https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fvllm-project%2Fvllm&report=vllm-project+%28user%29",
        "https://github.com/vllm-project/vllm/releases",
        "https://github.com/vllm-project/vllm/releases/tag/v0.8.2",
        "https://github.com/vllm-project/vllm/graphs/contributors",
        "https://github.com/WoosukKwon",
        "https://github.com/youkaichao",
        "https://github.com/DarkLight1337",
        "https://github.com/mgoin",
        "https://github.com/ywang96",
        "https://github.com/simon-mo",
        "https://github.com/Isotr0py",
        "https://github.com/njhill",
        "https://github.com/zhuohan123",
        "https://github.com/robertgshaw2-redhat",
        "https://github.com/tlrmchlsmth",
        "https://github.com/jeejeelee",
        "https://github.com/russellb",
        "https://github.com/comaniac",
        "https://github.com/vllm-project/vllm/search?l=python",
        "https://github.com/vllm-project/vllm/search?l=cuda",
        "https://github.com/vllm-project/vllm/search?l=c%2B%2B",
        "https://github.com/vllm-project/vllm/search?l=c",
        "https://github.com/vllm-project/vllm/search?l=shell",
        "https://github.com/vllm-project/vllm/search?l=cmake",
        "https://github.com/vllm-project/vllm/search?l=dockerfile",
        "https://github.com",
        "https://docs.github.com/site-policy/github-terms/github-terms-of-service",
        "https://docs.github.com/site-policy/privacy-policies/github-privacy-statement",
        "https://github.com/security",
        "https://docs.github.com/",
        "https://support.github.com?tags=dotcom-footer",
        "https://github.blog",
        "https://docs.vllm.ai",
        "https://vllm.ai",
        "https://arxiv.org/abs/2309.06180",
        "https://x.com/vllm_project",
        "https://discuss.vllm.ai",
        "https://slack.vllm.ai",
        "https://lu.ma/vllm-ollama",
        "https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day",
        "https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg",
        "https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing",
        "https://lu.ma/7mu4k4xx",
        "https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0",
        "https://lu.ma/h7g3kuj9",
        "https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing",
        "https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing",
        "https://blog.vllm.ai/2025/01/27/v1-alpha-release.html",
        "https://lu.ma/zep56hui",
        "https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing",
        "https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing",
        "https://pytorch.org/blog/vllm-joins-pytorch",
        "https://lu.ma/h0qvrajz",
        "https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing",
        "https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing",
        "https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing",
        "https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR",
        "https://lu.ma/87q3nvnh",
        "https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing",
        "https://lu.ma/lp0gyjqr",
        "https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing",
        "https://blog.vllm.ai/2024/07/23/llama31.html",
        "https://lu.ma/agivllm",
        "https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing",
        "https://robloxandvllmmeetup2024.splashthat.com/",
        "https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing",
        "https://lu.ma/ygxbpzhl",
        "https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing",
        "https://lu.ma/first-vllm-meetup",
        "https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing",
        "https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/",
        "https://chat.lmsys.org",
        "https://sky.cs.berkeley.edu",
        "https://blog.vllm.ai/2023/06/20/vllm.html",
        "https://arxiv.org/abs/2210.17323",
        "https://arxiv.org/abs/2306.00978",
        "https://blog.vllm.ai/2024/09/05/perf-update.html",
        "https://docs.vllm.ai/en/latest/models/supported_models.html",
        "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source",
        "https://docs.vllm.ai/en/latest/",
        "https://docs.vllm.ai/en/latest/getting_started/installation.html",
        "https://docs.vllm.ai/en/latest/getting_started/quickstart.html",
        "https://opencollective.com/vllm",
        "mailto:vllm-questions@lists.berkeley.edu",
        "https://www.githubstatus.com/"
    ]
}