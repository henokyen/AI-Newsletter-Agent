{
    "id": "d9cf0cd6c4fddcabd93d370a0ac6a611",
    "metadata": {
        "id": "d9cf0cd6c4fddcabd93d370a0ac6a611",
        "url": "https://huggingface.co/jinaai/ReaderLM-v2/",
        "title": "jinaai/ReaderLM-v2 · Hugging Face",
        "properties": {
            "description": "We’re on a journey to advance and democratize artificial intelligence through open source and open science.",
            "keywords": null,
            "author": null,
            "og:title": "jinaai/ReaderLM-v2 · Hugging Face",
            "og:type": "website",
            "og:url": "https://huggingface.co/jinaai/ReaderLM-v2",
            "og:image": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/ReaderLM-v2.png",
            "twitter:card": "summary_large_image",
            "twitter:site": "@huggingface",
            "twitter:image": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/ReaderLM-v2.png"
        }
    },
    "parent_metadata": {
        "id": "f65ae4903fe450fd86022e5a8ffe1a19",
        "url": "https://www.notion.so/Document-preprocessing-e-g-HTML-PDF-f65ae4903fe450fd86022e5a8ffe1a19",
        "title": "Document preprocessing (e.g., HTML, PDF)",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\n# \n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/AFoybzd5lpBQXEBrQHuTt.png)](https://huggingface.co/jinaai)\n[jinaai](https://huggingface.co/jinaai)\n/\n[ReaderLM-v2](https://huggingface.co/jinaai/ReaderLM-v2)\nlike 592\nFollow\n![](https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/AFoybzd5lpBQXEBrQHuTt.png) Jina AI 866\n[ Text Generation ](https://huggingface.co/models?pipeline_tag=text-generation)[ Transformers ](https://huggingface.co/models?library=transformers)[ ONNX ](https://huggingface.co/models?library=onnx)[ Safetensors ](https://huggingface.co/models?library=safetensors)[ multilingual ](https://huggingface.co/models?language=multilingual)[ qwen2 ](https://huggingface.co/models?other=qwen2)[ conversational ](https://huggingface.co/models?other=conversational)[ text-generation-inference ](https://huggingface.co/models?other=text-generation-inference)\narxiv: 2503.01151\nLicense: cc-by-nc-4.0\n[ 🇪🇺 Region: EU ](https://huggingface.co/models?other=region%3Aeu)\n[ Model card ](https://huggingface.co/jinaai/ReaderLM-v2)[ Files Files and versions ](https://huggingface.co/jinaai/ReaderLM-v2/tree/main)[ Community 12 ](https://huggingface.co/jinaai/ReaderLM-v2/discussions)\nTrain \nDeploy \nUse this model \n  * [ReaderLM-v2](https://huggingface.co/jinaai/ReaderLM-v2/#readerlm-v2 \"ReaderLM-v2\")\n    * [What's New in `ReaderLM-v2`](https://huggingface.co/jinaai/ReaderLM-v2/#whats-new-in-readerlm-v2 \"What&#39;s New in <code>ReaderLM-v2</code>\")\n    * [Model Overview](https://huggingface.co/jinaai/ReaderLM-v2/#model-overview \"Model Overview\")\n  * [Usage](https://huggingface.co/jinaai/ReaderLM-v2/#usage \"Usage\")\n    * [Via Reader API](https://huggingface.co/jinaai/ReaderLM-v2/#via-reader-api \"Via Reader API\")\n    * [On Google Colab](https://huggingface.co/jinaai/ReaderLM-v2/#on-google-colab \"On Google Colab\")\n    * [Local Usage](https://huggingface.co/jinaai/ReaderLM-v2/#local-usage \"Local Usage\")\n      * [HTML to Markdown Example](https://huggingface.co/jinaai/ReaderLM-v2/#html-to-markdown-example \"HTML to Markdown Example\")\n      * [HTML to JSON Example](https://huggingface.co/jinaai/ReaderLM-v2/#html-to-json-example \"HTML to JSON Example\")\n    * [Model Performance](https://huggingface.co/jinaai/ReaderLM-v2/#model-performance \"Model Performance\")\n      * [Quantitative Evaluation](https://huggingface.co/jinaai/ReaderLM-v2/#quantitative-evaluation \"Quantitative Evaluation\")\n      * [Qualitative Evaluation](https://huggingface.co/jinaai/ReaderLM-v2/#qualitative-evaluation \"Qualitative Evaluation\")\n    * [Training Details](https://huggingface.co/jinaai/ReaderLM-v2/#training-details \"Training Details\")\n\n\n![Jina AI: Your Search Foundation, Supercharged!](https://huggingface.co/datasets/jinaai/documentation-images/resolve/main/logo.webp)\n**Trained by[**Jina AI**](https://jina.ai/).**\n[Blog](https://jina.ai/news/readerlm-v2-frontier-small-language-model-for-html-to-markdown-and-json) | [API](https://jina.ai/reader) | [Colab](https://colab.research.google.com/drive/1FfPjZwkMSocOLsEYH45B3B4NxDryKLGI?usp=sharing) | [AWS](https://aws.amazon.com/marketplace/pp/prodview-jwfct4j4rvxk2?sr=0-21&ref_=beagle&applicationId=AWSMPContessa) | [Azure](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/jinaai.reader-lm-v2-vm)| [Arxiv](https://arxiv.org/abs/2503.01151)\n#  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#readerlm-v2) ReaderLM-v2 \n`ReaderLM-v2` is a 1.5B parameter language model that converts raw HTML into beautifully formatted markdown or JSON with superior accuracy and improved longer context handling. Supporting multiple languages (29 in total), `ReaderLM-v2` is specialized for tasks involving HTML parsing, transformation, and text extraction.\n##  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#whats-new-in-readerlm-v2) What's New in `ReaderLM-v2`\n`ReaderLM-v2` represents a significant leap forward from its predecessor, with several key improvements:\n  * **Better Markdown Generation** : Thanks to its new training paradigm and higher-quality training data, the model excels at generating complex elements like code fences, nested lists, tables, and LaTeX equations.\n  * **JSON Output** : Introduces direct HTML-to-JSON generation using predefined schemas, eliminating the need for intermediate markdown conversion.\n  * **Longer Context Handling** : Handles up to 512K tokens combined input and output length, with improved performance on long-form content.\n  * **Multilingual Support** : Comprehensive support across 29 languages for broader applications.\n  * **Enhanced Stability** : Greatly alleviates degeneration issues after generating long sequences through contrastive loss during training.\n\n\n##  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#model-overview) Model Overview \n  * **Model Type** : Autoregressive, decoder-only transformer\n  * **Parameter Count** : 1.54B\n  * **Context Window** : Up to 512K tokens (combined input and output)\n  * **Hidden Size** : 1536\n  * **Number of Layers** : 28\n  * **Query Heads** : 12\n  * **KV Heads** : 2\n  * **Head Size** : 128\n  * **Intermediate Size** : 8960\n  * **Supported Languages** : English, Chinese, Japanese, Korean, French, Spanish, Portuguese, German, Italian, Russian, Vietnamese, Thai, Arabic, and more (29 total)\n\n\n#  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#usage) Usage \nBelow, you will find instructions and examples for using `ReaderLM-v2` locally using the Hugging Face Transformers library. For a more hands-on experience in a hosted environment, see the [Google Colab Notebook](https://colab.research.google.com/drive/1FfPjZwkMSocOLsEYH45B3B4NxDryKLGI?usp=sharing).\n##  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#via-reader-api) Via Reader API \n`ReaderLM-v2` is now fully integrated with [Reader API](https://jina.ai/reader/). To use it, simply specify `x-engine: readerlm-v2` in your request headers and enable response streaming with `-H 'Accept: text/event-stream'`:\n```\ncurl https://r.jina.ai/https://news.ycombinator.com/ -H 'x-engine: readerlm-v2' -H 'Accept: text/event-stream'\n\n```\n\nYou can try it without an API key at a lower rate limit. For higher rate limits, you can purchase an API key. Please note that ReaderLM-v2 requests consume 3x the normal token count from your API key allocation. This is currently an experimental feature, and we're working with the GCP team to improve GPU efficiency.\n##  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#on-google-colab) On Google Colab \nYou can try `ReaderLM-v2` via our [Colab notebook](https://colab.research.google.com/drive/1FfPjZwkMSocOLsEYH45B3B4NxDryKLGI?usp=sharing), which demonstrates HTML-to-markdown conversion, JSON extraction, and instruction-following using the HackerNews frontpage as an example. The notebook is optimized for Colab's free T4 GPU tier and requires `vllm` and `triton` for acceleration and running.\nNote that the free T4 GPU has limitations—it doesn't support bfloat16 or flash attention 2, leading to higher memory usage and slower processing of longer inputs. Nevertheless, ReaderLM-v2 successfully processes large documents under these constraints, achieving processing speeds of 67 tokens/s input and 36 tokens/s output. For production use, we recommend an RTX 3090/4090 for optimal performance.\n##  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#local-usage) Local Usage \nTo use `ReaderLM-v2` locally:\n  1. Install the necessary dependencies:\n```\npip install transformers\n\n```\n\n  2. Load and run the model:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # or \"cpu\"\ntokenizer = AutoTokenizer.from_pretrained(\"jinaai/ReaderLM-v2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"jinaai/ReaderLM-v2\").to(device)\n\n```\n\n  3. (Optional) Pre-clean your HTML to remove scripts, styles, comments, to reduce the noise and length of the input:\n```\nimport re\n# Patterns\nSCRIPT_PATTERN = r\"<[ ]*script.*?\\/[ ]*script[ ]*>\"\nSTYLE_PATTERN = r\"<[ ]*style.*?\\/[ ]*style[ ]*>\"\nMETA_PATTERN = r\"<[ ]*meta.*?>\"\nCOMMENT_PATTERN = r\"<[ ]*!--.*?--[ ]*>\"\nLINK_PATTERN = r\"<[ ]*link.*?>\"\nBASE64_IMG_PATTERN = r'<img[^>]+src=\"data:image/[^;]+;base64,[^\"]+\"[^>]*>'\nSVG_PATTERN = r\"(<svg[^>]*>)(.*?)(<\\/svg>)\"\n\ndef replace_svg(html: str, new_content: str = \"this is a placeholder\") -> str:\n  return re.sub(\n    SVG_PATTERN,\n    lambda match: f\"{match.group(1)}{new_content}{match.group(3)}\",\n    html,\n    flags=re.DOTALL,\n  )\n\ndef replace_base64_images(html: str, new_image_src: str = \"#\") -> str:\n  return re.sub(BASE64_IMG_PATTERN, f'<img src=\"{new_image_src}\"/>', html)\n\ndef clean_html(html: str, clean_svg: bool = False, clean_base64: bool = False):\n  html = re.sub(\n    SCRIPT_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n  )\n  html = re.sub(\n    STYLE_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n  )\n  html = re.sub(\n    META_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n  )\n  html = re.sub(\n    COMMENT_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n  )\n  html = re.sub(\n    LINK_PATTERN, \"\", html, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n  )\n  if clean_svg:\n    html = replace_svg(html)\n  if clean_base64:\n    html = replace_base64_images(html)\n  return html\n\n```\n\n  4. Create a prompt for the model:\n```\ndef create_prompt(\n  text: str, tokenizer=None, instruction: str = None, schema: str = None\n) -> str:\n  \"\"\"\n  Create a prompt for the model with optional instruction and JSON schema.\n  \"\"\"\n  if not instruction:\n    instruction = \"Extract the main content from the given HTML and convert it to Markdown format.\"\n  if schema:\n    instruction = \"Extract the specified information from a list of news threads and present it in a structured JSON format.\"\n    prompt = f\"{instruction}\\n```html\\n{text}\\n```\\nThe JSON schema is as follows:```json\\n{schema}\\n```\"\n  else:\n    prompt = f\"{instruction}\\n```html\\n{text}\\n```\"\n  messages = [\n    {\n      \"role\": \"user\",\n      \"content\": prompt,\n    }\n  ]\n  return tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n  )\n\n```\n\n\n\n###  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#html-to-markdown-example) HTML to Markdown Example \n```\nhtml = \"<html><body><h1>Hello, world!</h1></body></html>\"\nhtml = clean_html(html)\ninput_prompt = create_prompt(html, tokenizer=tokenizer)\ninputs = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(device)\noutputs = model.generate(\n  inputs, max_new_tokens=1024, temperature=0, do_sample=False, repetition_penalty=1.08\n)\nprint(tokenizer.decode(outputs[0]))\n\n```\n\n###  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#html-to-json-example) HTML to JSON Example \n```\nschema = \"\"\"\n{\n \"type\": \"object\",\n \"properties\": {\n  \"title\": {\n   \"type\": \"string\"\n  },\n  \"author\": {\n   \"type\": \"string\"\n  },\n  \"date\": {\n   \"type\": \"string\"\n  },\n  \"content\": {\n   \"type\": \"string\"\n  }\n },\n \"required\": [\"title\", \"author\", \"date\", \"content\"]\n}\n\"\"\"\nhtml = clean_html(html)\ninput_prompt = create_prompt(html, tokenizer=tokenizer, schema=schema)\ninputs = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(device)\noutputs = model.generate(\n  inputs, max_new_tokens=1024, temperature=0, do_sample=False, repetition_penalty=1.08\n)\nprint(tokenizer.decode(outputs[0]))\n\n```\n\n##  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#model-performance) Model Performance \nReaderLM-v2 has been extensively evaluated on various tasks:\n###  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#quantitative-evaluation) Quantitative Evaluation \nFor HTML-to-Markdown tasks, the model outperforms much larger models like Qwen2.5-32B-Instruct and Gemini2-flash-expr, achieving:\n  * ROUGE-L: 0.84\n  * Levenshtein Distance: 0.22\n  * Jaro-Winkler Similarity: 0.82\n\n\nFor HTML-to-JSON tasks, it shows competitive performance with:\n  * F1 Score: 0.81\n  * Precision: 0.82\n  * Recall: 0.81\n  * Pass-Rate: 0.98\n\n\n###  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#qualitative-evaluation) Qualitative Evaluation \nThe model excels in three key dimensions:\n  * Content Integrity: 39/50\n  * Structural Accuracy: 35/50\n  * Format Compliance: 36/50\n\n\nThese scores demonstrate strong performance in preserving semantic information, maintaining structural accuracy, and adhering to markdown syntax standards.\n##  [ ](https://huggingface.co/jinaai/ReaderLM-v2/#training-details) Training Details \nReaderLM-v2 is built on Qwen2.5-1.5B-Instruction and trained using a sophisticated pipeline:\n  1. Data Preparation: Created html-markdown-1m dataset with 1 million HTML documents\n  2. Synthetic Data Generation: Three-step pipeline using Qwen2.5-32B-Instruction\n     * Drafting: Initial markdown and JSON generation\n     * Refinement: Content cleanup and structure alignment\n     * Critique: Quality evaluation and filtering\n  3. Training Process:\n     * Long-context pretraining\n     * Supervised fine-tuning\n     * Direct preference optimization\n     * Self-play reinforcement tuning\n\n\n\nDownloads last month\n    40,842 \nSafetensors[](https://huggingface.co/docs/safetensors)\nModel size\n1.54B params\nTensor type\nBF16 \n·\nInference Providers [NEW](https://huggingface.co/jinaai/ReaderLM-v2/?inference_api=true)\nHF Inference API\n[ Text Generation](https://huggingface.co/tasks/text-generation \"Learn more about text-generation\")\nInput a message to start chatting with **jinaai/ReaderLM-v2**.\nSend\nHF Inference deployability: The model authors have turned it off explicitly.\nView Code\n[ Open Playground](https://huggingface.co/playground?modelId=jinaai/ReaderLM-v2&provider=hf-inference)\n##  Model tree for jinaai/ReaderLM-v2 [](https://huggingface.co/docs/hub/model-cards#specifying-a-base-model)\nFinetunes\n[1 model](https://huggingface.co/models?other=base_model:finetune:jinaai/ReaderLM-v2)\nMerges\n[4 models](https://huggingface.co/models?other=base_model:merge:jinaai/ReaderLM-v2)\nQuantizations\n[30 models](https://huggingface.co/models?other=base_model:quantized:jinaai/ReaderLM-v2)\n##  Spaces using jinaai/ReaderLM-v2 9\n[💻 KBaba7/Quant](https://huggingface.co/spaces/KBaba7/Quant)[🏃 bhaskartripathi/LLM_Quantization](https://huggingface.co/spaces/bhaskartripathi/LLM_Quantization)[💻 FallnAI/Quantize-HF-Models](https://huggingface.co/spaces/FallnAI/Quantize-HF-Models)[🌍 Felguk/ReaderLM-v2](https://huggingface.co/spaces/Felguk/ReaderLM-v2)[🏃 K00B404/LLM_Quantization](https://huggingface.co/spaces/K00B404/LLM_Quantization)[💻 totolook/Quant](https://huggingface.co/spaces/totolook/Quant)[🔥 ruslanmv/convert_to_gguf](https://huggingface.co/spaces/ruslanmv/convert_to_gguf)[⚡ Nymbo/Markdown-Studio](https://huggingface.co/spaces/Nymbo/Markdown-Studio)[📚 kevcx2/jinaai-ReaderLM-v2](https://huggingface.co/spaces/kevcx2/jinaai-ReaderLM-v2) + 4 Spaces\n##  Collection including jinaai/ReaderLM-v2\n#### [Jina Reader-LM Collection  Convert HTML content to LLM-friendly Markdown/JSON content •  3 items •  Updated Jan 16 • 10](https://huggingface.co/collections/jinaai/jina-reader-lm-670628bfe0d782685cb53416)\nSystem theme \nCompany\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/) [](https://huggingface.co/)\nWebsite\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://huggingface.co/",
        "https://huggingface.co/models",
        "https://huggingface.co/datasets",
        "https://huggingface.co/spaces",
        "https://huggingface.co/posts",
        "https://huggingface.co/docs",
        "https://huggingface.co/enterprise",
        "https://huggingface.co/pricing",
        "https://huggingface.co/login",
        "https://huggingface.co/join",
        "https://huggingface.co/jinaai",
        "https://huggingface.co/jinaai/ReaderLM-v2",
        "https://huggingface.co/models?pipeline_tag=text-generation",
        "https://huggingface.co/models?library=transformers",
        "https://huggingface.co/models?library=onnx",
        "https://huggingface.co/models?library=safetensors",
        "https://huggingface.co/models?language=multilingual",
        "https://huggingface.co/models?other=qwen2",
        "https://huggingface.co/models?other=conversational",
        "https://huggingface.co/models?other=text-generation-inference",
        "https://huggingface.co/models?other=region%3Aeu",
        "https://huggingface.co/jinaai/ReaderLM-v2/tree/main",
        "https://huggingface.co/jinaai/ReaderLM-v2/discussions",
        "https://huggingface.co/jinaai/ReaderLM-v2/#readerlm-v2",
        "https://huggingface.co/jinaai/ReaderLM-v2/#whats-new-in-readerlm-v2",
        "https://huggingface.co/jinaai/ReaderLM-v2/#model-overview",
        "https://huggingface.co/jinaai/ReaderLM-v2/#usage",
        "https://huggingface.co/jinaai/ReaderLM-v2/#via-reader-api",
        "https://huggingface.co/jinaai/ReaderLM-v2/#on-google-colab",
        "https://huggingface.co/jinaai/ReaderLM-v2/#local-usage",
        "https://huggingface.co/jinaai/ReaderLM-v2/#html-to-markdown-example",
        "https://huggingface.co/jinaai/ReaderLM-v2/#html-to-json-example",
        "https://huggingface.co/jinaai/ReaderLM-v2/#model-performance",
        "https://huggingface.co/jinaai/ReaderLM-v2/#quantitative-evaluation",
        "https://huggingface.co/jinaai/ReaderLM-v2/#qualitative-evaluation",
        "https://huggingface.co/jinaai/ReaderLM-v2/#training-details",
        "https://huggingface.co/docs/safetensors",
        "https://huggingface.co/jinaai/ReaderLM-v2/?inference_api=true",
        "https://huggingface.co/tasks/text-generation",
        "https://huggingface.co/playground?modelId=jinaai/ReaderLM-v2&provider=hf-inference",
        "https://huggingface.co/docs/hub/model-cards#specifying-a-base-model",
        "https://huggingface.co/models?other=base_model:finetune:jinaai/ReaderLM-v2",
        "https://huggingface.co/models?other=base_model:merge:jinaai/ReaderLM-v2",
        "https://huggingface.co/models?other=base_model:quantized:jinaai/ReaderLM-v2",
        "https://huggingface.co/spaces/KBaba7/Quant",
        "https://huggingface.co/spaces/bhaskartripathi/LLM_Quantization",
        "https://huggingface.co/spaces/FallnAI/Quantize-HF-Models",
        "https://huggingface.co/spaces/Felguk/ReaderLM-v2",
        "https://huggingface.co/spaces/K00B404/LLM_Quantization",
        "https://huggingface.co/spaces/totolook/Quant",
        "https://huggingface.co/spaces/ruslanmv/convert_to_gguf",
        "https://huggingface.co/spaces/Nymbo/Markdown-Studio",
        "https://huggingface.co/spaces/kevcx2/jinaai-ReaderLM-v2",
        "https://huggingface.co/collections/jinaai/jina-reader-lm-670628bfe0d782685cb53416",
        "https://huggingface.co/terms-of-service",
        "https://huggingface.co/privacy",
        "https://huggingface.co/huggingface",
        "https://jina.ai/",
        "https://jina.ai/news/readerlm-v2-frontier-small-language-model-for-html-to-markdown-and-json",
        "https://jina.ai/reader",
        "https://colab.research.google.com/drive/1FfPjZwkMSocOLsEYH45B3B4NxDryKLGI?usp=sharing",
        "https://aws.amazon.com/marketplace/pp/prodview-jwfct4j4rvxk2?sr=0-21&ref_=beagle&applicationId=AWSMPContessa",
        "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/jinaai.reader-lm-v2-vm",
        "https://arxiv.org/abs/2503.01151",
        "https://jina.ai/reader/",
        "https://apply.workable.com/huggingface/"
    ]
}