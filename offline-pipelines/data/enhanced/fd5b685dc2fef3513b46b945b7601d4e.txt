[Skip to content](https://towardsdatascience.com/how-to-build-a-graph-rag-app-b323fc33ba06/#wp--skip-link--target)
[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)
The world’s leading publication for data science, AI, and ML professionals.
Sign in
Sign out
[Contributor Portal](https://contributor.insightmediagroup.io/)
  * [Latest](https://towardsdatascience.com/latest/)
  * [Editor’s Picks](https://towardsdatascience.com/tag/editors-pick/)
  * [Deep Dives](https://towardsdatascience.com/tag/deep-dives/)
  * [Contribute](https://towardsdatascience.com/questions-96667b06af5/)
  * [Newsletter](https://newsletter.towardsdatascience.com/subscription-to-the-newsletter)
[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)


Toggle Mobile Navigation
  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)
  * [X](https://x.com/TDataScience)


Toggle Search
Search
# How to Build a Graph RAG App
Using knowledge graphs and AI to retrieve, filter, and summarize medical journal articles 
[Steve Hedden](https://towardsdatascience.com/author/steve-hedden/)
Dec 30, 2024
30 min read
Share 
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_fsYI8Riyq2lAsH8wTCuKhw.webp)Image by Author
_The accompanying code for the app and notebook are_[ _here._](https://github.com/SteveHedden/kg_llm/tree/main/graphRAGapp)
Knowledge graphs (KGs) and Large Language Models (LLMs) are a match made in heaven. My [previous](https://medium.com/towards-data-science/how-to-implement-knowledge-graphs-and-large-language-models-llms-together-at-the-enterprise-level-cf2835475c47) [posts](https://medium.com/towards-data-science/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759) discuss the complementarities of these two technologies in more detail but the short version is, “some of the main weaknesses of LLMs, that they are black-box models and struggle with factual knowledge, are some of KGs’ greatest strengths. KGs are, essentially, collections of facts, and they are fully interpretable.”
This article is all about building a simple Graph RAG app. What is RAG? RAG, or Retrieval-Augmented Generation, is about **retrieving** relevant information to **augment** a prompt that is sent to an LLM, which **generates** a response. Graph RAG is RAG that uses a knowledge graph as part of the retrieval portion. If you’ve never heard of Graph RAG, or want a refresher, I’d watch [this video](https://www.youtube.com/watch?v=knDDGYHnnSI).
The basic idea is that, rather than sending your prompt directly to an LLM, which was not trained on your data, you can supplement your prompt with the relevant information needed for the LLM to answer your prompt accurately. The example I use often is copying a job description and my resume into ChatGPT to write a cover letter. The LLM is able to provide a much more relevant response to my prompt, ‘write me a cover letter,’ if I give it my resume and the description of the job I am applying for. Since knowledge graphs are built to store knowledge, they are a perfect way to store internal data and supplement LLM prompts with additional context, improving the accuracy and contextual understanding of the responses.
This technology has many, many, applications such [customer service bots](https://arxiv.org/pdf/2404.17723), [drug](https://academic.oup.com/bioinformatics/article/40/6/btae353/7687047) [discovery](https://blog.biostrand.ai/integrating-knowledge-graphs-and-large-language-models-for-next-generation-drug-discovery), [automated regulatory report generation in life sciences](https://www.weave.bio/), [talent acquisition and management for HR](https://beamery.com/resources/news/beamery-announces-talentgpt-the-world-s-first-generative-ai-for-hr), [legal research and writing](https://legal.thomsonreuters.com/blog/retrieval-augmented-generation-in-legal-tech/), and [wealth advisor assistants](https://www.cnbc.com/amp/2023/03/14/morgan-stanley-testing-openai-powered-chatbot-for-its-financial-advisors.html). Because of the wide applicability and the potential to improve the performance of LLM tools, Graph RAG (that’s the term I’ll use here) has been blowing up in popularity. Here is a graph showing interest over time based on Google searches.
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_vuRkEjW9AcQPld5PsqWjkg.png)Source: <https://trends.google.com/>
Graph RAG has experienced a surge in search interest, even surpassing terms like knowledge graphs and retrieval-augmented generation. Note that Google Trends measures  _relative_ search interest, not absolute number of searches. The spike in July 2024 for searches of Graph RAG coincides with the week Microsoft [announced](https://www.microsoft.com/en-us/research/blog/graphrag-new-tool-for-complex-data-discovery-now-on-github/) that their GraphRAG application would be available on [GitHub](https://github.com/microsoft/graphrag).
The excitement around Graph RAG is broader than just Microsoft, however. Samsung acquired RDFox, a knowledge graph company, in July of 2024. The [article announcing that acquisition](https://news.samsung.com/global/samsung-electronics-announces-acquisition-of-oxford-semantic-technologies-uk-based-knowledge-graph-startup) did not mention Graph RAG explicitly, but in [this article in Forbes](https://www.forbes.com/sites/zakdoffman/2024/11/09/samsung-confirms-new-upgrade-choice-millions-of-galaxy-owners-must-now-decide/) published in November 2024, a Samsung spokesperson stated, “We plan to develop knowledge graph technology, one of the main technologies of personalized AI, and organically connect with generated AI to support user-specific services.”
In October 2024, Ontotext, a leading graph database company, and Semantic Web company, the maker of PoolParty, a knowledge graph curation platform, merged to form [Graphwise](https://graphwise.ai/). According to [the press release](https://www.prnewswire.com/news-releases/semantic-web-company-and-ontotext-merge-to-create-knowledge-graph-and-ai-powerhouse-graphwise-302283427.html?utm_source=chatgpt.com), the merger aims to “democratize the evolution of Graph RAG as a category.”
While some of the buzz around Graph RAG may come from the broader excitement surrounding chatbots and generative AI, it reflects a genuine evolution in how knowledge graphs are being applied to solve complex, real-world problems. One example is that LinkedIn [applied Graph RAG](https://arxiv.org/pdf/2404.17723) to improve their customer service technical support. Because the tool was able to retrieve the relevant data (like previously solved similar tickets or questions) to feed the LLM, the responses were more accurate and the mean resolution time dropped from 40 hours to 15 hours.
This post will go through the construction of a pretty simple, but I think illustrative, example of how Graph RAG can work in practice. The end result is an app that a non-technical user can interact with. Like my last post, I will use a dataset consisting of medical journal articles from PubMed. The idea is that this is an app that someone in the medical field could use to do literature review. The same principles can be applied to many use cases however, which is why Graph RAG is so exciting.
The structure of the app, along with this post is as follows:
Step zero is preparing the data. I will explain the details below but the overall goal is to vectorize the raw data and, separately, turn it into an RDF graph. As long as we keep URIs tied to the articles before we vectorize, we can navigate across a graph of articles and a vector space of articles. Then, we can:
  1. **Search Articles:** use the power of the vector database to do an initial search of relevant articles given a search term. I will use vector similarity to retrieve articles with the most similar vectors to that of the search term.
  2. **Refine Terms:** explore the [Medical Subject Headings (MeSH) biomedical vocabulary](https://id.nlm.nih.gov/mesh/) to select terms to use to filter the articles from step 1. This controlled vocabulary contains medical terms, alternative names, narrower concepts, and many other properties and relationships.
  3. **Filter & Summarize: **use the MeSH terms to filter the articles to avoid ‘context poisoning’. Then send the remaining articles to an LLM along with an additional prompt like, “summarize in bullets.”


Some notes on this app and tutorial before we get started:
  * This set-up uses knowledge graphs exclusively for metadata. This is only possible because each article in my dataset has already been tagged with terms that are part of a rich controlled vocabulary. I am using the graph for structure and semantics and the vector database for similarity-based retrieval, ensuring each technology is used for what it does best. Vector similarity can tell us “esophageal cancer” is semantically similar to “mouth cancer”, but knowledge graphs can tell us the details of the relationship between “esophageal cancer” and “mouth cancer.”
  * The data I used for this app is a collection of medical journal articles from PubMed (more on the data below). I chose this dataset because it is structured (tabular) but also contains text in the form of abstracts for each article, and because it is already tagged with topical terms that are aligned with a well-established controlled vocabulary (MeSH). Because these are medical articles, I have called this app ‘Graph RAG for Medicine.’ But this same structure can be applied to any domain and is not specific to the medical field.
  * What I hope this tutorial and app demonstrate is that you can improve the results of your RAG application in terms of accuracy and explainability by incorporating a knowledge graph into the retrieval step. I will show how KGs can improve the accuracy of RAG applications in two ways: by giving the user a way of filtering the context to ensure the LLM is only being fed the most relevant information; and by using domain specific controlled vocabularies with dense relationships that are maintained and curated by domain experts to do the filtering.
  * What this tutorial and app don’t directly showcase are two other significant ways KGs can enhance RAG applications: governance, access control, and regulatory compliance; and efficiency and scalability. For governance, KGs can do more than filter content for relevancy to improve accuracy — they can enforce data governance policies. For instance, if a user lacks permission to access certain content, that content can be excluded from their RAG pipeline. On the efficiency and scalability side, KGs can help ensure RAG applications don’t die on the shelf. While it’s easy to create an impressive one-off RAG app (that’s literally the purpose of this tutorial), many companies struggle with a proliferation of disconnected POCs that lack a cohesive framework, structure, or platform. That means many of those apps are not going to survive long. A metadata layer powered by KGs can break down data silos, providing the foundation needed to build, scale, and maintain RAG applications effectively. Using a rich controlled vocabulary like MeSH for the metadata tags on these articles is a way of ensuring this Graph RAG app can be integrated with other systems and reducing the risk that it becomes a silo.


# Step 0: Prepare the data
_The code to prepare the data is in_[ _this_](https://github.com/SteveHedden/kg_llm/blob/main/graphRAGapp/VectorVsKG_updated.ipynb) _notebook._
As mentioned, I’ve again decided to use [this](https://www.kaggle.com/datasets/owaiskhan9654/pubmed-multilabel-text-classification) dataset of 50,000 research articles from the PubMed repository (License [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)). This dataset contains the title of the articles, their abstracts, as well as a field for metadata tags. These tags are from the Medical Subject Headings (MeSH) controlled vocabulary thesaurus. The PubMed articles are really just metadata on the articles — there are abstracts for each article but we don’t have the full text. The data is already in tabular format and tagged with MeSH terms.
We can vectorize this tabular dataset directly. We could turn it into a graph (RDF) before we vectorize, but I didn’t do that for this app and I don’t know that it would help the final results for this kind of data. The most important thing about vectorizing the raw data is that we add [Unique Resource Identifiers](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier) (URIs) to each article first. A URI is a unique ID for navigating RDF data and it is necessary for us to go back and forth between vectors and entities in our graph. Additionally, we will create a separate collection in our vector database for the MeSH terms. This will allow the user to search for relevant terms without having prior knowledge of this controlled vocabulary. Below is a diagram of what we are doing to prepare our data.
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_RiYNNyxGIEGClobHedTnuQ.png)Image by Author
We have two collections in our vector database to query: articles and terms. We also have the data represented as a graph in RDF format. Since MeSH has an API, I am just going to query the API directly to get alternative names and narrower concepts for terms.
## Vectorize data in Weaviate
First import the required packages and set up the Weaviate client:
```
import weaviate<br>from weaviate.util import generate_uuid5<br>from weaviate.classes.init import Auth<br>import os<br>import json<br>import pandas as pd<br><br>client = weaviate.connect_to_weaviate_cloud(<br>  cluster_url="XXX", # Replace with your Weaviate Cloud URL<br>  auth_credentials=Auth.api_key("XXX"), # Replace with your Weaviate Cloud key<br>  headers={'X-OpenAI-Api-key': "XXX"} # Replace with your OpenAI API key<br>)
```

Read in the PubMed journal articles. I am using [Databricks](https://www.databricks.com/) to run this notebook so you may need to change this, depending on where you run it. The goal here is just to get the data into a pandas DataFrame.
```
df = spark.sql("SELECT * FROM workspace.default.pub_med_multi_label_text_classification_dataset_processed").toPandas()
```

If you’re running this locally, just do:
```
df = pd.read_csv("PubMed Multi Label Text Classification Dataset Processed.csv")
```

Then clean the data up a bit:
```
import numpy as np<br># Replace infinity values with NaN and then fill NaN values<br>df.replace([np.inf, -np.inf], np.nan, inplace=True)<br>df.fillna('', inplace=True)<br><br># Convert columns to string type<br>df['Title'] = df['Title'].astype(str)<br>df['abstractText'] = df['abstractText'].astype(str)<br>df['meshMajor'] = df['meshMajor'].astype(str)
```

Now we need to create a URI for each article and add that in as a new column. This is important because the URI is the way we can connect the vector representation of an article with the knowledge graph representation of the article.
```
import urllib.parse<br>from rdflib import Graph, RDF, RDFS, Namespace, URIRef, Literal<br><br><br># Function to create a valid URI<br>def create_valid_uri(base_uri, text):<br>  if pd.isna(text):<br>    return None<br>  # Encode text to be used in URI<br>  sanitized_text = urllib.parse.quote(text.strip().replace(' ', '_').replace('"', '').replace('<', '').replace('>', '').replace("'", "_"))<br>  return URIRef(f"{base_uri}/{sanitized_text}")<br><br><br># Function to create a valid URI for Articles<br>def create_article_uri(title, base_namespace="http://example.org/article/"):<br>  """<br>  Creates a URI for an article by replacing non-word characters with underscores and URL-encoding.<br><br>  Args:<br>    title (str): The title of the article.<br>    base_namespace (str): The base namespace for the article URI.<br><br>  Returns:<br>    URIRef: The formatted article URI.<br>  """<br>  if pd.isna(title):<br>    return None<br>  # Replace non-word characters with underscores<br>  sanitized_title = re.sub(r'\W+', '_', title.strip())<br>  # Condense multiple underscores into a single underscore<br>  sanitized_title = re.sub(r'_+', '_', sanitized_title)<br>  # URL-encode the term<br>  encoded_title = quote(sanitized_title)<br>  # Concatenate with base_namespace without adding underscores<br>  uri = f"{base_namespace}{encoded_title}"<br>  return URIRef(uri)<br><br># Add a new column to the DataFrame for the article URIs<br>df['Article_URI'] = df['Title'].apply(lambda title: create_valid_uri("http://example.org/article", title))
```

We also want to create a DataFrame of all of the MeSH terms that are used to tag the articles. This will be helpful later when we want to search for similar MeSH terms.
```
# Function to clean and parse MeSH terms<br>def parse_mesh_terms(mesh_list):<br>  if pd.isna(mesh_list):<br>    return []<br>  return [<br>    term.strip().replace(' ', '_')<br>    for term in mesh_list.strip("[]'").split(',')<br>  ]<br><br># Function to create a valid URI for MeSH terms<br>def create_valid_uri(base_uri, text):<br>  if pd.isna(text):<br>    return None<br>  sanitized_text = urllib.parse.quote(<br>    text.strip()<br>    .replace(' ', '_')<br>    .replace('"', '')<br>    .replace('<', '')<br>    .replace('>', '')<br>    .replace("'", "_")<br>  )<br>  return f"{base_uri}/{sanitized_text}"<br><br># Extract and process all MeSH terms<br>all_mesh_terms = []<br>for mesh_list in df["meshMajor"]:<br>  all_mesh_terms.extend(parse_mesh_terms(mesh_list))<br><br># Deduplicate terms<br>unique_mesh_terms = list(set(all_mesh_terms))<br><br># Create a DataFrame of MeSH terms and their URIs<br>mesh_df = pd.DataFrame({<br>  "meshTerm": unique_mesh_terms,<br>  "URI": [create_valid_uri("http://example.org/mesh", term) for term in unique_mesh_terms]<br>})<br><br># Display the DataFrame<br>print(mesh_df)
```

Vectorize the articles DataFrame:
```
from weaviate.classes.config import Configure<br><br><br>#define the collection<br>articles = client.collections.create(<br>  name = "Article",<br>  vectorizer_config=Configure.Vectorizer.text2vec_openai(), # If set to "none" you must always provide vectors yourself. Could be any other "text2vec-*" also.<br>  generative_config=Configure.Generative.openai(), # Ensure the `generative-openai` module is used for generative queries<br>)<br><br>#add ojects<br>articles = client.collections.get("Article")<br><br>with articles.batch.dynamic() as batch:<br>  for index, row in df.iterrows():<br>    batch.add_object({<br>      "title": row["Title"],<br>      "abstractText": row["abstractText"],<br>      "Article_URI": row["Article_URI"],<br>      "meshMajor": row["meshMajor"],<br>    })
```

Now vectorize the MeSH terms:
```
#define the collection<br>terms = client.collections.create(<br>  name = "term",<br>  vectorizer_config=Configure.Vectorizer.text2vec_openai(), # If set to "none" you must always provide vectors yourself. Could be any other "text2vec-*" also.<br>  generative_config=Configure.Generative.openai(), # Ensure the `generative-openai` module is used for generative queries<br>)<br><br>#add ojects<br>terms = client.collections.get("term")<br><br>with terms.batch.dynamic() as batch:<br>  for index, row in mesh_df.iterrows():<br>    batch.add_object({<br>      "meshTerm": row["meshTerm"],<br>      "URI": row["URI"],<br>    })
```

You can, at this point, run semantic search, similarity search, and RAG directly against the vectorized dataset. I won’t go through all of that here but you can look at the code in my [accompanying notebook](https://github.com/SteveHedden/kg_llm/tree/main/graphRAGapp) to do that.
## Turn data into a knowledge graph
I am just using the same code we used in the [last post](https://medium.com/towards-data-science/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759) to do this. We are basically turning every row in the data into an “Article” entity in our KG. Then we are giving each of these articles properties for title, abstract, and MeSH terms. We are also turning every MeSH term into an entity as well. This code also adds random dates to each article for a property called date published and a random number between 1 and 10 to a property called access. We won’t use those properties in this demo. Below is a visual representation of the graph we are creating from the data.
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_R1NIm9fLYdKFTsXzZmH33w.png)
Here is how to iterate through the DataFrame and turn it into RDF data:
```
from rdflib import Graph, RDF, RDFS, Namespace, URIRef, Literal<br>from rdflib.namespace import SKOS, XSD<br>import pandas as pd<br>import urllib.parse<br>import random<br>from datetime import datetime, timedelta<br>import re<br>from urllib.parse import quote<br><br># --- Initialization ---<br>g = Graph()<br><br># Define namespaces<br>schema = Namespace('http://schema.org/')<br>ex = Namespace('http://example.org/')<br>prefixes = {<br>  'schema': schema,<br>  'ex': ex,<br>  'skos': SKOS,<br>  'xsd': XSD<br>}<br>for p, ns in prefixes.items():<br>  g.bind(p, ns)<br><br># Define classes and properties<br>Article = URIRef(ex.Article)<br>MeSHTerm = URIRef(ex.MeSHTerm)<br>g.add((Article, RDF.type, RDFS.Class))<br>g.add((MeSHTerm, RDF.type, RDFS.Class))<br><br>title = URIRef(schema.name)<br>abstract = URIRef(schema.description)<br>date_published = URIRef(schema.datePublished)<br>access = URIRef(ex.access)<br><br>g.add((title, RDF.type, RDF.Property))<br>g.add((abstract, RDF.type, RDF.Property))<br>g.add((date_published, RDF.type, RDF.Property))<br>g.add((access, RDF.type, RDF.Property))<br><br># Function to clean and parse MeSH terms<br>def parse_mesh_terms(mesh_list):<br>  if pd.isna(mesh_list):<br>    return []<br>  return [term.strip() for term in mesh_list.strip("[]'").split(',')]<br><br># Enhanced convert_to_uri function<br>def convert_to_uri(term, base_namespace="http://example.org/mesh/"):<br>  """<br>  Converts a MeSH term into a standardized URI by replacing spaces and special characters with underscores,<br>  ensuring it starts and ends with a single underscore, and URL-encoding the term.<br><br>  Args:<br>    term (str): The MeSH term to convert.<br>    base_namespace (str): The base namespace for the URI.<br><br>  Returns:<br>    URIRef: The formatted URI.<br>  """<br>  if pd.isna(term):<br>    return None # Handle NaN or None terms gracefully<br>  <br>  # Step 1: Strip existing leading and trailing non-word characters (including underscores)<br>  stripped_term = re.sub(r'^\W+|\W+$', '', term)<br>  <br>  # Step 2: Replace non-word characters with underscores (one or more)<br>  formatted_term = re.sub(r'\W+', '_', stripped_term)<br>  <br>  # Step 3: Replace multiple consecutive underscores with a single underscore<br>  formatted_term = re.sub(r'_+', '_', formatted_term)<br>  <br>  # Step 4: URL-encode the term to handle any remaining special characters<br>  encoded_term = quote(formatted_term)<br>  <br>  # Step 5: Add single leading and trailing underscores<br>  term_with_underscores = f"_{encoded_term}_"<br>  <br>  # Step 6: Concatenate with base_namespace without adding an extra underscore<br>  uri = f"{base_namespace}{term_with_underscores}"<br><br>  return URIRef(uri)<br><br># Function to generate a random date within the last 5 years<br>def generate_random_date():<br>  start_date = datetime.now() - timedelta(days=5*365)<br>  random_days = random.randint(0, 5*365)<br>  return start_date + timedelta(days=random_days)<br><br># Function to generate a random access value between 1 and 10<br>def generate_random_access():<br>  return random.randint(1, 10)<br><br># Function to create a valid URI for Articles<br>def create_article_uri(title, base_namespace="http://example.org/article"):<br>  """<br>  Creates a URI for an article by replacing non-word characters with underscores and URL-encoding.<br><br>  Args:<br>    title (str): The title of the article.<br>    base_namespace (str): The base namespace for the article URI.<br><br>  Returns:<br>    URIRef: The formatted article URI.<br>  """<br>  if pd.isna(title):<br>    return None<br>  # Encode text to be used in URI<br>  sanitized_text = urllib.parse.quote(title.strip().replace(' ', '_').replace('"', '').replace('<', '').replace('>', '').replace("'", "_"))<br>  return URIRef(f"{base_namespace}/{sanitized_text}")<br><br># Loop through each row in the DataFrame and create RDF triples<br>for index, row in df.iterrows():<br>  article_uri = create_article_uri(row['Title'])<br>  if article_uri is None:<br>    continue<br>  <br>  # Add Article instance<br>  g.add((article_uri, RDF.type, Article))<br>  g.add((article_uri, title, Literal(row['Title'], datatype=XSD.string)))<br>  g.add((article_uri, abstract, Literal(row['abstractText'], datatype=XSD.string)))<br>  <br>  # Add random datePublished and access<br>  random_date = generate_random_date()<br>  random_access = generate_random_access()<br>  g.add((article_uri, date_published, Literal(random_date.date(), datatype=XSD.date)))<br>  g.add((article_uri, access, Literal(random_access, datatype=XSD.integer)))<br>  <br>  # Add MeSH Terms<br>  mesh_terms = parse_mesh_terms(row['meshMajor'])<br>  for term in mesh_terms:<br>    term_uri = convert_to_uri(term, base_namespace="http://example.org/mesh/")<br>    if term_uri is None:<br>      continue<br>    <br>    # Add MeSH Term instance<br>    g.add((term_uri, RDF.type, MeSHTerm))<br>    g.add((term_uri, RDFS.label, Literal(term.replace('_', ' '), datatype=XSD.string)))<br>    <br>    # Link Article to MeSH Term<br>    g.add((article_uri, schema.about, term_uri))<br><br># Path to save the file<br>file_path = "/Workspace/PubMedGraph.ttl"<br><br># Save the file<br>g.serialize(destination=file_path, format='turtle')<br><br>print(f"File saved at {file_path}")
```

OK, so now we have a vectorized version of the data, and a graph (RDF) version of the data. Each vector has a URI associated with it, which corresponds to an entity in the KG, so we can go back and forth between the data formats.
# Build an app
I decided to use [Streamlit](https://streamlit.io/) to build the interface for this graph RAG app. Similar to the last blog post, I have kept the user flow the same.
  1. **Search Articles:** First, the user searches for articles using a search term. This relies exclusively on the vector database. The user’s search term(s) is sent to the vector database and the ten articles nearest the term in vector space are returned.
  2. **Refine Terms:** Second, the user decides the MeSH terms to use to filter the returned results. Since we also vectorized the MeSH terms, we can have the user enter a natural language prompt to get the most relevant MeSH terms. Then, we allow the user to expand these terms to see their alternative names and narrower concepts. The user can select as many terms as they want for their filter criteria.
  3. **Filter & Summarize: **Third, the user applies the selected terms as filters to the original ten journal articles. We can do this since the PubMed articles are tagged with MeSH terms. Finally, we let the user enter an additional prompt to send to the LLM along with the filtered journal articles. This is the generative step of the RAG app.


Let’s go through these steps one at a time. You can see the full app and code on my GitHub, but here is the structure:
```
-- app.py (a python file that drives the app and calls other functions as needed)<br>-- query_functions (a folder containing python files with queries)<br> -- rdf_queries.py (python file with RDF queries)<br> -- weaviate_queries.py (python file containing weaviate queries)<br>-- PubMedGraph.ttl (the pubmed data in RDF format, stored as a ttl file)
```

## Search Articles
First, want to do is implement Weaviate’s [vector similarity search](https://weaviate.io/developers/weaviate/search/similarity). Since our articles are vectorized, we can send a search term to the vector database and get similar articles back.
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_SkbzqnwLMiDoxavXhSoxyg.png)Image by Author
The main function that searches for relevant journal articles in the vector database is in app.py:
```
# --- TAB 1: Search Articles ---<br>with tab_search:<br>  st.header("Search Articles (Vector Query)")<br>  query_text = st.text_input("Enter your vector search term (e.g., Mouth Neoplasms):", key="vector_search")<br><br>  if st.button("Search Articles", key="search_articles_btn"):<br>    try:<br>      client = initialize_weaviate_client()<br>      article_results = query_weaviate_articles(client, query_text)<br><br>      # Extract URIs here<br>      article_uris = [<br>        result["properties"].get("article_URI")<br>        for result in article_results<br>        if result["properties"].get("article_URI")<br>      ]<br><br>      # Store article_uris in the session state<br>      st.session_state.article_uris = article_uris<br><br>      st.session_state.article_results = [<br>        {<br>          "Title": result["properties"].get("title", "N/A"),<br>          "Abstract": (result["properties"].get("abstractText", "N/A")[:100] + "..."),<br>          "Distance": result["distance"],<br>          "MeSH Terms": ", ".join(<br>            ast.literal_eval(result["properties"].get("meshMajor", "[]"))<br>            if result["properties"].get("meshMajor") else []<br>          ),<br><br>        }<br>        for result in article_results<br>      ]<br>      client.close()<br>    except Exception as e:<br>      st.error(f"Error during article search: {e}")<br><br>  if st.session_state.article_results:<br>    st.write("**Search Results for Articles:**")<br>    st.table(st.session_state.article_results)<br>  else:<br>    st.write("No articles found yet.")
```

This function uses the queries stored in weaviate_queries to establish the Weaviate client (initialize_weaviate_client) and search for articles (query_weaviate_articles). Then we display the returned articles in a table, along with their abstracts, distance (how close they are to the search term), and the MeSH terms that they are tagged with.
The function to query Weaviate in weaviate_queries.py looks like this:
```
# Function to query Weaviate for Articles<br>def query_weaviate_articles(client, query_text, limit=10):<br>  # Perform vector search on Article collection<br>  response = client.collections.get("Article").query.near_text(<br>    query=query_text,<br>    limit=limit,<br>    return_metadata=MetadataQuery(distance=True)<br>  )<br><br>  # Parse response<br>  results = []<br>  for obj in response.objects:<br>    results.append({<br>      "uuid": obj.uuid,<br>      "properties": obj.properties,<br>      "distance": obj.metadata.distance,<br>    })<br>  return results
```

As you can see, I put a limit of ten results here just to make it simpler, but you can change that. This is just using vector similarity search in Weaviate to return relevant results.
The end result in the app looks like this:
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_SghQV-iph3ZQftZiNC6fNg.png)Image by Author
As a demo, I will search the term “treatments for mouth cancer”. As you can see, 10 articles are returned, mostly relevant. This demonstrates both the strengths and weaknesses of vector based retrieval.
The strength is that we can build a semantic search functionality on our data with minimal effort. As you can see above, all we did was set up the client and send the data to a vector database. Once our data has been vectorized, we can do semantic searches, similarity searches, and even RAG. I have put some of that in the notebook accompanying this post, but there’s a lot more in Weaviate’s [official docs](https://weaviate.io/developers/weaviate).
The weakness of vector based retrieval, as I mentioned above are that they are black-box and struggle with factual knowledge. In our example, it looks like most of the articles are about some kind of treatment or therapy for some kind of cancer. Some of the articles are about mouth cancer specifically, some are about a sub-type of mouth cancer like gingival cancer (cancer of the gums), and palatal cancer (cancer of the palate). But there are also articles about nasopharyngeal cancer (cancer of the upper throat), mandibular cancer (cancer of the jaw), and esophageal cancer (cancer of the esophagus). None of these (upper throat, jaw, or esophagus) are considered mouth cancer. It is understandable why an article about a specific cancer radiation therapy for nasopharyngeal neoplasms would be considered similar to the prompt “treatments for mouth cancer” but it may not be relevant if you are only looking for treatments for mouth cancer. If we were to plug these ten articles directly into our prompt to the LLM and ask it to “summarize the different treatment options,” we would be getting incorrect information.
The purpose of RAG is to give an LLM a very specific set of additional information to better answer your question — if that information is incorrect or irrelevant, it can lead to misleading responses from the LLM. This is often referred to as “context poisoning”. What is especially dangerous about context poisoning is that the response isn’t necessarily factually inaccurate (the LLM may accurately summarize the treatment options we feed it), and it isn’t necessarily based on an inaccurate piece of data (presumably the journal articles themselves are accurate), it’s just using the wrong data to answer your question. In this example, the user could be reading about how to treat the wrong kind of cancer, which seems very bad.
## Refine Terms
KGs can help improve the accuracy of responses and reduce the likelihood of context poisoning by refining the results from the vector database. The next step is for selecting what MeSH terms we want to use to filter the articles. First, we do another vector similarity search against the vector database but on the Terms collection. This is because the user may not be familiar with the MeSH controlled vocabulary. In our example above, I searched for, “therapies for mouth cancer”, but “mouth cancer” is not a term in MeSH — they use “Mouth Neoplasms”. We want the user to be able to start exploring the MeSH terms without having a prior understanding of them — this is good practice regardless of the metadata used to tag the content.
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_38L-k83P82FOmNwCnKhPdQ.png)Image by Author
The function to get relevant MeSH terms is nearly identical to the previous Weaviate query. Just replace Article with term:
```
# Function to query Weaviate for MeSH Terms<br>def query_weaviate_terms(client, query_text, limit=10):<br>  # Perform vector search on MeshTerm collection<br>  response = client.collections.get("term").query.near_text(<br>    query=query_text,<br>    limit=limit,<br>    return_metadata=MetadataQuery(distance=True)<br>  )<br><br>  # Parse response<br>  results = []<br>  for obj in response.objects:<br>    results.append({<br>      "uuid": obj.uuid,<br>      "properties": obj.properties,<br>      "distance": obj.metadata.distance,<br>    })<br>  return results
```

Here is what it looks like in the app:
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_OI1hVbBIMyHTEZy7Oy4FGQ.png)Image by Author
As you can see, I searched for “mouth cancer” and the most similar terms were returned. Mouth cancer was not returned, as that is not a term in MeSH, but Mouth Neoplasms is on the list.
The next step is to allow the user to expand the returned terms to see alternative names and narrower concepts. This requires querying the [MeSH API](https://id.nlm.nih.gov/mesh/). This was the trickiest part of this app for a number of reasons. The biggest problem is that Streamlit requires that everything has a unique ID but MeSH terms can repeat — if one of the returned concepts is a child of another, then when you expand the parent you will have a duplicate of the child. I think I took care of most of the big issues and the app should work, but there are probably bugs to find at this stage.
The functions we rely on are found in rdf_queries.py. We need one to get the alternative names for a term:
```
# Fetch alternative names and triples for a MeSH term<br>def get_concept_triples_for_term(term):<br>  term = sanitize_term(term) # Sanitize input term<br>  sparql = SPARQLWrapper("https://id.nlm.nih.gov/mesh/sparql")<br>  query = f"""<br>  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#><br>  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#><br>  PREFIX meshv: <http://id.nlm.nih.gov/mesh/vocab#><br>  PREFIX mesh: <http://id.nlm.nih.gov/mesh/><br><br>  SELECT ?subject ?p ?pLabel ?o ?oLabel<br>  FROM <http://id.nlm.nih.gov/mesh><br>  WHERE {{<br>    ?subject rdfs:label "{term}"@en .<br>    ?subject ?p ?o .<br>    FILTER(CONTAINS(STR(?p), "concept"))<br>    OPTIONAL {{ ?p rdfs:label ?pLabel . }}<br>    OPTIONAL {{ ?o rdfs:label ?oLabel . }}<br>  }}<br>  """<br>  try:<br>    sparql.setQuery(query)<br>    sparql.setReturnFormat(JSON)<br>    results = sparql.query().convert()<br><br>    triples = set()<br>    for result in results["results"]["bindings"]:<br>      obj_label = result.get("oLabel", {}).get("value", "No label")<br>      triples.add(sanitize_term(obj_label)) # Sanitize term before adding<br><br>    # Add the sanitized term itself to ensure it's included<br>    triples.add(sanitize_term(term))<br>    return list(triples)<br><br>  except Exception as e:<br>    print(f"Error fetching concept triples for term '{term}': {e}")<br>    return []
```

We also need functions to get the narrower (child) concepts for a given term. I have two functions that achieve this — one that gets the immediate children of a term and one recursive function that returns all children of a given depth.
```
# Fetch narrower concepts for a MeSH term<br>def get_narrower_concepts_for_term(term):<br>  term = sanitize_term(term) # Sanitize input term<br>  sparql = SPARQLWrapper("https://id.nlm.nih.gov/mesh/sparql")<br>  query = f"""<br>  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#><br>  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#><br>  PREFIX meshv: <http://id.nlm.nih.gov/mesh/vocab#><br>  PREFIX mesh: <http://id.nlm.nih.gov/mesh/><br><br>  SELECT ?narrowerConcept ?narrowerConceptLabel<br>  WHERE {{<br>    ?broaderConcept rdfs:label "{term}"@en .<br>    ?narrowerConcept meshv:broaderDescriptor ?broaderConcept .<br>    ?narrowerConcept rdfs:label ?narrowerConceptLabel .<br>  }}<br>  """<br>  try:<br>    sparql.setQuery(query)<br>    sparql.setReturnFormat(JSON)<br>    results = sparql.query().convert()<br><br>    concepts = set()<br>    for result in results["results"]["bindings"]:<br>      subject_label = result.get("narrowerConceptLabel", {}).get("value", "No label")<br>      concepts.add(sanitize_term(subject_label)) # Sanitize term before adding<br><br>    return list(concepts)<br><br>  except Exception as e:<br>    print(f"Error fetching narrower concepts for term '{term}': {e}")<br>    return []<br><br># Recursive function to fetch narrower concepts to a given depth<br>def get_all_narrower_concepts(term, depth=2, current_depth=1):<br>  term = sanitize_term(term) # Sanitize input term<br>  all_concepts = {}<br>  try:<br>    narrower_concepts = get_narrower_concepts_for_term(term)<br>    all_concepts[sanitize_term(term)] = narrower_concepts<br><br>    if current_depth < depth:<br>      for concept in narrower_concepts:<br>        child_concepts = get_all_narrower_concepts(concept, depth, current_depth + 1)<br>        all_concepts.update(child_concepts)<br><br>  except Exception as e:<br>    print(f"Error fetching all narrower concepts for term '{term}': {e}")<br><br>  return all_concepts
```

The other important part of step 2 is to allow the user to select terms to add to a list of “Selected Terms”. These will appear in the sidebar on the left of the screen. There are a lot of things that can improve this step like:
  * There is no way to clear all but you can clear the cache or refresh the browser if needed.
  * There is no way to ‘select all narrower concepts’ which would be helpful.
  * There is no option to add rules for filtering. Right now, we are just assuming that the article must contain term A OR term B OR term C etc. The rankings at the end are based on the number of terms the articles are tagged with.


Here is what it looks like in the app:
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_PHOQxQnPdDCiISTrbIeCHA.png)Image by Author
I can expand Mouth Neoplasms to see the alternative names, in this case, “Cancer of Mouth”, along with all of the narrower concepts. As you can see, most of the narrower concepts have their own children, which you can expand as well. For the purposes of this demo, I am going to select all children of Mouth Neoplasms.
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_1Nodj6EQ51R8GPuliWqONQ.png)Image by Author
This step is important not just because it allows the user to filter the search results, but also because it is a way for the user to explore the MeSH graph itself and learn from it. For example, this would be the place for the user to learn that nasopharyngeal neoplasms are not a subset of mouth neoplasms.
## Filter & Summarize
Now that you’ve got your articles and your filter terms, you can apply the filter and summarize the results. This is where we bring the original 10 articles returned in step one together with the refined list of MeSH terms. We allow the user to add additional context to the prompt before sending it to the LLM.
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_fIT2zmJGryYgpUT6iZr01A.png)Image by Author
The way we do this filtering is that we need to get the URIs for the 10 articles from the original search. Then we can query our knowledge graph for which of those articles have been tagged with the associated MeSH terms. Additionally, we save the abstracts of these articles for use in the next step. This would be the place where we could filter based on access control or other user-controlled parameters like author, filetype, date published, etc. I didn’t include any of that in this app but I did add in properties for access control and date published in case we want to add that in this UI later.
Here is what the code looks like in app.py:
```
    if st.button("Filter Articles"):<br>      try:<br>        # Check if we have URIs from tab 1<br>        if "article_uris" in st.session_state and st.session_state.article_uris:<br>          article_uris = st.session_state.article_uris<br><br>          # Convert list of URIs into a string for the VALUES clause or FILTER<br>          article_uris_string = ", ".join([f"<{str(uri)}>" for uri in article_uris])<br><br>          SPARQL_QUERY = """<br>          PREFIX schema: <http://schema.org/><br>          PREFIX ex: <http://example.org/><br><br>          SELECT ?article ?title ?abstract ?datePublished ?access ?meshTerm<br>          WHERE {{<br>           ?article a ex:Article ;<br>                schema:name ?title ;<br>                schema:description ?abstract ;<br>                schema:datePublished ?datePublished ;<br>                ex:access ?access ;<br>                schema:about ?meshTerm .<br><br>           ?meshTerm a ex:MeSHTerm .<br><br>           FILTER (?article IN ({article_uris}))<br>          }}<br>          """<br>          # Insert the article URIs into the query<br>          query = SPARQL_QUERY.format(article_uris=article_uris_string)<br>        else:<br>          st.write("No articles selected from Tab 1.")<br>          st.stop()<br><br>        # Query the RDF and save results in session state<br>        top_articles = query_rdf(LOCAL_FILE_PATH, query, final_terms)<br>        st.session_state.filtered_articles = top_articles<br><br>        if top_articles:<br><br>          # Combine abstracts from top articles and save in session state<br>          def combine_abstracts(ranked_articles):<br>            combined_text = " ".join(<br>              [f"Title: {data['title']} Abstract: {data['abstract']}" for article_uri, data in<br>               ranked_articles]<br>            )<br>            return combined_text<br><br><br>          st.session_state.combined_text = combine_abstracts(top_articles)<br><br>        else:<br>          st.write("No articles found for the selected terms.")<br>      except Exception as e:<br>        st.error(f"Error filtering articles: {e}")
```

This uses the function query_rdf in the rdf_queries.py file. That function looks like this:
```
# Function to query RDF using SPARQL<br>def query_rdf(local_file_path, query, mesh_terms, base_namespace="http://example.org/mesh/"):<br>  if not mesh_terms:<br>    raise ValueError("The list of MeSH terms is empty or invalid.")<br><br>  print("SPARQL Query:", query)<br><br>  # Create and parse the RDF graph<br>  g = Graph()<br>  g.parse(local_file_path, format="ttl")<br><br>  article_data = {}<br><br>  for term in mesh_terms:<br>    # Convert the term to a valid URI<br>    mesh_term_uri = convert_to_uri(term, base_namespace)<br>    #print("Term:", term, "URI:", mesh_term_uri)<br><br>    # Perform SPARQL query with initBindings<br>    results = g.query(query, initBindings={'meshTerm': mesh_term_uri})<br><br>    for row in results:<br>      article_uri = row['article']<br>      if article_uri not in article_data:<br>        article_data[article_uri] = {<br>          'title': row['title'],<br>          'abstract': row['abstract'],<br>          'datePublished': row['datePublished'],<br>          'access': row['access'],<br>          'meshTerms': set()<br>        }<br>      article_data[article_uri]['meshTerms'].add(str(row['meshTerm']))<br>    #print("DEBUG article_data:", article_data)<br><br>  # Rank articles by the number of matching MeSH terms<br>  ranked_articles = sorted(<br>    article_data.items(),<br>    key=lambda item: len(item[1]['meshTerms']),<br>    reverse=True<br>  )<br>  return ranked_articles[:10]
```

As you can see, this function also converts the MeSH terms to URIs so we can filter using the graph. Be careful in the way you convert terms to URIs and ensure it aligns with the other functions.
Here is what it looks like in the app:
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_lAAPdTjfFOzD1DM3QUofAA.png)Image by Author
As you can see, the two MeSH terms we selected from the previous step are here. If I click “Filter Articles,” it will filter the original 10 articles using our filter criteria in step 2. The articles will be returned with their full abstracts, along with their tagged MeSH terms (see image below).
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_wUsJ9sIQdMcro6ximwPxFg.png)Image by Author
There are 5 articles returned. Two are tagged with “mouth neoplasms,” one with “gingival neoplasms,” and two with “palatal neoplasms”.
Now that we have a refined list of articles we want to use to generate a response, we can move to the final step. We want to send these articles to an LLM to generate a response but we can also add in additional context to the prompt. I have a default prompt that says, “Summarize the key information here in bullet points. Make it understandable to someone without a medical degree.” For this demo, I am going to adjust the prompt to reflect our original search term:
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_zu6wCN1c8f4AqI3Gzfq5tA.png)
The results are as follows:
![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_3efzwyaNjWsWFizCwhCb0g.png)
The results look better to me, mostly because I know that the articles we are summarizing are, presumably, about treatments for mouth cancer. The dataset doesn’t contain the actual journal articles, just the abstracts. So these results are just summaries of summaries. There may be some value to this, but if we were building a real app and not just a demo, this is the step where we could incorporate the full text of the articles. Alternatively, this is where the user/researcher would go read these articles themselves, rather than relying exclusively on the LLM for the summaries.
# Conclusion
This tutorial demonstrates how combining vector databases and knowledge graphs can significantly enhance RAG applications. By leveraging vector similarity for initial searches and structured knowledge graph metadata for filtering and organization, we can build a system that delivers accurate, explainable, and domain-specific results. The integration of MeSH, a well-established controlled vocabulary, highlights the power of domain expertise in curating metadata, which ensures that the retrieval step aligns with the unique needs of the application while maintaining interoperability with other systems. This approach is not limited to medicine — its principles can be applied across domains wherever structured data and textual information coexist.
This tutorial underscores the importance of leveraging each technology for what it does best. Vector databases excel at similarity-based retrieval, while knowledge graphs shine in providing context, structure, and semantics. Additionally, scaling RAG applications demands a metadata layer to break down data silos and enforce governance policies. Thoughtful design, rooted in domain-specific metadata and robust governance, is the path to building RAG systems that are not only accurate but also scalable.
Written By
Steve Hedden
[See all from Steve Hedden](https://towardsdatascience.com/author/steve-hedden/)
Topics:
Share this article:
  * [ Share on Facebook  ](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-graph-rag-app-b323fc33ba06%2F&title=How%20to%20Build%20a%20Graph%20RAG%20App)
  * [ Share on LinkedIn  ](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-graph-rag-app-b323fc33ba06%2F&title=How%20to%20Build%20a%20Graph%20RAG%20App)
  * [ Share on X  ](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-graph-rag-app-b323fc33ba06%2F&text=How%20to%20Build%20a%20Graph%20RAG%20App)


## Related Articles
  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/Hybrid-Architecture_01-snapshot.png)
## [The Art of Hybrid Architectures](https://towardsdatascience.com/the-art-of-hybrid-architectures/)
[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)
Combining CNNs and Transformers to Elevate Fine-Grained Visual Classification 
[Eric Chung](https://towardsdatascience.com/author/eric-chung/)
March 28, 2025
32 min read
  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/ai-generated-city-banner-integration.png)
## [A Little More Conversation, A Little Less Action — A Case Against Premature Data Integration](https://towardsdatascience.com/a-little-more-conversation-a-little-less-action-a-case-against-premature-data-integration/)
[ Data Science ](https://towardsdatascience.com/category/data-science/)
Running a large data integration project before embarking on the ML part is easily a… 
[Daniel Bakkelund](https://towardsdatascience.com/author/danielbakkelund/)
March 28, 2025
14 min read
  * ![3D Reconstruction cover image](https://towardsdatascience.com/wp-content/uploads/2025/03/cover-3d-reconstruction.jpg)
## [Master the 3D Reconstruction Process: A Step-by-Step Guide](https://towardsdatascience.com/master-the-3d-reconstruction-process-step-by-step-guide/)
[ Data Science ](https://towardsdatascience.com/category/data-science/)
Learn the complete 3D reconstruction pipeline from feature extraction to dense matching. Master photogrammetry with… 
[Florent Poux, Ph.D.](https://towardsdatascience.com/author/florentpoux/)
March 28, 2025
17 min read
  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/nik-n1ccr-zVG68-unsplash-scaled-1.jpg)
## [AI Agents from Zero to Hero — Part 3](https://towardsdatascience.com/ai-agents-from-zero-to-hero-part-3/)
[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)
Build from scratch using only Ollama (no GPU, no APIKEY) 
[Mauro Di Pietro](https://towardsdatascience.com/author/maurodp/)
March 28, 2025
14 min read
  * ## [From Physics to Probability: Hamiltonian Mechanics for Generative Modeling and MCMC](https://towardsdatascience.com/from-physics-to-probability-hamiltonian-mechanics-for-generative-modeling-and-mcmc/)
[ Math ](https://towardsdatascience.com/category/math/)
Hamiltonian mechanics is a way to describe how physical systems, like planets or pendulums, move… 
[Soran Ghaderi](https://towardsdatascience.com/author/soran-ghaderi/)
March 28, 2025
17 min read
  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/Screenshot-2025-03-26-at-10.54.07 PM.png)
## [How to Format Your TDS Draft: A Quick(ish) Guide](https://towardsdatascience.com/how-to-format-your-tds-draft-a-quickish-guide/)
[ Writing ](https://towardsdatascience.com/category/writing/)
Everything you need to know about creating a draft on our Contributor Portal 
[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)
March 28, 2025
12 min read
  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/image1-1024x576-1.png)
## [Data Science: From School to Work, Part III](https://towardsdatascience.com/data-science-from-school-to-work-part-iii/)
[ Data Science ](https://towardsdatascience.com/category/data-science/)
Good practices for Python error handling and logging 
[Vincent Margot](https://towardsdatascience.com/author/vmargot/)
March 27, 2025
12 min read
  * ![A colorful, hand-drawn, cartoon-style illustration set against a vibrant blue background. In the center stands a cute, vintage-style robot with a white body and red accents \(shoulders, hands, waist, and feet\). Above the robot's head are three speech bubbles. The left bubble is orange and contains the text "EN" in bold black letters, representing English. The center bubble is white with the Chinese character "中" \(for Chinese\), and the right bubble is yellow with the Japanese character "日" \(for Japanese\). The robot appears to be choosing or processing languages, symbolizing multilingual communication or translation.](https://towardsdatascience.com/wp-content/uploads/2025/03/article-cover-robot-ratio.png)
## [Japanese-Chinese Translation with GenAI: What Works and What Doesn’t](https://towardsdatascience.com/japanese-chinese-translation-with-genai-what-works-and-what-doesnt/)
[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)
Evaluating GenAI in Japanese-Chinese translation: current limits and opportunities 
[Qian (Alex) Wan](https://towardsdatascience.com/author/qianalex-wan/)
March 27, 2025
20 min read
  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/featured-image.png)
## [Talk to Videos](https://towardsdatascience.com/talk-to-videos/)
[ Large Language Models ](https://towardsdatascience.com/category/artificial-intelligence/large-language-models/)
Developing an interactive AI application for video-based learning in education and business 
[Umair Ali Khan](https://towardsdatascience.com/author/umairali-khan/)
March 27, 2025
28 min read


  * [YouTube](https://www.youtube.com/c/TowardsDataScience)
  * [X](https://x.com/TDataScience)
  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)
  * [Threads](https://www.threads.net/@towardsdatascience)
  * [Bluesky](https://bsky.app/profile/towardsdatascience.com)


[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)
Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.
©  Insight Media Group, LLC 2025 
  * [About](https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/)
  * [Privacy Policy](https://towardsdatascience.com/privacy-policy/)
  * [Terms of Use](https://towardsdatascience.com/website-terms-of-use/)


[Towards Data Science is now independent!](https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/)
Cookies Settings
## Sign up to our newsletter
Email address*
First name*
Last name*
Job title*
Job level*
Please SelectC-LevelVP/DirectorManager/SupervisorMid Level or Senior Non-Managerial StaffEntry Level/Junior StaffFreelancer/ContractorStudent/InternOther
Company name*
  * I consent to receive newsletters and other communications from Towards Data Science publications.*


![Company Logo](https://cdn.cookielaw.org/logos/static/ot_company_logo.png)
## Privacy Preference Center
When you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer. [More information](https://cookiepedia.co.uk/giving-consent-to-cookies)
Allow All
###  Manage Consent Preferences
#### Functional Cookies
Functional Cookies Active
These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.
#### Strictly Necessary Cookies
Always Active
These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.
#### Performance Cookies
Performance Cookies Active
These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.
#### Targeting Cookies
Targeting Cookies Active
These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.
Back Button
### Cookie List
Search Icon
Filter Icon
Clear
checkbox label label
Apply Cancel
Consent Leg.Interest
checkbox label label
checkbox label label
checkbox label label
Reject All Confirm My Choices
[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-consent/)
Some areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.
