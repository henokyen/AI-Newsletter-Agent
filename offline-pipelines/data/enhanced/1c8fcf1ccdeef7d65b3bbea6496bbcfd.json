{
    "id": "1c8fcf1ccdeef7d65b3bbea6496bbcfd",
    "metadata": {
        "id": "1c8fcf1ccdeef7d65b3bbea6496bbcfd",
        "url": "https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c/",
        "title": "Introduction to Weight Quantization | Towards Data Science",
        "properties": {
            "description": null,
            "keywords": null,
            "author": "Maxime Labonne",
            "og:locale": "en_US",
            "og:type": "article",
            "og:title": "Introduction to Weight Quantization | Towards Data Science",
            "og:description": "Reducing the size of Large Language Models with 8-bit quantization",
            "og:url": "https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c/",
            "og:site_name": "Towards Data Science",
            "og:image": "https://towardsdatascience.com/wp-content/uploads/2023/07/1hWIaIAQ7GWbrjfbaoUoYxw.jpeg",
            "og:image:width": "1820",
            "og:image:height": "1024",
            "og:image:type": "image/jpeg",
            "twitter:card": "summary_large_image",
            "twitter:creator": "@TDataScience",
            "twitter:site": "@TDataScience",
            "twitter:label1": "Written by",
            "twitter:data1": "Maxime Labonne",
            "twitter:label2": "Est. reading time",
            "twitter:data2": "17 minutes"
        }
    },
    "parent_metadata": {
        "id": "9a2f231e378cf9f2918e8baadfbcc4b8",
        "url": "https://www.notion.so/Compression-9a2f231e378cf9f2918e8baadfbcc4b8",
        "title": "Compression",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Skip to content](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c/#wp--skip-link--target)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nThe world‚Äôs leading publication for data science, AI, and ML professionals.\nSign in\nSign out\n[Contributor Portal](https://contributor.insightmediagroup.io/)\n  * [Latest](https://towardsdatascience.com/latest/)\n  * [Editor‚Äôs Picks](https://towardsdatascience.com/tag/editors-pick/)\n  * [Deep Dives](https://towardsdatascience.com/tag/deep-dives/)\n  * [Contribute](https://towardsdatascience.com/questions-96667b06af5/)\n  * [Newsletter](https://newsletter.towardsdatascience.com/subscription-to-the-newsletter)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\n\n\nToggle Mobile Navigation\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [X](https://x.com/TDataScience)\n\n\nToggle Search\nSearch\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\n# Introduction to Weight Quantization\nReducing the size of Large Language Models with 8-bit quantization \n[Maxime Labonne](https://towardsdatascience.com/author/mlabonne/)\nJul 7, 2023\n15 min read\nShare \n![](https://towardsdatascience.com/wp-content/uploads/2023/07/1hWIaIAQ7GWbrjfbaoUoYxw.jpeg)\n[Large Language Models](https://towardsdatascience.com/tag/large-language-models/ \"Large Language Models\") (LLMs) are known for their extensive computational requirements. Typically, the size of a model is calculated by multiplying the number of parameters (**size**) by the precision of these values (**data type**). However, to save memory, weights can be stored using lower-precision data types through a process known as quantization.\nWe distinguish two main families of weight quantization techniques in the literature:\n  * **Post-Training[Quantization](https://towardsdatascience.com/tag/quantization/ \"Quantization\")** (PTQ) is a straightforward technique where the weights of an already trained model are converted to lower precision without necessitating any retraining. Although easy to implement, PTQ is associated with potential performance degradation.\n  * **Quantization-Aware Training** (QAT) incorporates the weight conversion process during the pre-training or fine-tuning stage, resulting in enhanced model performance. However, QAT is computationally expensive and demands representative training data.\n\n\nIn this article, we focus on PTQ to reduce the precision of our parameters. To get a good intuition, we will apply both na√Øve and more sophisticated techniques to a toy example using a GPT-2 model.\nThe entire code is freely available on [Google Colab](https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing) and [GitHub](https://github.com/mlabonne/llm-course/blob/main/Introduction_to_Weight_Quantization.ipynb).\n## üìö Background on Floating Point Representation\nThe choice of data type dictates the quantity of computational resources required, affecting the speed and efficiency of the model. In deep learning applications, balancing precision and computational performance becomes a vital exercise as higher precision often implies greater computational demands.\nAmong various data types, floating point numbers are predominantly employed in deep learning due to their ability to represent a wide range of values with high precision. Typically, a floating point number uses _n_ bits to store a numerical value. These _n_ bits are further partitioned into three distinct components:\n  1. **Sign** : The sign bit indicates the positive or negative nature of the number. It uses one bit where 0 indicates a positive number and 1 signals a negative number.\n  2. **Exponent** : The exponent is a segment of bits that represents the power to which the base (usually 2 in binary representation) is raised. The exponent can also be positive or negative, allowing the number to represent very large or very small values.\n  3. **Significand/Mantissa** : The remaining bits are used to store the significand, also referred to as the mantissa. This represents the significant digits of the number. The precision of the number heavily depends on the length of the significand.\n\n\nThis design allows floating point numbers to cover a wide range of values with varying levels of precision. The formula used for this representation is:\n![](https://towardsdatascience.com/wp-content/uploads/2023/07/1gVngsuChFURCIf0N9biYWw.png)\nTo understand this better, let‚Äôs delve into some of the most commonly used data types in deep learning: float32 (FP32), float16 (FP16), and bfloat16 (BF16):\n  * **FP32** uses 32 bits to represent a number: one bit for the sign, eight for the exponent, and the remaining 23 for the significand. While it provides a high degree of precision, the downside of FP32 is its high computational and memory footprint.\n  * **FP16** uses 16 bits to store a number: one is used for the sign, five for the exponent, and ten for the significand. Although this makes it more memory-efficient and accelerates computations, the reduced range and precision can introduce numerical instability, potentially impacting model accuracy.\n  * **BF16** is also a 16-bit format but with one bit for the sign, _eight_ for the exponent, and _seven_ for the significand. BF16 expands the representable range compared to FP16, thus decreasing underflow and overflow risks. Despite a reduction in precision due to fewer significand bits, BF16 typically does not significantly impact model performance and is a useful compromise for deep learning tasks.\n\n![Image by author](https://towardsdatascience.com/wp-content/uploads/2023/07/1maBUGGMedWvvDyk6FdvkWw.png)Image by author\nIn ML jargon, FP32 is often termed \"full precision\" (4 bytes), while BF16 and FP16 are \"half-precision\" (2 bytes). But could we do even better and store weights using a single byte? The answer is the INT8 data type, which consists of an 8-bit representation capable of storing 2‚Å∏ = 256 different values. In the next section, we‚Äôll see how to convert FP32 weights into an INT8 format.\n## üî∞ Na√Øve 8-bit Quantization\nIn this section, we will implement two quantization techniques: a symmetric one with **absolute maximum (absmax) quantization** and an asymmetric one with **zero-point quantization**. In both cases, the goal is to map an FP32 tensor **X** (original weights) to an INT8 tensor **X_quant** (quantized weights).\nWith **absmax quantization** , the original number is divided by the absolute maximum value of the tensor and multiplied by a scaling factor (127) to map inputs into the range [-127, 127]. To retrieve the original FP16 values, the INT8 number is divided by the quantization factor, acknowledging some loss of precision due to rounding.\n![](https://towardsdatascience.com/wp-content/uploads/2023/07/1jNl_x4JF0lpRA4_6Cae9cg.png)\nFor instance, let‚Äôs say we have an absolution maximum value of 3.2. A weight of 0.1 would be quantized to _round(0.1 √ó 127/3.2) = 4_. If we want to dequantize it, we would get _4 √ó 3.2/127 = 0.1008_ , which implies an error of 0.008. Here‚Äôs the corresponding Python implementation:\n```\nimport torch\ndef absmax_quantize(X):\n  # Calculate scale\n  scale = 127 / torch.max(torch.abs(X))\n  # Quantize\n  X_quant = (scale * X).round()\n  # Dequantize\n  X_dequant = X_quant / scale\n  return X_quant.to(torch.int8), X_dequant\n```\n\nWith **zero-point quantization** , we can consider asymmetric input distributions, which is useful when you consider the output of a ReLU function (only positive values), for example. The input values are first scaled by the total range of values (255) divided by the difference between the maximum and minimum values. This distribution is then shifted by the zero-point to map it into the range [-128, 127] (notice the extra value compared to absmax). First, we calculate the scale factor and the zero-point value:\n![](https://towardsdatascience.com/wp-content/uploads/2023/07/1GiCuvGWBtdU4-hXcvetTnw.png)\nThen, we can use these variables to quantize or dequantize our weights:\n![](https://towardsdatascience.com/wp-content/uploads/2023/07/1SalTtt_eNoYOeHLD1XiEfw.png)\nLet‚Äôs take an example: we have a maximum value of 3.2 and a minimum value of -3.0. We can calculate the scale is _255/(3.2 + 3.0) = 41.13_ and the zero-point _-round(41.13 √ó -3.0) ‚Äì 128 = 123 -128 = -5_ , so our previous weight of 0.1 would be quantized to _round(41.13 √ó 0.1 -5) = -1_. This is very different from the previous value obtained using absmax (4 vs. -1).\n![Image by author](https://towardsdatascience.com/wp-content/uploads/2023/07/1n5nqoJUXp65JahKsLzQS-A.png)Image by author\nThe Python implementation is quite straightforward:\n```\ndef zeropoint_quantize(X):\n  # Calculate value range (denominator)\n  x_range = torch.max(X) - torch.min(X)\n  x_range = 1 if x_range == 0 else x_range\n  # Calculate scale\n  scale = 255 / x_range\n  # Shift by zero-point\n  zeropoint = (-scale * torch.min(X) - 128).round()\n  # Scale and round the inputs\n  X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n  # Dequantize\n  X_dequant = (X_quant - zeropoint) / scale\n  return X_quant.to(torch.int8), X_dequant\n```\n\nInstead of relying on complete toy examples, we can use these two functions on a real model thanks to the `transformers`library.\nWe start by loading the model and tokenizer for GPT-2. This is a very small model we probably don‚Äôt want to quantize, but it will be good enough for this tutorial. First, we want to observe the model‚Äôs size so we can compare it later and evaluate the **memory savings** due to 8-bit quantization.\n```\n!pip install -q bitsandbytes>=0.39.0\n!pip install -q git+https://github.com/huggingface/accelerate.git\n!pip install -q git+https://github.com/huggingface/transformers.git\n```\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntorch.manual_seed(0)\n# Set device to CPU for now\ndevice = 'cpu'\n# Load model and tokenizer\nmodel_id = 'gpt2'\nmodel = AutoModelForCausalLM.from_pretrained(model_id).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# Print model size\nprint(f\"Model size: {model.get_memory_footprint():,} bytes\")\n```\n```\nModel size: 510,342,192 bytes\n```\n\nThe size of the GPT-2 model is approximately 487MB in FP32. The next step consists of quantizing the weights using zero-point and absmax quantization. In the following example, we apply these techniques to the first attention layer of GPT-2 to see the results.\n```\n# Extract weights of the first layer\nweights = model.transformer.h[0].attn.c_attn.weight.data\nprint(\"Original weights:\")\nprint(weights)\n# Quantize layer using absmax quantization\nweights_abs_quant, _ = absmax_quantize(weights)\nprint(\"nAbsmax quantized weights:\")\nprint(weights_abs_quant)\n# Quantize layer using absmax quantization\nweights_zp_quant, _ = zeropoint_quantize(weights)\nprint(\"nZero-point quantized weights:\")\nprint(weights_zp_quant)\n```\n```\nOriginal weights:\ntensor([[-0.4738, -0.2614, -0.0978, ..., 0.0513, -0.0584, 0.0250],\n    [ 0.0874, 0.1473, 0.2387, ..., -0.0525, -0.0113, -0.0156],\n    [ 0.0039, 0.0695, 0.3668, ..., 0.1143, 0.0363, -0.0318],\n    ...,\n    [-0.2592, -0.0164, 0.1991, ..., 0.0095, -0.0516, 0.0319],\n    [ 0.1517, 0.2170, 0.1043, ..., 0.0293, -0.0429, -0.0475],\n    [-0.4100, -0.1924, -0.2400, ..., -0.0046, 0.0070, 0.0198]])\nAbsmax quantized weights:\ntensor([[-21, -12, -4, ...,  2, -3,  1],\n    [ 4,  7, 11, ..., -2, -1, -1],\n    [ 0,  3, 16, ...,  5,  2, -1],\n    ...,\n    [-12, -1,  9, ...,  0, -2,  1],\n    [ 7, 10,  5, ...,  1, -2, -2],\n    [-18, -9, -11, ...,  0,  0,  1]], dtype=torch.int8)\nZero-point quantized weights:\ntensor([[-20, -11, -3, ...,  3, -2,  2],\n    [ 5,  8, 12, ..., -1,  0,  0],\n    [ 1,  4, 18, ...,  6,  3,  0],\n    ...,\n    [-11,  0, 10, ...,  1, -1,  2],\n    [ 8, 11,  6, ...,  2, -1, -1],\n    [-18, -8, -10, ...,  1,  1,  2]], dtype=torch.int8)\n```\n\nThe difference between the original (FP32) and quantized values (INT8) is clear, but the difference between absmax and zero-point weights is more subtle. In this case, the inputs look shifted by a value of -1. This suggests that the weight distribution in this layer is quite symmetric.\nWe can compare these techniques by quantizing every layer in GPT-2 (linear layers, attention layers, etc.) and create two new models: `model_abs` and `model_zp`. To be precise, we will actually replace the original weights with _**de**_ -quantized ones. This has two benefits: it allows us to 1/ compare the distribution of our weights (same scale) and 2/ actually run the models.\nIndeed, PyTorch doesn‚Äôt allow INT8 matrix multiplication by default. In a real scenario, we would dequantize them to run the model (in FP16 for example) but store them as INT8. In the next section, we will use the `[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)` library to solve this issue.\n```\nimport numpy as np\nfrom copy import deepcopy\n# Store original weights\nweights = [param.data.clone() for param in model.parameters()]\n# Create model to quantize\nmodel_abs = deepcopy(model)\n# Quantize all model weights\nweights_abs = []\nfor param in model_abs.parameters():\n  _, dequantized = absmax_quantize(param.data)\n  param.data = dequantized\n  weights_abs.append(dequantized)\n# Create model to quantize\nmodel_zp = deepcopy(model)\n# Quantize all model weights\nweights_zp = []\nfor param in model_zp.parameters():\n  _, dequantized = zeropoint_quantize(param.data)\n  param.data = dequantized\n  weights_zp.append(dequantized)\n```\n\nNow that our models have been quantized, we want to check the impact of this process. Intuitively, we want to make sure that the quantized weights are **close to the original ones**. A visual way to check it is to plot the distribution of the dequantized and original weights. If the quantization is lossy, it would drastically change the weight distribution.\nThe following figure shows this comparison, where the blue histogram represents the original (FP32) weights, and the red one represents the dequantized (from INT8) weights. Note that we only display this plot between -2 and 2 because of outliers with very high absolute values (more on that later).\n![](https://towardsdatascience.com/wp-content/uploads/2023/07/15FkPW5PQDn3zqP7x9QSPLg.png)\nBoth plots are quite similar, with a surprising spike around 0. This spike shows that our quantization is quite lossy since reversing the process doesn‚Äôt output the original values. This is particularly true for the absmax model, which displays both a lower valley and a higher spike around 0.\nLet‚Äôs compare the performance of the original and quantized models. For this purpose, we define a `generate_text()` function to generate 50 tokens with [top-k sampling](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html).\n```\ndef generate_text(model, input_text, max_length=50):\n  input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n  output = model.generate(inputs=input_ids,\n              max_length=max_length,\n              do_sample=True,\n              top_k=30,\n              pad_token_id=tokenizer.eos_token_id,\n              attention_mask=input_ids.new_ones(input_ids.shape))\n  return tokenizer.decode(output[0], skip_special_tokens=True)\n# Generate text with original and quantized models\noriginal_text = generate_text(model, \"I have a dream\")\nabsmax_text  = generate_text(model_abs, \"I have a dream\")\nzp_text    = generate_text(model_zp, \"I have a dream\")\nprint(f\"Original model:n{original_text}\")\nprint(\"-\" * 50)\nprint(f\"Absmax model:n{absmax_text}\")\nprint(\"-\" * 50)\nprint(f\"Zeropoint model:n{zp_text}\")\n```\n```\nOriginal model:\nI have a dream, and it is a dream I believe I would get to live in my future. I love my mother, and there was that one time I had been told that my family wasn't even that strong. And then I got the\n--------------------------------------------------\nAbsmax model:\nI have a dream to find out the origin of her hair. She loves it. But there's no way you could be honest about how her hair is made. She must be crazy.\nWe found a photo of the hairstyle posted on\n--------------------------------------------------\nZeropoint model:\nI have a dream of creating two full-time jobs in America‚Äîone for people with mental health issues, and one for people who do not suffer from mental illness‚Äîor at least have an employment and family history of substance abuse, to work part\n```\n\nInstead of trying to see if one output makes more sense than the others, we can quantify it by calculating the **perplexity** of each output. This is a common metric used to evaluate language models, which measures the uncertainty of a model in predicting the next token in a sequence. In this comparison, we make the common assumption that the lower the score, the better the model is. In practice, a sentence with a high perplexity could also be correct.\nWe implement it using a minimal function since it doesn‚Äôt need to consider details like the length of the context window since our sentences are short.\n```\ndef calculate_perplexity(model, text):\n  # Encode the text\n  encodings = tokenizer(text, return_tensors='pt').to(device)\n  # Define input_ids and target_ids\n  input_ids = encodings.input_ids\n  target_ids = input_ids.clone()\n  with torch.no_grad():\n    outputs = model(input_ids, labels=target_ids)\n  # Loss calculation\n  neg_log_likelihood = outputs.loss\n  # Perplexity calculation\n  ppl = torch.exp(neg_log_likelihood)\n  return ppl\nppl   = calculate_perplexity(model, original_text)\nppl_abs = calculate_perplexity(model_abs, absmax_text)\nppl_zp = calculate_perplexity(model_zp, absmax_text)\nprint(f\"Original perplexity: {ppl.item():.2f}\")\nprint(f\"Absmax perplexity:  {ppl_abs.item():.2f}\")\nprint(f\"Zeropoint perplexity: {ppl_zp.item():.2f}\")\n```\n```\nOriginal perplexity: 15.53\nAbsmax perplexity:  17.92\nZeropoint perplexity: 17.97\n```\n\nWe see that the perplexity of the original model is **slightly lower** than the two others. A single experiment is not very reliable, but we could repeat this process multiple times to see the difference between each model. In theory, zero-point quantization should be slightly better than absmax, but is also more costly to compute.\nIn this example, we applied quantization techniques to entire layers (per-tensor basis). However, we could apply it at different granularity levels: from the entire model to individual values. Quantizing the entire model in one pass would seriously degrade the performance, while quantizing individual values would create a big overhead. In practice, we often prefer the **vector-wise quantization** , which considers the variability of values in rows and columns inside of the same tensor.\nHowever, even vector-wise quantization doesn‚Äôt solve the problem of outlier features. Outlier features are extreme values (negative or positive) that appear in all transformer layers when the model reach a certain scale (>6.7B parameters). This is an issue since a single outlier can reduce the precision for all other values. But discarding these outlier features is not an option since it would **greatly degrade** the model‚Äôs performance.\n## üî¢ 8-bit Quantization with LLM.int8()\nIntroduced by [Dettmers et al. (2022)](https://arxiv.org/abs/2208.07339), LLM.int8() is a solution to the outlier problem. It relies on a vector-wise (absmax) quantization scheme and introduces mixed-precision quantization. This means that outlier features are processed in a FP16 format to retain their precision, while the other values are processed in an INT8 format. As outliers represent about 0.1% of values, this effectively reduces the memory footprint of the LLM by almost 2x.\n![Image by author](https://towardsdatascience.com/wp-content/uploads/2023/07/1SFxEfA8009WcgWJG5Vsh0A.png)Image by author\nLLM.int8() works by conducting matrix multiplication computation in three key steps:\n  1. Extract columns from the input hidden states **X** containing outlier features using a custom threshold.\n  2. Perform the matrix multiplication of the outliers using FP16 and the non-outliers using INT8 with vector-wise quantization (row-wise for the hidden state **X** and column-wise for the weight matrix **W**).\n  3. Dequantize the non-outlier results (INT8 to FP16) and add them to the outlier results to get the full result in FP16.\n\n![Image by author](https://towardsdatascience.com/wp-content/uploads/2023/07/11xt5F9kFLFGfiFmRtF11Pg.png)Image by author\nThis approach is necessary because 8-bit precision is limited and can lead to substantial errors when quantizing a vector with large values. These errors also tend to amplify as they propagate through multiple layers.\nWe can easily use this technique thanks to the integration of the `bitsandbytes` library into the Hugging Face ecosystem. We just need to specify `load_in_8bit=True` when loading the model (it also requires a GPU).\n```\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_int8 = AutoModelForCausalLM.from_pretrained(model_id,\n                       device_map='auto',\n                       load_in_8bit=True,\n                       )\nprint(f\"Model size: {model_int8.get_memory_footprint():,} bytes\")\n```\n```\nModel size: 176,527,896 bytes\n```\n\nWith this extra line of code, the model is now almost three times smaller (168MB vs. 487MB). We can even compare the distribution of the original and quantized weights as we did earlier:\n![](https://towardsdatascience.com/wp-content/uploads/2023/07/1gvBKaY8nDJgTfMwPVi77KQ.png)\nIn this case, we see spikes around -2, -1, 0, 1, 2, etc. These values correspond to the parameters stored in the INT8 format (non-outliers). You can verify it by printing the model‚Äôs weights using `model_int8.parameters()`.\nWe can also generate text with this quantized model and compare it to the original model.\n```\n# Generate text with quantized model\ntext_int8 = generate_text(model_int8, \"I have a dream\")\nprint(f\"Original model:n{original_text}\")\nprint(\"-\" * 50)\nprint(f\"LLM.int8() model:n{text_int8}\")\n```\n```\nOriginal model:\nI have a dream, and it is a dream I believe I would get to live in my future. I love my mother, and there was that one time I had been told that my family wasn't even that strong. And then I got the\n--------------------------------------------------\nLLM.int8() model:\nI have a dream. I don't know what will come of it, but I am going to have to look for something that will be right. I haven't thought about it for a long time, but I have to try to get that thing\n```\n\nOnce again, it is difficult to judge what is the best output, but we can rely on the perplexity metric to give us an (approximate) answer.\n```\nprint(f\"Perplexity (original):  {ppl.item():.2f}\")\nppl = calculate_perplexity(model_int8, text_int8)\nprint(f\"Perplexity (LLM.int8()): {ppl.item():.2f}\")\n```\n```\nPerplexity (original):  15.53\nPerplexity (LLM.int8()): 7.93\n```\n\nIn this case, the perplexity of the quantized model is twice as low as the original one. In general, this is not the case, but it shows that this quantization technique is very competitive. In fact, the authors of LLM.int8() show that the performance degradation is so low it‚Äôs negligible (<1%). However, it has an additional cost in terms of computation: LLM.int8() is roughly about 20% slower for large models.\n## Conclusion\nThis article provided an overview of the most popular weight quantization techniques. We started by gaining an understanding of floating point representation, before introducing two techniques for 8-bit quantization: **absmax** and **zero-point quantization**. However, their limitations, particularly when it comes to handling outliers, led to **LLM.int8()** , a technique that also preserves the model‚Äôs performance. This approach underlines the progress being made in the field of weight quantization, revealing the importance of properly addressing outliers.\nLooking forward, our next article will explore the GPTQ weight quantization technique in depth. This technique, introduced by [Frantar et al.](https://arxiv.org/abs/2210.17323), only utilizes 4 bits and represents a significant advancement in the field of weight quantization. We will provide a comprehensive guide on how to implement GPTQ using the AutoGPTQ library.\nIf you‚Äôre interested in more technical content around LLMs, follow me on Twitter [@maximelabonne](https://twitter.com/maximelabonne).\n## References\n  * T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339). 2022.\n  * Y. Beldaka, and T. Dettmers, [A Gentle Introduction to 8-bit Matrix Multiplication](https://huggingface.co/blog/hf-bitsandbytes-integration), Hugging Face Blog (2022).\n  * A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, [A Survey of Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/abs/2103.13630). 2021.\n  * H. Wu, P. Judd, X. Zhang, M. Isaev, and P. Micikevicius, [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602). 2020.\n  * Lilian Weng, [Large Transformer Model Inference Optimization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/), Lil‚ÄôLog (2023).\n  * Kamil Czarnogorski, [Local Large Language Models](https://int8.io/local-large-language-models-beginners-guide/), Int8 (2023).\n\n\nWritten By\nMaxime Labonne\n[See all from Maxime Labonne](https://towardsdatascience.com/author/mlabonne/)\nTopics:\n[Data Science](https://towardsdatascience.com/tag/data-science/), [Hands On Tutorials](https://towardsdatascience.com/tag/hands-on-tutorials/), [Large Language Models](https://towardsdatascience.com/tag/large-language-models/), [Machine Learning](https://towardsdatascience.com/tag/machine-learning/), [Quantization](https://towardsdatascience.com/tag/quantization/)\nShare this article:\n  * [ Share on Facebook  ](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c%2F&title=Introduction%20to%20Weight%20Quantization)\n  * [ Share on LinkedIn  ](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c%2F&title=Introduction%20to%20Weight%20Quantization)\n  * [ Share on X  ](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c%2F&text=Introduction%20to%20Weight%20Quantization)\n\n\n## Related Articles\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/0c09RmbCCpfjAbSMq.png)\n## [Implementing Convolutional Neural Networks in TensorFlow](https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nStep-by-step code guide to building a Convolutional Neural Network \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nAugust 20, 2024\n6 min read\n  * ## [What Do Large Language Models ‚ÄúUnderstand‚Äù?](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA deep dive on the meaning of understanding and how it applies to LLMs \n[Tarik Dzekman](https://towardsdatascience.com/author/tarikdzekman/)\nAugust 21, 2024\n31 min read\n  * ![Photo by Krista Mangulsone on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/0GyVVTbgotH-DhGPH-scaled.jpg)\n## [How to Forecast Hierarchical Time Series](https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA beginner‚Äôs guide to forecast reconciliation \n[Dr. Robert K√ºbler](https://towardsdatascience.com/author/dr-robert-kuebler/)\nAugust 20, 2024\n13 min read\n  * ![Photo by davisuko on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/1bAABgtZtAIG5YW1oEjW3pA-scaled.jpeg)\n## [Hands-on Time Series Anomaly Detection using Autoencoders, with Python](https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nHere‚Äôs how to use Autoencoders to detect signals with anomalies in a few lines of‚Ä¶ \n[Piero Paialunga](https://towardsdatascience.com/author/piero-paialunga/)\nAugust 21, 2024\n12 min read\n  * ![Image from Canva.](https://towardsdatascience.com/wp-content/uploads/2024/08/1UAA9jQVdqMXnwzYiz8Q53Q.png)\n## [3 AI Use Cases (That Are Not a Chatbot)](https://towardsdatascience.com/3-ai-use-cases-that-are-not-a-chatbot-f4f328a2707a/)\n[ Machine Learning ](https://towardsdatascience.com/category/artificial-intelligence/machine-learning/)\nFeature engineering, structuring unstructured data, and lead scoring \n[Shaw Talebi](https://towardsdatascience.com/author/shawhin/)\nAugust 21, 2024\n7 min read\n  * ## [Solving a Constrained Project Scheduling Problem with Quantum Annealing](https://towardsdatascience.com/solving-a-constrained-project-scheduling-problem-with-quantum-annealing-d0640e657a3b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nSolving the resource constrained project scheduling problem (RCPSP) with D-Wave‚Äôs hybrid constrained quadratic model (CQM) \n[Luis Fernando P√âREZ ARMAS, Ph.D.](https://towardsdatascience.com/author/luisfernandopa1212/)\nAugust 20, 2024\n29 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2023/02/1VEUgT5T4absnTqBMOEuNig.png)\n## [Back To Basics, Part Uno: Linear Regression and Cost Function](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nAn illustrated guide on essential machine learning concepts \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nFebruary 3, 2023\n6 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/1kM8tfYcdaoccB1HX71YDig.png)\n## [Must-Know in Statistics: The Bivariate Normal Projection Explained](https://towardsdatascience.com/must-know-in-statistics-the-bivariate-normal-projection-explained-ace7b2f70b5b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nDerivation and practical examples of this powerful concept \n[Luigi Battistoni](https://towardsdatascience.com/author/lu-battistoni/)\nAugust 14, 2024\n7 min read\n  * ![Photo by Jess Bailey on Unsplash](https://towardsdatascience.com/wp-content/uploads/2022/09/11tHmNYFaWWtWG5I7bNeN6g-scaled.jpeg)\n## [How to Make the Most of Your Experience as a TDS Author](https://towardsdatascience.com/how-to-make-the-most-of-your-experience-as-a-tds-author-b1e056be63f1/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nA quick guide to our resources and FAQ \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nSeptember 13, 2022\n4 min read\n\n\n  * [YouTube](https://www.youtube.com/c/TowardsDataScience)\n  * [X](https://x.com/TDataScience)\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [Threads](https://www.threads.net/@towardsdatascience)\n  * [Bluesky](https://bsky.app/profile/towardsdatascience.com)\n\n\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nYour home for data science and Al. The world‚Äôs leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\n¬©  Insight Media Group, LLC 2025 \n  * [About](https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/)\n  * [Privacy Policy](https://towardsdatascience.com/privacy-policy/)\n  * [Terms of Use](https://towardsdatascience.com/website-terms-of-use/)\n\n\n[Towards Data Science is now independent!](https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/)\nCookies Settings\n## Sign up to our newsletter\nEmail address*\nFirst name*\nLast name*\nJob title*\nJob level*\nPlease SelectC-LevelVP/DirectorManager/SupervisorMid Level or Senior Non-Managerial StaffEntry Level/Junior StaffFreelancer/ContractorStudent/InternOther\nCompany name*\n  * I consent to receive newsletters and other communications from Towards Data Science publications.*\n\n\nSome areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c/#wp--skip-link--target",
        "https://towardsdatascience.com/",
        "https://towardsdatascience.com/latest/",
        "https://towardsdatascience.com/tag/editors-pick/",
        "https://towardsdatascience.com/tag/deep-dives/",
        "https://towardsdatascience.com/questions-96667b06af5/",
        "https://newsletter.towardsdatascience.com/subscription-to-the-newsletter",
        "https://towardsdatascience.com/category/data-science/",
        "https://towardsdatascience.com/author/mlabonne/",
        "https://towardsdatascience.com/tag/large-language-models/",
        "https://towardsdatascience.com/tag/quantization/",
        "https://towardsdatascience.com/tag/data-science/",
        "https://towardsdatascience.com/tag/hands-on-tutorials/",
        "https://towardsdatascience.com/tag/machine-learning/",
        "https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/",
        "https://towardsdatascience.com/category/artificial-intelligence/",
        "https://towardsdatascience.com/author/shreya-rao/",
        "https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/",
        "https://towardsdatascience.com/author/tarikdzekman/",
        "https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/",
        "https://towardsdatascience.com/author/dr-robert-kuebler/",
        "https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/",
        "https://towardsdatascience.com/author/piero-paialunga/",
        "https://towardsdatascience.com/3-ai-use-cases-that-are-not-a-chatbot-f4f328a2707a/",
        "https://towardsdatascience.com/category/artificial-intelligence/machine-learning/",
        "https://towardsdatascience.com/author/shawhin/",
        "https://towardsdatascience.com/solving-a-constrained-project-scheduling-problem-with-quantum-annealing-d0640e657a3b/",
        "https://towardsdatascience.com/author/luisfernandopa1212/",
        "https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/",
        "https://towardsdatascience.com/must-know-in-statistics-the-bivariate-normal-projection-explained-ace7b2f70b5b/",
        "https://towardsdatascience.com/author/lu-battistoni/",
        "https://towardsdatascience.com/how-to-make-the-most-of-your-experience-as-a-tds-author-b1e056be63f1/",
        "https://towardsdatascience.com/author/towardsdatascience/",
        "https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/",
        "https://towardsdatascience.com/privacy-policy/",
        "https://towardsdatascience.com/website-terms-of-use/",
        "https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/",
        "https://contributor.insightmediagroup.io/",
        "https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca",
        "https://x.com/TDataScience",
        "https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing",
        "https://github.com/mlabonne/llm-course/blob/main/Introduction_to_Weight_Quantization.ipynb",
        "https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html",
        "https://arxiv.org/abs/2208.07339",
        "https://arxiv.org/abs/2210.17323",
        "https://twitter.com/maximelabonne",
        "https://huggingface.co/blog/hf-bitsandbytes-integration",
        "https://arxiv.org/abs/2103.13630",
        "https://arxiv.org/abs/2004.09602",
        "https://lilianweng.github.io/posts/2023-01-10-inference-optimization/",
        "https://int8.io/local-large-language-models-beginners-guide/",
        "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c%2F&title=Introduction%20to%20Weight%20Quantization",
        "https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c%2F&title=Introduction%20to%20Weight%20Quantization",
        "https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-weight-quantization-2494701b9c0c%2F&text=Introduction%20to%20Weight%20Quantization",
        "https://www.youtube.com/c/TowardsDataScience",
        "https://www.threads.net/@towardsdatascience",
        "https://bsky.app/profile/towardsdatascience.com"
    ]
}