[Skip to content](https://github.com/NVIDIA/TensorRT-LLM/#start-of-content)
## Navigation Menu
Toggle navigation
[ ](https://github.com/)
[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT-LLM%2F)
  * Product 
    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)
    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)
    * [ Actions Automate any workflow  ](https://github.com/features/actions)
    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)
    * [ Issues Plan and track work  ](https://github.com/features/issues)
    * [ Code Review Manage code changes  ](https://github.com/features/code-review)
    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)
    * [ Code Search Find more, search less  ](https://github.com/features/code-search)
Explore
    * [ All features ](https://github.com/features)
    * [ Documentation ](https://docs.github.com)
    * [ GitHub Skills ](https://skills.github.com)
    * [ Blog ](https://github.blog)
  * Solutions 
By company size
    * [ Enterprises ](https://github.com/enterprise)
    * [ Small and medium teams ](https://github.com/team)
    * [ Startups ](https://github.com/enterprise/startups)
    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)
By use case
    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)
    * [ DevOps ](https://github.com/solutions/use-case/devops)
    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)
    * [ View all use cases ](https://github.com/solutions/use-case)
By industry
    * [ Healthcare ](https://github.com/solutions/industry/healthcare)
    * [ Financial services ](https://github.com/solutions/industry/financial-services)
    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)
    * [ Government ](https://github.com/solutions/industry/government)
    * [ View all industries ](https://github.com/solutions/industry)
[ View all solutions ](https://github.com/solutions)
  * Resources 
Topics
    * [ AI ](https://github.com/resources/articles/ai)
    * [ DevOps ](https://github.com/resources/articles/devops)
    * [ Security ](https://github.com/resources/articles/security)
    * [ Software Development ](https://github.com/resources/articles/software-development)
    * [ View all ](https://github.com/resources/articles)
Explore
    * [ Learning Pathways ](https://resources.github.com/learn/pathways)
    * [ Events & Webinars ](https://resources.github.com)
    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)
    * [ Customer Stories ](https://github.com/customer-stories)
    * [ Partners ](https://partner.github.com)
    * [ Executive Insights ](https://github.com/solutions/executive-insights)
  * Open Source 
    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)
    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)
Repositories
    * [ Topics ](https://github.com/topics)
    * [ Trending ](https://github.com/trending)
    * [ Collections ](https://github.com/collections)
  * Enterprise 
    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)
Available add-ons
    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)
    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)
    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)
  * [Pricing](https://github.com/pricing)


Search or jump to...
# Search code, repositories, users, issues, pull requests...
Search 
Clear
[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)
#  Provide feedback 
We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
Cancel  Submit feedback 
#  Saved searches 
## Use saved searches to filter your results more quickly
Name
Query
To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). 
Cancel  Create saved search 
[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT-LLM%2F)
[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=NVIDIA%2FTensorRT-LLM) Reseting focus
You signed in with another tab or window. [Reload](https://github.com/NVIDIA/TensorRT-LLM/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/NVIDIA/TensorRT-LLM/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/NVIDIA/TensorRT-LLM/) to refresh your session. Dismiss alert
{{ message }}
[ NVIDIA ](https://github.com/NVIDIA) / **[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) ** Public
  * [ Notifications ](https://github.com/login?return_to=%2FNVIDIA%2FTensorRT-LLM) You must be signed in to change notification settings
  * [ Fork 1.3k ](https://github.com/login?return_to=%2FNVIDIA%2FTensorRT-LLM)
  * [ Star  10k ](https://github.com/login?return_to=%2FNVIDIA%2FTensorRT-LLM)


TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. 
[nvidia.github.io/TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM "https://nvidia.github.io/TensorRT-LLM")
### License
[ Apache-2.0 license ](https://github.com/NVIDIA/TensorRT-LLM/blob/main/LICENSE)
[ 10k stars ](https://github.com/NVIDIA/TensorRT-LLM/stargazers) [ 1.3k forks ](https://github.com/NVIDIA/TensorRT-LLM/forks) [ Branches ](https://github.com/NVIDIA/TensorRT-LLM/branches) [ Tags ](https://github.com/NVIDIA/TensorRT-LLM/tags) [ Activity ](https://github.com/NVIDIA/TensorRT-LLM/activity)
[ Star  ](https://github.com/login?return_to=%2FNVIDIA%2FTensorRT-LLM)
[ Notifications ](https://github.com/login?return_to=%2FNVIDIA%2FTensorRT-LLM) You must be signed in to change notification settings
  * [ Code ](https://github.com/NVIDIA/TensorRT-LLM)
  * [ Issues 447 ](https://github.com/NVIDIA/TensorRT-LLM/issues)
  * [ Pull requests 142 ](https://github.com/NVIDIA/TensorRT-LLM/pulls)
  * [ Discussions ](https://github.com/NVIDIA/TensorRT-LLM/discussions)
  * [ Actions ](https://github.com/NVIDIA/TensorRT-LLM/actions)
  * [ Projects 0 ](https://github.com/NVIDIA/TensorRT-LLM/projects)
  * [ Security ](https://github.com/NVIDIA/TensorRT-LLM/security)
  * [ Insights ](https://github.com/NVIDIA/TensorRT-LLM/pulse)


Additional navigation options
  * [ Code  ](https://github.com/NVIDIA/TensorRT-LLM)
  * [ Issues  ](https://github.com/NVIDIA/TensorRT-LLM/issues)
  * [ Pull requests  ](https://github.com/NVIDIA/TensorRT-LLM/pulls)
  * [ Discussions  ](https://github.com/NVIDIA/TensorRT-LLM/discussions)
  * [ Actions  ](https://github.com/NVIDIA/TensorRT-LLM/actions)
  * [ Projects  ](https://github.com/NVIDIA/TensorRT-LLM/projects)
  * [ Security  ](https://github.com/NVIDIA/TensorRT-LLM/security)
  * [ Insights  ](https://github.com/NVIDIA/TensorRT-LLM/pulse)


# NVIDIA/TensorRT-LLM
main
[Branches](https://github.com/NVIDIA/TensorRT-LLM/branches)[Tags](https://github.com/NVIDIA/TensorRT-LLM/tags)
[](https://github.com/NVIDIA/TensorRT-LLM/branches)[](https://github.com/NVIDIA/TensorRT-LLM/tags)
Go to file
Code
## Folders and files
Name| Name| Last commit message| Last commit date  
---|---|---|---  
## Latest commit
## History
[208 Commits](https://github.com/NVIDIA/TensorRT-LLM/commits/main/)[](https://github.com/NVIDIA/TensorRT-LLM/commits/main/)  
[.devcontainer](https://github.com/NVIDIA/TensorRT-LLM/tree/main/.devcontainer ".devcontainer")| [.devcontainer](https://github.com/NVIDIA/TensorRT-LLM/tree/main/.devcontainer ".devcontainer")  
[.github](https://github.com/NVIDIA/TensorRT-LLM/tree/main/.github ".github")| [.github](https://github.com/NVIDIA/TensorRT-LLM/tree/main/.github ".github")  
[3rdparty](https://github.com/NVIDIA/TensorRT-LLM/tree/main/3rdparty "3rdparty")| [3rdparty](https://github.com/NVIDIA/TensorRT-LLM/tree/main/3rdparty "3rdparty")  
[benchmarks](https://github.com/NVIDIA/TensorRT-LLM/tree/main/benchmarks "benchmarks")| [benchmarks](https://github.com/NVIDIA/TensorRT-LLM/tree/main/benchmarks "benchmarks")  
[cpp](https://github.com/NVIDIA/TensorRT-LLM/tree/main/cpp "cpp")| [cpp](https://github.com/NVIDIA/TensorRT-LLM/tree/main/cpp "cpp")  
[docker](https://github.com/NVIDIA/TensorRT-LLM/tree/main/docker "docker")| [docker](https://github.com/NVIDIA/TensorRT-LLM/tree/main/docker "docker")  
[docs](https://github.com/NVIDIA/TensorRT-LLM/tree/main/docs "docs")| [docs](https://github.com/NVIDIA/TensorRT-LLM/tree/main/docs "docs")  
[examples](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples "examples")| [examples](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples "examples")  
[jenkins](https://github.com/NVIDIA/TensorRT-LLM/tree/main/jenkins "jenkins")| [jenkins](https://github.com/NVIDIA/TensorRT-LLM/tree/main/jenkins "jenkins")  
[scripts](https://github.com/NVIDIA/TensorRT-LLM/tree/main/scripts "scripts")| [scripts](https://github.com/NVIDIA/TensorRT-LLM/tree/main/scripts "scripts")  
[tensorrt_llm](https://github.com/NVIDIA/TensorRT-LLM/tree/main/tensorrt_llm "tensorrt_llm")| [tensorrt_llm](https://github.com/NVIDIA/TensorRT-LLM/tree/main/tensorrt_llm "tensorrt_llm")  
[tests](https://github.com/NVIDIA/TensorRT-LLM/tree/main/tests "tests")| [tests](https://github.com/NVIDIA/TensorRT-LLM/tree/main/tests "tests")  
[.clang-format](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.clang-format ".clang-format")| [.clang-format](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.clang-format ".clang-format")  
[.clang-tidy](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.clang-tidy ".clang-tidy")| [.clang-tidy](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.clang-tidy ".clang-tidy")  
[.clangd](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.clangd ".clangd")| [.clangd](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.clangd ".clangd")  
[.cursorignore](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.cursorignore ".cursorignore")| [.cursorignore](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.cursorignore ".cursorignore")  
[.dockerignore](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.dockerignore ".dockerignore")| [.dockerignore](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.dockerignore ".dockerignore")  
[.gitattributes](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.gitattributes ".gitattributes")| [.gitattributes](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.gitattributes ".gitattributes")  
[.gitignore](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.gitignore ".gitignore")| [.gitignore](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.gitignore ".gitignore")  
[.gitmodules](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.gitmodules ".gitmodules")| [.gitmodules](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.gitmodules ".gitmodules")  
[.pre-commit-config.yaml](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.pre-commit-config.yaml ".pre-commit-config.yaml")| [.pre-commit-config.yaml](https://github.com/NVIDIA/TensorRT-LLM/blob/main/.pre-commit-config.yaml ".pre-commit-config.yaml")  
[CODE_OF_CONDUCT.md](https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODE_OF_CONDUCT.md "CODE_OF_CONDUCT.md")| [CODE_OF_CONDUCT.md](https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODE_OF_CONDUCT.md "CODE_OF_CONDUCT.md")  
[CODING_GUIDELINES.md](https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md "CODING_GUIDELINES.md")| [CODING_GUIDELINES.md](https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md "CODING_GUIDELINES.md")  
[CONTRIBUTING.md](https://github.com/NVIDIA/TensorRT-LLM/blob/main/CONTRIBUTING.md "CONTRIBUTING.md")| [CONTRIBUTING.md](https://github.com/NVIDIA/TensorRT-LLM/blob/main/CONTRIBUTING.md "CONTRIBUTING.md")  
[LICENSE](https://github.com/NVIDIA/TensorRT-LLM/blob/main/LICENSE "LICENSE")| [LICENSE](https://github.com/NVIDIA/TensorRT-LLM/blob/main/LICENSE "LICENSE")  
[README.md](https://github.com/NVIDIA/TensorRT-LLM/blob/main/README.md "README.md")| [README.md](https://github.com/NVIDIA/TensorRT-LLM/blob/main/README.md "README.md")  
[pyproject.toml](https://github.com/NVIDIA/TensorRT-LLM/blob/main/pyproject.toml "pyproject.toml")| [pyproject.toml](https://github.com/NVIDIA/TensorRT-LLM/blob/main/pyproject.toml "pyproject.toml")  
[requirements-dev.txt](https://github.com/NVIDIA/TensorRT-LLM/blob/main/requirements-dev.txt "requirements-dev.txt")| [requirements-dev.txt](https://github.com/NVIDIA/TensorRT-LLM/blob/main/requirements-dev.txt "requirements-dev.txt")  
[requirements.txt](https://github.com/NVIDIA/TensorRT-LLM/blob/main/requirements.txt "requirements.txt")| [requirements.txt](https://github.com/NVIDIA/TensorRT-LLM/blob/main/requirements.txt "requirements.txt")  
[setup.py](https://github.com/NVIDIA/TensorRT-LLM/blob/main/setup.py "setup.py")| [setup.py](https://github.com/NVIDIA/TensorRT-LLM/blob/main/setup.py "setup.py")  
View all files  
## Repository files navigation
  * [README](https://github.com/NVIDIA/TensorRT-LLM/)
  * [Code of conduct](https://github.com/NVIDIA/TensorRT-LLM/)
  * [Apache-2.0 license](https://github.com/NVIDIA/TensorRT-LLM/)


# TensorRT-LLM
[](https://github.com/NVIDIA/TensorRT-LLM/#tensorrt-llm)
####  A TensorRT Toolbox for Optimized Large Language Model Inference
[](https://github.com/NVIDIA/TensorRT-LLM/#-a-tensorrt-toolbox-for-optimized-large-language-model-inference)
[![Documentation](https://camo.githubusercontent.com/c191e0610a0b83ed5bef19569b59b7f27f7da4b11c720841af9028c9f239bf97/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d627269676874677265656e2e7376673f7374796c653d666c6174)](https://nvidia.github.io/TensorRT-LLM/) [![python](https://camo.githubusercontent.com/c9460927ab208c58be7bb60094b09a7a29f7ab9e09e59c555992fceef38d236e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31322d677265656e)](https://www.python.org/downloads/release/python-3123/) [![python](https://camo.githubusercontent.com/390d871e5bf777de2ed63aa5243962314ec1871c19ffc6d89cf29022cd4bf374/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31302d677265656e)](https://www.python.org/downloads/release/python-31012/) [![cuda](https://camo.githubusercontent.com/9c5d4216906c5fad8e51924bcc91bd8982e6ba715c1c97c51756bc515648f303/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f637564612d31322e382e302d677265656e)](https://developer.nvidia.com/cuda-downloads) [![trt](https://camo.githubusercontent.com/f441034dbbaf679670a627a8860ca4f813d66b7ea2e887411270b43e52d65819/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5452542d31302e382e302d677265656e)](https://developer.nvidia.com/tensorrt) [![version](https://camo.githubusercontent.com/d0e4e36fb8421a0fe81fd0d1133f6dc6cfa19a5a831a34a17b8b73d59c22783f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72656c656173652d302e31392e302e6465762d677265656e)](https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/version.py) [![license](https://camo.githubusercontent.com/babc55b476ce60b545de3012f13503eea326b5d8d8b9957b2d850c2e3f0cf507/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322d626c7565)](https://github.com/NVIDIA/TensorRT-LLM/blob/main/LICENSE)
[Architecture](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/overview.md) | [Performance](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md) | [Examples](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples) | [Documentation](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source) | [Roadmap](https://docs.google.com/presentation/d/1gycPmtdh7uUcH6laOvW65Dbp9F1McUkGDIcAyjicBZs/edit?usp=sharing)
## Latest News
[](https://github.com/NVIDIA/TensorRT-LLM/#latest-news)
  * [2025/02/28] Spotlight🌟🌟🌟 NAVER Place Optimizes SLM-Based Vertical Services with NVIDIA TensorRT-LLM [➡️ link](https://developer.nvidia.com/blog/spotlight-naver-place-optimizes-slm-based-vertical-services-with-nvidia-tensorrt-llm/)


[![](https://camo.githubusercontent.com/dc665ae848ea9726001f87703cc2c9ed10170ff6271a08f01c121d01ac8d51ca/68747470733a2f2f646576656c6f7065722d626c6f67732e6e76696469612e636f6d2f77702d636f6e74656e742f75706c6f6164732f323032352f30322f6e617665722d706c6163652d6170702d677261706869632e6a7067)](https://camo.githubusercontent.com/dc665ae848ea9726001f87703cc2c9ed10170ff6271a08f01c121d01ac8d51ca/68747470733a2f2f646576656c6f7065722d626c6f67732e6e76696469612e636f6d2f77702d636f6e74656e742f75706c6f6164732f323032352f30322f6e617665722d706c6163652d6170702d677261706869632e6a7067)
  * [2025/02/25] 🌟 DeepSeek-R1 performance now optimized for Blackwell [➡️ link](https://huggingface.co/nvidia/DeepSeek-R1-FP4)
  * [2025/02/20] Stay ahead of the curve 📈 Leverage the most performant & capable platform for inference. 🚀Explore the complete guide to achieve great accuracy, high throughput, and low latency at the lowest cost for your business ➡️ [➡️ link](https://www.nvidia.com/en-us/solutions/ai/inference/balancing-cost-latency-and-performance-ebook/?ncid=so-twit-348956&linkId=100000341423615)
  * [2025/02/18] Unlock #LLM inference with auto-scaling on @AWS EKS ✨ ✅ Set up EFA networking & EFS storage with NVLink-connected H100 GPUs on P5.48xlarge ✅ Deploy multi-node Triton Inference Server + TRT-LLM to scale LLaMa 3.1 405B models ✅ Use LeaderWorkerSet & Prometheus metrics for autoscaling and orchestration [➡️ link](https://aws.amazon.com/blogs/hpc/scaling-your-llm-inference-workloads-multi-node-deployment-with-tensorrt-llm-and-triton-on-amazon-eks/)
  * [2025/02/12] 🦸⚡ Automating GPU Kernel Generation with DeepSeek-R1 and Inference Time Scaling [➡️ link](https://developer.nvidia.com/blog/automating-gpu-kernel-generation-with-deepseek-r1-and-inference-time-scaling/?ncid=so-twit-997075&linkId=100000338909937)
  * [2025/02/12] 🌟 How Scaling Laws Drive Smarter, More Powerful AI [➡️ link](https://blogs.nvidia.com/blog/ai-scaling-laws/?ncid=so-link-889273&linkId=100000338837832)
  * [2025/01/25] 🌟 Nvidia moves AI focus to inference cost, efficiency 🌟 [➡️ link](https://www.fierceelectronics.com/ai/nvidia-moves-ai-focus-inference-cost-efficiency?linkId=100000332985606)
  * [2025/01/24] 🏎️🏎️🏎️ Optimize AI Inference Performance with NVIDIA Full-Stack Solutions [➡️ link](https://developer.nvidia.com/blog/optimize-ai-inference-performance-with-nvidia-full-stack-solutions/?ncid=so-twit-400810&linkId=100000332621049)
  * [2025/01/23] 🚀🚀🚀 Fast, Low-Cost Inference Offers Key to Profitable AI [➡️ link](https://blogs.nvidia.com/blog/ai-inference-platform/?ncid=so-twit-693236-vt04&linkId=100000332307804)
  * [2025/01/16] 🐍 Introducing New KV Cache Reuse Optimizations in NVIDIA TensorRT-LLM [➡️ link](https://developer.nvidia.com/blog/introducing-new-kv-cache-reuse-optimizations-in-nvidia-tensorrt-llm/?ncid=so-twit-363876&linkId=100000330323229)
  * [2025/01/14] 📣 Bing's Transition to LLM/SLM Models: Optimizing Search with TensorRT-LLM [➡️ link](https://blogs.bing.com/search-quality-insights/December-2024/Bing-s-Transition-to-LLM-SLM-Models-Optimizing-Search-with-TensorRT-LLM)
  * [2025/01/04] ⚡Boost Llama 3.3 70B Inference Throughput 3x with NVIDIA TensorRT-LLM Speculative Decoding [➡️ link](https://developer.nvidia.com/blog/boost-llama-3-3-70b-inference-throughput-3x-with-nvidia-tensorrt-llm-speculative-decoding/)

Previous News
  * [2024/12/10] ⚡ Llama 3.3 70B from AI at Meta is accelerated by TensorRT-LLM. 🌟 State-of-the-art model on par with Llama 3.1 405B for reasoning, math, instruction following and tool use. Explore the preview [➡️ link](https://build.nvidia.com/meta/llama-3_3-70b-instruct)
  * [2024/12/03] 🌟 Boost your AI inference throughput by up to 3.6x. We now support speculative decoding and tripling token throughput with our NVIDIA TensorRT-LLM. Perfect for your generative AI apps. ⚡Learn how in this technical deep dive [➡️ link](https://nvda.ws/3ZCZTzD)
  * [2024/12/02] Working on deploying ONNX models for performance-critical applications? Try our NVIDIA Nsight Deep Learning Designer ⚡ A user-friendly GUI and tight integration with NVIDIA TensorRT that offers: ✅ Intuitive visualization of ONNX model graphs ✅ Quick tweaking of model architecture and parameters ✅ Detailed performance profiling with either ORT or TensorRT ✅ Easy building of TensorRT engines [➡️ link](https://developer.nvidia.com/nsight-dl-designer?ncid=so-link-485689&linkId=100000315016072)
  * [2024/11/26] 📣 Introducing TensorRT-LLM for Jetson AGX Orin, making it even easier to deploy on Jetson AGX Orin with initial support in JetPack 6.1 via the v0.12.0-jetson branch of the TensorRT-LLM repo. ✅ Pre-compiled TensorRT-LLM wheels & containers for easy integration ✅ Comprehensive guides & docs to get you started [➡️ link](https://forums.developer.nvidia.com/t/tensorrt-llm-for-jetson/313227?linkId=100000312718869)
  * [2024/11/21] NVIDIA TensorRT-LLM Multiblock Attention Boosts Throughput by More Than 3x for Long Sequence Lengths on NVIDIA HGX H200 [➡️ link](https://developer.nvidia.com/blog/nvidia-tensorrt-llm-multiblock-attention-boosts-throughput-by-more-than-3x-for-long-sequence-lengths-on-nvidia-hgx-h200/)
  * [2024/11/19] Llama 3.2 Full-Stack Optimizations Unlock High Performance on NVIDIA GPUs [➡️ link](https://developer.nvidia.com/blog/llama-3-2-full-stack-optimizations-unlock-high-performance-on-nvidia-gpus/?ncid=so-link-721194)
  * [2024/11/09] 🚀🚀🚀 3x Faster AllReduce with NVSwitch and TensorRT-LLM MultiShot [➡️ link](https://developer.nvidia.com/blog/3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot/)
  * [2024/11/09] ✨ NVIDIA advances the AI ecosystem with the AI model of LG AI Research 🙌 [➡️ link](https://blogs.nvidia.co.kr/blog/nvidia-lg-ai-research/)
  * [2024/11/02] 🌟🌟🌟 NVIDIA and LlamaIndex Developer Contest 🙌 Enter for a chance to win prizes including an NVIDIA® GeForce RTX™ 4080 SUPER GPU, DLI credits, and more🙌 [➡️ link](https://developer.nvidia.com/llamaindex-developer-contest)
  * [2024/10/28] 🏎️🏎️🏎️ NVIDIA GH200 Superchip Accelerates Inference by 2x in Multiturn Interactions with Llama Models [➡️ link](https://developer.nvidia.com/blog/nvidia-gh200-superchip-accelerates-inference-by-2x-in-multiturn-interactions-with-llama-models/)
  * [2024/10/22] New 📝 Step-by-step instructions on how to ✅ Optimize LLMs with NVIDIA TensorRT-LLM, ✅ Deploy the optimized models with Triton Inference Server, ✅ Autoscale LLMs deployment in a Kubernetes environment. 🙌 Technical Deep Dive: [➡️ link](https://nvda.ws/3YgI8UT)
  * [2024/10/07] 🚀🚀🚀Optimizing Microsoft Bing Visual Search with NVIDIA Accelerated Libraries [➡️ link](https://developer.nvidia.com/blog/optimizing-microsoft-bing-visual-search-with-nvidia-accelerated-libraries/)
  * [2024/09/29] 🌟 AI at Meta PyTorch + TensorRT v2.4 🌟 ⚡TensorRT 10.1 ⚡PyTorch 2.4 ⚡CUDA 12.4 ⚡Python 3.12 [➡️ link](https://github.com/pytorch/TensorRT/releases/tag/v2.4.0)
  * [2024/09/17] ✨ NVIDIA TensorRT-LLM Meetup [➡️ link](https://drive.google.com/file/d/1RR8GqC-QbuaKuHj82rZcXb3MS20SWo6F/view?usp=share_link)
  * [2024/09/17] ✨ Accelerating LLM Inference at Databricks with TensorRT-LLM [➡️ link](https://drive.google.com/file/d/1NeSmrLaWRJAY1rxD9lJmzpB9rzr38j8j/view?usp=sharing)
  * [2024/09/17] ✨ TensorRT-LLM @ Baseten [➡️ link](https://drive.google.com/file/d/1Y7L2jqW-aRmt31mCdqhwvGMmCSOzBUjG/view?usp=share_link)
  * [2024/09/04] 🏎️🏎️🏎️ Best Practices for Tuning TensorRT-LLM for Optimal Serving with BentoML [➡️ link](https://www.bentoml.com/blog/tuning-tensor-rt-llm-for-optimal-serving-with-bentoml)
  * [2024/08/20] 🏎️SDXL with #TensorRT Model Optimizer ⏱️⚡ 🏁 cache diffusion 🏁 quantization aware training 🏁 QLoRA 🏁 #Python 3.12 [➡️ link](https://developer.nvidia.com/blog/nvidia-tensorrt-model-optimizer-v0-15-boosts-inference-performance-and-expands-model-support/)
  * [2024/08/13] 🐍 DIY Code Completion with #Mamba ⚡ #TensorRT #LLM for speed 🤖 NIM for ease ☁️ deploy anywhere [➡️ link](https://developer.nvidia.com/blog/revolutionizing-code-completion-with-codestral-mamba-the-next-gen-coding-llm/)
  * [2024/08/06] 🗫 Multilingual Challenge Accepted 🗫 🤖 #TensorRT #LLM boosts low-resource languages like Hebrew, Indonesian and Vietnamese ⚡[➡️ link](https://developer.nvidia.com/blog/accelerating-hebrew-llm-performance-with-nvidia-tensorrt-llm/?linkId=100000278659647)
  * [2024/07/30] Introducing🍊 @SliceXAI ELM Turbo 🤖 train ELM once ⚡ #TensorRT #LLM optimize ☁️ deploy anywhere [➡️ link](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms)
  * [2024/07/23] 👀 @AIatMeta Llama 3.1 405B trained on 16K NVIDIA H100s - inference is #TensorRT #LLM optimized ⚡ 🦙 400 tok/s - per node 🦙 37 tok/s - per user 🦙 1 node inference [➡️ link](https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms)
  * [2024/07/09] Checklist to maximize multi-language performance of @meta #Llama3 with #TensorRT #LLM inference: ✅ MultiLingual ✅ NIM ✅ LoRA tuned adaptors[➡️ Tech blog](https://developer.nvidia.com/blog/deploy-multilingual-llms-with-nvidia-nim/)
  * [2024/07/02] Let the @MistralAI MoE tokens fly 📈 🚀 #Mixtral 8x7B with NVIDIA #TensorRT #LLM on #H100. [➡️ Tech blog](https://developer.nvidia.com/blog/achieving-high-mixtral-8x7b-performance-with-nvidia-h100-tensor-core-gpus-and-tensorrt-llm?ncid=so-twit-928467)
  * [2024/06/24] Enhanced with NVIDIA #TensorRT #LLM, @upstage.ai’s solar-10.7B-instruct is ready to power your developer projects through our API catalog 🏎️. ✨[➡️ link](https://build.nvidia.com/upstage/solar-10_7b-instruct?snippet_tab=Try)
  * [2024/06/18] CYMI: 🤩 Stable Diffusion 3 dropped last week 🎊 🏎️ Speed up your SD3 with #TensorRT INT8 Quantization[➡️ link](https://build.nvidia.com/upstage/solar-10_7b-instruct?snippet_tab=Try)
  * [2024/06/18] 🧰Deploying ComfyUI with TensorRT? Here’s your setup guide [➡️ link](https://github.com/comfyanonymous/ComfyUI_TensorRT)
  * [2024/06/11] ✨#TensorRT Weight-Stripped Engines ✨ Technical Deep Dive for serious coders ✅+99% compression ✅1 set of weights → ** GPUs ✅0 performance loss ✅** models…LLM, CNN, etc.[➡️ link](https://developer.nvidia.com/blog/maximum-performance-and-minimum-footprint-for-ai-apps-with-nvidia-tensorrt-weight-stripped-engines/)
  * [2024/06/04] ✨ #TensorRT and GeForce #RTX unlock ComfyUI SD superhero powers 🦸⚡ 🎥 Demo: [➡️ link](https://youtu.be/64QEVfbPHyg) 📗 DIY notebook: [➡️ link](https://console.brev.dev/launchable/deploy?userID=2x2sil999&orgID=ktj33l4xj&name=ComfyUI_TensorRT&instance=L4%40g2-standard-4%3Anvidia-l4%3A1&diskStorage=500&cloudID=GCP&baseImage=docker.io%2Fpytorch%2Fpytorch%3A2.2.0-cuda12.1-cudnn8-runtime&ports=ComfUI%3A8188&file=https%3A%2F%2Fgithub.com%2Fbrevdev%2Fnotebooks%2Fblob%2Fmain%2Ftensorrt-comfyui.ipynb&launchableID=env-2hQX3n7ae5mq3NjNZ32DfAG0tJf)
  * [2024/05/28] ✨#TensorRT weight stripping for ResNet-50 ✨ ✅+99% compression ✅1 set of weights → ** GPUs\ ✅0 performance loss ✅** models…LLM, CNN, etc 👀 📚 DIY [➡️ link](https://console.brev.dev/launchable/deploy?userID=2x2sil999&orgID=ktj33l4xj&launchableID=env-2h6bym7h5GFNho3vpWQQeUYMwTM&instance=L4%40g6.xlarge&diskStorage=500&cloudID=devplane-brev-1&baseImage=nvcr.io%2Fnvidia%2Ftensorrt%3A24.05-py3&file=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT%2Fblob%2Frelease%2F10.0%2Fsamples%2Fpython%2Fsample_weight_stripping%2Fnotebooks%2Fweight_stripping.ipynb&name=tensorrt_weight_stripping_resnet50)
  * [2024/05/21] ✨@modal_labs has the codes for serverless @AIatMeta Llama 3 on #TensorRT #LLM ✨👀 📚 Marvelous Modal Manual: Serverless TensorRT-LLM (LLaMA 3 8B) | Modal Docs [➡️ link](https://modal.com/docs/examples/trtllm_llama)
  * [2024/05/08] NVIDIA TensorRT Model Optimizer -- the newest member of the #TensorRT ecosystem is a library of post-training and training-in-the-loop model optimization techniques ✅quantization ✅sparsity ✅QAT [➡️ blog](https://developer.nvidia.com/blog/accelerate-generative-ai-inference-performance-with-nvidia-tensorrt-model-optimizer-now-publicly-available/)
  * [2024/05/07] 🦙🦙🦙 24,000 tokens per second 🛫Meta Llama 3 takes off with #TensorRT #LLM 📚[➡️ link](https://blogs.nvidia.com/blog/meta-llama3-inference-acceleration/)
  * [2024/02/06] [🚀 Speed up inference with SOTA quantization techniques in TRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/quantization-in-TRT-LLM.md)
  * [2024/01/30] [ New XQA-kernel provides 2.4x more Llama-70B throughput within the same latency budget](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/XQA-kernel.md)
  * [2023/12/04] [Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/Falcon180B-H200.md)
  * [2023/11/27] [SageMaker LMI now supports TensorRT-LLM - improves throughput by 60%, compared to previous version](https://aws.amazon.com/blogs/machine-learning/boost-inference-performance-for-llms-with-new-amazon-sagemaker-containers/)
  * [2023/11/13] [H200 achieves nearly 12,000 tok/sec on Llama2-13B](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/H200launch.md)
  * [2023/10/22] [🚀 RAG on Windows using TensorRT-LLM and LlamaIndex 🦙](https://github.com/NVIDIA/trt-llm-rag-windows#readme)
  * [2023/10/19] Getting Started Guide - [Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available ](https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/)
  * [2023/10/17] [Large Language Models up to 4x Faster on RTX With TensorRT-LLM for Windows ](https://blogs.nvidia.com/blog/2023/10/17/tensorrt-llm-windows-stable-diffusion-rtx/)


## TensorRT-LLM Overview
[](https://github.com/NVIDIA/TensorRT-LLM/#tensorrt-llm-overview)
TensorRT-LLM is a library for optimizing Large Language Model (LLM) inference. It provides state-of-the-art optimizations, including custom attention kernels, inflight batching, paged KV caching, quantization (FP8, INT4 [AWQ](https://arxiv.org/abs/2306.00978), INT8 [SmoothQuant](https://arxiv.org/abs/2211.10438), ++) and much more, to perform inference efficiently on NVIDIA GPUs
TensorRT-LLM provides a Python API to build LLMs into optimized [TensorRT](https://developer.nvidia.com/tensorrt) engines. It contains runtimes in Python (bindings) and C++ to execute those TensorRT engines. It also includes a [backend](https://github.com/triton-inference-server/tensorrtllm_backend) for integration with the [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server). Models built with TensorRT-LLM can be executed on a wide range of configurations from a single GPU to multiple nodes with multiple GPUs (using [Tensor Parallelism](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/parallelisms.html#tensor-parallelism) and/or [Pipeline Parallelism](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/parallelisms.html#pipeline-parallelism)).
TensorRT-LLM comes with several popular models pre-defined. They can easily be modified and extended to fit custom needs via a PyTorch-like Python API. Refer to the [Support Matrix](https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html) for a list of supported models.
TensorRT-LLM is built on top of the [TensorRT](https://developer.nvidia.com/tensorrt) Deep Learning Inference library. It leverages much of TensorRT's deep learning optimizations and adds LLM-specific optimizations on top, as described above. TensorRT is an ahead-of-time compiler; it builds "[Engines](https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#ecosystem)" which are optimized representations of the compiled model containing the entire execution graph. These engines are optimized for a specific GPU architecture, and can be validated, benchmarked, and serialized for later deployment in a production environment.
## Getting Started
[](https://github.com/NVIDIA/TensorRT-LLM/#getting-started)
To get started with TensorRT-LLM, visit our documentation:
  * [Quick Start Guide](https://nvidia.github.io/TensorRT-LLM/quick-start-guide.html)
  * [Release Notes](https://nvidia.github.io/TensorRT-LLM/release-notes.html)
  * [Installation Guide for Linux](https://nvidia.github.io/TensorRT-LLM/installation/linux.html)
  * [Installation Guide for Grace Hopper](https://nvidia.github.io/TensorRT-LLM/installation/grace-hopper.html)
  * [Supported Hardware, Models, and other Software](https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html)


## Community
[](https://github.com/NVIDIA/TensorRT-LLM/#community)
  * [Model zoo](https://huggingface.co/TheFloat16) (generated by TRT-LLM rel 0.9 a9356d4b7610330e89c1010f342a9ac644215c52)


## About
TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. 
[nvidia.github.io/TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM "https://nvidia.github.io/TensorRT-LLM")
### Resources
[ Readme ](https://github.com/NVIDIA/TensorRT-LLM/#readme-ov-file)
### License
[ Apache-2.0 license ](https://github.com/NVIDIA/TensorRT-LLM/#Apache-2.0-1-ov-file)
### Code of conduct
[ Code of conduct ](https://github.com/NVIDIA/TensorRT-LLM/#coc-ov-file)
[ Activity](https://github.com/NVIDIA/TensorRT-LLM/activity)
[ Custom properties](https://github.com/NVIDIA/TensorRT-LLM/custom-properties)
### Stars
[ **10k** stars](https://github.com/NVIDIA/TensorRT-LLM/stargazers)
### Watchers
[ **112** watching](https://github.com/NVIDIA/TensorRT-LLM/watchers)
### Forks
[ **1.3k** forks](https://github.com/NVIDIA/TensorRT-LLM/forks)
[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT-LLM&report=NVIDIA+%28user%29)
##  [Releases 13](https://github.com/NVIDIA/TensorRT-LLM/releases)
[ TensorRT-LLM Release 0.17.0 Latest  Feb 7, 2025 ](https://github.com/NVIDIA/TensorRT-LLM/releases/tag/v0.17.0)
[+ 12 releases](https://github.com/NVIDIA/TensorRT-LLM/releases)
##  [Packages 0](https://github.com/orgs/NVIDIA/packages?repo_name=TensorRT-LLM)
No packages published 
##  [Contributors 119](https://github.com/NVIDIA/TensorRT-LLM/graphs/contributors)
[+ 105 contributors](https://github.com/NVIDIA/TensorRT-LLM/graphs/contributors)
## Languages
  * [ C++ 99.6% ](https://github.com/NVIDIA/TensorRT-LLM/search?l=c%2B%2B)
  * [ Python 0.3% ](https://github.com/NVIDIA/TensorRT-LLM/search?l=python)
  * [ Cuda 0.1% ](https://github.com/NVIDIA/TensorRT-LLM/search?l=cuda)
  * [ CMake 0.0% ](https://github.com/NVIDIA/TensorRT-LLM/search?l=cmake)
  * [ Groovy 0.0% ](https://github.com/NVIDIA/TensorRT-LLM/search?l=groovy)
  * [ Shell 0.0% ](https://github.com/NVIDIA/TensorRT-LLM/search?l=shell)


## Footer
[ ](https://github.com "GitHub") © 2025 GitHub, Inc. 
### Footer navigation
  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
  * [Security](https://github.com/security)
  * [Status](https://www.githubstatus.com/)
  * [Docs](https://docs.github.com/)
  * [Contact](https://support.github.com?tags=dotcom-footer)
  * Manage cookies 
  * Do not share my personal information 


You can’t perform that action at this time. 
