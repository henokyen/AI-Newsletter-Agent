{
    "id": "6afe59f9c15cd4e7b4cbaebbd5b19bd3",
    "metadata": {
        "id": "6afe59f9c15cd4e7b4cbaebbd5b19bd3",
        "url": "https://towardsdatascience.com/multi-rep-colbert-retrieval-models-for-rags-fe05381b8819/",
        "title": "Building RAGs Without A Retrieval Model Is a Terrible Mistake | Towards Data Science",
        "properties": {
            "description": null,
            "keywords": null,
            "author": "Thuwarakesh Murallie",
            "og:locale": "en_US",
            "og:type": "article",
            "og:title": "Building RAGs Without A Retrieval Model Is a Terrible Mistake | Towards Data Science",
            "og:description": "Here are my favorite techniques - one is faster, the other is more accurate.",
            "og:url": "https://towardsdatascience.com/multi-rep-colbert-retrieval-models-for-rags-fe05381b8819/",
            "og:site_name": "Towards Data Science",
            "og:image": "https://towardsdatascience.com/wp-content/uploads/2024/09/1AuRmIY2jCQyt-eMynSF3mQ-scaled.jpeg",
            "og:image:width": "2560",
            "og:image:height": "1707",
            "og:image:type": "image/jpeg",
            "twitter:card": "summary_large_image",
            "twitter:creator": "@TDataScience",
            "twitter:site": "@TDataScience",
            "twitter:label1": "Written by",
            "twitter:data1": "Thuwarakesh Murallie",
            "twitter:label2": "Est. reading time",
            "twitter:data2": "10 minutes"
        }
    },
    "parent_metadata": {
        "id": "f5cdb0fb930d58d0eca7f6beed23ee29",
        "url": "https://www.notion.so/RAG-f5cdb0fb930d58d0eca7f6beed23ee29",
        "title": "RAG",
        "properties": {
            "Type": [
                "Leaf"
            ]
        }
    },
    "content": "[Skip to content](https://towardsdatascience.com/multi-rep-colbert-retrieval-models-for-rags-fe05381b8819/#wp--skip-link--target)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nThe world‚Äôs leading publication for data science, AI, and ML professionals.\nSign in\nSign out\n[Contributor Portal](https://contributor.insightmediagroup.io/)\n  * [Latest](https://towardsdatascience.com/latest/)\n  * [Editor‚Äôs Picks](https://towardsdatascience.com/tag/editors-pick/)\n  * [Deep Dives](https://towardsdatascience.com/tag/deep-dives/)\n  * [Contribute](https://towardsdatascience.com/questions-96667b06af5/)\n  * [Newsletter](https://newsletter.towardsdatascience.com/subscription-to-the-newsletter)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\n\n\nToggle Mobile Navigation\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [X](https://x.com/TDataScience)\n\n\nToggle Search\nSearch\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\n# Building RAGs Without A Retrieval Model Is a Terrible Mistake\nHere are my favorite techniques ‚Äì one is faster, the other is more accurate. \n[Thuwarakesh Murallie](https://towardsdatascience.com/author/thuwarakesh/)\nSep 17, 2024\n10 min read\nShare \n![Photo by Alexander Grey](https://towardsdatascience.com/wp-content/uploads/2024/09/1AuRmIY2jCQyt-eMynSF3mQ-scaled.jpeg)Photo by [Alexander Grey](https://www.pexels.com/photo/person-holding-multicolored-container-1209870/)\nI build RAG apps; it‚Äôs fun!\nBut the apps I build don‚Äôt do well in production. They‚Äôre promising prototypes, but they never go live!\nThe culprit is almost always the retrieval. Come on, this is the heart of RAGs. What are we supposed to build without this?\nThis is until I **index documents** for faster or better retrieval.\nIndexing helps us engineer solutions that retrieve data faster. It significantly reduces latency, improving the overall app experience. We use indexing in almost every app we build. It has nothing to do with LLMs or RAGs.\nAlmost all the databases ship with indexing support. For instance, [Postgres](https://www.freecodecamp.org/news/postgresql-indexing-strategies/) can do _B-Tree, GiST, SP-GiST, BRIN, GIN, and Hash_ types of indexing. That‚Äôs a list long enough to go to a separate future post.\nIn this post, I‚Äôll discuss the popular indexing strategies I frequently use for better document retrieval. These techniques are, however, specific to RAG apps. You‚Äôll see why in a moment.\nMy two go-to indexing techniques are **multi-representation and ColBERT**. These aren‚Äôt the only methods we have. And it‚Äôs an evolving sub-field in RAGs.\nIf there are many, why do I prefer these two over any other? There are two main reasons.\n> They are both easy to understand and implement\n> But they also do well in most instances\nYou‚Äôll also see that indexing improves speed and helps retrieve more relevant documents, especially ColBERT, compared to a system without indexing.\nReal quick‚Äîbefore moving on, you should know **how to chunk large documents** into smaller pieces for embedding and retrieval. Whenever I say document, I mean these chunks. As always, we have more than one method for chunking documents. Your job as an engineer is to pick the best for your case.\nI‚Äôve discussed these in my previous posts. Take a moment to check [position-based churning vs. semantic chunking](https://towardsdatascience.com/semantic-chunking-for-rag-35b7675ffafd) and [agentic chunking](https://towardsdatascience.com/agentic-chunking-for-rags-091beccd94b1). I even tried [clustering as a cheap and fast alternative for agentic chunking](https://towardsdatascience.com/improving-rag-chunking-with-clustering-03c1cf41f1cd).\n## Multi-representation: Faster and reasonably accurate\nThe name speaks for itself. ryte?\nWe store two versions of our documents ‚Äì One for indexing. But the other is the actual document that we retrieve.\nSimple!\nOnce we chunk our documents, we create a version optimized for retrieval. This would usually be a summary of the original chunk, which should contain all the vital information.\nWe store the **optimized version in a vector store** and the **original chunk in a document store**. We bind them both with the **same key** to trace which optimized version is for which document.\nNow, we retrieve as usual. We ask the vector store to fetch the documents closer to our text input in the vector space. But as we retrieve, we use the key we set before to fetch the original document instead of the optimized version.\nHere‚Äôs a code walkthrough.\n```\n# Multi-Representation Indexing Tutorial\n## 1. Import required libraries and load environment variables\nimport uuid\nfrom dotenv import load_dotenv\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain.storage import InMemoryByteStore\nfrom langchain.vectorstores import Chroma\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain_core.documents import Document\n# Load environment variables (e.g., API keys)\nload_dotenv()\n## 2. Load and preprocess the document\n# Load the document from a web page. We use Django's security page as an example\nloader = WebBaseLoader(\"https://docs.djangoproject.com/en/5.1/topics/security/\")\ndocs = loader.load()\n# Split the document into smaller chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\nchunks = text_splitter.split_documents(docs)\n## 3. Generate optimized summaries for each chunk\n# Set up the language model and prompt\nmodel = ChatOpenAI()\ntemplate = \"\"\"\nGenerate a concise and coherent summary of the following document, ensuring that all key details, important concepts, and relevant information are preserved. \nHighlight the main points and conclusions while maintaining clarity and context.\n{document}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\noutput_parser = StrOutputParser()\n# Create a chain for summarization\nchain = prompt | model | output_parser\n# Generate optimized summaries for each chunk\noptimized_chunks = chain.batch(chunks)\n## 4. Set up the multi-vector retriever\n# Initialize vector store and document store\nvectorstore = Chroma(embedding_function=OpenAIEmbeddings())\ndocumentstore = InMemoryByteStore()\n# Generate unique keys for each chunk\nkey = \"unique_id\"\nkeys = [str(uuid.uuid4()) for _ in range(len(optimized_chunks))]\n# Create Document objects with metadata\ndocs = [\n  Document(page_content=chunk, metadata={key: keys[ix]})\n  for ix, chunk in enumerate(optimized_chunks)\n]\n# Set up the multi-vector retriever\nretriever = MultiVectorRetriever(\n  vectorstore=vectorstore,\n  byte_store=documentstore,\n  id_key=key,\n)\n## 5. Add documents to the retriever\n# Add documents to the vector store\nretriever.vectorstore.add_documents(docs)\n# Add documents to the document store\nretriever.docstore.mset(\n  list(zip(keys, [Document(page_content=d) for d in optimized_chunks]))\n)\n## 6. Retrieve relevant documents\n# Example query\nquery = \"How to restrict access to certain hosts?\"\nrelevant_docs = retriever.get_relevant_documents(query)\n# Print the retrieved documents\nfor doc in relevant_docs:\n  print(f\"Content: {doc.page_content}n\")\n  print(f\"Metadata: {doc.metadata}n\")\n  print(\"---n\")\n```\n\nIf we ignore the library loading part, five key steps are involved. Let‚Äôs look at them in detail.\n![Multi-representation - Image by the author](https://towardsdatascience.com/wp-content/uploads/2024/09/1syORwpSl-Nq6C_jGdK0nVw.png)Multi-representation ‚Äì Image by the author\nFirstly, we load our sources. Then we chunk them.\nI‚Äôve used recursive character splitting here, but I do not like position-based chunking. I use it here for simplicity‚Äôs sake.\nThen, we create an LLM chain to summarize our original chunks. The summaries we create should have all the essential details. This is the crucial part of multi-representation indexing. Although I‚Äôve given a short prompt, you should spend most of your time here.\nBetter prompts make better summaries. But they also get you the best documents retrieved.\nNote that we use the batch function to convert all the chunks into summaries in one go. This comes in handy for large datasets.\nIn steps 4 and 5, we create a MultiVectorRetriever. Essentially, this is the multi-representation indexing process. You create a retriever aware of your vector store and the document store and how the optimized vector versions relate to the documents in the document store. That‚Äôs the document key.\nNow, we‚Äôre ready to run queries and retrieve documents. Here‚Äôs the output we get.\n```\nContent: Django now requires setting ALLOWED_HOSTS explicitly to enhance security, even when using a virtual host with ServerName configured. This change prevents potential security vulnerabilities from fake Host headers in HTTP requests. It is recommended to update Django configurations accordingly to mitigate risks effectively. Source: https://docs.djangoproject.com/en/5.1/topics/security/\nMetadata: {}\n---\n```\n```\nMetadata: {}\n```\n```\n---\n```\n\nThis is the original chunk from the website. However, we only have the optimized version in the vector store for retrieval.\n### How does it get more relevant documents?\nYou might ask, \"Okay, this may improve performance, but how does it help retrieve more relevant documents?\"\nHere‚Äôs how.\nA large chunk of text mainly contains noise. That‚Äôs less relevant information to the main point. The cosine similarity, or any other distance-based similarity search, goes wrong with this noise.\nBy only including the necessary details in the chunk, we reduce the noise to a greater extent. Now, similarity searches only have to worry about ‚Äòsimilarity,‚Äô not the noise.\n## ColBERT: When accuracy matters more than speed\nThe idea behind ColBERT is complex‚Äîat least to my immature mind‚Äîbut not too complicated to grasp.\nBut here‚Äôs the need for a different approach.\nThe chunk and the query are embedded into vector forms in regular retrieval. Then, a similarity search is performed to find the nearest neighbors.\nThis approach is straightforward ‚Äì but has an issue.\nDocuments are often comparatively larger. Queries are not. The comparison is not apple-to-apple.\nColBERT handles it in a slightly different way. Here‚Äôs a diagram that illustrates it.\n![ColBERT indexing for RAG - Image by the Author](https://towardsdatascience.com/wp-content/uploads/2024/09/1SCa4padRDYHGreB1F3ejoA.png)ColBERT indexing for RAG ‚Äì Image by the Author\nWe take chunks one by one and tokenize them. We do the same for our search query.\nThen, we compute the similarities between every token in the search query and the tokens in the chunk. We then take the maximum and keep a note of it.\nWe do this process for every token in our search query. Now, you‚Äôve got an array of maximum similarity scores. Each number in this represents a token in the chunk. You take the sum of this array. Now you‚Äôve got a single score for a chunk.\nOkay, we need to do this for all the chunks in the vector store. Now, we‚Äôve got an array of similarity values. This time, each number in the array represents a document.\nLastly, you can compare the values you‚Äôve computed for all the chunks to find the most similar chunks.\nThat‚Äôs a lot of work.\nI know what you‚Äôre thinking.\n> If it‚Äôs complex, why would you still pick it as your first choice?\nThe credit goes to a library called \"[RAGatouille](https://python.langchain.com/v0.2/docs/integrations/retrievers/ragatouille/)\" (Not [Ratatouille](https://python.langchain.com/v0.2/docs/integrations/retrievers/ragatouille/) üêÄ .), making ColBERT implementation effortless.\nHere‚Äôs how.\nLet‚Äôs first install RAGatouille using Pypi.\n```\n!pip install ragatouille\n```\n\nIf you do it on your local computer, you‚Äôll need to download many files and wait a long time. You‚Äôre lucky if you do it on a Colab notebook or a cloud resource.\nWe have to import the pre-trained model for ColBERT. The following lines would do.\n```\nfrom ragatouille import RAGPretrainedModel\nRAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n```\n![Image by the Author](https://towardsdatascience.com/wp-content/uploads/2024/09/17etb3NuPdd7yW7pJN5rYIQ.png)Image by the Author\nWe‚Äôre now ready to index and retrieve using the ColBERT model. But let‚Äôs fetch and prepare our text for this. We‚Äôll use the same [Django security documentation](https://docs.djangoproject.com/en/5.1/topics/security/) for this purpose.\n```\nPython\">import requests\nfrom bs4 import BeautifulSoup, SoupStrainer\nurl = \"https://docs.djangoproject.com/en/5.1/topics/security/\"\nonly_content = SoupStrainer(id=\"docs-content\")\nsoup = BeautifulSoup(requests.get(url).content, \"html.parser\", parse_only=only_content)\nfull_document = soup.get_text(separator='n', strip=True)\nfull_document\n```\n![Image by the Author](https://towardsdatascience.com/wp-content/uploads/2024/09/1NGqn-FFH4SFhRXaYiBRlMw.png)Image by the Author\nHere‚Äôs the magical part. Indexing with RAGatouille is just a single line. Retrieval is another.\n```\nRAG.index(\n  collection=[full_document],\n  index_name=\"Djagno-security\",\n  max_document_length=180,\n  split_documents=True,\n)\n```\n\nThis step is going to take a lot of time. ColBERT isn‚Äôt good at speed. We will come to that later.\nThe point here is that it‚Äôs now simple to implement. It is simpler than the other methods.\nAnd here‚Äôs how the retrieval step goes.\n```\nresults = RAG.search(query=\"How to restrict access based on host names?\", k=3)\nresults\n```\n![Image by the Author](https://towardsdatascience.com/wp-content/uploads/2024/09/1GIO9G9_XCNxOmS-Gczk0MQ.png)Image by the Author\nThe first time you run a query against your documents, it will take a lot of time. This is because RAGatouille fetches and indexes the document simultaneously.\nDon‚Äôt be alarmed!\nYour subsequent queries will take little time, and you will get more accurate document chunks with reasonably low latency.\n### The big caveat of ColBERT\nI love ColBERT just as much as I hate it.\nEven though ColBERT is excellent at retrieving relevant information, it has a high computing cost and indexing time.\nIf you‚Äôve followed this post, you‚Äôd have already noticed it takes a lot of time to index even a tiny web page. Imagine if you‚Äôre indexing all the documents an organization that has lived for a couple of decades has produced.\nYou‚Äôll need a giant computer, or your vacations will be canceled.\nI haven‚Äôt used ColBERT for any of the projects because of this, but it‚Äôs still in all my prototypes.\n## Final thoughts\nYou can build an RAG app overnight, and the following day, you can put it live.\nYet, without making critical decisions, the app won‚Äôt survive in production for various reasons. Often, the complaints are latency and inaccuracy.\nThis is almost always the retrieval or communication with the LLMs phases for RAG apps.\nIn this post, I‚Äôve shared some fantastic techniques I learned that helped me reduce retrieval-related issues.\nI hope it helps you too.\n_Thanks for reading, friend! Besides**[Medium](https://thuwarakesh.medium.com/)**_ , _I‚Äôm on**[LinkedIn](https://www.linkedin.com/in/thuwarakesh/)** and **[X,](https://twitter.com/Thuwarakesh)** too!_\nWritten By\nThuwarakesh Murallie\n[See all from Thuwarakesh Murallie](https://towardsdatascience.com/author/thuwarakesh/)\nTopics:\n[Data Science](https://towardsdatascience.com/tag/data-science/), [Langchain](https://towardsdatascience.com/tag/langchain/), [Large Language Models](https://towardsdatascience.com/tag/large-language-models/), [Python](https://towardsdatascience.com/tag/python/), [Retrieval Augmented](https://towardsdatascience.com/tag/retrieval-augmented/)\nShare this article:\n  * [ Share on Facebook  ](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-rep-colbert-retrieval-models-for-rags-fe05381b8819%2F&title=Building%20RAGs%20Without%20A%20Retrieval%20Model%20Is%20a%20Terrible%20Mistake)\n  * [ Share on LinkedIn  ](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-rep-colbert-retrieval-models-for-rags-fe05381b8819%2F&title=Building%20RAGs%20Without%20A%20Retrieval%20Model%20Is%20a%20Terrible%20Mistake)\n  * [ Share on X  ](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-rep-colbert-retrieval-models-for-rags-fe05381b8819%2F&text=Building%20RAGs%20Without%20A%20Retrieval%20Model%20Is%20a%20Terrible%20Mistake)\n\n\n## Related Articles\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/0c09RmbCCpfjAbSMq.png)\n## [Implementing Convolutional Neural Networks in TensorFlow](https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nStep-by-step code guide to building a Convolutional Neural Network \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nAugust 20, 2024\n6 min read\n  * ## [What Do Large Language Models ‚ÄúUnderstand‚Äù?](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA deep dive on the meaning of understanding and how it applies to LLMs \n[Tarik Dzekman](https://towardsdatascience.com/author/tarikdzekman/)\nAugust 21, 2024\n31 min read\n  * ![Photo by davisuko on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/1bAABgtZtAIG5YW1oEjW3pA-scaled.jpeg)\n## [Hands-on Time Series Anomaly Detection using Autoencoders, with Python](https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nHere‚Äôs how to use Autoencoders to detect signals with anomalies in a few lines of‚Ä¶ \n[Piero Paialunga](https://towardsdatascience.com/author/piero-paialunga/)\nAugust 21, 2024\n12 min read\n  * ## [Solving a Constrained Project Scheduling Problem with Quantum Annealing](https://towardsdatascience.com/solving-a-constrained-project-scheduling-problem-with-quantum-annealing-d0640e657a3b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nSolving the resource constrained project scheduling problem (RCPSP) with D-Wave‚Äôs hybrid constrained quadratic model (CQM) \n[Luis Fernando P√âREZ ARMAS, Ph.D.](https://towardsdatascience.com/author/luisfernandopa1212/)\nAugust 20, 2024\n29 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2023/02/1VEUgT5T4absnTqBMOEuNig.png)\n## [Back To Basics, Part Uno: Linear Regression and Cost Function](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nAn illustrated guide on essential machine learning concepts \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nFebruary 3, 2023\n6 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/1kM8tfYcdaoccB1HX71YDig.png)\n## [Must-Know in Statistics: The Bivariate Normal Projection Explained](https://towardsdatascience.com/must-know-in-statistics-the-bivariate-normal-projection-explained-ace7b2f70b5b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nDerivation and practical examples of this powerful concept \n[Luigi Battistoni](https://towardsdatascience.com/author/lu-battistoni/)\nAugust 14, 2024\n7 min read\n  * ![Photo by Jess Bailey on Unsplash](https://towardsdatascience.com/wp-content/uploads/2022/09/11tHmNYFaWWtWG5I7bNeN6g-scaled.jpeg)\n## [How to Make the Most of Your Experience as a TDS Author](https://towardsdatascience.com/how-to-make-the-most-of-your-experience-as-a-tds-author-b1e056be63f1/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nA quick guide to our resources and FAQ \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nSeptember 13, 2022\n4 min read\n  * ![Photo by Alex Geerts on Unsplash](https://towardsdatascience.com/wp-content/uploads/2020/11/0BF38u2sw4WQdaMLS-scaled.jpg)\n## [Our Columns](https://towardsdatascience.com/our-columns-53501f74c86d/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nColumns on TDS are carefully curated collections of posts on a particular idea or category‚Ä¶ \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nNovember 14, 2020\n4 min read\n  * ![Image created by authors with GPT-4o](https://towardsdatascience.com/wp-content/uploads/2024/08/1vilI3Q4nlwqsAQLq3TOzSA.jpg)\n## [Optimizing Marketing Campaigns with Budgeted Multi-Armed Bandits](https://towardsdatascience.com/optimizing-marketing-campaigns-with-budgeted-multi-armed-bandits-a65fccd61878/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nWith demos, our new solution, and a video \n[Vadim Arzamasov](https://towardsdatascience.com/author/vadim-arzamasov/)\nAugust 16, 2024\n10 min read\n\n\n  * [YouTube](https://www.youtube.com/c/TowardsDataScience)\n  * [X](https://x.com/TDataScience)\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [Threads](https://www.threads.net/@towardsdatascience)\n  * [Bluesky](https://bsky.app/profile/towardsdatascience.com)\n\n\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nYour home for data science and Al. The world‚Äôs leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\n¬©  Insight Media Group, LLC 2025 \n  * [About](https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/)\n  * [Privacy Policy](https://towardsdatascience.com/privacy-policy/)\n  * [Terms of Use](https://towardsdatascience.com/website-terms-of-use/)\n\n\n[Towards Data Science is now independent!](https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/)\nCookies Settings\n## Sign up to our newsletter\nEmail address*\nFirst name*\nLast name*\nJob title*\nJob level*\nPlease SelectC-LevelVP/DirectorManager/SupervisorMid Level or Senior Non-Managerial StaffEntry Level/Junior StaffFreelancer/ContractorStudent/InternOther\nCompany name*\n  * I consent to receive newsletters and other communications from Towards Data Science publications.*\n\n\n![Company Logo](https://cdn.cookielaw.org/logos/static/ot_company_logo.png)\n## Privacy Preference Center\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer. [More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nAllow All\n###  Manage Consent Preferences\n#### Functional Cookies\nFunctional Cookies Active\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\n#### Strictly Necessary Cookies\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n#### Performance Cookies\nPerformance Cookies Active\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n#### Targeting Cookies\nTargeting Cookies Active\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nBack Button\n### Cookie List\nSearch Icon\nFilter Icon\nClear\ncheckbox label label\nApply Cancel\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nReject All Confirm My Choices\n[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-consent/)\nSome areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://towardsdatascience.com/multi-rep-colbert-retrieval-models-for-rags-fe05381b8819/#wp--skip-link--target",
        "https://towardsdatascience.com/",
        "https://towardsdatascience.com/latest/",
        "https://towardsdatascience.com/tag/editors-pick/",
        "https://towardsdatascience.com/tag/deep-dives/",
        "https://towardsdatascience.com/questions-96667b06af5/",
        "https://newsletter.towardsdatascience.com/subscription-to-the-newsletter",
        "https://towardsdatascience.com/category/data-science/",
        "https://towardsdatascience.com/author/thuwarakesh/",
        "https://towardsdatascience.com/semantic-chunking-for-rag-35b7675ffafd",
        "https://towardsdatascience.com/agentic-chunking-for-rags-091beccd94b1",
        "https://towardsdatascience.com/improving-rag-chunking-with-clustering-03c1cf41f1cd",
        "https://towardsdatascience.com/tag/data-science/",
        "https://towardsdatascience.com/tag/langchain/",
        "https://towardsdatascience.com/tag/large-language-models/",
        "https://towardsdatascience.com/tag/python/",
        "https://towardsdatascience.com/tag/retrieval-augmented/",
        "https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/",
        "https://towardsdatascience.com/category/artificial-intelligence/",
        "https://towardsdatascience.com/author/shreya-rao/",
        "https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/",
        "https://towardsdatascience.com/author/tarikdzekman/",
        "https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/",
        "https://towardsdatascience.com/author/piero-paialunga/",
        "https://towardsdatascience.com/solving-a-constrained-project-scheduling-problem-with-quantum-annealing-d0640e657a3b/",
        "https://towardsdatascience.com/author/luisfernandopa1212/",
        "https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/",
        "https://towardsdatascience.com/must-know-in-statistics-the-bivariate-normal-projection-explained-ace7b2f70b5b/",
        "https://towardsdatascience.com/author/lu-battistoni/",
        "https://towardsdatascience.com/how-to-make-the-most-of-your-experience-as-a-tds-author-b1e056be63f1/",
        "https://towardsdatascience.com/author/towardsdatascience/",
        "https://towardsdatascience.com/our-columns-53501f74c86d/",
        "https://towardsdatascience.com/optimizing-marketing-campaigns-with-budgeted-multi-armed-bandits-a65fccd61878/",
        "https://towardsdatascience.com/author/vadim-arzamasov/",
        "https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/",
        "https://towardsdatascience.com/privacy-policy/",
        "https://towardsdatascience.com/website-terms-of-use/",
        "https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/",
        "https://contributor.insightmediagroup.io/",
        "https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca",
        "https://x.com/TDataScience",
        "https://www.pexels.com/photo/person-holding-multicolored-container-1209870/",
        "https://www.freecodecamp.org/news/postgresql-indexing-strategies/",
        "https://python.langchain.com/v0.2/docs/integrations/retrievers/ragatouille/",
        "https://docs.djangoproject.com/en/5.1/topics/security/",
        "https://thuwarakesh.medium.com/",
        "https://www.linkedin.com/in/thuwarakesh/",
        "https://twitter.com/Thuwarakesh",
        "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-rep-colbert-retrieval-models-for-rags-fe05381b8819%2F&title=Building%20RAGs%20Without%20A%20Retrieval%20Model%20Is%20a%20Terrible%20Mistake",
        "https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-rep-colbert-retrieval-models-for-rags-fe05381b8819%2F&title=Building%20RAGs%20Without%20A%20Retrieval%20Model%20Is%20a%20Terrible%20Mistake",
        "https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-rep-colbert-retrieval-models-for-rags-fe05381b8819%2F&text=Building%20RAGs%20Without%20A%20Retrieval%20Model%20Is%20a%20Terrible%20Mistake",
        "https://www.youtube.com/c/TowardsDataScience",
        "https://www.threads.net/@towardsdatascience",
        "https://bsky.app/profile/towardsdatascience.com",
        "https://cookiepedia.co.uk/giving-consent-to-cookies",
        "https://www.onetrust.com/products/cookie-consent/"
    ]
}