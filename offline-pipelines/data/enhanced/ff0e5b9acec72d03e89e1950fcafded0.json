{
    "id": "ff0e5b9acec72d03e89e1950fcafded0",
    "metadata": {
        "id": "ff0e5b9acec72d03e89e1950fcafded0",
        "url": "https://www.llamaindex.ai/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4/",
        "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray — LlamaIndex - Build Knowledge Assistants over your Enterprise Data",
        "properties": {
            "description": "LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data.",
            "keywords": null,
            "author": null,
            "og:title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray — LlamaIndex - Build Knowledge Assistants over your Enterprise Data",
            "og:description": "LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data.",
            "og:image": "https://www.llamaindex.ai/og.png"
        }
    },
    "parent_metadata": {
        "id": "9e5cb97bb4a939ddccc5badc5cd4d41b",
        "url": "https://www.notion.so/Distributed-Computing-9e5cb97bb4a939ddccc5badc5cd4d41b",
        "title": "Distributed Computing",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Announcing our LlamaCloud General Availability (and our $19M series A)!](https://www.llamaindex.ai/blog/announcing-our-series-a-and-llamacloud-general-availability)[![LlamaIndex](https://www.llamaindex.ai/llamaindex.svg)](https://www.llamaindex.ai/)\n  * Products\n  * Solutions\n  * [Community](https://www.llamaindex.ai/community)\n  * [Careers](https://www.llamaindex.ai/careers)\n  * [Blog](https://www.llamaindex.ai/blog)\n\n\n[Book a demo](https://www.llamaindex.ai/contact)[ Get started](https://cloud.llamaindex.ai/)\n  * Products \n    * [Document parsingThe first and leading GenAI-native parser over your most complex data.](https://www.llamaindex.ai/llamaparse)\n    * [Knowledge ManagementConnect, transform, and index your enterprise data into an agent-accessible knowledge base](https://www.llamaindex.ai/enterprise)\n    * [Agent FrameworkOrchestrate and deploy multi-agent applications over your data with the #1 agent framework.](https://www.llamaindex.ai/framework)\n  * Solutions \n    * [Financial Analysts](https://www.llamaindex.ai/solutions/finance)\n    * [Administrative Operations](https://www.llamaindex.ai/solutions/administrative-operations)\n    * [Engineering & R&D](https://www.llamaindex.ai/solutions/engineering)\n    * [Customer Support](https://www.llamaindex.ai/solutions/customer-support)\n    * [Healthcare / Pharma](https://www.llamaindex.ai/solutions/healthcare-pharma)\n  * [Community](https://www.llamaindex.ai/community)\n  * [Careers](https://www.llamaindex.ai/careers)\n  * [Blog](https://www.llamaindex.ai/blog)\n\n[ Talk to us](https://www.llamaindex.ai/contact)\n  * [](https://github.com/run-llama/llama_index)\n  * [](https://discord.com/invite/eN6D2HQ4aX)\n  * [](https://twitter.com/llama_index)\n  * [](https://www.linkedin.com/company/91154103/)\n  * [](https://www.youtube.com/@LlamaIndex)\n\n\n© 2025 LlamaIndex\n![](https://www.llamaindex.ai/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F958a7e3655d67819e61eab2b7606fca78e37aec7-1200x557.png%3Ffit%3Dmax%26auto%3Dformat&w=1920&q=75)\n[Jerry Liu](https://www.llamaindex.ai/blog/author/jerry-liu) • 2023-06-27\n# Build and Scale a Powerful Query Engine with LlamaIndex and Ray\n  * [NLP](https://www.llamaindex.ai/blog/tag/nlp)\n  * [Distributed Systems](https://www.llamaindex.ai/blog/tag/distributed-systems)\n  * [AI](https://www.llamaindex.ai/blog/tag/ai)\n  * [Large Language Models](https://www.llamaindex.ai/blog/tag/large-language-models)\n  * [Parallel Computing](https://www.llamaindex.ai/blog/tag/parallel-computing)\n\n\nCo-authors: Jerry Liu (CEO at LlamaIndex), Amog Kamsetty (Software Engineer at Anyscale)\n(**note:** this is cross-posted from the original blog post on Anyscale’s website. [Check it out here](https://www.anyscale.com/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-ray)!)\nIn this blog, we showcase how you can use LlamaIndex and Ray to build a query engine to answer questions and generate insights about Ray itself, given its documentation and blog posts.\nWe’ll give a quick introduction of LlamaIndex + Ray, and then walk through a step-by-step tutorial on building and deploying this query engine. We make use of both Ray Datasets to parallelize building indices as well as Ray Serve to build deployments.\n# Introduction\nLarge Language Models (LLMs) offer the promise of allowing users to extract complex insights from their unstructured text data. Retrieval-augmented generation pipelines have emerged as a common pattern for developing LLM applications allowing users to effectively perform semantic search over a collection of documents.\n_Example of retrieval augmented generation. Relevant context is pulled from a set of documents and included in the LLM input prompt._![](https://www.llamaindex.ai/blog/images/1*euY0oGTyi5vnt2aqJ9hFjw.png)\nHowever, when productionizing these applications over many different data sources, there are a few challenges:\n  1. Tooling for indexing data from many different data sources\n  2. Handling complex queries over different data sources\n  3. Scaling indexing to thousands or millions of documents\n  4. Deploying a scalable LLM application into production\n\n\nHere, we showcase how [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/) and [Ray](https://docs.ray.io/en/latest/) are the perfect setup for this task.\nLlamaIndex is a data framework for building LLM applications, and solves Challenges #1 and #2. It also provides a comprehensive toolkit allowing users to connect their private data with a language model. It offers a variety of tools to help users first ingest and index their data — convert different formats of unstructured and structured data into a format that the language model can use, and query their private data.\nRay is a powerful framework for scalable AI that solves Challenges #3 and #4. We can use it to dramatically accelerate ingest, inference, pretraining, and also effortlessly deploy and scale the query capabilities of LlamaIndex into the cloud.\nMore specifically, we showcase a very relevant use case — highlighting Ray features that are present in both the documentation as well as the Ray blog posts!\n# Data Ingestion and Embedding Pipeline\nWe use LlamaIndex + Ray to ingest, parse, embed and store Ray docs and blog posts in a parallel fashion. For the most part, these steps are duplicated across the two data sources, so we show the steps for just the documentation below.\nCode for this part of the blog is [available here](https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py).\n_Sequential pipeline with “ingest”, “parse” and “embed” stages. Files are processed sequentially resulting in poor hardware utilization and long computation time._![](https://www.llamaindex.ai/blog/images/1*uQpJXp_A-1-AOwz3LMyl3Q.png)_Parallel pipeline. Thanks to Ray we can process multiple input files simultaneously. Parallel processing has much better performance, because hardware is better utilized._![](https://www.llamaindex.ai/blog/images/1*im0zUrKp8ABSRrZlic0L8Q.png)\n# Load Data\nWe start by ingesting these two sources of data. We first fetch both data sources and download the HTML files.\nWe then need to load and parse these files. We can do this with the help of LlamaHub, our community-driven repository of 100+ data loaders from various API’s, file formats (.pdf, .html, .docx), and databases. We use an HTML data loader offered by [Unstructured](https://github.com/Unstructured-IO/unstructured).\n```\nfrom typing import Dict, List\nfrom pathlib import Path\nfrom llama_index import download_loader\nfrom llama_index import Document\n# Step 1: Logic for loading and parsing the files into llama_index documents.\nUnstructuredReader = download_loader(\"UnstructuredReader\")\nloader = UnstructuredReader()\ndef load_and_parse_files(file_row: Dict[str, Path]) -&gt; Dict[str, Document]:\n  documents = []\n  file = file_row[\"path\"]\n  if file.is_dir():\n    return []\n  # Skip all non-html files like png, jpg, etc.\n  if file.suffix.lower() == \".html\":\n    loaded_doc = loader.load_data(file=file, split_documents=False)\n    loaded_doc[0].extra_info = {\"path\": str(file)}\n    documents.extend(loaded_doc)\n  return [{\"doc\": doc} for doc in documents]\n```\n\nUnstructured offers a robust suite of parsing tools on top of various files. It is able to help sanitize HTML documents by stripping out information like tags and formatting the text accordingly.\n## Scaling Data Ingest\n![](https://www.llamaindex.ai/blog/images/1*prVxsm5aR-a5IQItiiWZXQ.png)\nSince we have many HTML documents to process, loading/processing each one serially is inefficient and slow. This is an opportunity to use Ray and distribute execution of the `load_and_parse_files` method across multiple CPUs or GPUs.\n```\nimport ray\n# Get the paths for the locally downloaded documentation.\nall_docs_gen = Path(\"./docs.ray.io/\").rglob(\"*\")\nall_docs = [{\"path\": doc.resolve()} for doc in all_docs_gen]\n# Create the Ray Dataset pipeline\nds = ray.data.from_items(all_docs)\n# Use `flat_map` since there is a 1:N relationship.\n# Each filepath returns multiple documents.\nloaded_docs = ds.flat_map(load_and_parse_files)\n```\n\n# Parse Files\nNow that we’ve loaded the documents, the next step is to parse them into Node objects — a “Node” object represents a more granular chunk of text, derived from the source documents. Node objects can be used in the input prompt as context; by setting a small enough chunk size, we can make sure that inserting Node objects do not overflow the context limits.\nWe define a function called `convert_documents_into_nodes` which converts documents into nodes using a simple text splitting strategy.\n```\n# Step 2: Convert the loaded documents into llama_index Nodes. This will split the documents into chunks.\nfrom llama_index.node_parser import SimpleNodeParser\nfrom llama_index.data_structs import Node\ndef convert_documents_into_nodes(documents: Dict[str, Document]) -&gt; Dict[str, Node]:\n  parser = SimpleNodeParser()\n  document = documents[\"doc\"]\n  nodes = parser.get_nodes_from_documents([document]) \n  return [{\"node\": node} for node in nodes]\n```\n\n## Run Parsing in Parallel\n![](https://www.llamaindex.ai/blog/images/1*W3LNUEzK6QyH52shEAr4yQ.png)\nSince we have many documents, processing each document into nodes serially is inefficient and slow. We use Ray `flat_map` method to process documents into nodes in parallel:\n```\n\n# Use `flat_map` since there is a 1:N relationship. Each document returns multiple nodes.\nnodes = loaded_docs.flat_map(convert_documents_into_nodes)\n```\n\n# Generate Embeddings\n![](https://www.llamaindex.ai/blog/images/1*bJMxNgwzfg_ThixXNhOwww.png)\nWe then generate embeddings for each Node using a Hugging Face Sentence Transformers model. We can do this with the help of LangChain’s embedding abstraction.\nSimilar to document loading/parsing, embedding generation can similarly be parallelized with Ray. We wrap these embedding operations into a helper class, called `EmbedNodes`, to take advantage of Ray abstractions.\n```\n# Step 3: Embed each node using a local embedding model.\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nclass EmbedNodes:\n  def __init__(self):\n    self.embedding_model = HuggingFaceEmbeddings(\n      # Use all-mpnet-base-v2 Sentence_transformer.\n      # This is the default embedding model for LlamaIndex/Langchain.\n      model_name=\"sentence-transformers/all-mpnet-base-v2\", \n      model_kwargs={\"device\": \"cuda\"},\n      # Use GPU for embedding and specify a large enough batch size to maximize GPU utilization.\n      # Remove the \"device\": \"cuda\" to use CPU instead.\n      encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100}\n      )\n  def __call__(self, node_batch: Dict[str, List[Node]]) -&gt; Dict[str, List[Node]]:\n    nodes = node_batch[\"node\"]\n    text = [node.text for node in nodes]\n    embeddings = self.embedding_model.embed_documents(text)\n    assert len(nodes) == len(embeddings)\n    for node, embedding in zip(nodes, embeddings):\n      node.embedding = embedding\n    return {\"embedded_nodes\": nodes}\n```\n\nAfterwards, generating an embedding for each node is as simple as calling the following operation in Ray:\n```\n# Use `map_batches` to specify a batch size to maximize GPU utilization.\n# We define `EmbedNodes` as a class instead of a function so we only initialize the embedding model once. \n# This state can be reused for multiple batches.\nembedded_nodes = nodes.map_batches(\n  EmbedNodes, \n  batch_size=100, \n  # Use 1 GPU per actor.\n  num_gpus=1,\n  # There are 4 GPUs in the cluster. Each actor uses 1 GPU. So we want 4 total actors.\n  compute=ActorPoolStrategy(size=4))\n# Step 5: Trigger execution and collect all the embedded nodes.\nray_docs_nodes = []\nfor row in embedded_nodes.iter_rows():\n  node = row[\"embedded_nodes\"]\n  assert node.embedding is not None\n  ray_docs_nodes.append(node)\n```\n\n# Data Indexing\n![](https://www.llamaindex.ai/blog/images/1*WvvbV91UFrPTXViKiF_TBA.png)\nThe next step is to store these nodes within an “index” in LlamaIndex. An index is a core abstraction in LlamaIndex to “structure” your data in a certain way — this structure can then be used for downstream LLM retrieval + querying. An index can interface with a storage or vector store abstraction.\nThe most commonly used index abstraction within LlamaIndex is our vector index, where each node is stored along with an embedding. In this example, we use a simple in-memory vector store, but you can also choose to specify any one of LlamaIndex’s 10+ vector store integrations as the storage provider (e.g. Pinecone, Weaviate, Chroma).\nWe build two vector indices: one over the documentation nodes, and another over the blog post nodes and persist them to disk. Code is [available here](https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py#L102:L131).\n```\nfrom llama_index import GPTVectorStoreIndex\n# Store Ray Documentation embeddings\nray_docs_index = GPTVectorStoreIndex(nodes=ray_docs_nodes)\nray_docs_index.storage_context.persist(persist_dir=\"/tmp/ray_docs_index\")\n# Store Anyscale blog post embeddings\nray_blogs_index = GPTVectorStoreIndex(nodes=ray_blogs_nodes)\nray_blogs_index.storage_context.persist(persist_dir=\"/tmp/ray_blogs_index\")\n```\n\n**That’s it in terms of building a data pipeline using LlamaIndex + Ray Data**!\nYour data is now ready to be used within your LLM application. Check out our next section for how to use advanced LlamaIndex query capabilities on top of your data.\n# Data Querying\n![](https://www.llamaindex.ai/blog/images/1*yp1AZoi-B6ZT2O7eqjIfcQ.png)\nLlamaIndex provides both simple and advanced query capabilities on top of your data + indices. The central abstraction within LlamaIndex is called a “query engine.” A query engine takes in a natural language query input and returns a natural language “output”. Each index has a “default” corresponding query engine. For instance, the default query engine for a vector index first performs top-k retrieval over the vector store to fetch the most relevant documents.\nThese query engines can be easily derived from each index:\n```\nray_docs_engine = ray_docs_index.as_query_engine(similarity_top_k=5, service_context=service_context)\nray_blogs_engine = ray_blogs_index.as_query_engine(similarity_top_k=5, service_context=service_context)\n```\n\nLlamaIndex also provides more advanced query engines for multi-document use cases — for instance, we may want to ask how a given feature in Ray is highlighted in both the documentation and blog. `SubQuestionQueryEngine` can take in other query engines as input. Given an existing question, it can decide to break down the question into simpler questions over any subset of query engines; it will execute the simpler questions and combine results at the top-level.\nThis abstraction is quite powerful; it can perform semantic search over one document, or combine results across multiple documents.\nFor instance, given the following question “What is Ray?”, we can break this into sub-questions “What is Ray according to the documentation”, and “What is Ray according to the blog posts” over the document query engine and blog query engine respectively.\n```\n# Define a sub-question query engine, that can use the individual query engines as tools.\n    query_engine_tools = [\n      QueryEngineTool(\n        query_engine=self.ray_docs_engine,\n        metadata=ToolMetadata(name=\"ray_docs_engine\", description=\"Provides information about the Ray documentation\")\n      ),\n      QueryEngineTool(\n        query_engine=self.ray_blogs_engine, \n        metadata=ToolMetadata(name=\"ray_blogs_engine\", description=\"Provides information about Ray blog posts\")\n      ),\n    ]\nsub_query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools, service_context=service_context, use_async=False)\n```\n\nHave a look at [deploy_app.py](https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py#L22:L56) to review the full implementation.\n# Deploying with Ray Serve\n![](https://www.llamaindex.ai/blog/images/1*j0ZjPhwy7L6nyb9krfZMrA.png)\nWe’ve now created an incredibly powerful query module over your data. As a next step, what if we could seamlessly deploy this function to production and serve users? Ray Serve makes this incredibly easy to do. Ray Serve is a scalable compute layer for serving ML models and LLMs that enables serving individual models or creating composite model pipelines where you can independently deploy, update, and scale individual components.\nTo do this, you just need to do the following steps:\n  1. Define an outer class that can “wrap” a query engine, and expose a “query” endpoint\n  2. Add a `@ray.serve.deployment` decorator on this class\n  3. Deploy the Ray Serve application\n\n\nIt will look something like the following:\n```\nfrom ray import serve\n@serve.deployment\nclass QADeployment:\n  def __init__(self):\n self.query_engine = ...\n  def query(self, query: str):\n      response = self.query_engine.query(query)\n      source_nodes = response.source_nodes\n      source_str = \"\"\n      for i in range(len(source_nodes)):\n        node = source_nodes[i]\n        source_str += f\"Sub-question {i+1}:\\n\"\n        source_str += node.node.text\n        source_str += \"\\n\\n\"\n      return f\"Response: {str(response)} \\n\\n\\n {source_str}\\n\"\n  async def __call__(self, request: Request):\n    query = request.query_params[\"query\"]\n    return str(self.query(query))\n# Deploy the Ray Serve application.\ndeployment = QADeployment.bind()\n```\n\nHave a look at the [deploy_app.py](https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py) for full implementation.\n# Example Queries\nOnce we’ve deployed the application, we can query it with questions about Ray.\nWe can query just one of the data sources:\n```\nQ: \"What is Ray Serve?\"\nRay Serve is a system for deploying and managing applications on a Ray\ncluster. It provides APIs for deploying applications, managing replicas, and\nmaking requests to applications. It also provides a command line interface\n(CLI) for managing applications and a dashboard for monitoring applications.\n```\n\nBut, we can also provide complex queries that require synthesis across both the documentation and the blog posts. These complex queries are easily handled by the subquestion-query engine that we defined.\n```\nQ: \"Compare and contrast how the Ray docs and the Ray blogs present Ray Serve\"\nResponse: \nThe Ray docs and the Ray blogs both present Ray Serve as a web interface\nthat provides metrics, charts, and other features to help Ray users\nunderstand and debug Ray applications. However, the Ray docs provide more\ndetailed information, such as a Quick Start guide, user guide, production\nguide, performance tuning guide, development workflow guide, API reference,\nexperimental Java API, and experimental gRPC support. Additionally, the Ray\ndocs provide a guide for migrating from 1.x to 2.x. On the other hand, the\nRay blogs provide a Quick Start guide, a User Guide, and Advanced Guides to\nhelp users get started and understand the features of Ray Serve.\nAdditionally, the Ray blogs provide examples and use cases to help users\nunderstand how to use Ray Serve in their own projects.\n---\nSub-question 1\nSub question: How does the Ray docs present Ray Serve\nResponse: \nThe Ray docs present Ray Serve as a web interface that provides metrics,\ncharts, and other features to help Ray users understand and debug Ray\napplications. It provides a Quick Start guide, user guide, production guide,\nperformance tuning guide, and development workflow guide. It also provides\nan API reference, experimental Java API, and experimental gRPC support.\nFinally, it provides a guide for migrating from 1.x to 2.x.\n---\nSub-question 2\nSub question: How does the Ray blogs present Ray Serve\nResponse: \nThe Ray blog presents Ray Serve as a framework for distributed applications\nthat enables users to handle HTTP requests, scale and allocate resources,\ncompose models, and more. It provides a Quick Start guide, a User Guide, and\nAdvanced Guides to help users get started and understand the features of Ray\nServe. Additionally, it provides examples and use cases to help users\nunderstand how to use Ray Serve in their own projects.\n```\n\n# Conclusion\nIn this example, we showed how you can build a scalable data pipeline and a powerful query engine using LlamaIndex + Ray. We also demonstrated how to deploy LlamaIndex applications using Ray Serve. This allows you to effortlessly ask questions and synthesize insights about Ray across disparate data sources!\nWe used LlamaIndex — a data framework for building LLM applications — to load, parse, embed and index the data. We ensured efficient and fast parallel execution by using Ray. Then, we used LlamaIndex querying capabilities to perform semantic search over a single document, or combine results across multiple documents. Finally, we used Ray Serve to package the application for production use.\nImplementation in open source, code is available on GitHub: [LlamaIndex-Ray-app](https://github.com/amogkam/llama_index_ray)\n# What’s next?\nVisit LlamaIndex [site](https://www.llamaindex.ai/) and [docs](https://gpt-index.readthedocs.io/en/latest/) to learn more about this data framework for building LLM applications.\nVisit [Ray docs](https://docs.ray.io/en/latest/ray-overview/use-cases.html#llms-and-gen-ai) to learn more about how to build and deploy scalable LLM apps.\nJoin our communities!\n  * [Join Ray community](https://forms.gle/9TSdDYUgxYs8SA9e8) on Slack and Ray #LLM channel.\n  * You can also join the LlamaIndex [community on discord](https://discord.gg/UB58qbeq).\n\n\nWe have our [Ray Summit 2023](https://raysummit.anyscale.com/) early-bird registration open until 6/30. Secure your spot, save some money, savor the community camaraderie at the summit.\n## Related articles\n  * ![](https://www.llamaindex.ai/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&w=828&q=75)\n[Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations](https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations)\n2024-03-19\n  * ![](https://www.llamaindex.ai/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&w=828&q=75)\n[LlamaIndex Newsletter 2024-03-19](https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19)\n2024-03-19\n  * ![](https://www.llamaindex.ai/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&w=828&q=75)\n[LlamaIndex Newsletter 2024-03-05](https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05)\n2024-03-05\n  * ![](https://www.llamaindex.ai/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&w=828&q=75)\n[Querying a network of knowledge with llama-index-networks](https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f)\n2024-02-27\n\n\n![LlamaIndex](https://www.llamaindex.ai/llamaindex.svg)\n  * [](https://github.com/run-llama/llama_index)\n  * [](https://discord.com/invite/eN6D2HQ4aX)\n  * [](https://twitter.com/llama_index)\n  * [](https://www.linkedin.com/company/91154103/)\n  * [](https://www.youtube.com/@LlamaIndex)\n\n\n### [LlamaIndex](https://www.llamaindex.ai/)\n  * [Blog](https://www.llamaindex.ai/blog)\n  * [Partners](https://www.llamaindex.ai/partners)\n  * [Careers](https://www.llamaindex.ai/careers)\n  * [Contact](https://www.llamaindex.ai/contact)\n  * [Brand](https://www.llamaindex.ai/brand)\n  * [Status](https://llamaindex.statuspage.io)\n  * [Trust Center](https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu)\n\n\n### [Enterprise](https://www.llamaindex.ai/enterprise)\n  * [LlamaCloud](https://cloud.llamaindex.ai)\n  * [LlamaParse](https://cloud.llamaindex.ai/parse)\n  * [Customers](https://www.llamaindex.ai/customers)\n  * [SharePoint](https://www.llamaindex.ai/llamacloud-sharepoint-data-loading-for-generative-ai)\n  * [AWS S3](https://www.llamaindex.ai/llamacloud-aws-s3-data-loading-for-generative-ai)\n  * [Azure Blob Storage](https://www.llamaindex.ai/llamacloud-azure-blob-storage-data-loading-for-generative-ai)\n  * [Google Drive](https://www.llamaindex.ai/llamacloud-google-drive-data-loading-for-generative-ai)\n\n\n### [Framework](https://www.llamaindex.ai/framework)\n  * [Python package](https://pypi.org/project/llama-index/)\n  * [Python docs](https://docs.llamaindex.ai)\n  * [TypeScript package](https://www.npmjs.com/package/llamaindex)\n  * [TypeScript docs](https://ts.llamaindex.ai)\n  * [LlamaHub](https://llamahub.ai)\n  * [GitHub](https://github.com/run-llama)\n\n\n### [Community](https://www.llamaindex.ai/community)\n  * [Newsletter](https://www.llamaindex.ai/community#newsletter)\n  * [Discord](https://discord.com/invite/eN6D2HQ4aX)\n  * [LinkedIn](https://www.linkedin.com/company/91154103/)\n  * [Twitter/X](https://twitter.com/llama_index)\n  * [YouTube](https://www.youtube.com/@LlamaIndex)\n  * [BlueSky](https://bsky.app/profile/llamaindex.bsky.social)\n\n\n### Starter projects\n  * [create-llama](https://www.npmjs.com/package/create-llama)\n  * [SEC Insights](https://secinsights.ai)\n  * [LlamaBot](https://github.com/run-llama/llamabot)\n  * [RAG CLI](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html)\n\n\n© 2025 LlamaIndex\n[Privacy Notice](https://www.llamaindex.ai/files/privacy-notice.pdf)\n[Terms of Service](https://www.llamaindex.ai/files/terms-of-service.pdf)\n",
    "content_quality_score": 0.8,
    "summary": null,
    "child_urls": [
        "https://www.llamaindex.ai/blog/announcing-our-series-a-and-llamacloud-general-availability",
        "https://www.llamaindex.ai/",
        "https://www.llamaindex.ai/community",
        "https://www.llamaindex.ai/careers",
        "https://www.llamaindex.ai/blog",
        "https://www.llamaindex.ai/contact",
        "https://cloud.llamaindex.ai/",
        "https://www.llamaindex.ai/llamaparse",
        "https://www.llamaindex.ai/enterprise",
        "https://www.llamaindex.ai/framework",
        "https://www.llamaindex.ai/solutions/finance",
        "https://www.llamaindex.ai/solutions/administrative-operations",
        "https://www.llamaindex.ai/solutions/engineering",
        "https://www.llamaindex.ai/solutions/customer-support",
        "https://www.llamaindex.ai/solutions/healthcare-pharma",
        "https://www.llamaindex.ai/blog/author/jerry-liu",
        "https://www.llamaindex.ai/blog/tag/nlp",
        "https://www.llamaindex.ai/blog/tag/distributed-systems",
        "https://www.llamaindex.ai/blog/tag/ai",
        "https://www.llamaindex.ai/blog/tag/large-language-models",
        "https://www.llamaindex.ai/blog/tag/parallel-computing",
        "https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations",
        "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19",
        "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-05",
        "https://www.llamaindex.ai/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f",
        "https://www.llamaindex.ai/partners",
        "https://www.llamaindex.ai/brand",
        "https://cloud.llamaindex.ai",
        "https://cloud.llamaindex.ai/parse",
        "https://www.llamaindex.ai/customers",
        "https://www.llamaindex.ai/llamacloud-sharepoint-data-loading-for-generative-ai",
        "https://www.llamaindex.ai/llamacloud-aws-s3-data-loading-for-generative-ai",
        "https://www.llamaindex.ai/llamacloud-azure-blob-storage-data-loading-for-generative-ai",
        "https://www.llamaindex.ai/llamacloud-google-drive-data-loading-for-generative-ai",
        "https://docs.llamaindex.ai",
        "https://ts.llamaindex.ai",
        "https://www.llamaindex.ai/community#newsletter",
        "https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html",
        "https://www.llamaindex.ai/files/privacy-notice.pdf",
        "https://www.llamaindex.ai/files/terms-of-service.pdf",
        "https://github.com/run-llama/llama_index",
        "https://discord.com/invite/eN6D2HQ4aX",
        "https://twitter.com/llama_index",
        "https://www.linkedin.com/company/91154103/",
        "https://www.youtube.com/@LlamaIndex",
        "https://www.anyscale.com/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-ray",
        "https://gpt-index.readthedocs.io/en/latest/",
        "https://docs.ray.io/en/latest/",
        "https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py",
        "https://github.com/Unstructured-IO/unstructured",
        "https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py#L102:L131",
        "https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py#L22:L56",
        "https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py",
        "https://github.com/amogkam/llama_index_ray",
        "https://docs.ray.io/en/latest/ray-overview/use-cases.html#llms-and-gen-ai",
        "https://forms.gle/9TSdDYUgxYs8SA9e8",
        "https://discord.gg/UB58qbeq",
        "https://raysummit.anyscale.com/",
        "https://llamaindex.statuspage.io",
        "https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu",
        "https://pypi.org/project/llama-index/",
        "https://www.npmjs.com/package/llamaindex",
        "https://llamahub.ai",
        "https://github.com/run-llama",
        "https://bsky.app/profile/llamaindex.bsky.social",
        "https://www.npmjs.com/package/create-llama",
        "https://secinsights.ai",
        "https://github.com/run-llama/llamabot"
    ]
}