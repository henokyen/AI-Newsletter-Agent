{
    "id": "19cd291c5a2ab2d36afb90c89f9b0c07",
    "metadata": {
        "id": "19cd291c5a2ab2d36afb90c89f9b0c07",
        "url": "https://huggingface.co/docs/sagemaker/en/train/",
        "title": "Run training on Amazon SageMaker",
        "properties": {
            "description": "We’re on a journey to advance and democratize artificial intelligence through open source and open science.",
            "keywords": null,
            "author": null,
            "og:title": "Run training on Amazon SageMaker",
            "og:type": "website",
            "og:url": "https://huggingface.co/docs/sagemaker/en/train",
            "og:image": "https://huggingface.co/front/thumbnails/docs/sagemaker.png",
            "twitter:card": "summary_large_image",
            "twitter:site": "@huggingface",
            "twitter:image": "https://huggingface.co/front/thumbnails/docs/sagemaker.png"
        }
    },
    "parent_metadata": {
        "id": "021a0e4da54f8a31eeb178d8f7b928ed",
        "url": "https://www.notion.so/SageMaker-021a0e4da54f8a31eeb178d8f7b928ed",
        "title": "SageMaker",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\nAmazon SageMaker documentation\nRun training on Amazon SageMaker\n# Amazon SageMaker\n🏡 View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersDistilabelEvaluateGradioHubHub Python LibraryHugging Face Generative AI Services (HUGS)Huggingface.jsInference API (serverless)Inference Endpoints (dedicated)LeaderboardsLightevalOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jssmolagentstimm\nSearch documentation\n`⌘K`\nmain EN [ ](https://github.com/huggingface/hub-docs)\n[Hugging Face on Amazon SageMaker ](https://huggingface.co/docs/sagemaker/en/index)[Get started ](https://huggingface.co/docs/sagemaker/en/getting-started)[Run training on Amazon SageMaker ](https://huggingface.co/docs/sagemaker/en/train)[Deploy models to Amazon SageMaker ](https://huggingface.co/docs/sagemaker/en/inference)[Reference ](https://huggingface.co/docs/sagemaker/en/reference)\n![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\nJoin the Hugging Face community\nand get access to the augmented documentation experience \nCollaborate on models, datasets and Spaces \nFaster examples with accelerated inference \nSwitch between documentation themes \n[Sign Up](https://huggingface.co/join)\nto get started\n# [](https://huggingface.co/docs/sagemaker/en/train/#run-training-on-amazon-sagemaker) Run training on Amazon SageMaker\nThis guide will show you how to train a 🤗 Transformers model with the `HuggingFace` SageMaker Python SDK. Learn how to:\n  * [Install and setup your training environment](https://huggingface.co/docs/sagemaker/en/train/#installation-and-setup).\n  * [Prepare a training script](https://huggingface.co/docs/sagemaker/en/train/#prepare-a-transformers-fine-tuning-script).\n  * [Create a Hugging Face Estimator](https://huggingface.co/docs/sagemaker/en/train/#create-a-hugging-face-estimator).\n  * [Run training with the `fit` method](https://huggingface.co/docs/sagemaker/en/train/#execute-training).\n  * [Access your trained model](https://huggingface.co/docs/sagemaker/en/train/#access-trained-model).\n  * [Perform distributed training](https://huggingface.co/docs/sagemaker/en/train/#distributed-training).\n  * [Create a spot instance](https://huggingface.co/docs/sagemaker/en/train/#spot-instances).\n  * [Load a training script from a GitHub repository](https://huggingface.co/docs/sagemaker/en/train/#git-repository).\n  * [Collect training metrics](https://huggingface.co/docs/sagemaker/en/train/#sagemaker-metrics).\n\n\n## [](https://huggingface.co/docs/sagemaker/en/train/#installation-and-setup) Installation and setup\nBefore you can train a 🤗 Transformers model with SageMaker, you need to sign up for an AWS account. If you don’t have an AWS account yet, learn more [here](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html).\nOnce you have an AWS account, get started using one of the following:\n  * [SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html)\n  * [SageMaker notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)\n  * Local environment\n\n\nTo start training locally, you need to setup an appropriate [IAM role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\nUpgrade to the latest `sagemaker` version:\nCopied\n```\npip install sagemaker --upgrade\n```\n\n**SageMaker environment**\nSetup your SageMaker environment as shown below:\nCopied\n```\nimport sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n```\n\n_Note: The execution role is only available when running a notebook within SageMaker. If you run`get_execution_role` in a notebook not on SageMaker, expect a `region` error._\n**Local environment**\nSetup your local environment as shown below:\nCopied\n```\nimport sagemaker\nimport boto3\niam_client = boto3.client('iam')\nrole = iam_client.get_role(RoleName='role-name-of-your-iam-role-with-right-permissions')['Role']['Arn']\nsess = sagemaker.Session()\n```\n\n## [](https://huggingface.co/docs/sagemaker/en/train/#prepare-a--transformers-fine-tuning-script) Prepare a 🤗 Transformers fine-tuning script\nOur training script is very similar to a training script you might run outside of SageMaker. However, you can access useful properties about the training environment through various environment variables (see [here](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md) for a complete list), such as:\n  * `SM_MODEL_DIR`: A string representing the path to which the training job writes the model artifacts. After training, artifacts in this directory are uploaded to S3 for model hosting. `SM_MODEL_DIR` is always set to `/opt/ml/model`.\n  * `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n  * `SM_CHANNEL_XXXX:` A string representing the path to the directory that contains the input data for the specified channel. For example, when you specify `train` and `test` in the Hugging Face Estimator `fit` method, the environment variables are set to `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST`.\n\n\nThe `hyperparameters` defined in the [Hugging Face Estimator](https://huggingface.co/docs/sagemaker/en/train/#create-an-huggingface-estimator) are passed as named arguments and processed by `ArgumentParser()`.\nCopied\n```\nimport transformers\nimport datasets\nimport argparse\nimport os\nif __name__ == \"__main__\":\n  parser = argparse.ArgumentParser()\n  # hyperparameters sent by the client are passed as command-line arguments to the script\n  parser.add_argument(\"--epochs\", type=int, default=3)\n  parser.add_argument(\"--per_device_train_batch_size\", type=int, default=32)\n  parser.add_argument(\"--model_name_or_path\", type=str)\n  # data, model, and output directories\n  parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n  parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n  parser.add_argument(\"--test_dir\", type=str, default=os.environ[\"SM_CHANNEL_TEST\"])\n```\n\n_Note that SageMaker doesn’t support argparse actions. For example, if you want to use a boolean hyperparameter, specify`type` as `bool` in your script and provide an explicit `True` or `False` value._\nLook [train.py file](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/scripts/train.py) for a complete example of a 🤗 Transformers training script.\n## [](https://huggingface.co/docs/sagemaker/en/train/#training-output-management) Training Output Management\nIf `output_dir` in the `TrainingArguments` is set to ‘/opt/ml/model’ the Trainer saves all training artifacts, including logs, checkpoints, and models. Amazon SageMaker archives the whole ‘/opt/ml/model’ directory as `model.tar.gz` and uploads it at the end of the training job to Amazon S3. Depending on your Hyperparameters and `TrainingArguments` this could lead to a large artifact (> 5GB), which can slow down deployment for Amazon SageMaker Inference. You can control how checkpoints, logs, and artifacts are saved by customization the [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). For example by providing `save_total_limit` as `TrainingArgument` you can control the limit of the total amount of checkpoints. Deletes the older checkpoints in `output_dir` if new ones are saved and the maximum limit is reached.\nIn addition to the options already mentioned above, there is another option to save the training artifacts during the training session. Amazon SageMaker supports [Checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html), which allows you to continuously save your artifacts during training to Amazon S3 rather than at the end of your training. To enable [Checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html) you need to provide the `checkpoint_s3_uri` parameter pointing to an Amazon S3 location in the `HuggingFace` estimator and set `output_dir` to `/opt/ml/checkpoints`. _Note: If you set`output_dir` to `/opt/ml/checkpoints` make sure to call `trainer.save_model(\"/opt/ml/model\")` or model.save_pretrained(“/opt/ml/model”)/`tokenizer.save_pretrained(\"/opt/ml/model\")` at the end of your training to be able to deploy your model seamlessly to Amazon SageMaker for Inference._\n## [](https://huggingface.co/docs/sagemaker/en/train/#create-a-hugging-face-estimator) Create a Hugging Face Estimator\nRun 🤗 Transformers training scripts on SageMaker by creating a [Hugging Face Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#huggingface-estimator). The Estimator handles end-to-end SageMaker training. There are several parameters you should define in the Estimator:\n  1. `entry_point` specifies which fine-tuning script to use.\n  2. `instance_type` specifies an Amazon instance to launch. Refer [here](https://aws.amazon.com/sagemaker/pricing/) for a complete list of instance types.\n  3. `hyperparameters` specifies training hyperparameters. View additional available hyperparameters in [train.py file](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/scripts/train.py).\n\n\nThe following code sample shows how to train with a custom script `train.py` with three hyperparameters (`epochs`, `per_device_train_batch_size`, and `model_name_or_path`):\nCopied\n```\nfrom sagemaker.huggingface import HuggingFace\n\n# hyperparameters which are passed to the training job\nhyperparameters={'epochs': 1,\n         'per_device_train_batch_size': 32,\n         'model_name_or_path': 'distilbert-base-uncased'\n         }\n# create the Estimator\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='./scripts',\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n    role=role,\n    transformers_version='4.26',\n    pytorch_version='1.13',\n    py_version='py39',\n    hyperparameters = hyperparameters\n)\n```\n\nIf you are running a `TrainingJob` locally, define `instance_type='local'` or `instance_type='local_gpu'` for GPU usage. Note that this will not work with SageMaker Studio.\n## [](https://huggingface.co/docs/sagemaker/en/train/#execute-training) Execute training\nStart your `TrainingJob` by calling `fit` on a Hugging Face Estimator. Specify your input training data in `fit`. The input training data can be a:\n  * S3 URI such as `s3://my-bucket/my-training-data`.\n  * `FileSystemInput` for Amazon Elastic File System or FSx for Lustre. See [here](https://sagemaker.readthedocs.io/en/stable/overview.html?highlight=FileSystemInput#use-file-systems-as-training-inputs) for more details about using these file systems as input.\n\n\nCall `fit` to begin training:\nCopied\n```\nhuggingface_estimator.fit(\n {'train': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/train',\n  'test': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/test'}\n)\n```\n\nSageMaker starts and manages all the required EC2 instances and initiates the `TrainingJob` by running:\nCopied\n```\n/opt/conda/bin/python train.py --epochs 1 --model_name_or_path distilbert-base-uncased --per_device_train_batch_size 32\n```\n\n## [](https://huggingface.co/docs/sagemaker/en/train/#access-trained-model) Access trained model\nOnce training is complete, you can access your model through the [AWS console](https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-signin) or download it directly from S3.\nCopied\n```\nfrom sagemaker.s3 import S3Downloader\nS3Downloader.download(\n  s3_uri=huggingface_estimator.model_data, # S3 URI where the trained model is located\n  local_path='.',             # local path where *.targ.gz is saved\n  sagemaker_session=sess          # SageMaker session used for training the model\n)\n```\n\n## [](https://huggingface.co/docs/sagemaker/en/train/#distributed-training) Distributed training\nSageMaker provides two strategies for distributed training: data parallelism and model parallelism. Data parallelism splits a training set across several GPUs, while model parallelism splits a model across several GPUs.\n### [](https://huggingface.co/docs/sagemaker/en/train/#data-parallelism) Data parallelism\nThe Hugging Face [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) supports SageMaker’s data parallelism library. If your training script uses the Trainer API, you only need to define the distribution parameter in the Hugging Face Estimator:\nCopied\n```\n# configuration for running training on smdistributed data parallel\ndistribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n# create the Estimator\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='./scripts',\n    instance_type='ml.p3dn.24xlarge',\n    instance_count=2,\n    role=role,\n    transformers_version='4.26.0',\n    pytorch_version='1.13.1',\n    py_version='py39',\n    hyperparameters = hyperparameters,\n    distribution = distribution\n)\n```\n\n📓 Open the [sagemaker-notebook.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/07_tensorflow_distributed_training_data_parallelism/sagemaker-notebook.ipynb) for an example of how to run the data parallelism library with TensorFlow.\n### [](https://huggingface.co/docs/sagemaker/en/train/#model-parallelism) Model parallelism\nThe Hugging Face [Trainer] also supports SageMaker’s model parallelism library. If your training script uses the Trainer API, you only need to define the distribution parameter in the Hugging Face Estimator (see [here](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html?highlight=modelparallel#required-sagemaker-python-sdk-parameters) for more detailed information about using model parallelism):\nCopied\n```\n# configuration for running training on smdistributed model parallel\nmpi_options = {\n  \"enabled\" : True,\n  \"processes_per_host\" : 8\n}\nsmp_options = {\n  \"enabled\":True,\n  \"parameters\": {\n    \"microbatches\": 4,\n    \"placement_strategy\": \"spread\",\n    \"pipeline\": \"interleaved\",\n    \"optimize\": \"speed\",\n    \"partitions\": 4,\n    \"ddp\": True,\n  }\n}\ndistribution={\n  \"smdistributed\": {\"modelparallel\": smp_options},\n  \"mpi\": mpi_options\n}\n # create the Estimator\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='./scripts',\n    instance_type='ml.p3dn.24xlarge',\n    instance_count=2,\n    role=role,\n    transformers_version='4.26.0',\n    pytorch_version='1.13.1',\n    py_version='py39',\n    hyperparameters = hyperparameters,\n    distribution = distribution\n)\n```\n\n📓 Open the [sagemaker-notebook.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/04_distributed_training_model_parallelism/sagemaker-notebook.ipynb) for an example of how to run the model parallelism library.\n## [](https://huggingface.co/docs/sagemaker/en/train/#spot-instances) Spot instances\nThe Hugging Face extension for the SageMaker Python SDK means we can benefit from [fully-managed EC2 spot instances](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html). This can help you save up to 90% of training costs!\n_Note: Unless your training job completes quickly, we recommend you use[checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html) with managed spot training. In this case, you need to define the `checkpoint_s3_uri`._\nSet `use_spot_instances=True` and define your `max_wait` and `max_run` time in the Estimator to use spot instances:\nCopied\n```\n# hyperparameters which are passed to the training job\nhyperparameters={'epochs': 1,\n         'train_batch_size': 32,\n         'model_name':'distilbert-base-uncased',\n         'output_dir':'/opt/ml/checkpoints'\n         }\n# create the Estimator\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='./scripts',\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n\t  checkpoint_s3_uri=f's3://{sess.default_bucket()}/checkpoints'\n    use_spot_instances=True,\n    # max_wait should be equal to or greater than max_run in seconds\n    max_wait=3600,\n    max_run=1000,\n    role=role,\n    transformers_version='4.26',\n    pytorch_version='1.13',\n    py_version='py39',\n    hyperparameters = hyperparameters\n)\n# Training seconds: 874\n# Billable seconds: 262\n# Managed Spot Training savings: 70.0%\n```\n\n📓 Open the [sagemaker-notebook.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/05_spot_instances/sagemaker-notebook.ipynb) for an example of how to use spot instances.\n## [](https://huggingface.co/docs/sagemaker/en/train/#git-repository) Git repository\nThe Hugging Face Estimator can load a training script [stored in a GitHub repository](https://sagemaker.readthedocs.io/en/stable/overview.html#use-scripts-stored-in-a-git-repository). Provide the relative path to the training script in `entry_point` and the relative path to the directory in `source_dir`.\nIf you are using `git_config` to run the [🤗 Transformers example scripts](https://github.com/huggingface/transformers/tree/main/examples), you need to configure the correct `'branch'` in `transformers_version` (e.g. if you use `transformers_version='4.4.2` you have to use `'branch':'v4.4.2'`).\n_Tip: Save your model to S3 by setting`output_dir=/opt/ml/model` in the hyperparameter of your training script._\nCopied\n```\n# configure git settings\ngit_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.4.2'} # v4.4.2 refers to the transformers_version you use in the estimator\n # create the Estimator\nhuggingface_estimator = HuggingFace(\n    entry_point='run_glue.py',\n    source_dir='./examples/pytorch/text-classification',\n    git_config=git_config,\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n    role=role,\n    transformers_version='4.26',\n    pytorch_version='1.13',\n    py_version='py39',\n    hyperparameters=hyperparameters\n)\n```\n\n## [](https://huggingface.co/docs/sagemaker/en/train/#sagemaker-metrics) SageMaker metrics\n[SageMaker metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html#define-train-metrics) automatically parses training job logs for metrics and sends them to CloudWatch. If you want SageMaker to parse the logs, you must specify the metric’s name and a regular expression for SageMaker to use to find the metric.\nCopied\n```\n# define metrics definitions\nmetric_definitions = [\n  {\"Name\": \"train_runtime\", \"Regex\": \"train_runtime.*=\\D*(.*?)$\"},\n  {\"Name\": \"eval_accuracy\", \"Regex\": \"eval_accuracy.*=\\D*(.*?)$\"},\n  {\"Name\": \"eval_loss\", \"Regex\": \"eval_loss.*=\\D*(.*?)$\"},\n]\n# create the Estimator\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='./scripts',\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n    role=role,\n    transformers_version='4.26',\n    pytorch_version='1.13',\n    py_version='py39',\n    metric_definitions=metric_definitions,\n    hyperparameters = hyperparameters)\n```\n\n📓 Open the [notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/06_sagemaker_metrics/sagemaker-notebook.ipynb) for an example of how to capture metrics in SageMaker.\n[< > Update on GitHub](https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md)\n[←Get started](https://huggingface.co/docs/sagemaker/en/getting-started) [Deploy models to Amazon SageMaker→](https://huggingface.co/docs/sagemaker/en/inference)\n[Run training on Amazon SageMaker](https://huggingface.co/docs/sagemaker/en/train/#run-training-on-amazon-sagemaker) [Installation and setup](https://huggingface.co/docs/sagemaker/en/train/#installation-and-setup) [Prepare a 🤗 Transformers fine-tuning script](https://huggingface.co/docs/sagemaker/en/train/#prepare-a--transformers-fine-tuning-script) [Training Output Management](https://huggingface.co/docs/sagemaker/en/train/#training-output-management) [Create a Hugging Face Estimator](https://huggingface.co/docs/sagemaker/en/train/#create-a-hugging-face-estimator) [Execute training](https://huggingface.co/docs/sagemaker/en/train/#execute-training) [Access trained model](https://huggingface.co/docs/sagemaker/en/train/#access-trained-model) [Distributed training](https://huggingface.co/docs/sagemaker/en/train/#distributed-training) [Data parallelism](https://huggingface.co/docs/sagemaker/en/train/#data-parallelism) [Model parallelism](https://huggingface.co/docs/sagemaker/en/train/#model-parallelism) [Spot instances](https://huggingface.co/docs/sagemaker/en/train/#spot-instances) [Git repository](https://huggingface.co/docs/sagemaker/en/train/#git-repository) [SageMaker metrics](https://huggingface.co/docs/sagemaker/en/train/#sagemaker-metrics)\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://huggingface.co/",
        "https://huggingface.co/models",
        "https://huggingface.co/datasets",
        "https://huggingface.co/spaces",
        "https://huggingface.co/posts",
        "https://huggingface.co/docs",
        "https://huggingface.co/enterprise",
        "https://huggingface.co/pricing",
        "https://huggingface.co/login",
        "https://huggingface.co/join",
        "https://huggingface.co/docs/sagemaker/en/index",
        "https://huggingface.co/docs/sagemaker/en/getting-started",
        "https://huggingface.co/docs/sagemaker/en/train",
        "https://huggingface.co/docs/sagemaker/en/inference",
        "https://huggingface.co/docs/sagemaker/en/reference",
        "https://huggingface.co/docs/sagemaker/en/train/#run-training-on-amazon-sagemaker",
        "https://huggingface.co/docs/sagemaker/en/train/#installation-and-setup",
        "https://huggingface.co/docs/sagemaker/en/train/#prepare-a-transformers-fine-tuning-script",
        "https://huggingface.co/docs/sagemaker/en/train/#create-a-hugging-face-estimator",
        "https://huggingface.co/docs/sagemaker/en/train/#execute-training",
        "https://huggingface.co/docs/sagemaker/en/train/#access-trained-model",
        "https://huggingface.co/docs/sagemaker/en/train/#distributed-training",
        "https://huggingface.co/docs/sagemaker/en/train/#spot-instances",
        "https://huggingface.co/docs/sagemaker/en/train/#git-repository",
        "https://huggingface.co/docs/sagemaker/en/train/#sagemaker-metrics",
        "https://huggingface.co/docs/sagemaker/en/train/#prepare-a--transformers-fine-tuning-script",
        "https://huggingface.co/docs/sagemaker/en/train/#create-an-huggingface-estimator",
        "https://huggingface.co/docs/sagemaker/en/train/#training-output-management",
        "https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments",
        "https://huggingface.co/docs/sagemaker/en/train/#data-parallelism",
        "https://huggingface.co/docs/transformers/main_classes/trainer",
        "https://huggingface.co/docs/sagemaker/en/train/#model-parallelism",
        "https://github.com/huggingface/hub-docs",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html",
        "https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md",
        "https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/scripts/train.py",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html",
        "https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#huggingface-estimator",
        "https://aws.amazon.com/sagemaker/pricing/",
        "https://sagemaker.readthedocs.io/en/stable/overview.html?highlight=FileSystemInput#use-file-systems-as-training-inputs",
        "https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-signin",
        "https://github.com/huggingface/notebooks/blob/main/sagemaker/07_tensorflow_distributed_training_data_parallelism/sagemaker-notebook.ipynb",
        "https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html?highlight=modelparallel#required-sagemaker-python-sdk-parameters",
        "https://github.com/huggingface/notebooks/blob/main/sagemaker/04_distributed_training_model_parallelism/sagemaker-notebook.ipynb",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html",
        "https://github.com/huggingface/notebooks/blob/main/sagemaker/05_spot_instances/sagemaker-notebook.ipynb",
        "https://sagemaker.readthedocs.io/en/stable/overview.html#use-scripts-stored-in-a-git-repository",
        "https://github.com/huggingface/transformers/tree/main/examples",
        "https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html#define-train-metrics",
        "https://github.com/huggingface/notebooks/blob/main/sagemaker/06_sagemaker_metrics/sagemaker-notebook.ipynb",
        "https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md"
    ]
}