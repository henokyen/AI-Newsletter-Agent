{
    "id": "c95e2ff023109fcbdead94bb0e9042c9",
    "metadata": {
        "id": "c95e2ff023109fcbdead94bb0e9042c9",
        "url": "https://arxiv.org/abs/2408.03314?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN/",
        "title": "[2408.03314] Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
        "properties": {
            "description": "Abstract page for arXiv paper 2408.03314: Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
            "keywords": null,
            "author": null,
            "og:type": "website",
            "og:site_name": "arXiv.org",
            "og:title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
            "og:url": "https://arxiv.org/abs/2408.03314v1",
            "og:image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
            "og:image:secure_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
            "og:image:width": "1200",
            "og:image:height": "700",
            "og:image:alt": "arXiv logo",
            "og:description": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a \"compute-optimal\" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.",
            "twitter:site": "@arxiv",
            "twitter:card": "summary",
            "twitter:title": "Scaling LLM Test-Time Compute Optimally can be More Effective than...",
            "twitter:description": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In...",
            "twitter:image": "https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png",
            "twitter:image:alt": "arXiv logo"
        }
    },
    "parent_metadata": {
        "id": "8cefae312c0ac046c39fa8aaa882abbc",
        "url": "https://www.notion.so/Agents-8cefae312c0ac046c39fa8aaa882abbc",
        "title": "Agents",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Skip to main content](https://arxiv.org/abs/2408.03314?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN/#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/) > [cs](https://arxiv.org/list/cs/recent) > arXiv:2408.03314 \n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[ ![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg) ](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n## quick links\n  * [Login](https://arxiv.org/login)\n  * [Help Pages](https://info.arxiv.org/help)\n  * [About](https://info.arxiv.org/about)\n\n\n# Computer Science > Machine Learning\n**arXiv:2408.03314** (cs) \n[Submitted on 6 Aug 2024]\n# Title:Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters\nAuthors:[Charlie Snell](https://arxiv.org/search/cs?searchtype=author&query=Snell,+C), [Jaehoon Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+J), [Kelvin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+K), [Aviral Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar,+A)\nView a PDF of the paper titled Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters, by Charlie Snell and 3 other authors\n[View PDF](https://arxiv.org/pdf/2408.03314) [HTML (experimental)](https://arxiv.org/html/2408.03314v1)\n> Abstract:Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a \"compute-optimal\" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model. \nSubjects: |  Machine Learning (cs.LG); Computation and Language (cs.CL)  \n---|---  \nCite as: | [arXiv:2408.03314](https://arxiv.org/abs/2408.03314) [cs.LG]  \n(or  [arXiv:2408.03314v1](https://arxiv.org/abs/2408.03314v1) [cs.LG] for this version)   \n<https://doi.org/10.48550/arXiv.2408.03314> Focus to learn more arXiv-issued DOI via DataCite  \n## Submission history\nFrom: Charlie Snell [[view email](https://arxiv.org/show-email/dfa16c4c/2408.03314)] **[v1]** Tue, 6 Aug 2024 17:35:05 UTC (4,152 KB) \nFull-text links:\n## Access Paper:\nView a PDF of the paper titled Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters, by Charlie Snell and 3 other authors\n  * [View PDF](https://arxiv.org/pdf/2408.03314)\n  * [HTML (experimental)](https://arxiv.org/html/2408.03314v1)\n  * [TeX Source](https://arxiv.org/src/2408.03314)\n  * [Other Formats](https://arxiv.org/format/2408.03314)\n\n\n[ ![license icon](https://arxiv.org/icons/licenses/by-4.0.png) view license ](http://creativecommons.org/licenses/by/4.0/ \"Rights to this article\")\nCurrent browse context: \ncs.LG\n[< prev](https://arxiv.org/prevnext?id=2408.03314&function=prev&context=cs.LG \"previous in cs.LG \\(accesskey p\\)\") |  [next >](https://arxiv.org/prevnext?id=2408.03314&function=next&context=cs.LG \"next in cs.LG \\(accesskey n\\)\")\n[new](https://arxiv.org/list/cs.LG/new) |  [recent](https://arxiv.org/list/cs.LG/recent) | [2024-08](https://arxiv.org/list/cs.LG/2024-08)\nChange to browse by: \n[cs](https://arxiv.org/abs/2408.03314?context=cs) [cs.CL](https://arxiv.org/abs/2408.03314?context=cs.CL)\n### References & Citations\n  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2408.03314)\n  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2408.03314)\n  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:2408.03314)\n\n\n### [ 1 blog link](https://arxiv.org/tb/2408.03314)\n([what is this?](https://info.arxiv.org/help/trackback.html)) \n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citation Loading...\n## BibTeX formatted citation\n×\nloading...\nData provided by: \n### Bookmark\n[ ![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png) ](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2408.03314&description=Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters \"Bookmark on BibSonomy\") [ ![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png) ](https://reddit.com/submit?url=https://arxiv.org/abs/2408.03314&title=Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters \"Bookmark on Reddit\")\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\nConnected Papers Toggle\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\nLitmaps Toggle\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\nscite.ai Toggle\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_\nDagsHub Toggle\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\nGotitPub Toggle\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\nHuggingface Toggle\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\nLinks to Code Toggle\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\nScienceCast Toggle\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\nDemos\n# Demos\nReplicate Toggle\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\nSpaces Toggle\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\nSpaces Toggle\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\nCore recommender toggle\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\nIArxiv recommender toggle\nIArxiv Recommender _([What is IArxiv?](https://iarxiv.org/about))_\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n\nAbout arXivLabs \n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2408.03314) | [Disable MathJax](javascript:setMathjaxCookie\\(\\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n  * [About](https://info.arxiv.org/about)\n  * [Help](https://info.arxiv.org/help)\n\n\n  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)\n  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)\n\n\n  * [Copyright](https://info.arxiv.org/help/license/index.html)\n  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n\n  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n  * [arXiv Operational Status ](https://status.arxiv.org) Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n\n\n",
    "content_quality_score": 0.8,
    "summary": null,
    "child_urls": [
        "https://arxiv.org/abs/2408.03314?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN/#content",
        "https://info.arxiv.org/about/ourmembers.html",
        "https://info.arxiv.org/about/donate.html",
        "https://arxiv.org/IgnoreMe",
        "https://arxiv.org/",
        "https://arxiv.org/list/cs/recent",
        "https://info.arxiv.org/help",
        "https://arxiv.org/search/advanced",
        "https://arxiv.org/login",
        "https://info.arxiv.org/about",
        "https://arxiv.org/search/cs?searchtype=author&query=Snell,+C",
        "https://arxiv.org/search/cs?searchtype=author&query=Lee,+J",
        "https://arxiv.org/search/cs?searchtype=author&query=Xu,+K",
        "https://arxiv.org/search/cs?searchtype=author&query=Kumar,+A",
        "https://arxiv.org/pdf/2408.03314",
        "https://arxiv.org/html/2408.03314v1",
        "https://arxiv.org/abs/2408.03314",
        "https://arxiv.org/abs/2408.03314v1",
        "https://arxiv.org/show-email/dfa16c4c/2408.03314",
        "https://arxiv.org/src/2408.03314",
        "https://arxiv.org/format/2408.03314",
        "https://arxiv.org/prevnext?id=2408.03314&function=prev&context=cs.LG",
        "https://arxiv.org/prevnext?id=2408.03314&function=next&context=cs.LG",
        "https://arxiv.org/list/cs.LG/new",
        "https://arxiv.org/list/cs.LG/recent",
        "https://arxiv.org/list/cs.LG/2024-08",
        "https://arxiv.org/abs/2408.03314?context=cs",
        "https://arxiv.org/abs/2408.03314?context=cs.CL",
        "https://arxiv.org/tb/2408.03314",
        "https://info.arxiv.org/help/trackback.html",
        "https://arxiv.org/static/browse/0.3.4/css/cite.css",
        "https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer",
        "https://iarxiv.org/about",
        "https://info.arxiv.org/labs/index.html",
        "https://arxiv.org/auth/show-endorsers/2408.03314",
        "https://info.arxiv.org/help/mathjax.html",
        "https://info.arxiv.org/help/contact.html",
        "https://info.arxiv.org/help/subscribe",
        "https://info.arxiv.org/help/license/index.html",
        "https://info.arxiv.org/help/policies/privacy_policy.html",
        "https://info.arxiv.org/help/web_accessibility.html",
        "https://status.arxiv.org",
        "https://www.cornell.edu/",
        "https://doi.org/10.48550/arXiv.2408.03314",
        "http://creativecommons.org/licenses/by/4.0/",
        "https://ui.adsabs.harvard.edu/abs/arXiv:2408.03314",
        "https://scholar.google.com/scholar_lookup?arxiv_id=2408.03314",
        "https://api.semanticscholar.org/arXiv:2408.03314",
        "http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2408.03314&description=Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
        "https://reddit.com/submit?url=https://arxiv.org/abs/2408.03314&title=Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
        "https://www.connectedpapers.com/about",
        "https://www.litmaps.co/",
        "https://www.scite.ai/",
        "https://alphaxiv.org/",
        "https://www.catalyzex.com",
        "https://dagshub.com/",
        "http://gotit.pub/faq",
        "https://huggingface.co/huggingface",
        "https://paperswithcode.com/",
        "https://sciencecast.org/welcome",
        "https://replicate.com/docs/arxiv/about",
        "https://huggingface.co/docs/hub/spaces",
        "https://txyz.ai",
        "https://influencemap.cmlab.dev/",
        "https://core.ac.uk/services/recommender",
        "javascript:setMathjaxCookie()",
        "https://subscribe.sorryapp.com/24846f03/email/new",
        "https://subscribe.sorryapp.com/24846f03/slack/new"
    ]
}