{
    "id": "852beaa1db5ca7de478cea2dfb932bc7",
    "metadata": {
        "id": "852beaa1db5ca7de478cea2dfb932bc7",
        "url": "https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use/",
        "title": "Templates",
        "properties": {
            "description": "We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.",
            "keywords": null,
            "author": null,
            "og:title": "Templates",
            "og:type": "website",
            "og:url": "https://huggingface.co/docs/transformers/en/chat_templating",
            "og:image": "https://huggingface.co/front/thumbnails/docs/transformers.png",
            "twitter:card": "summary_large_image",
            "twitter:site": "@huggingface",
            "twitter:image": "https://huggingface.co/front/thumbnails/docs/transformers.png"
        }
    },
    "parent_metadata": {
        "id": "dbd53f71d37c285b3c4bcaadc5ddee79",
        "url": "https://www.notion.so/Training-Fine-tuning-LLMs-dbd53f71d37c285b3c4bcaadc5ddee79",
        "title": "Training & Fine-tuning LLMs",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\nTransformers documentation \nTemplates\n# Transformers\nüè° View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersDistilabelEvaluateGradioHubHub Python LibraryHugging Face Generative AI Services (HUGS)Huggingface.jsInference API (serverless)Inference Endpoints (dedicated)LeaderboardsLightevalOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jssmolagentstimm\nSearch documentation\nmainv4.50.0v4.49.0v4.48.2v4.47.1v4.46.3v4.45.2v4.44.2v4.43.4v4.42.4v4.41.2v4.40.2v4.39.3v4.38.2v4.37.2v4.36.1v4.35.2v4.34.1v4.33.3v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-builder-html ARDEENESFRHIITJAKOPTTETRZH [ ](https://github.com/huggingface/transformers)\nGet started\n[Transformers ](https://huggingface.co/docs/transformers/en/index)[Installation ](https://huggingface.co/docs/transformers/en/installation)[Quickstart ](https://huggingface.co/docs/transformers/en/quicktour)\nBase classes\nInference\nPipeline API\nLLMs\nChat with models\n[Chat basics ](https://huggingface.co/docs/transformers/en/conversations)[Templates ](https://huggingface.co/docs/transformers/en/chat_templating)[Multimodal templates ](https://huggingface.co/docs/transformers/en/chat_templating_multimodal)[Template writing ](https://huggingface.co/docs/transformers/en/chat_templating_writing)[Tools and RAG ](https://huggingface.co/docs/transformers/en/chat_extras)\nOptimization\n[Agents ](https://huggingface.co/docs/transformers/en/agents)[Tools ](https://huggingface.co/docs/transformers/en/tools)\nTraining\nQuantization\nExport to production\nResources\nContribute\nAPI\n![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\nJoin the Hugging Face community\nand get access to the augmented documentation experience \nCollaborate on models, datasets and Spaces \nFaster examples with accelerated inference \nSwitch between documentation themes \n[Sign Up](https://huggingface.co/join)\nto get started\n# [](https://huggingface.co/docs/transformers/en/chat_templating#templates) Templates\nThe [chat pipeline](https://huggingface.co/docs/transformers/en/conversations) guide introduced [TextGenerationPipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.TextGenerationPipeline) and the concept of a chat prompt or chat template for conversing with a model. Underlying this high-level pipeline is the `apply_chat_template` method. A chat template is a part of the tokenizer and it specifies how to convert conversations into a single tokenizable string in the expected model format.\nIn the example below, Mistral-7B-Instruct and Zephyr-7B are finetuned from the same base model but they‚Äôre trained with different chat formats. Without chat templates, you have to manually write formatting code for each model and even minor errors can hurt performance. Chat templates offer a universal way to format chat inputs to any model.\nMistral \nZephyr \nCopied\n```\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\nchat = [\n {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]\ntokenizer.apply_chat_template(chat, tokenize=False)\n```\n\nCopied\n```\n<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\n```\n\nThis guide explores `apply_chat_template` and chat templates in more detail.\n## [](https://huggingface.co/docs/transformers/en/chat_templating#applychattemplate) apply_chat_template\nChats should be structured as a list of dictionaries with `role` and `content` keys. The `role` key specifies the speaker (usually between you and the system), and the `content` key contains your message. For the system, the `content` is a high-level description of how the model should behave and respond when you‚Äôre chatting with it.\nPass your messages to `apply_chat_template` to tokenize and format them. You can set [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` to indicate the start of a message.\nCopied\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n  {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n ]\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\nprint(tokenizer.decode(tokenized_chat[0]))\n```\n\nCopied\n```\n<|system|>\nYou are a friendly chatbot who always responds in the style of a pirate</s>\n<|user|>\nHow many helicopters can a human eat in one sitting?</s>\n<|assistant|>\n```\n\nNow pass the tokenized chat to [generate()](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/text_generation#transformers.GenerationMixin.generate) to generate a response.\nCopied\n```\noutputs = model.generate(tokenized_chat, max_new_tokens=128) \nprint(tokenizer.decode(outputs[0]))\n```\n\nCopied\n```\n<|system|>\nYou are a friendly chatbot who always responds in the style of a pirate</s>\n<|user|>\nHow many helicopters can a human eat in one sitting?</s>\n<|assistant|>\nMatey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n```\n\n### [](https://huggingface.co/docs/transformers/en/chat_templating#addgenerationprompt) add_generation_prompt\nThe [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) parameter adds tokens that indicate the start of a response. This ensures the chat model generates a system response instead of continuing a users message.\nNot all models require generation prompts, and some models, like [Llama](https://huggingface.co/docs/transformers/en/model_doc/llama), don‚Äôt have any special tokens before the system response. In this case, [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) has no effect.\nCopied\n```\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\ntokenized_chat\n```\n\nCopied\n```\n<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n```\n\n### [](https://huggingface.co/docs/transformers/en/chat_templating#continuefinalmessage) continue_final_message\nThe [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) parameter controls whether the final message in the chat should be continued or not instead of starting a new one. It removes end of sequence tokens so that the model continues generation from the final message.\nThis is useful for ‚Äúprefilling‚Äù a model response. In the example below, the model generates text that continues the JSON string rather than starting a new message. It can be very useful for improving the accuracy for instruction following when you know how to start its replies.\nCopied\n```\nchat = [\n  {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n  {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n]\nformatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)\nmodel.generate(**formatted_chat)\n```\n\nYou shouldn‚Äôt use [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) and [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) together. The former adds tokens that start a new message, while the latter removes end of sequence tokens. Using them together returns an error.\n[TextGenerationPipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.TextGenerationPipeline) sets [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` by default to start a new message. However, if the final message in the chat has the ‚Äúassistant‚Äù role, it assumes the message is a prefill and switches to `continue_final_message=True`. This is because most models don‚Äôt support multiple consecutive assistant messages. To override this behavior, explicitly pass the [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) to the pipeline.\n## [](https://huggingface.co/docs/transformers/en/chat_templating#multiple-templates) Multiple templates\nA model may have several different templates for different use cases. For example, a model may have a template for regular chat, tool use, and RAG.\nWhen there are multiple templates, the chat template is a dictionary. Each key corresponds to the name of a template. `apply_chat_template` handles multiple templates based on their name. It looks for a template named `default` in most cases and if it can‚Äôt find one, it raises an error.\nFor a tool calling template, if a user passes a `tools` parameter and a `tool_use` template exists, the tool calling template is used instead of `default`.\nTo access templates with other names, pass the template name to the `chat_template` parameter in `apply_chat_template`. For example, if you‚Äôre using a RAG template then set `chat_template=\"rag\"`.\nIt can be confusing to manage multiple templates though, so we recommend using a single template for all use cases. Use Jinja statements like `if tools is defined` and `{% macro %}` definitions to wrap multiple code paths in a single template.\n## [](https://huggingface.co/docs/transformers/en/chat_templating#template-selection) Template selection\nIt is important to set a chat template format that matches the template format a model was pretrained on, otherwise performance may suffer. Even if you‚Äôre training the model further, performance is best if the chat tokens are kept constant.\nBut if you‚Äôre training a model from scratch or finetuning a model for chat, you have more options to select a template. For example, [ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md) is a popular format that is flexbile enough to handle many use cases. It even includes support for [generation prompts](https://huggingface.co/docs/transformers/en/chat_templating#add_generation_prompt), but it doesn‚Äôt add beginning-of-string (`BOS`) or end-of-string (`EOS`) tokens. If your model expects `BOS` and `EOS` tokens, set `add_special_tokens=True` and make sure to add them to your template.\nCopied\n```\n{%- for message in messages %}\n  {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n{%- endfor %}\n```\n\nSet the template with the following logic to support [generation prompts](https://huggingface.co/docs/transformers/en/chat_templating#add_generation_prompt). The template wraps each message with `<|im_start|>` and `<|im_end|>` tokens and writes the role as a string. This allows you to easily customize the roles you want to train with.\nCopied\n```\ntokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n```\n\nThe `user`, `system` and `assistant` roles are standard roles in chat templates. We recommend using these roles when it makes sense, especially if you‚Äôre using your model with the [TextGenerationPipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.TextGenerationPipeline).\nCopied\n```\n<|im_start|>system\nYou are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n<|im_start|>user\nHow are you?<|im_end|>\n<|im_start|>assistant\nI'm doing great!<|im_end|>\n```\n\n## [](https://huggingface.co/docs/transformers/en/chat_templating#model-training) Model training\nTraining a model with a chat template is a good way to ensure a chat template matches the tokens a model is trained on. Apply the chat template as a preprocessing step to your dataset. Set `add_generation_prompt=False` because the additional tokens to prompt an assistant response aren‚Äôt helpful during training.\nAn example of preprocessing a dataset with a chat template is shown below.\nCopied\n```\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\nchat1 = [\n  {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n  {\"role\": \"assistant\", \"content\": \"The sun.\"}\n]\nchat2 = [\n  {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n  {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n]\ndataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\ndataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\nprint(dataset['formatted_chat'][0])\n```\n\nCopied\n```\n<|user|>\nWhich is bigger, the moon or the sun?</s>\n<|assistant|>\nThe sun.</s>\n```\n\nAfter this step, you can continue following the [training recipe](https://huggingface.co/docs/transformers/en/tasks/language_modeling) for causal language models using the `formatted_chat` column.\nSome tokenizers add special `<bos>` and `<eos>` tokens. Chat templates should already include all the necessary special tokens, and adding additional special tokens is often incorrect or duplicated, hurting model performance. When you format text with `apply_chat_template(tokenize=False)`, make sure you set `add_special_tokens=False` as well to avoid duplicating them.\nCopied\n```\napply_chat_template(messages, tokenize=False, add_special_tokens=False)\n```\n\nThis isn‚Äôt an issue if `apply_chat_template(tokenize=True)`.\n[< > Update on GitHub](https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md)\n[‚ÜêChat basics](https://huggingface.co/docs/transformers/en/conversations) [Multimodal templates‚Üí](https://huggingface.co/docs/transformers/en/chat_templating_multimodal)\n[Templates](https://huggingface.co/docs/transformers/en/chat_templating#templates) [apply_chat_template](https://huggingface.co/docs/transformers/en/chat_templating#applychattemplate) [add_generation_prompt](https://huggingface.co/docs/transformers/en/chat_templating#addgenerationprompt) [continue_final_message](https://huggingface.co/docs/transformers/en/chat_templating#continuefinalmessage) [Multiple templates](https://huggingface.co/docs/transformers/en/chat_templating#multiple-templates) [Template selection](https://huggingface.co/docs/transformers/en/chat_templating#template-selection) [Model training](https://huggingface.co/docs/transformers/en/chat_templating#model-training)\n",
    "content_quality_score": 0.5,
    "summary": null,
    "child_urls": [
        "https://huggingface.co/",
        "https://huggingface.co/models",
        "https://huggingface.co/datasets",
        "https://huggingface.co/spaces",
        "https://huggingface.co/posts",
        "https://huggingface.co/docs",
        "https://huggingface.co/enterprise",
        "https://huggingface.co/pricing",
        "https://huggingface.co/login",
        "https://huggingface.co/join",
        "https://huggingface.co/docs/transformers/en/index",
        "https://huggingface.co/docs/transformers/en/installation",
        "https://huggingface.co/docs/transformers/en/quicktour",
        "https://huggingface.co/docs/transformers/en/conversations",
        "https://huggingface.co/docs/transformers/en/chat_templating",
        "https://huggingface.co/docs/transformers/en/chat_templating_multimodal",
        "https://huggingface.co/docs/transformers/en/chat_templating_writing",
        "https://huggingface.co/docs/transformers/en/chat_extras",
        "https://huggingface.co/docs/transformers/en/agents",
        "https://huggingface.co/docs/transformers/en/tools",
        "https://huggingface.co/docs/transformers/en/chat_templating#templates",
        "https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.TextGenerationPipeline",
        "https://huggingface.co/docs/transformers/en/chat_templating#applychattemplate",
        "https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt",
        "https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/text_generation#transformers.GenerationMixin.generate",
        "https://huggingface.co/docs/transformers/en/chat_templating#addgenerationprompt",
        "https://huggingface.co/docs/transformers/en/model_doc/llama",
        "https://huggingface.co/docs/transformers/en/chat_templating#continuefinalmessage",
        "https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message",
        "https://huggingface.co/docs/transformers/en/chat_templating#multiple-templates",
        "https://huggingface.co/docs/transformers/en/chat_templating#template-selection",
        "https://huggingface.co/docs/transformers/en/chat_templating#add_generation_prompt",
        "https://huggingface.co/docs/transformers/en/chat_templating#model-training",
        "https://huggingface.co/docs/transformers/en/tasks/language_modeling",
        "https://github.com/huggingface/transformers",
        "https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md",
        "https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md"
    ]
}