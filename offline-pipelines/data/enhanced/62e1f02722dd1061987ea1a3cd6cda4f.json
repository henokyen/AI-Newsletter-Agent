{
    "id": "62e1f02722dd1061987ea1a3cd6cda4f",
    "metadata": {
        "id": "62e1f02722dd1061987ea1a3cd6cda4f",
        "url": "https://github.com/unslothai/unsloth/",
        "title": "GitHub - unslothai/unsloth: Finetune Llama 3.3, DeepSeek-R1, Gemma 3 & Reasoning LLMs 2x faster with 70% less memory! ü¶•",
        "properties": {
            "description": "Finetune Llama 3.3, DeepSeek-R1, Gemma 3 & Reasoning LLMs 2x faster with 70% less memory! ü¶• - unslothai/unsloth",
            "keywords": null,
            "author": null,
            "og:image": "https://opengraph.githubassets.com/80008b8b4efae382a46905b9837475acda850a5848c0c5afd000ebbe9f648acc/unslothai/unsloth",
            "og:image:alt": "Finetune Llama 3.3, DeepSeek-R1, Gemma 3 & Reasoning LLMs 2x faster with 70% less memory! ü¶• - unslothai/unsloth",
            "og:image:width": "1200",
            "og:image:height": "600",
            "og:site_name": "GitHub",
            "og:type": "object",
            "og:title": "GitHub - unslothai/unsloth: Finetune Llama 3.3, DeepSeek-R1, Gemma 3 & Reasoning LLMs 2x faster with 70% less memory! ü¶•",
            "og:url": "https://github.com/unslothai/unsloth",
            "og:description": "Finetune Llama 3.3, DeepSeek-R1, Gemma 3 & Reasoning LLMs 2x faster with 70% less memory! ü¶• - unslothai/unsloth",
            "twitter:image": "https://opengraph.githubassets.com/80008b8b4efae382a46905b9837475acda850a5848c0c5afd000ebbe9f648acc/unslothai/unsloth",
            "twitter:site": "@github",
            "twitter:card": "summary_large_image",
            "twitter:title": "GitHub - unslothai/unsloth: Finetune Llama 3.3, DeepSeek-R1, Gemma 3 & Reasoning LLMs 2x faster with 70% less memory! ü¶•",
            "twitter:description": "Finetune Llama 3.3, DeepSeek-R1, Gemma 3 & Reasoning LLMs 2x faster with 70% less memory! ü¶• - unslothai/unsloth"
        }
    },
    "parent_metadata": {
        "id": "efa45eaceaa2cf654ea0dde5c79cbdfd",
        "url": "https://www.notion.so/Training-Fine-tuning-LLMs-efa45eaceaa2cf654ea0dde5c79cbdfd",
        "title": "Training & Fine-tuning LLMs",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Skip to content](https://github.com/unslothai/unsloth/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=unslothai%2Funsloth) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/unslothai/unsloth/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/unslothai/unsloth/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/unslothai/unsloth/) to refresh your session. Dismiss alert\n{{ message }}\n[ unslothai ](https://github.com/unslothai) / **[unsloth](https://github.com/unslothai/unsloth) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Funslothai%2Funsloth) You must be signed in to change notification settings\n  * [ Fork 2.8k ](https://github.com/login?return_to=%2Funslothai%2Funsloth)\n  * [ Star  36.2k ](https://github.com/login?return_to=%2Funslothai%2Funsloth)\n\n\nFinetune Llama 3.3, DeepSeek-R1, Gemma 3 & Reasoning LLMs 2x faster with 70% less memory! ü¶• \n[unsloth.ai](https://unsloth.ai \"https://unsloth.ai\")\n### License\n[ Apache-2.0 license ](https://github.com/unslothai/unsloth/blob/main/LICENSE)\n[ 36.2k stars ](https://github.com/unslothai/unsloth/stargazers) [ 2.8k forks ](https://github.com/unslothai/unsloth/forks) [ Branches ](https://github.com/unslothai/unsloth/branches) [ Tags ](https://github.com/unslothai/unsloth/tags) [ Activity ](https://github.com/unslothai/unsloth/activity)\n[ Star  ](https://github.com/login?return_to=%2Funslothai%2Funsloth)\n[ Notifications ](https://github.com/login?return_to=%2Funslothai%2Funsloth) You must be signed in to change notification settings\n  * [ Code ](https://github.com/unslothai/unsloth)\n  * [ Issues 924 ](https://github.com/unslothai/unsloth/issues)\n  * [ Pull requests 65 ](https://github.com/unslothai/unsloth/pulls)\n  * [ Discussions ](https://github.com/unslothai/unsloth/discussions)\n  * [ Actions ](https://github.com/unslothai/unsloth/actions)\n  * [ Wiki ](https://github.com/unslothai/unsloth/wiki)\n  * [ Security ](https://github.com/unslothai/unsloth/security)\n  * [ Insights ](https://github.com/unslothai/unsloth/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/unslothai/unsloth)\n  * [ Issues  ](https://github.com/unslothai/unsloth/issues)\n  * [ Pull requests  ](https://github.com/unslothai/unsloth/pulls)\n  * [ Discussions  ](https://github.com/unslothai/unsloth/discussions)\n  * [ Actions  ](https://github.com/unslothai/unsloth/actions)\n  * [ Wiki  ](https://github.com/unslothai/unsloth/wiki)\n  * [ Security  ](https://github.com/unslothai/unsloth/security)\n  * [ Insights  ](https://github.com/unslothai/unsloth/pulse)\n\n\n# unslothai/unsloth\nmain\n[Branches](https://github.com/unslothai/unsloth/branches)[Tags](https://github.com/unslothai/unsloth/tags)\n[](https://github.com/unslothai/unsloth/branches)[](https://github.com/unslothai/unsloth/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[1,157 Commits](https://github.com/unslothai/unsloth/commits/main/)[](https://github.com/unslothai/unsloth/commits/main/)  \n[.github](https://github.com/unslothai/unsloth/tree/main/.github \".github\")| [.github](https://github.com/unslothai/unsloth/tree/main/.github \".github\")  \n[images](https://github.com/unslothai/unsloth/tree/main/images \"images\")| [images](https://github.com/unslothai/unsloth/tree/main/images \"images\")  \n[tests](https://github.com/unslothai/unsloth/tree/main/tests \"tests\")| [tests](https://github.com/unslothai/unsloth/tree/main/tests \"tests\")  \n[unsloth](https://github.com/unslothai/unsloth/tree/main/unsloth \"unsloth\")| [unsloth](https://github.com/unslothai/unsloth/tree/main/unsloth \"unsloth\")  \n[CONTRIBUTING.md](https://github.com/unslothai/unsloth/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/unslothai/unsloth/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")  \n[LICENSE](https://github.com/unslothai/unsloth/blob/main/LICENSE \"LICENSE\")| [LICENSE](https://github.com/unslothai/unsloth/blob/main/LICENSE \"LICENSE\")  \n[README.md](https://github.com/unslothai/unsloth/blob/main/README.md \"README.md\")| [README.md](https://github.com/unslothai/unsloth/blob/main/README.md \"README.md\")  \n[pyproject.toml](https://github.com/unslothai/unsloth/blob/main/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/unslothai/unsloth/blob/main/pyproject.toml \"pyproject.toml\")  \n[unsloth-cli.py](https://github.com/unslothai/unsloth/blob/main/unsloth-cli.py \"unsloth-cli.py\")| [unsloth-cli.py](https://github.com/unslothai/unsloth/blob/main/unsloth-cli.py \"unsloth-cli.py\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/unslothai/unsloth/)\n  * [Apache-2.0 license](https://github.com/unslothai/unsloth/)\n\n\n[ ![unsloth logo](https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png) ](https://unsloth.ai)\n[![](https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_\\(8B\\)-Alpaca.ipynb) [![](https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png)](https://discord.com/invite/unsloth) [![](https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png)](https://docs.unsloth.ai)\n### Finetune Llama 3.3, Gemma 3, Phi-4, Qwen 2.5 & Mistral 2x faster with 80% less VRAM!\n[](https://github.com/unslothai/unsloth/#finetune-llama-33-gemma-3-phi-4-qwen-25--mistral-2x-faster-with-80-less-vram)\n[![](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)\n## ‚ú® Finetune for Free\n[](https://github.com/unslothai/unsloth/#-finetune-for-free)\nNotebooks are beginner friendly. Read our [guide](https://docs.unsloth.ai/get-started/fine-tuning-guide). Add your dataset, click \"Run All\", and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.\nUnsloth supports | Free Notebooks | Performance | Memory use  \n---|---|---|---  \n**GRPO (R1 reasoning)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_\\(8B\\)-GRPO.ipynb) | 2x faster | 80% less  \n**Gemma 3 (4B)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_\\(4B\\).ipynb) | 1.6x faster | 60% less  \n**Llama 3.2 (3B)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_\\(1B_and_3B\\)-Conversational.ipynb) | 2x faster | 70% less  \n**Phi-4 (14B)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb) | 2x faster | 70% less  \n**Llama 3.2 Vision (11B)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_\\(11B\\)-Vision.ipynb) | 2x faster | 50% less  \n**Llama 3.1 (8B)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_\\(8B\\)-Alpaca.ipynb) | 2x faster | 70% less  \n**Qwen 2.5 (7B)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_\\(7B\\)-Alpaca.ipynb) | 2x faster | 70% less  \n**Mistral v0.3 (7B)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_\\(7B\\)-Conversational.ipynb) | 2.2x faster | 75% less  \n**Ollama** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_\\(8B\\)-Ollama.ipynb) | 1.9x faster | 60% less  \n**DPO Zephyr** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_\\(7B\\)-DPO.ipynb) | 1.9x faster | 50% less  \n  * See [all our notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) and [all our models](https://docs.unsloth.ai/get-started/all-our-models)\n  * **Kaggle Notebooks** for [Llama 3.2 Kaggle notebook](https://www.kaggle.com/danielhanchen/kaggle-llama-3-2-1b-3b-unsloth-notebook), [Llama 3.1 (8B)](https://www.kaggle.com/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook), [Phi-4 (14B)](https://www.kaggle.com/code/danielhanchen/phi-4-finetuning-unsloth-notebook), [Mistral (7B)](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n  * See detailed documentation for Unsloth [here](https://docs.unsloth.ai/).\n\n\n## ‚ö° Quickstart\n[](https://github.com/unslothai/unsloth/#-quickstart)\n  * **Install with pip (recommended)** for Linux devices:\n\n\n```\npip install unsloth\n\n```\n\nFor Windows install instructions, see [here](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation).\n## ü¶• Unsloth.ai News\n[](https://github.com/unslothai/unsloth/#-unslothai-news)\n  * üì£ NEW! [**EVERYTHING** is now supported](https://unsloth.ai/blog/gemma3#everything) incuding: FFT, ALL models (Mixtral, MOE, Cohere, Mamba) and all training algorithms (KTO, DoRA) etc. MultiGPU support coming very soon. To enable full-finetuning, set `full_finetuning = True` and for 8-bit finetuning, set `load_in_8bit = True`\n  * üì£ NEW! **Gemma 3** by Google: [Read Blog](https://unsloth.ai/blog/gemma3). We [uploaded GGUFs, 4-bit models](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa).\n  * üì£ NEW! Introducing Long-context [Reasoning (GRPO)](https://unsloth.ai/blog/grpo) in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!\n  * üì£ NEW! [DeepSeek-R1](https://unsloth.ai/blog/deepseek-r1) - the most powerful open reasoning models with Llama & Qwen distillations. Run or fine-tune them now [with our guide](https://unsloth.ai/blog/deepseek-r1). All model uploads: [here](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5).\n  * üì£ NEW! [Phi-4](https://unsloth.ai/blog/phi4) by Microsoft: We also [fixed bugs](https://unsloth.ai/blog/phi4) in Phi-4 and [uploaded GGUFs, 4-bit](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa).\n  * üì£ NEW! [Llama 3.3 (70B)](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f), Meta's latest model is supported.\n  * üì£ Introducing Unsloth [Dynamic 4-bit Quantization](https://unsloth.ai/blog/dynamic-4bit)! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using <10% more VRAM than BnB 4-bit. See our collection on [Hugging Face here.](https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7)\n  * üì£ [Vision models](https://unsloth.ai/blog/vision) now supported! [Llama 3.2 Vision (11B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_\\(11B\\)-Vision.ipynb), [Qwen 2.5 VL (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_\\(7B\\)-Vision.ipynb) and [Pixtral (12B) 2409](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_\\(12B\\)-Vision.ipynb)\n\nClick for more news\n  * üì£ NEW! We worked with Apple to add [Cut Cross Entropy](https://arxiv.org/abs/2411.09009). Unsloth now supports 89K context for Meta's Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.\n  * üì£ We found and helped fix a [gradient accumulation bug](https://unsloth.ai/blog/gradient)! Please update Unsloth and transformers.\n  * üì£ Try out [Chat interface](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb)!\n  * üì£ NEW! Qwen-2.5 including [Coder](https://unsloth.ai/blog/qwen-coder) models are now supported with bugfixes. 14b fits in a Colab GPU! [Qwen 2.5 conversational notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_\\(14B\\)-Conversational.ipynb)\n  * üì£ NEW! [Mistral Small 22b notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_\\(22B\\)-Alpaca.ipynb) finetuning fits in under 16GB of VRAM!\n  * üì£ NEW! `pip install unsloth` now works! Head over to [pypi](https://pypi.org/project/unsloth/) to check it out! This allows non git pull installs. Use `pip install unsloth[colab-new]` for non dependency installs.\n  * üì£ NEW! Continued Pretraining [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_\\(7B\\)-CPT.ipynb) for other languages like Korean!\n  * üì£ [2x faster inference](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_\\(8B\\)-Inference.ipynb) added for all our models\n  * üì£ We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support [4x longer context windows](https://unsloth.ai/blog/long-context)!\n\n\n## üîó Links and Resources\n[](https://github.com/unslothai/unsloth/#-links-and-resources)\nType | Links  \n---|---  \nüìö **Documentation & Wiki** | [Read Our Docs](https://docs.unsloth.ai)  \n[![](https://camo.githubusercontent.com/2d318d2de87051f80a781377bcf094af5e9a9648beb1f93cb3471cf4d5da0af0/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f362f36662f4c6f676f5f6f665f547769747465722e737667)](https://camo.githubusercontent.com/2d318d2de87051f80a781377bcf094af5e9a9648beb1f93cb3471cf4d5da0af0/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f362f36662f4c6f676f5f6f665f547769747465722e737667) **Twitter (aka X)** | [Follow us on X](https://twitter.com/unslothai)  \nüíæ **Installation** | [Pip install](https://docs.unsloth.ai/get-started/installing-+-updating)  \nüîÆ **Our Models** | [Unsloth Releases](https://docs.unsloth.ai/get-started/all-our-models)  \n‚úçÔ∏è **Blog** | [Read our Blogs](https://unsloth.ai/blog)  \n[![](https://camo.githubusercontent.com/366c9dc2ccd72d171a31e5af3cdfb0b42ce61a6f22a20d3e53609ad9ccb47655/68747470733a2f2f726564646974696e632e636f6d2f68732d66732f68756266732f526564646974253230496e632f4272616e642f5265646469745f4c6f676f2e706e67)](https://camo.githubusercontent.com/366c9dc2ccd72d171a31e5af3cdfb0b42ce61a6f22a20d3e53609ad9ccb47655/68747470733a2f2f726564646974696e632e636f6d2f68732d66732f68756266732f526564646974253230496e632f4272616e642f5265646469745f4c6f676f2e706e67) **Reddit** | [Join our Reddit page](https://reddit.com/r/unsloth)  \n## ‚≠ê Key Features\n[](https://github.com/unslothai/unsloth/#-key-features)\n  * Supports **full-finetuning** , pretraining, 4b-bit, 16-bit and **8-bit** training\n  * All kernels written in [OpenAI's Triton](https://openai.com/index/triton/) language. **Manual backprop engine**.\n  * **0% loss in accuracy** - no approximation methods - all exact.\n  * No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\n  * Works on **Linux** and **Windows**\n  * Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\n  * If you trained a model with ü¶•Unsloth, you can use this cool sticker! [![](https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png)](https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png)\n\n\n## üíæ Install Unsloth\n[](https://github.com/unslothai/unsloth/#-install-unsloth)\nYou can also see our documentation for more detailed installation and updating instructions [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n### Pip Installation\n[](https://github.com/unslothai/unsloth/#pip-installation)\n**Install with pip (recommended) for Linux devices:**\n```\npip install unsloth\n\n```\n\nSee [here](https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation) for advanced pip install instructions.\n### Windows Installation\n[](https://github.com/unslothai/unsloth/#windows-installation)\nWarning\nPython 3.13 does not support Unsloth. Use 3.12, 3.11 or 3.10\n  1. **Install NVIDIA Video Driver:** You should install the latest version of your GPUs driver. Download drivers here: [NVIDIA GPU Drive](https://www.nvidia.com/Download/index.aspx).\n  2. **Install Visual Studio C++:** You will need Visual Studio, with C++ installed. By default, C++ is not installed with [Visual Studio](https://visualstudio.microsoft.com/vs/community/), so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n  3. **Install CUDA Toolkit:** Follow the instructions to install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive).\n  4. **Install PyTorch:** You will need the correct version of PyTorch that is compatibile with your CUDA drivers, so make sure to select them carefully. [Install PyTorch](https://pytorch.org/get-started/locally/).\n  5. **Install Unsloth:**\n\n\n```\npip install unsloth\n```\n\n#### Notes\n[](https://github.com/unslothai/unsloth/#notes)\nTo run Unsloth directly on Windows:\n  * Install Triton from this Windows fork and follow the instructions [here](https://github.com/woct0rdho/triton-windows) (be aware that the Windows fork requires PyTorch >= 2.4 and CUDA 12)\n  * In the SFTTrainer, set `dataset_num_proc=1` to avoid a crashing issue:\n\n\n```\ntrainer = SFTTrainer(\n  dataset_num_proc=1,\n  ...\n)\n```\n\n#### Advanced/Troubleshooting\n[](https://github.com/unslothai/unsloth/#advancedtroubleshooting)\nFor **advanced installation instructions** or if you see weird errors during installations:\n  1. Install `torch` and `triton`. Go to <https://pytorch.org> to install it. For example `pip install torch torchvision torchaudio triton`\n  2. Confirm if CUDA is installated correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.\n  3. Install `xformers` manually. You can try installing `vllm` and seeing if `vllm` succeeds. Check if `xformers` succeeded with `python -m xformers.info` Go to <https://github.com/facebookresearch/xformers>. Another option is to install `flash-attn` for Ampere GPUs.\n  4. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful.\n  5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`\n\n\n### Conda Installation (Optional)\n[](https://github.com/unslothai/unsloth/#conda-installation-optional)\n`‚ö†Ô∏èOnly use Conda if you have it. If not, use Pip`. Select either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.\n```\nconda create --name unsloth_env \\\n  python=3.11 \\\n  pytorch-cuda=12.1 \\\n  pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\n  -y\nconda activate unsloth_env\npip install unsloth\n```\n\nIf you're looking to install Conda in a Linux environment, [read here](https://docs.anaconda.com/miniconda/), or run the below üîΩ\n```\nmkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\n```\n\n### Advanced Pip Installation\n[](https://github.com/unslothai/unsloth/#advanced-pip-installation)\n`‚ö†Ô∏èDo **NOT** use this if you have Conda.` Pip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5` and CUDA versions.\nFor other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.\nFor example, if you have `torch 2.4` and `CUDA 12.1`, use:\n```\npip install --upgrade pip\npip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n```\n\nAnother example, if you have `torch 2.5` and `CUDA 12.4`, use:\n```\npip install --upgrade pip\npip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n```\n\nAnd other examples:\n```\npip install \"unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n```\n\nOr, run the below in a terminal to get the **optimal** pip installation command:\n```\nwget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n```\n\nOr, run the below manually in a Python REPL:\n```\ntry: import torch\nexcept: raise ImportError('Install torch via `pip install torch`')\nfrom packaging.version import Version as V\nv = V(torch.__version__)\ncuda = str(torch.version.cuda)\nis_ampere = torch.cuda.get_device_capability()[0] >= 8\nif cuda != \"12.1\" and cuda != \"11.8\" and cuda != \"12.4\": raise RuntimeError(f\"CUDA = {cuda} not supported!\")\nif  v <= V('2.1.0'): raise RuntimeError(f\"Torch = {v} too old!\")\nelif v <= V('2.1.1'): x = 'cu{}{}-torch211'\nelif v <= V('2.1.2'): x = 'cu{}{}-torch212'\nelif v < V('2.3.0'): x = 'cu{}{}-torch220'\nelif v < V('2.4.0'): x = 'cu{}{}-torch230'\nelif v < V('2.5.0'): x = 'cu{}{}-torch240'\nelif v < V('2.6.0'): x = 'cu{}{}-torch250'\nelse: raise RuntimeError(f\"Torch = {v} too new!\")\nx = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\")\nprint(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')\n```\n\n## üìú Documentation\n[](https://github.com/unslothai/unsloth/#-documentation)\n  * Go to our official [Documentation](https://docs.unsloth.ai) for saving to GGUF, checkpointing, evaluation and more!\n  * We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\n  * We're in ü§óHugging Face's official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\n  * If you want to download models from the ModelScope community, please use an environment variable: `UNSLOTH_USE_MODELSCOPE=1`, and install the modelscope library by: `pip install modelscope -U`.\n\n\n> unsloth_cli.py also supports `UNSLOTH_USE_MODELSCOPE=1` to download models and datasets. please remember to use the model and dataset id in the ModelScope community.\n```\nfrom unsloth import FastLanguageModel \nimport torch\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\nmax_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!\n# Get LAION dataset\nurl = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\ndataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n  \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",   # Llama-3.1 2x faster\n  \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n  \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n  \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",  # 4bit for 405b!\n  \"unsloth/Mistral-Small-Instruct-2409\",   # Mistral 22b 2x faster!\n  \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n  \"unsloth/Phi-3.5-mini-instruct\",      # Phi-3.5 2x faster!\n  \"unsloth/Phi-3-medium-4k-instruct\",\n  \"unsloth/gemma-2-9b-bnb-4bit\",\n  \"unsloth/gemma-2-27b-bnb-4bit\",      # Gemma 2x faster!\n  \"unsloth/Llama-3.2-1B-bnb-4bit\",      # NEW! Llama 3.2 models\n  \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n  \"unsloth/Llama-3.2-3B-bnb-4bit\",\n  \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n  \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n] # More models at https://huggingface.co/unsloth\nmodel, tokenizer = FastModel.from_pretrained(\n  model_name = \"unsloth/gemma-3-4B-it\",\n  max_seq_length = 2048, # Choose any for long context!\n  load_in_4bit = True, # 4 bit quantization to reduce memory\n  load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n  full_finetuning = False, # [NEW!] We have full finetuning now!\n  # token = \"hf_...\", # use one if using gated models\n)\n# Do model patching and add fast LoRA weights\nmodel = FastLanguageModel.get_peft_model(\n  model,\n  r = 16,\n  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n           \"gate_proj\", \"up_proj\", \"down_proj\",],\n  lora_alpha = 16,\n  lora_dropout = 0, # Supports any, but = 0 is optimized\n  bias = \"none\",  # Supports any, but = \"none\" is optimized\n  # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n  use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n  random_state = 3407,\n  max_seq_length = max_seq_length,\n  use_rslora = False, # We support rank stabilized LoRA\n  loftq_config = None, # And LoftQ\n)\ntrainer = SFTTrainer(\n  model = model,\n  train_dataset = dataset,\n  tokenizer = tokenizer,\n  args = SFTConfig(\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    warmup_steps = 10,\n    max_steps = 60,\n    logging_steps = 1,\n    output_dir = \"outputs\",\n    optim = \"adamw_8bit\",\n    seed = 3407,\n  ),\n)\ntrainer.train()\n# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like\n# (1) Saving to GGUF / merging to 16bit for vLLM\n# (2) Continued training from a saved LoRA adapter\n# (3) Adding an evaluation loop / OOMs\n# (4) Customized chat templates\n```\n\n## üí° Reinforcement Learning\n[](https://github.com/unslothai/unsloth/#-reinforcement-learning)\nRL including DPO, GRPO, PPO, Reward Modelling, Online DPO all work with Unsloth. We're in ü§óHugging Face's official docs! We're on the [GRPO docs](https://huggingface.co/learn/nlp-course/en/chapter12/6) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)! List of RL notebooks:\n  * ORPO notebook: [Link](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_\\(8B\\)-ORPO.ipynb)\n  * DPO Zephyr notebook: [Link](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_\\(7B\\)-DPO.ipynb)\n  * KTO notebook: [Link](https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing)\n  * SimPO notebook: [Link](https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing)\n\nClick for DPO code\n```\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Optional set GPU device ID\nfrom unsloth import FastLanguageModel\nimport torch\nfrom trl import DPOTrainer, DPOConfig\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n  model_name = \"unsloth/zephyr-sft-bnb-4bit\",\n  max_seq_length = max_seq_length,\n  load_in_4bit = True,\n)\n# Do model patching and add fast LoRA weights\nmodel = FastLanguageModel.get_peft_model(\n  model,\n  r = 64,\n  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n           \"gate_proj\", \"up_proj\", \"down_proj\",],\n  lora_alpha = 64,\n  lora_dropout = 0, # Supports any, but = 0 is optimized\n  bias = \"none\",  # Supports any, but = \"none\" is optimized\n  # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n  use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n  random_state = 3407,\n  max_seq_length = max_seq_length,\n)\ndpo_trainer = DPOTrainer(\n  model = model,\n  ref_model = None,\n  train_dataset = YOUR_DATASET_HERE,\n  # eval_dataset = YOUR_DATASET_HERE,\n  tokenizer = tokenizer,\n  args = DPOConfig(\n    per_device_train_batch_size = 4,\n    gradient_accumulation_steps = 8,\n    warmup_ratio = 0.1,\n    num_train_epochs = 3,\n    logging_steps = 1,\n    optim = \"adamw_8bit\",\n    seed = 42,\n    output_dir = \"outputs\",\n    max_length = 1024,\n    max_prompt_length = 512,\n    beta = 0.1,\n  ),\n)\ndpo_trainer.train()\n```\n\n## ü•á Performance Benchmarking\n[](https://github.com/unslothai/unsloth/#-performance-benchmarking)\n  * For our most detailed benchmarks, read our [Llama 3.3 Blog](https://unsloth.ai/blog/llama3-3).\n  * Benchmarking of Unsloth was also conducted by [ü§óHugging Face](https://huggingface.co/blog/unsloth-trl).\n\n\nWe tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):\nModel | VRAM | ü¶• Unsloth speed | ü¶• VRAM reduction | ü¶• Longer context | üòä Hugging Face + FA2  \n---|---|---|---|---|---  \nLlama 3.3 (70B) | 80GB | 2x | >75% | 13x longer | 1x  \nLlama 3.1 (8B) | 80GB | 2x | >70% | 12x longer | 1x  \n### Context length benchmarks\n[](https://github.com/unslothai/unsloth/#context-length-benchmarks)\n#### Llama 3.1 (8B) max. context length\n[](https://github.com/unslothai/unsloth/#llama-31-8b-max-context-length)\nWe tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.\nGPU VRAM | ü¶•Unsloth context length | Hugging Face + FA2  \n---|---|---  \n8 GB | 2,972 | OOM  \n12 GB | 21,848 | 932  \n16 GB | 40,724 | 2,551  \n24 GB | 78,475 | 5,789  \n40 GB | 153,977 | 12,264  \n48 GB | 191,728 | 15,502  \n80 GB | 342,733 | 28,454  \n#### Llama 3.3 (70B) max. context length\n[](https://github.com/unslothai/unsloth/#llama-33-70b-max-context-length)\nWe tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.\nGPU VRAM | ü¶•Unsloth context length | Hugging Face + FA2  \n---|---|---  \n48 GB | 12,106 | OOM  \n80 GB | 89,389 | 6,916  \n[![](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)](https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67)\n### Citation\n[](https://github.com/unslothai/unsloth/#citation)\nYou can cite the Unsloth repo as follows:\n```\n@software{unsloth,\n author = {Daniel Han, Michael Han and Unsloth team},\n title = {Unsloth},\n url = {http://github.com/unslothai/unsloth},\n year = {2023}\n}\n```\n\n### Thank You to\n[](https://github.com/unslothai/unsloth/#thank-you-to)\n  * Hugging Face's [TRL library](https://github.com/huggingface/trl) which serves as the basis foundation for Unsloth\n  * [Erik](https://github.com/erikwijmans) for his help adding [Apple's ML Cross Entropy](https://github.com/apple/ml-cross-entropy) in Unsloth\n  * [HuyNguyen-hust](https://github.com/HuyNguyen-hust) for making [RoPE Embeddings 28% faster](https://github.com/unslothai/unsloth/pull/238)\n  * [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\n  * [152334H](https://github.com/152334H) for experimental DPO support\n\n\n## About\nFinetune Llama 3.3, DeepSeek-R1, Gemma 3 & Reasoning LLMs 2x faster with 70% less memory! ü¶• \n[unsloth.ai](https://unsloth.ai \"https://unsloth.ai\")\n### Topics\n[ llama ](https://github.com/topics/llama \"Topic: llama\") [ lora ](https://github.com/topics/lora \"Topic: lora\") [ gemma ](https://github.com/topics/gemma \"Topic: gemma\") [ mistral ](https://github.com/topics/mistral \"Topic: mistral\") [ fine-tuning ](https://github.com/topics/fine-tuning \"Topic: fine-tuning\") [ finetuning ](https://github.com/topics/finetuning \"Topic: finetuning\") [ llm ](https://github.com/topics/llm \"Topic: llm\") [ llms ](https://github.com/topics/llms \"Topic: llms\") [ qlora ](https://github.com/topics/qlora \"Topic: qlora\") [ deepseek ](https://github.com/topics/deepseek \"Topic: deepseek\") [ unsloth ](https://github.com/topics/unsloth \"Topic: unsloth\") [ llama3 ](https://github.com/topics/llama3 \"Topic: llama3\") [ phi3 ](https://github.com/topics/phi3 \"Topic: phi3\") [ gemma2 ](https://github.com/topics/gemma2 \"Topic: gemma2\") [ deepseek-r1 ](https://github.com/topics/deepseek-r1 \"Topic: deepseek-r1\") [ gemma3 ](https://github.com/topics/gemma3 \"Topic: gemma3\")\n### Resources\n[ Readme ](https://github.com/unslothai/unsloth/#readme-ov-file)\n### License\n[ Apache-2.0 license ](https://github.com/unslothai/unsloth/#Apache-2.0-1-ov-file)\n[ Activity](https://github.com/unslothai/unsloth/activity)\n[ Custom properties](https://github.com/unslothai/unsloth/custom-properties)\n### Stars\n[ **36.2k** stars](https://github.com/unslothai/unsloth/stargazers)\n### Watchers\n[ **213** watching](https://github.com/unslothai/unsloth/watchers)\n### Forks\n[ **2.8k** forks](https://github.com/unslothai/unsloth/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth&report=unslothai+%28user%29)\n##  [Releases 12](https://github.com/unslothai/unsloth/releases)\n[ Gemma 3 + FFT Support Latest  Mar 14, 2025 ](https://github.com/unslothai/unsloth/releases/tag/2025-03)\n[+ 11 releases](https://github.com/unslothai/unsloth/releases)\n## Sponsor this project\n  * ![ko_fi](https://github.githubassets.com/assets/ko_fi-53a60c17e75c.svg) [ko-fi.com/**unsloth**](https://ko-fi.com/unsloth)\n\n\n##  [Packages 0](https://github.com/orgs/unslothai/packages?repo_name=unsloth)\nNo packages published \n##  [Contributors 71](https://github.com/unslothai/unsloth/graphs/contributors)\n  * [ ![@danielhanchen](https://avatars.githubusercontent.com/u/23090290?s=64&v=4) ](https://github.com/danielhanchen)\n  * [ ![@shimmyshimmer](https://avatars.githubusercontent.com/u/107991372?s=64&v=4) ](https://github.com/shimmyshimmer)\n  * [ ![@Erland366](https://avatars.githubusercontent.com/u/68678137?s=64&v=4) ](https://github.com/Erland366)\n  * [ ![@Datta0](https://avatars.githubusercontent.com/u/39181234?s=64&v=4) ](https://github.com/Datta0)\n  * [ ![@NinoRisteski](https://avatars.githubusercontent.com/u/95188570?s=64&v=4) ](https://github.com/NinoRisteski)\n  * [ ![@xyangk](https://avatars.githubusercontent.com/u/9495054?s=64&v=4) ](https://github.com/xyangk)\n  * [ ![@KareemMusleh](https://avatars.githubusercontent.com/u/81531392?s=64&v=4) ](https://github.com/KareemMusleh)\n  * [ ![@sebdg](https://avatars.githubusercontent.com/u/1187529?s=64&v=4) ](https://github.com/sebdg)\n  * [ ![@bet0x](https://avatars.githubusercontent.com/u/778862?s=64&v=4) ](https://github.com/bet0x)\n  * [ ![@neph1](https://avatars.githubusercontent.com/u/7988802?s=64&v=4) ](https://github.com/neph1)\n  * [ ![@t-vi](https://avatars.githubusercontent.com/u/20787943?s=64&v=4) ](https://github.com/t-vi)\n  * [ ![@Oseltamivir](https://avatars.githubusercontent.com/u/58582368?s=64&v=4) ](https://github.com/Oseltamivir)\n  * [ ![@chrehall68](https://avatars.githubusercontent.com/u/60240707?s=64&v=4) ](https://github.com/chrehall68)\n  * [ ![@mahiatlinux](https://avatars.githubusercontent.com/u/110882203?s=64&v=4) ](https://github.com/mahiatlinux)\n\n\n[+ 57 contributors](https://github.com/unslothai/unsloth/graphs/contributors)\n## Languages\n  * [ Python 100.0% ](https://github.com/unslothai/unsloth/search?l=python)\n\n\n## Footer\n[ ](https://github.com \"GitHub\") ¬© 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can‚Äôt perform that action at this time. \n",
    "content_quality_score": null,
    "summary": null,
    "child_urls": [
        "https://github.com/unslothai/unsloth/#start-of-content",
        "https://github.com/",
        "https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth%2F",
        "https://github.com/features/copilot",
        "https://github.com/features/security",
        "https://github.com/features/actions",
        "https://github.com/features/codespaces",
        "https://github.com/features/issues",
        "https://github.com/features/code-review",
        "https://github.com/features/discussions",
        "https://github.com/features/code-search",
        "https://github.com/features",
        "https://docs.github.com",
        "https://skills.github.com",
        "https://github.com/enterprise",
        "https://github.com/team",
        "https://github.com/enterprise/startups",
        "https://github.com/solutions/industry/nonprofits",
        "https://github.com/solutions/use-case/devsecops",
        "https://github.com/solutions/use-case/devops",
        "https://github.com/solutions/use-case/ci-cd",
        "https://github.com/solutions/use-case",
        "https://github.com/solutions/industry/healthcare",
        "https://github.com/solutions/industry/financial-services",
        "https://github.com/solutions/industry/manufacturing",
        "https://github.com/solutions/industry/government",
        "https://github.com/solutions/industry",
        "https://github.com/solutions",
        "https://github.com/resources/articles/ai",
        "https://github.com/resources/articles/devops",
        "https://github.com/resources/articles/security",
        "https://github.com/resources/articles/software-development",
        "https://github.com/resources/articles",
        "https://resources.github.com/learn/pathways",
        "https://resources.github.com",
        "https://github.com/resources/whitepapers",
        "https://github.com/customer-stories",
        "https://partner.github.com",
        "https://github.com/solutions/executive-insights",
        "https://github.com/sponsors",
        "https://github.com/readme",
        "https://github.com/topics",
        "https://github.com/trending",
        "https://github.com/collections",
        "https://github.com/enterprise/advanced-security",
        "https://github.com/features/copilot/copilot-business",
        "https://github.com/premium-support",
        "https://github.com/pricing",
        "https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax",
        "https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=unslothai%2Funsloth",
        "https://github.com/unslothai",
        "https://github.com/unslothai/unsloth",
        "https://github.com/login?return_to=%2Funslothai%2Funsloth",
        "https://github.com/unslothai/unsloth/blob/main/LICENSE",
        "https://github.com/unslothai/unsloth/stargazers",
        "https://github.com/unslothai/unsloth/forks",
        "https://github.com/unslothai/unsloth/branches",
        "https://github.com/unslothai/unsloth/tags",
        "https://github.com/unslothai/unsloth/activity",
        "https://github.com/unslothai/unsloth/issues",
        "https://github.com/unslothai/unsloth/pulls",
        "https://github.com/unslothai/unsloth/discussions",
        "https://github.com/unslothai/unsloth/actions",
        "https://github.com/unslothai/unsloth/wiki",
        "https://github.com/unslothai/unsloth/security",
        "https://github.com/unslothai/unsloth/pulse",
        "https://github.com/unslothai/unsloth/commits/main/",
        "https://github.com/unslothai/unsloth/tree/main/.github",
        "https://github.com/unslothai/unsloth/tree/main/images",
        "https://github.com/unslothai/unsloth/tree/main/tests",
        "https://github.com/unslothai/unsloth/tree/main/unsloth",
        "https://github.com/unslothai/unsloth/blob/main/CONTRIBUTING.md",
        "https://github.com/unslothai/unsloth/blob/main/README.md",
        "https://github.com/unslothai/unsloth/blob/main/pyproject.toml",
        "https://github.com/unslothai/unsloth/blob/main/unsloth-cli.py",
        "https://github.com/unslothai/unsloth/",
        "https://github.com/unslothai/unsloth/#finetune-llama-33-gemma-3-phi-4-qwen-25--mistral-2x-faster-with-80-less-vram",
        "https://github.com/unslothai/unsloth/#-finetune-for-free",
        "https://github.com/unslothai/unsloth/#-quickstart",
        "https://github.com/unslothai/unsloth/#-unslothai-news",
        "https://github.com/unslothai/unsloth/#-links-and-resources",
        "https://github.com/unslothai/unsloth/#-key-features",
        "https://github.com/TimDettmers/bitsandbytes",
        "https://github.com/unslothai/unsloth/#-install-unsloth",
        "https://github.com/unslothai/unsloth/#pip-installation",
        "https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation",
        "https://github.com/unslothai/unsloth/#windows-installation",
        "https://github.com/unslothai/unsloth/#notes",
        "https://github.com/woct0rdho/triton-windows",
        "https://github.com/unslothai/unsloth/#advancedtroubleshooting",
        "https://github.com/facebookresearch/xformers",
        "https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix",
        "https://github.com/unslothai/unsloth/#conda-installation-optional",
        "https://github.com/unslothai/unsloth/#advanced-pip-installation",
        "https://github.com/unslothai/unsloth/#-documentation",
        "https://github.com/unslothai/unsloth/#-reinforcement-learning",
        "https://github.com/unslothai/unsloth/#-performance-benchmarking",
        "https://github.com/unslothai/unsloth/#context-length-benchmarks",
        "https://github.com/unslothai/unsloth/#llama-31-8b-max-context-length",
        "https://github.com/unslothai/unsloth/#llama-33-70b-max-context-length",
        "https://github.com/unslothai/unsloth/#citation",
        "https://github.com/unslothai/unsloth/#thank-you-to",
        "https://github.com/huggingface/trl",
        "https://github.com/erikwijmans",
        "https://github.com/apple/ml-cross-entropy",
        "https://github.com/HuyNguyen-hust",
        "https://github.com/unslothai/unsloth/pull/238",
        "https://github.com/RandomInternetPreson",
        "https://github.com/152334H",
        "https://github.com/topics/llama",
        "https://github.com/topics/lora",
        "https://github.com/topics/gemma",
        "https://github.com/topics/mistral",
        "https://github.com/topics/fine-tuning",
        "https://github.com/topics/finetuning",
        "https://github.com/topics/llm",
        "https://github.com/topics/llms",
        "https://github.com/topics/qlora",
        "https://github.com/topics/deepseek",
        "https://github.com/topics/unsloth",
        "https://github.com/topics/llama3",
        "https://github.com/topics/phi3",
        "https://github.com/topics/gemma2",
        "https://github.com/topics/deepseek-r1",
        "https://github.com/topics/gemma3",
        "https://github.com/unslothai/unsloth/#readme-ov-file",
        "https://github.com/unslothai/unsloth/#Apache-2.0-1-ov-file",
        "https://github.com/unslothai/unsloth/custom-properties",
        "https://github.com/unslothai/unsloth/watchers",
        "https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth&report=unslothai+%28user%29",
        "https://github.com/unslothai/unsloth/releases",
        "https://github.com/unslothai/unsloth/releases/tag/2025-03",
        "https://github.com/orgs/unslothai/packages?repo_name=unsloth",
        "https://github.com/unslothai/unsloth/graphs/contributors",
        "https://github.com/danielhanchen",
        "https://github.com/shimmyshimmer",
        "https://github.com/Erland366",
        "https://github.com/Datta0",
        "https://github.com/NinoRisteski",
        "https://github.com/xyangk",
        "https://github.com/KareemMusleh",
        "https://github.com/sebdg",
        "https://github.com/bet0x",
        "https://github.com/neph1",
        "https://github.com/t-vi",
        "https://github.com/Oseltamivir",
        "https://github.com/chrehall68",
        "https://github.com/mahiatlinux",
        "https://github.com/unslothai/unsloth/search?l=python",
        "https://github.com",
        "https://docs.github.com/site-policy/github-terms/github-terms-of-service",
        "https://docs.github.com/site-policy/privacy-policies/github-privacy-statement",
        "https://github.com/security",
        "https://docs.github.com/",
        "https://support.github.com?tags=dotcom-footer",
        "https://github.blog",
        "https://unsloth.ai",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb",
        "https://discord.com/invite/unsloth",
        "https://docs.unsloth.ai",
        "https://camo.githubusercontent.com/e93226ff5ba5be911d9fa41864c58db621a09ba17e14fe8f98f63e124431c927/68747470733a2f2f692e6962622e636f2f734a37526847472f696d6167652d34312e706e67",
        "https://docs.unsloth.ai/get-started/fine-tuning-guide",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb",
        "https://docs.unsloth.ai/get-started/unsloth-notebooks",
        "https://docs.unsloth.ai/get-started/all-our-models",
        "https://www.kaggle.com/danielhanchen/kaggle-llama-3-2-1b-3b-unsloth-notebook",
        "https://www.kaggle.com/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook",
        "https://www.kaggle.com/code/danielhanchen/phi-4-finetuning-unsloth-notebook",
        "https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook",
        "https://docs.unsloth.ai/",
        "https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation",
        "https://unsloth.ai/blog/gemma3#everything",
        "https://unsloth.ai/blog/gemma3",
        "https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa",
        "https://unsloth.ai/blog/grpo",
        "https://unsloth.ai/blog/deepseek-r1",
        "https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5",
        "https://unsloth.ai/blog/phi4",
        "https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f",
        "https://unsloth.ai/blog/dynamic-4bit",
        "https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7",
        "https://unsloth.ai/blog/vision",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb",
        "https://arxiv.org/abs/2411.09009",
        "https://unsloth.ai/blog/gradient",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb",
        "https://unsloth.ai/blog/qwen-coder",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(14B)-Conversational.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb",
        "https://pypi.org/project/unsloth/",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-CPT.ipynb",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Inference.ipynb",
        "https://unsloth.ai/blog/long-context",
        "https://camo.githubusercontent.com/2d318d2de87051f80a781377bcf094af5e9a9648beb1f93cb3471cf4d5da0af0/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f362f36662f4c6f676f5f6f665f547769747465722e737667",
        "https://twitter.com/unslothai",
        "https://docs.unsloth.ai/get-started/installing-+-updating",
        "https://unsloth.ai/blog",
        "https://camo.githubusercontent.com/366c9dc2ccd72d171a31e5af3cdfb0b42ce61a6f22a20d3e53609ad9ccb47655/68747470733a2f2f726564646974696e632e636f6d2f68732d66732f68756266732f526564646974253230496e632f4272616e642f5265646469745f4c6f676f2e706e67",
        "https://reddit.com/r/unsloth",
        "https://openai.com/index/triton/",
        "https://developer.nvidia.com/cuda-gpus",
        "https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png",
        "https://www.nvidia.com/Download/index.aspx",
        "https://visualstudio.microsoft.com/vs/community/",
        "https://developer.nvidia.com/cuda-toolkit-archive",
        "https://pytorch.org/get-started/locally/",
        "https://pytorch.org",
        "https://docs.anaconda.com/miniconda/",
        "https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth",
        "https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth",
        "https://huggingface.co/learn/nlp-course/en/chapter12/6",
        "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb",
        "https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing",
        "https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing",
        "https://unsloth.ai/blog/llama3-3",
        "https://huggingface.co/blog/unsloth-trl",
        "https://ko-fi.com/unsloth",
        "https://www.githubstatus.com/"
    ]
}