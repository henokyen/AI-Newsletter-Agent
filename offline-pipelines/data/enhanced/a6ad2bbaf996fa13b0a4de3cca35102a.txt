[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4e7df1a62f83&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=post_page---top_nav_layout_nav-----------------------------------------)
Sign up
[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fteetracker.medium.com%2Flangchain-llama-index-rag-with-multi-query-retrieval-4e7df1a62f83&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)
[](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)
[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)
[](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)
Sign up
[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fteetracker.medium.com%2Flangchain-llama-index-rag-with-multi-query-retrieval-4e7df1a62f83&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)
![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)
[Home](https://medium.com/?source=post_page--------------------------------------------)
Following
Library
[Your lists](https://medium.com/me/lists?source=post_page--------------------------------------------)[Saved lists](https://medium.com/me/lists/saved?source=post_page--------------------------------------------)[Highlights](https://medium.com/me/list/highlights?source=post_page--------------------------------------------)[Reading history](https://medium.com/me/lists/reading-history?source=post_page--------------------------------------------)
[Stories](https://medium.com/me/stories/drafts?source=post_page--------------------------------------------)[Stats](https://medium.com/me/stats?source=post_page--------------------------------------------)
# LangChain / Llama-Index: RAG with Multi-Query Retrieval
[![TeeTracker](https://miro.medium.com/v2/resize:fill:88:88/1*yYq5U3pKuZKMAs0UTQc_HA.png)](https://teetracker.medium.com/?source=post_page---byline--4e7df1a62f83---------------------------------------)
[TeeTracker](https://teetracker.medium.com/?source=post_page---byline--4e7df1a62f83---------------------------------------)
·
[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F984171777958&operation=register&redirect=https%3A%2F%2Fteetracker.medium.com%2Flangchain-llama-index-rag-with-multi-query-retrieval-4e7df1a62f83&user=TeeTracker&userId=984171777958&source=post_page-984171777958--byline--4e7df1a62f83---------------------post_header------------------)
4 min read
·
Feb 14, 2024
[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F4e7df1a62f83&operation=register&redirect=https%3A%2F%2Fteetracker.medium.com%2Flangchain-llama-index-rag-with-multi-query-retrieval-4e7df1a62f83&user=TeeTracker&userId=984171777958&source=---header_actions--4e7df1a62f83---------------------clap_footer------------------)
--
[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e7df1a62f83&operation=register&redirect=https%3A%2F%2Fteetracker.medium.com%2Flangchain-llama-index-rag-with-multi-query-retrieval-4e7df1a62f83&source=---header_actions--4e7df1a62f83---------------------bookmark_footer------------------)
Listen
Share
Enhance query context with intermediate queries during RAG to improve information retrieval for the original query.
# Query Expansion
Query expansion works by extending the original query with additional terms or phrases that are related or synonymous.
**Multi-Query Retrieva** l is a type of query expansion.
# Mechanisms
When I input a query request, we first use a large language model to generate a similar query. I will use a similar query to retrieve relevant documents (nodes in the llama-index). This retrieved information will be used to query the context of the original query.
![](https://miro.medium.com/v2/resize:fit:700/1*wa27F8CbKqYtaRRGyRKy0Q.png)
## 2 times LLM interactions
To generate queries, we need to make additional parallel requests to LLM. This means adding a total of 2 requests, with the option of using gpt3 as the first request and gpt4 or better for the final one.
# Implementation method
## LangChain
```
loader = UnstructuredPDFLoader(FILE_NAME)docs = loader.load()text_splitter = SentenceTransformersTokenTextSplitter()texts = text_splitter.split_documents(docs)emb = OpenAIEmbeddings(openai_api_key=openai.api_key)vec_db = Chroma.from_documents(documents=texts, embedding=emb)lc_model = ChatOpenAI(openai_api_key=openai.api_key, temperature=1.5)base_retriever = vec_db.as_retriever(k=K)final_retriever = MultiQueryRetriever.from_llm(base_retriever, lc_model)tmpl = """You are an assistant to answer a question from user with a context.Context:{context}Question:{question}The response should be presented as a list of key points, after creating the title of the content,  formatted in HTML with appropriate markup for clarity and organization."""prompt = ChatPromptTemplate.from_template(tmpl)chain = {"question": RunnablePassthrough(), "context": final_retriever} \    | prompt \    | lc_model \    | StrOutputParser() \result = chain.invoke("Waht is the doc talking about?") 
```

**MultiQueryRetriever** provides ready-made classes to accomplish this task. The key point is to provide a**base retriever**. All “generated queries” will be automatically implemented, by default 3 of them. Their retrieval process will also be securely encapsulated.
As you can see in the [colab](https://colab.research.google.com/drive/1HKv85boODXbU944s3tanL-nBRwin7JAq?usp=sharing), You will observe the intermediate generated queries such as:
```
INFO:langchain.retrievers.multi_query:Generated queries: ['1. What is the main topic discussed in the document?', '2. Could you provide a brief summary of the subject matter of the document?', '3. What does the document primarily cover and discuss?']
```

Those queries will be used later to retrieve the relevant from the indices.
## Llama-Index
The implementation of Llama-Index is quite tricky because we have to manually generate “generated queries” and their retrieval process is manually implemented. Since there are multiple queries, we will use the necessary coroutine mechanism.
```
vector_index: BaseIndex = VectorStoreIndex.from_documents(  docs,  service_context=service_context,   show_progress=True,)base_retriever = vector_index.as_retriever(similarity_top_k=K)class MultiQueriesRetriever(BaseRetriever):  def __init__(self, base_retriever: BaseRetriever, model:OpenAI):    self.template = PromptTemplate("""You are an AI language model assistant. Your task is to generate Five  different versions of the given user question to retrieve relevant documents from a vector  database. By generating multiple perspectives on the user question, your goal is to help  the user overcome some of the limitations of the distance-based similarity search.  Provide these alternative questions seperated by newlines.  Original question: {question}""")    self._retrievers = [base_retriever]    self.base_retriever = base_retriever    self.model = model    def gen_queries(self, query) -> List[str]:    gen_queries_model = OpenAI(model="gpt-3-turbo", temperature=1.5)    prompt = self.template.format(question=query)    res = self.model.complete(prompt)    return res.text.split("\n")  async def run_gen_queries(self,generated_queries: List[str]) -> List[NodeWithScore]:    tasks = list(map(lambda q: self.base_retriever.aretrieve(q), generated_queries))     res = await tqdm.gather(*tasks)    return res[0]  def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:    return list()  async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:    query = query_bundle.query_str    generated_queries = self.gen_queries(query)    query_res = await self.run_gen_queries(generated_queries)    return query_res  mr = MultiQueriesRetriever(base_retriever, li_model)final_res = await RetrieverQueryEngine(mr).aquery(query_text)
```

The key point is that we inherit BaseRetriever, which means we combine it with a base retriever to query relevant information based on generated queries. _aretrieve must be overridden because generated queries are implemented through coroutine. No further details are explained here.
Furthermore, you have the opportunity to view the queries that were generated at an intermediate stage, if you print.
## SubQuestionQueryEngine
Llama-Index provides a class called **SubQuestionQueryEngine** that basically meets our needs, the difference is, it’s break-down, not generate a “similar” query. According to the documentation, you can use the following code:
```
query_engine_tools = [  QueryEngineTool(    query_engine=vector_query_engine,    metadata=ToolMetadata(      name="pg_essay",      description="Paul Graham essay on What I Worked On",    ),  ),]query_engine = SubQuestionQueryEngine.from_defaults(  query_engine_tools=query_engine_tools,  use_async=True,)response = query_engine.query(  "How was Paul Grahams life different before, during, and after YC?")
```

[Full notebook](https://github.com/run-llama/llama_index/blob/main/docs/examples/query_engine/sub_question_query_engine.ipynb)
The **SubQuestionQueryEngine** works by**breaking down** the original query into sub-questions, each of which is directed to a relevant data source. The intermediate answers from these sub-questions are used **to provide context** and **contribute to the overall answer**. Each sub-question is designed to extract a specific piece of information from the data source it is directed to. The responses to these sub-questions are then combined to form a comprehensive answer to the original query.
On the other hand, the **SubQuestionQueryEngine** breaks down a complex query into many sub-questions and their target query engine for execution. After executing all sub-questions, _all responses are gathered and sent to a response synthesizer to produce the final response_. The **SubQuestionQueryEngine** decides which **QueryEngineTool** to use for each sub-question based on the **tool_name** attribute of the SubQuestion object.
# Code
[colab](https://github.com/XinyueZ/chat-your-doc/blob/master/notebooks/multi_queries_retrieval.ipynb)
[direct-run](https://colab.research.google.com/drive/1HKv85boODXbU944s3tanL-nBRwin7JAq?usp=sharing)
![](https://miro.medium.com/v2/da:true/resize:fit:0/5c50caa54067fd622d2f0fac18392213bf92f6e2fae89b691e62bceb40885e74)
## Sign up to discover human stories that deepen your understanding of the world.
## Free
Distraction-free reading. No ads.
Organize your knowledge with lists and highlights.
Tell your story. Find your audience.
Sign up for free
## Membership
Read member-only stories
Support writers you read most
Earn money for your writing
Listen to audio narrations
Read offline with the Medium app
Try for $5/month
[Langchain](https://medium.com/tag/langchain?source=post_page-----4e7df1a62f83---------------------------------------)
[Llamaindex](https://medium.com/tag/llamaindex?source=post_page-----4e7df1a62f83---------------------------------------)
[Retrieval Augmented](https://medium.com/tag/retrieval-augmented?source=post_page-----4e7df1a62f83---------------------------------------)
[AI](https://medium.com/tag/ai?source=post_page-----4e7df1a62f83---------------------------------------)
[Artificial Intelligence](https://medium.com/tag/artificial-intelligence?source=post_page-----4e7df1a62f83---------------------------------------)
[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F4e7df1a62f83&operation=register&redirect=https%3A%2F%2Fteetracker.medium.com%2Flangchain-llama-index-rag-with-multi-query-retrieval-4e7df1a62f83&user=TeeTracker&userId=984171777958&source=---footer_actions--4e7df1a62f83---------------------clap_footer------------------)
--
[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F4e7df1a62f83&operation=register&redirect=https%3A%2F%2Fteetracker.medium.com%2Flangchain-llama-index-rag-with-multi-query-retrieval-4e7df1a62f83&user=TeeTracker&userId=984171777958&source=---footer_actions--4e7df1a62f83---------------------clap_footer------------------)
--
[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e7df1a62f83&operation=register&redirect=https%3A%2F%2Fteetracker.medium.com%2Flangchain-llama-index-rag-with-multi-query-retrieval-4e7df1a62f83&source=---footer_actions--4e7df1a62f83---------------------bookmark_footer------------------)
[![TeeTracker](https://miro.medium.com/v2/resize:fill:96:96/1*yYq5U3pKuZKMAs0UTQc_HA.png)](https://teetracker.medium.com/?source=post_page---post_author_info--4e7df1a62f83---------------------------------------)
[![TeeTracker](https://miro.medium.com/v2/resize:fill:128:128/1*yYq5U3pKuZKMAs0UTQc_HA.png)](https://teetracker.medium.com/?source=post_page---post_author_info--4e7df1a62f83---------------------------------------)
Follow
## [Written by TeeTracker](https://teetracker.medium.com/?source=post_page---post_author_info--4e7df1a62f83---------------------------------------)
[438 Followers](https://teetracker.medium.com/followers?source=post_page---post_author_info--4e7df1a62f83---------------------------------------)
·[87 Following](https://teetracker.medium.com/following?source=post_page---post_author_info--4e7df1a62f83---------------------------------------)
AI advocate // All about AI
Follow
## No responses yet
[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--4e7df1a62f83---------------------------------------)
![](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)
Write a response
[What are your thoughts?](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fteetracker.medium.com%2Flangchain-llama-index-rag-with-multi-query-retrieval-4e7df1a62f83&source=---post_responses--4e7df1a62f83---------------------respond_sidebar------------------)
Cancel
Respond
Also publish to my profile
[Help](https://help.medium.com/hc/en-us?source=post_page-----4e7df1a62f83---------------------------------------)
[Status](https://medium.statuspage.io/?source=post_page-----4e7df1a62f83---------------------------------------)
[About](https://medium.com/about?autoplay=1&source=post_page-----4e7df1a62f83---------------------------------------)
[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----4e7df1a62f83---------------------------------------)
Press
[Blog](https://blog.medium.com/?source=post_page-----4e7df1a62f83---------------------------------------)
[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4e7df1a62f83---------------------------------------)
[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----4e7df1a62f83---------------------------------------)
[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4e7df1a62f83---------------------------------------)
[Text to speech](https://speechify.com/medium?source=post_page-----4e7df1a62f83---------------------------------------)
