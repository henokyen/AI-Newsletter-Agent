{
    "id": "394f0f6e5e43ef73aae0d562ae9d81ea",
    "metadata": {
        "id": "394f0f6e5e43ef73aae0d562ae9d81ea",
        "url": "https://qdrant.tech/articles/sparse-vectors/",
        "title": "What is a Sparse Vector? How to Achieve Vector-based Hybrid Search - Qdrant",
        "properties": {
            "description": "Learn what sparse vectors are, how they work, and their importance in modern data processing. Explore methods like SPLADE for creating and leveraging sparse vectors efficiently.",
            "keywords": "sparse vectors,SPLADE,hybrid search,vector search,qdrant",
            "author": "Nirant Kasliwal",
            "og:url": "https://qdrant.tech/articles/sparse-vectors/",
            "og:type": "website",
            "og:title": "What is a Sparse Vector? How to Achieve Vector-based Hybrid Search - Qdrant",
            "og:description": "Learn what sparse vectors are, how they work, and their importance in modern data processing. Explore methods like SPLADE for creating and leveraging sparse vectors efficiently.",
            "og:image": "https://qdrant.tech/articles_data/sparse-vectors/social_preview.png",
            "og:image:secure_url": "https://qdrant.tech/articles_data/sparse-vectors/social_preview.png",
            "og:image:type": "image/jpeg",
            "og:image:width": "1200",
            "og:image:height": "630",
            "twitter:card": "summary_large_image",
            "twitter:domain": "qdrant",
            "twitter:url": "https://qdrant.tech/articles/sparse-vectors/",
            "twitter:title": "What is a Sparse Vector? How to Achieve Vector-based Hybrid Search - Qdrant",
            "twitter:description": "Learn what sparse vectors are, how they work, and their importance in modern data processing. Explore methods like SPLADE for creating and leveraging sparse vectors efficiently.",
            "twitter:image:src": "https://qdrant.tech/articles_data/sparse-vectors/social_preview.png"
        }
    },
    "parent_metadata": {
        "id": "e8faa7f280255eda185ec2fd4cadf8cc",
        "url": "https://www.notion.so/RAG-e8faa7f280255eda185ec2fd4cadf8cc",
        "title": "RAG",
        "properties": {
            "Type": [
                "Leaf"
            ]
        }
    },
    "content": "[![logo](https://qdrant.tech/img/qdrant-logo.svg)](https://qdrant.tech/)\n  * [Qdrant](https://qdrant.tech/documentation/)\n  * [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n  * [Build](https://qdrant.tech/documentation/build/)\n  * [Learn](https://qdrant.tech/articles/)\n  * [API Reference](https://api.qdrant.tech/api-reference)\n\n\nSearch\n[Log in](https://cloud.qdrant.io/login) [Start Free](https://cloud.qdrant.io/signup)\n![logo](https://qdrant.tech/img/qdrant-logo.svg)\nSearch\n  * [ Qdrant](https://qdrant.tech/documentation/)\n  * [ Cloud](https://qdrant.tech/documentation/cloud-intro/)\n  * [ Build](https://qdrant.tech/documentation/build/)\n  * [ Learn](https://qdrant.tech/articles/)\n  * [ API Reference](https://api.qdrant.tech/api-reference)\n\n\n### Learn\n[Vector Search Manuals](https://qdrant.tech/articles/vector-search-manuals/)\n[Qdrant Internals](https://qdrant.tech/articles/qdrant-internals/)\n[Data Exploration](https://qdrant.tech/articles/data-exploration/)\n[Machine Learning](https://qdrant.tech/articles/machine-learning/)\n[RAG & GenAI](https://qdrant.tech/articles/rag-and-genai/)\n[Practical Examples](https://qdrant.tech/articles/practicle-examples/)\n[Ecosystem](https://qdrant.tech/articles/ecosystem/)\n### Learn\n[Vector Search Manuals](https://qdrant.tech/articles/vector-search-manuals/)\n[Qdrant Internals](https://qdrant.tech/articles/qdrant-internals/)\n[Data Exploration](https://qdrant.tech/articles/data-exploration/)\n[Machine Learning](https://qdrant.tech/articles/machine-learning/)\n[RAG & GenAI](https://qdrant.tech/articles/rag-and-genai/)\n[Practical Examples](https://qdrant.tech/articles/practicle-examples/)\n[Ecosystem](https://qdrant.tech/articles/ecosystem/)\n  * [Articles](https://qdrant.tech/articles/)\n  * What is a Sparse Vector? How to Achieve Vector-based Hybrid Search\n\n\n[ Back to Vector Search Manuals](https://qdrant.tech/articles/vector-search-manuals/)\n# What is a Sparse Vector? How to Achieve Vector-based Hybrid Search\nNirant Kasliwal\n¬∑\nDecember 09, 2023\n![What is a Sparse Vector? How to Achieve Vector-based Hybrid Search](https://qdrant.tech/articles_data/sparse-vectors/preview/title.jpg)\nThink of a library with a vast index card system. Each index card only has a few keywords marked out (sparse vector) of a large possible set for each book (document). This is what sparse vectors enable for text.\n## What are sparse and dense vectors?\nSparse vectors are like the Marie Kondo of data‚Äîkeeping only what sparks joy (or relevance, in this case).\nConsider a simplified example of 2 documents, each with 200 words. A dense vector would have several hundred non-zero values, whereas a sparse vector could have, much fewer, say only 20 non-zero values.\nIn this example: We assume it selects only 2 words or tokens from each document. The rest of the values are zero. This is why it‚Äôs called a sparse vector.\n```\ndense = [0.2, 0.3, 0.5, 0.7, ...] # several hundred floats\nsparse = [{331: 0.5}, {14136: 0.7}] # 20 key value pairs\n\n```\n\nThe numbers 331 and 14136 map to specific tokens in the vocabulary e.g. `['chocolate', 'icecream']`. The rest of the values are zero. This is why it‚Äôs called a sparse vector.\nThe tokens aren‚Äôt always words though, sometimes they can be sub-words: `['ch', 'ocolate']` too.\nThey‚Äôre pivotal in information retrieval, especially in ranking and search systems. BM25, a standard ranking function used by search engines like [Elasticsearch](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors), exemplifies this. BM25 calculates the relevance of documents to a given search query.\nBM25‚Äôs capabilities are well-established, yet it has its limitations.\nBM25 relies solely on the frequency of words in a document and does not attempt to comprehend the meaning or the contextual importance of the words. Additionally, it requires the computation of the entire corpus‚Äôs statistics in advance, posing a challenge for large datasets.\nSparse vectors harness the power of neural networks to surmount these limitations while retaining the ability to query exact words and phrases. They excel in handling large text data, making them crucial in modern data processing a and marking an advancement over traditional methods such as BM25.\n## Understanding sparse vectors\nSparse Vectors are a representation where each dimension corresponds to a word or subword, greatly aiding in interpreting document rankings. This clarity is why sparse vectors are essential in modern search and recommendation systems, complimenting the meaning-rich embedding or dense vectors.\nDense vectors from models like OpenAI Ada-002 or Sentence Transformers contain non-zero values for every element. In contrast, sparse vectors focus on relative word weights per document, with most values being zero. This results in a more efficient and interpretable system, especially in text-heavy applications like search.\nSparse Vectors shine in domains and scenarios where many rare keywords or specialized terms are present. For example, in the medical domain, many rare terms are not present in the general vocabulary, so general-purpose dense vectors cannot capture the nuances of the domain.\nFeature| Sparse Vectors| Dense Vectors  \n---|---|---  \n**Data Representation**|  Majority of elements are zero| All elements are non-zero  \n**Computational Efficiency**|  Generally higher, especially in operations involving zero elements| Lower, as operations are performed on all elements  \n**Information Density**|  Less dense, focuses on key features| Highly dense, capturing nuanced relationships  \n**Example Applications**|  Text search, Hybrid search| [RAG](https://qdrant.tech/articles/what-is-rag-in-ai/), many general machine learning tasks  \nWhere do sparse vectors fail though? They‚Äôre not great at capturing nuanced relationships between words. For example, they can‚Äôt capture the relationship between ‚Äúking‚Äù and ‚Äúqueen‚Äù as well as dense vectors.\n## SPLADE\nLet‚Äôs check out [SPLADE](https://europe.naverlabs.com/research/computer-science/splade-a-sparse-bi-encoder-bert-based-model-achieves-effective-and-efficient-full-text-document-ranking/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors), an excellent way to make sparse vectors. Let‚Äôs look at some numbers first. Higher is better:\nModel| MRR@10 (MS MARCO Dev)| Type  \n---|---|---  \nBM25| 0.184| Sparse  \nTCT-ColBERT| 0.359| Dense  \ndoc2query-T5 [link](https://github.com/castorini/docTTTTTquery)| 0.277| Sparse  \nSPLADE| 0.322| Sparse  \nSPLADE-max| 0.340| Sparse  \nSPLADE-doc| 0.322| Sparse  \nDistilSPLADE-max| 0.368| Sparse  \nAll numbers are from [SPLADEv2](https://arxiv.org/abs/2109.10086). MRR is [Mean Reciprocal Rank](https://www.wikiwand.com/en/Mean_reciprocal_rank#References), a standard metric for ranking. [MS MARCO](https://microsoft.github.io/MSMARCO-Passage-Ranking/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) is a dataset for evaluating ranking and retrieval for passages.\nSPLADE is quite flexible as a method, with regularization knobs that can be tuned to obtain [different models](https://github.com/naver/splade) as well:\n> SPLADE is more a class of models rather than a model per se: depending on the regularization magnitude, we can obtain different models (from very sparse to models doing intense query/doc expansion) with different properties and performance.\nFirst, let‚Äôs look at how to create a sparse vector. Then, we‚Äôll look at the concepts behind SPLADE.\n## Creating a sparse vector\nWe‚Äôll explore two different ways to create a sparse vector. The higher performance way to create a sparse vector from dedicated document and query encoders. We‚Äôll look at a simpler approach ‚Äì here we will use the same model for both document and query. We will get a dictionary of token ids and their corresponding weights for a sample text - representing a document.\nIf you‚Äôd like to follow along, here‚Äôs a [Colab Notebook](https://colab.research.google.com/gist/NirantK/ad658be3abefc09b17ce29f45255e14e/splade-single-encoder.ipynb), [alternate link](https://gist.github.com/NirantK/ad658be3abefc09b17ce29f45255e14e) with all the code.\n### Setting Up\n```\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\nmodel_id = \"naver/splade-cocondenser-ensembledistil\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForMaskedLM.from_pretrained(model_id)\ntext = \"\"\"Arthur Robert Ashe Jr. (July 10, 1943 ‚Äì February 6, 1993) was an American professional tennis player. He won three Grand Slam titles in singles and two in doubles.\"\"\"\n\n```\n\n### Computing the sparse vector\n```\nimport torch\ndef compute_vector(text):\n  \"\"\"\n  Computes a vector from logits and attention mask using ReLU, log, and max operations.\n  \"\"\"\n  tokens = tokenizer(text, return_tensors=\"pt\")\n  output = model(**tokens)\n  logits, attention_mask = output.logits, tokens.attention_mask\n  relu_log = torch.log(1 + torch.relu(logits))\n  weighted_log = relu_log * attention_mask.unsqueeze(-1)\n  max_val, _ = torch.max(weighted_log, dim=1)\n  vec = max_val.squeeze()\n  return vec, tokens\nvec, tokens = compute_vector(text)\nprint(vec.shape)\n\n```\n\nYou‚Äôll notice that there are 38 tokens in the text based on this tokenizer. This will be different from the number of tokens in the vector. In a TF-IDF, we‚Äôd assign weights only to these tokens or words. In SPLADE, we assign weights to all the tokens in the vocabulary using this vector using our learned model.\n## Term expansion and weights\n```\ndef extract_and_map_sparse_vector(vector, tokenizer):\n  \"\"\"\n  Extracts non-zero elements from a given vector and maps these elements to their human-readable tokens using a tokenizer. The function creates and returns a sorted dictionary where keys are the tokens corresponding to non-zero elements in the vector, and values are the weights of these elements, sorted in descending order of weights.\n  This function is useful in NLP tasks where you need to understand the significance of different tokens based on a model's output vector. It first identifies non-zero values in the vector, maps them to tokens, and sorts them by weight for better interpretability.\n  Args:\n  vector (torch.Tensor): A PyTorch tensor from which to extract non-zero elements.\n  tokenizer: The tokenizer used for tokenization in the model, providing the mapping from tokens to indices.\n  Returns:\n  dict: A sorted dictionary mapping human-readable tokens to their corresponding non-zero weights.\n  \"\"\"\n  # Extract indices and values of non-zero elements in the vector\n  cols = vector.nonzero().squeeze().cpu().tolist()\n  weights = vector[cols].cpu().tolist()\n  # Map indices to tokens and create a dictionary\n  idx2token = {idx: token for token, idx in tokenizer.get_vocab().items()}\n  token_weight_dict = {\n    idx2token[idx]: round(weight, 2) for idx, weight in zip(cols, weights)\n  }\n  # Sort the dictionary by weights in descending order\n  sorted_token_weight_dict = {\n    k: v\n    for k, v in sorted(\n      token_weight_dict.items(), key=lambda item: item[1], reverse=True\n    )\n  }\n  return sorted_token_weight_dict\n# Usage example\nsorted_tokens = extract_and_map_sparse_vector(vec, tokenizer)\nsorted_tokens\n\n```\n\nThere will be 102 sorted tokens in total. This has expanded to include tokens that weren‚Äôt in the original text. This is the term expansion we will talk about next.\nHere are some terms that are added: ‚ÄúBerlin‚Äù, and ‚Äúfounder‚Äù - despite having no mention of Arthur‚Äôs race (which leads to Owen‚Äôs Berlin win) and his work as the founder of Arthur Ashe Institute for Urban Health. Here are the top few `sorted_tokens` with a weight of more than 1:\n```\n{\n  \"ashe\": 2.95,\n  \"arthur\": 2.61,\n  \"tennis\": 2.22,\n  \"robert\": 1.74,\n  \"jr\": 1.55,\n  \"he\": 1.39,\n  \"founder\": 1.36,\n  \"doubles\": 1.24,\n  \"won\": 1.22,\n  \"slam\": 1.22,\n  \"died\": 1.19,\n  \"singles\": 1.1,\n  \"was\": 1.07,\n  \"player\": 1.06,\n  \"titles\": 0.99, \n  ...\n}\n\n```\n\nIf you‚Äôre interested in using the higher-performance approach, check out the following models:\n  1. [naver/efficient-splade-VI-BT-large-doc](https://huggingface.co/naver/efficient-splade-vi-bt-large-doc)\n  2. [naver/efficient-splade-VI-BT-large-query](https://huggingface.co/naver/efficient-splade-vi-bt-large-doc)\n\n\n## Why SPLADE works: term expansion\nConsider a query ‚Äúsolar energy advantages‚Äù. SPLADE might expand this to include terms like ‚Äúrenewable,‚Äù ‚Äúsustainable,‚Äù and ‚Äúphotovoltaic,‚Äù which are contextually relevant but not explicitly mentioned. This process is called term expansion, and it‚Äôs a key component of SPLADE.\nSPLADE learns the query/document expansion to include other relevant terms. This is a crucial advantage over other sparse methods which include the exact word, but completely miss the contextually relevant ones.\nThis expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.\nFor example, assume a 1M document corpus. Say, we use 100 sparse token ids + weights per document. Correspondingly, dense BERT vector would be 768M floats, the OpenAI Embedding would be 1.536B floats, and the sparse vector would be a maximum of 100M integers + 100M floats. This could mean a **10x reduction in memory usage** , which is a huge win for large-scale systems:\nVector Type| Memory (GB)  \n---|---  \nDense BERT Vector| 6.144  \nOpenAI Embedding| 12.288  \nSparse Vector| 1.12  \n### How SPLADE works: leveraging BERT\nSPLADE leverages a transformer architecture to generate sparse representations of documents and queries, enabling efficient retrieval. Let‚Äôs dive into the process.\nThe output logits from the transformer backbone are inputs upon which SPLADE builds. The transformer architecture can be something familiar like BERT. Rather than producing dense probability distributions, SPLADE utilizes these logits to construct sparse vectors‚Äîthink of them as a distilled essence of tokens, where each dimension corresponds to a term from the vocabulary and its associated weight in the context of the given document or query.\nThis sparsity is critical; it mirrors the probability distributions from a typical [Masked Language Modeling](http://jalammar.github.io/illustrated-bert/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) task but is tuned for retrieval effectiveness, emphasizing terms that are both:\n  1. Contextually relevant: Terms that represent a document well should be given more weight.\n  2. Discriminative across documents: Terms that a document has, and other documents don‚Äôt, should be given more weight.\n\n\nThe token-level distributions that you‚Äôd expect in a standard transformer model are now transformed into token-level importance scores in SPLADE. These scores reflect the significance of each term in the context of the document or query, guiding the model to allocate more weight to terms that are likely to be more meaningful for retrieval purposes.\nThe resulting sparse vectors are not only memory-efficient but also tailored for precise matching in the high-dimensional space of a search engine like Qdrant.\n### Interpreting SPLADE\nA downside of dense vectors is that they are not interpretable, making it difficult to understand why a document is relevant to a query.\nSPLADE importance estimation can provide insights into the ‚Äòwhy‚Äô behind a document‚Äôs relevance to a query. By shedding light on which tokens contribute most to the retrieval score, SPLADE offers some degree of interpretability alongside performance, a rare feat in the realm of neural IR systems. For engineers working on search, this transparency is invaluable.\n## Known limitations of SPLADE\n### Pooling strategy\nThe switch to max pooling in SPLADE improved its performance on the MS MARCO and TREC datasets. However, this indicates a potential limitation of the baseline SPLADE pooling method, suggesting that SPLADE‚Äôs performance is sensitive to the choice of pooling strategy‚Äã‚Äã.\n### Document and query Eecoder\nThe SPLADE model variant that uses a document encoder with max pooling but no query encoder reaches the same performance level as the prior SPLADE model. This suggests a limitation in the necessity of a query encoder, potentially affecting the efficiency of the model‚Äã‚Äã.\n### Other sparse vector methods\nSPLADE is not the only method to create sparse vectors.\nEssentially, sparse vectors are a superset of TF-IDF and BM25, which are the most popular text retrieval methods. In other words, you can create a sparse vector using the term frequency and inverse document frequency (TF-IDF) to reproduce the BM25 score exactly.\nAdditionally, attention weights from Sentence Transformers can be used to create sparse vectors. This method preserves the ability to query exact words and phrases but avoids the computational overhead of query expansion used in SPLADE.\nWe will cover these methods in detail in a future article.\n## Leveraging sparse vectors in Qdrant for hybrid search\nQdrant supports a separate index for Sparse Vectors. This enables you to use the same collection for both dense and sparse vectors. Each ‚ÄúPoint‚Äù in Qdrant can have both dense and sparse vectors.\nBut let‚Äôs first take a look at how you can work with sparse vectors in Qdrant.\n## Practical implementation in Python\nLet‚Äôs dive into how Qdrant handles sparse vectors with an example. Here is what we will cover:\n  1. Setting Up Qdrant Client: Initially, we establish a connection with Qdrant using the QdrantClient. This setup is crucial for subsequent operations.\n  2. Creating a Collection with Sparse Vector Support: In Qdrant, a collection is a container for your vectors. Here, we create a collection specifically designed to support sparse vectors. This is done using the create_collection method where we define the parameters for sparse vectors, such as setting the index configuration.\n  3. Inserting Sparse Vectors: Once the collection is set up, we can insert sparse vectors into it. This involves defining the sparse vector with its indices and values, and then upserting this point into the collection.\n  4. Querying with Sparse Vectors: To perform a search, we first prepare a query vector. This involves computing the vector from a query text and extracting its indices and values. We then use these details to construct a query against our collection.\n  5. Retrieving and Interpreting Results: The search operation returns results that include the id of the matching document, its score, and other relevant details. The score is a crucial aspect, reflecting the similarity between the query and the documents in the collection.\n\n\n### 1. Set up\n```\n# Qdrant client setup\nclient = QdrantClient(\":memory:\")\n# Define collection name\nCOLLECTION_NAME = \"example_collection\"\n# Insert sparse vector into Qdrant collection\npoint_id = 1 # Assign a unique ID for the point\n\n```\n\n### 2. Create a collection with sparse vector support\n```\nclient.create_collection(\n  collection_name=COLLECTION_NAME,\n  vectors_config={},\n  sparse_vectors_config={\n    \"text\": models.SparseVectorParams(\n      index=models.SparseIndexParams(\n        on_disk=False,\n      )\n    )\n  },\n)\n\n```\n\n### 3. Insert sparse vectors\nHere, we see the process of inserting a sparse vector into the Qdrant collection. This step is key to building a dataset that can be quickly retrieved in the first stage of the retrieval process, utilizing the efficiency of sparse vectors. Since this is for demonstration purposes, we insert only one point with Sparse Vector and no dense vector.\n```\nclient.upsert(\n  collection_name=COLLECTION_NAME,\n  points=[\n    models.PointStruct(\n      id=point_id,\n      payload={}, # Add any additional payload if necessary\n      vector={\n        \"text\": models.SparseVector(\n          indices=indices.tolist(), values=values.tolist()\n        )\n      },\n    )\n  ],\n)\n\n```\n\nBy upserting points with sparse vectors, we prepare our dataset for rapid first-stage retrieval, laying the groundwork for subsequent detailed analysis using dense vectors. Notice that we use ‚Äútext‚Äù to denote the name of the sparse vector.\nThose familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API ‚Äì this is to make it easier to use sparse vectors in existing codebases. As always, you‚Äôre able to **apply payload filters** , shard keys, and other advanced features you‚Äôve come to expect from Qdrant. To make things easier for you, the indices and values don‚Äôt have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.\n### 4. Query with sparse vectors\nWe use the same process to prepare a query vector as well. This involves computing the vector from a query text and extracting its indices and values. We then use these details to construct a query against our collection.\n```\n# Preparing a query vector\nquery_text = \"Who was Arthur Ashe?\"\nquery_vec, query_tokens = compute_vector(query_text)\nquery_vec.shape\nquery_indices = query_vec.nonzero().numpy().flatten()\nquery_values = query_vec.detach().numpy()[query_indices]\n\n```\n\nIn this example, we use the same model for both document and query. This is not a requirement, but it‚Äôs a simpler approach.\n### 5. Retrieve and interpret results\nAfter setting up the collection and inserting sparse vectors, the next critical step is retrieving and interpreting the results. This process involves executing a search query and then analyzing the returned results.\n```\n# Searching for similar documents\nresult = client.search(\n  collection_name=COLLECTION_NAME,\n  query_vector=models.NamedSparseVector(\n    name=\"text\",\n    vector=models.SparseVector(\n      indices=query_indices,\n      values=query_values,\n    ),\n  ),\n  with_vectors=True,\n)\nresult\n\n```\n\nIn the above code, we execute a search against our collection using the prepared sparse vector query. The `client.search` method takes the collection name and the query vector as inputs. The query vector is constructed using the `models.NamedSparseVector`, which includes the indices and values derived from the query text. This is a crucial step in efficiently retrieving relevant documents.\n```\nScoredPoint(\n  id=1,\n  version=0,\n  score=3.4292831420898438,\n  payload={},\n  vector={\n    \"text\": SparseVector(\n      indices=[2001, 2002, 2010, 2018, 2032, ...],\n      values=[\n        1.0660614967346191,\n        1.391068458557129,\n        0.8903818726539612,\n        0.2502821087837219,\n        ...,\n      ],\n    )\n  },\n)\n\n```\n\nThe result, as shown above, is a `ScoredPoint` object containing the ID of the retrieved document, its version, a similarity score, and the sparse vector. The score is a key element as it quantifies the similarity between the query and the document, based on their respective vectors.\nTo understand how this scoring works, we use the familiar dot product method:\n$$\\text{Similarity}(\\text{Query}, \\text{Document}) = \\sum_{i \\in I} \\text{Query}_i \\times \\text{Document}_i$$\nThis formula calculates the similarity score by multiplying corresponding elements of the query and document vectors and summing these products. This method is particularly effective with sparse vectors, where many elements are zero, leading to a computationally efficient process. The higher the score, the greater the similarity between the query and the document, making it a valuable metric for assessing the relevance of the retrieved documents.\n## Hybrid search: combining sparse and dense vectors\nBy combining search results from both dense and sparse vectors, you can achieve a hybrid search that is both efficient and accurate. Results from sparse vectors will guarantee, that all results with the required keywords are returned, while dense vectors will cover the semantically similar results.\nThe mixture of dense and sparse results can be presented directly to the user, or used as a first stage of a two-stage retrieval process.\nLet‚Äôs see how you can make a hybrid search query in Qdrant.\nFirst, you need to create a collection with both dense and sparse vectors:\n```\nclient.create_collection(\n  collection_name=COLLECTION_NAME,\n  vectors_config={\n    \"text-dense\": models.VectorParams(\n      size=1536, # OpenAI Embeddings\n      distance=models.Distance.COSINE,\n    )\n  },\n  sparse_vectors_config={\n    \"text-sparse\": models.SparseVectorParams(\n      index=models.SparseIndexParams(\n        on_disk=False,\n      )\n    )\n  },\n)\n\n```\n\nThen, assuming you have upserted both dense and sparse vectors, you can query them together:\n```\nquery_text = \"Who was Arthur Ashe?\"\n# Compute sparse and dense vectors\nquery_indices, query_values = compute_sparse_vector(query_text)\nquery_dense_vector = compute_dense_vector(query_text)\nclient.search_batch(\n  collection_name=COLLECTION_NAME,\n  requests=[\n    models.SearchRequest(\n      vector=models.NamedVector(\n        name=\"text-dense\",\n        vector=query_dense_vector,\n      ),\n      limit=10,\n    ),\n    models.SearchRequest(\n      vector=models.NamedSparseVector(\n        name=\"text-sparse\",\n        vector=models.SparseVector(\n          indices=query_indices,\n          values=query_values,\n        ),\n      ),\n      limit=10,\n    ),\n  ],\n)\n\n```\n\nThe result will be a pair of result lists, one for dense and one for sparse vectors.\nHaving those results, there are several ways to combine them:\n### Mixing or fusion\nYou can mix the results from both dense and sparse vectors, based purely on their relative scores. This is a simple and effective approach, but it doesn‚Äôt take into account the semantic similarity between the results. Among the [popular mixing methods](https://medium.com/plain-simple-software/distribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18) are:\n```\n- Reciprocal Ranked Fusion (RRF)\n- Relative Score Fusion (RSF)\n- Distribution-Based Score Fusion (DBSF)\n\n```\n![Relative Score Fusion](https://qdrant.tech/articles_data/sparse-vectors/mixture.png)\nRelative Score Fusion\n[Ranx](https://github.com/AmenRa/ranx) is a great library for mixing results from different sources.\n### Re-ranking\nYou can use obtained results as a first stage of a two-stage retrieval process. In the second stage, you can re-rank the results from the first stage using a more complex model, such as [Cross-Encoders](https://www.sbert.net/examples/applications/cross-encoder/README.html) or services like [Cohere Rerank](https://txt.cohere.com/rerank/).\nAnd that‚Äôs it! You‚Äôve successfully achieved hybrid search with Qdrant!\n## Additional resources\nFor those who want to dive deeper, here are the top papers on the topic most of which have code available:\n  1. Problem Motivation: [Sparse Overcomplete Word Vector Representations](https://ar5iv.org/abs/1506.02004?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n  2. [SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval](https://ar5iv.org/abs/2109.10086?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n  3. [SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking](https://ar5iv.org/abs/2107.05720?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n  4. Late Interaction - [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](https://ar5iv.org/abs/2112.01488?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n  5. [SparseEmbed: Learning Sparse Lexical Representations with Contextual Embeddings for Retrieval](https://research.google/pubs/pub52289/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n\n\n**Why just read when you can try it out?**\nWe‚Äôve packed an easy-to-use Colab for you on how to make a Sparse Vector: [Sparse Vectors Single Encoder Demo](https://colab.research.google.com/drive/1wa2Yr5BCOgV0MTOFFTude99BOXCLHXky?usp=sharing). Run it, tinker with it, and start seeing the magic unfold in your projects. We can‚Äôt wait to hear how you use it!\n## Conclusion\nAlright, folks, let‚Äôs wrap it up. Better search isn‚Äôt a ‚Äônice-to-have,‚Äô it‚Äôs a game-changer, and Qdrant can get you there.\nGot questions? Our [Discord community](https://qdrant.to/discord?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) is teeming with answers.\nIf you enjoyed reading this, why not sign up for our [newsletter](https://qdrant.tech/subscribe/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) to stay ahead of the curve.\nAnd, of course, a big thanks to you, our readers, for pushing us to make ranking better for everyone.\n##### Was this page useful?\n![Thumb up icon](https://qdrant.tech/icons/outline/thumb-up.svg) Yes  ![Thumb down icon](https://qdrant.tech/icons/outline/thumb-down.svg) No\nThank you for your feedback! üôè\nWe are sorry to hear that. üòî You can [edit](https://qdrant.tech/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/sparse-vectors.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue.\nOn this page:\n  * [What are sparse and dense vectors?](https://qdrant.tech/articles/sparse-vectors/#what-are-sparse-and-dense-vectors)\n  * [Understanding sparse vectors](https://qdrant.tech/articles/sparse-vectors/#understanding-sparse-vectors)\n  * [SPLADE](https://qdrant.tech/articles/sparse-vectors/#splade)\n  * [Creating a sparse vector](https://qdrant.tech/articles/sparse-vectors/#creating-a-sparse-vector)\n    * [Setting Up](https://qdrant.tech/articles/sparse-vectors/#setting-up)\n    * [Computing the sparse vector](https://qdrant.tech/articles/sparse-vectors/#computing-the-sparse-vector)\n  * [Term expansion and weights](https://qdrant.tech/articles/sparse-vectors/#term-expansion-and-weights)\n  * [Why SPLADE works: term expansion](https://qdrant.tech/articles/sparse-vectors/#why-splade-works-term-expansion)\n    * [How SPLADE works: leveraging BERT](https://qdrant.tech/articles/sparse-vectors/#how-splade-works-leveraging-bert)\n    * [Interpreting SPLADE](https://qdrant.tech/articles/sparse-vectors/#interpreting-splade)\n  * [Known limitations of SPLADE](https://qdrant.tech/articles/sparse-vectors/#known-limitations-of-splade)\n    * [Pooling strategy](https://qdrant.tech/articles/sparse-vectors/#pooling-strategy)\n    * [Document and query Eecoder](https://qdrant.tech/articles/sparse-vectors/#document-and-query-eecoder)\n    * [Other sparse vector methods](https://qdrant.tech/articles/sparse-vectors/#other-sparse-vector-methods)\n  * [Leveraging sparse vectors in Qdrant for hybrid search](https://qdrant.tech/articles/sparse-vectors/#leveraging-sparse-vectors-in-qdrant-for-hybrid-search)\n  * [Practical implementation in Python](https://qdrant.tech/articles/sparse-vectors/#practical-implementation-in-python)\n    * [1. Set up](https://qdrant.tech/articles/sparse-vectors/#1-set-up)\n    * [2. Create a collection with sparse vector support](https://qdrant.tech/articles/sparse-vectors/#2-create-a-collection-with-sparse-vector-support)\n    * [3. Insert sparse vectors](https://qdrant.tech/articles/sparse-vectors/#3-insert-sparse-vectors)\n    * [4. Query with sparse vectors](https://qdrant.tech/articles/sparse-vectors/#4-query-with-sparse-vectors)\n    * [5. Retrieve and interpret results](https://qdrant.tech/articles/sparse-vectors/#5-retrieve-and-interpret-results)\n  * [Hybrid search: combining sparse and dense vectors](https://qdrant.tech/articles/sparse-vectors/#hybrid-search-combining-sparse-and-dense-vectors)\n    * [Mixing or fusion](https://qdrant.tech/articles/sparse-vectors/#mixing-or-fusion)\n    * [Re-ranking](https://qdrant.tech/articles/sparse-vectors/#re-ranking)\n  * [Additional resources](https://qdrant.tech/articles/sparse-vectors/#additional-resources)\n  * [Conclusion](https://qdrant.tech/articles/sparse-vectors/#conclusion)\n\n\n  * [ Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/sparse-vectors.md)\n  * [ Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)\n\n\n#### Ready to get started with Qdrant?\n[Start Free](https://qdrant.to/cloud/)\n¬© 2025 Qdrant. All Rights Reserved\n[Terms](https://qdrant.tech/legal/terms_and_conditions/) [Privacy Policy](https://qdrant.tech/legal/privacy-policy/) [Impressum](https://qdrant.tech/legal/impressum/)\n√ó\n[ Powered by ](https://qdrant.tech/)\nWe use cookies to learn more about you. At any time you can delete or block cookies through your browser settings.\n[Learn more](https://qdrant.tech/legal/privacy-policy/)[I accept](https://qdrant.tech/articles/sparse-vectors/)\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://qdrant.tech/",
        "https://qdrant.tech/documentation/",
        "https://qdrant.tech/documentation/cloud-intro/",
        "https://qdrant.tech/documentation/build/",
        "https://qdrant.tech/articles/",
        "https://api.qdrant.tech/api-reference",
        "https://qdrant.tech/articles/vector-search-manuals/",
        "https://qdrant.tech/articles/qdrant-internals/",
        "https://qdrant.tech/articles/data-exploration/",
        "https://qdrant.tech/articles/machine-learning/",
        "https://qdrant.tech/articles/rag-and-genai/",
        "https://qdrant.tech/articles/practicle-examples/",
        "https://qdrant.tech/articles/ecosystem/",
        "https://qdrant.tech/articles/what-is-rag-in-ai/",
        "https://qdrant.tech/subscribe/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors",
        "https://qdrant.tech/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/sparse-vectors.md",
        "https://qdrant.tech/articles/sparse-vectors/#what-are-sparse-and-dense-vectors",
        "https://qdrant.tech/articles/sparse-vectors/#understanding-sparse-vectors",
        "https://qdrant.tech/articles/sparse-vectors/#splade",
        "https://qdrant.tech/articles/sparse-vectors/#creating-a-sparse-vector",
        "https://qdrant.tech/articles/sparse-vectors/#setting-up",
        "https://qdrant.tech/articles/sparse-vectors/#computing-the-sparse-vector",
        "https://qdrant.tech/articles/sparse-vectors/#term-expansion-and-weights",
        "https://qdrant.tech/articles/sparse-vectors/#why-splade-works-term-expansion",
        "https://qdrant.tech/articles/sparse-vectors/#how-splade-works-leveraging-bert",
        "https://qdrant.tech/articles/sparse-vectors/#interpreting-splade",
        "https://qdrant.tech/articles/sparse-vectors/#known-limitations-of-splade",
        "https://qdrant.tech/articles/sparse-vectors/#pooling-strategy",
        "https://qdrant.tech/articles/sparse-vectors/#document-and-query-eecoder",
        "https://qdrant.tech/articles/sparse-vectors/#other-sparse-vector-methods",
        "https://qdrant.tech/articles/sparse-vectors/#leveraging-sparse-vectors-in-qdrant-for-hybrid-search",
        "https://qdrant.tech/articles/sparse-vectors/#practical-implementation-in-python",
        "https://qdrant.tech/articles/sparse-vectors/#1-set-up",
        "https://qdrant.tech/articles/sparse-vectors/#2-create-a-collection-with-sparse-vector-support",
        "https://qdrant.tech/articles/sparse-vectors/#3-insert-sparse-vectors",
        "https://qdrant.tech/articles/sparse-vectors/#4-query-with-sparse-vectors",
        "https://qdrant.tech/articles/sparse-vectors/#5-retrieve-and-interpret-results",
        "https://qdrant.tech/articles/sparse-vectors/#hybrid-search-combining-sparse-and-dense-vectors",
        "https://qdrant.tech/articles/sparse-vectors/#mixing-or-fusion",
        "https://qdrant.tech/articles/sparse-vectors/#re-ranking",
        "https://qdrant.tech/articles/sparse-vectors/#additional-resources",
        "https://qdrant.tech/articles/sparse-vectors/#conclusion",
        "https://qdrant.tech/legal/terms_and_conditions/",
        "https://qdrant.tech/legal/privacy-policy/",
        "https://qdrant.tech/legal/impressum/",
        "https://qdrant.tech/articles/sparse-vectors/",
        "https://cloud.qdrant.io/login",
        "https://cloud.qdrant.io/signup",
        "https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors",
        "https://europe.naverlabs.com/research/computer-science/splade-a-sparse-bi-encoder-bert-based-model-achieves-effective-and-efficient-full-text-document-ranking/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors",
        "https://github.com/castorini/docTTTTTquery",
        "https://arxiv.org/abs/2109.10086",
        "https://www.wikiwand.com/en/Mean_reciprocal_rank#References",
        "https://microsoft.github.io/MSMARCO-Passage-Ranking/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors",
        "https://github.com/naver/splade",
        "https://colab.research.google.com/gist/NirantK/ad658be3abefc09b17ce29f45255e14e/splade-single-encoder.ipynb",
        "https://gist.github.com/NirantK/ad658be3abefc09b17ce29f45255e14e",
        "https://huggingface.co/naver/efficient-splade-vi-bt-large-doc",
        "http://jalammar.github.io/illustrated-bert/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors",
        "https://medium.com/plain-simple-software/distribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18",
        "https://github.com/AmenRa/ranx",
        "https://www.sbert.net/examples/applications/cross-encoder/README.html",
        "https://txt.cohere.com/rerank/",
        "https://ar5iv.org/abs/1506.02004?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors",
        "https://ar5iv.org/abs/2109.10086?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors",
        "https://ar5iv.org/abs/2107.05720?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors",
        "https://ar5iv.org/abs/2112.01488?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors",
        "https://research.google/pubs/pub52289/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors",
        "https://colab.research.google.com/drive/1wa2Yr5BCOgV0MTOFFTude99BOXCLHXky?usp=sharing",
        "https://qdrant.to/discord?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors",
        "https://github.com/qdrant/landing_page/issues/new/choose",
        "https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/sparse-vectors.md",
        "https://qdrant.to/cloud/"
    ]
}