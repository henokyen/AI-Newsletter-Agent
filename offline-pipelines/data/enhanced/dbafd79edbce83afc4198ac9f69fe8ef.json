{
    "id": "dbafd79edbce83afc4198ac9f69fe8ef",
    "metadata": {
        "id": "dbafd79edbce83afc4198ac9f69fe8ef",
        "url": "https://arxiv.org/abs/2302.13971/",
        "title": "[2302.13971] LLaMA: Open and Efficient Foundation Language Models",
        "properties": {
            "description": "Abstract page for arXiv paper 2302.13971: LLaMA: Open and Efficient Foundation Language Models",
            "keywords": null,
            "author": null,
            "og:type": "website",
            "og:site_name": "arXiv.org",
            "og:title": "LLaMA: Open and Efficient Foundation Language Models",
            "og:url": "https://arxiv.org/abs/2302.13971v1",
            "og:image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
            "og:image:secure_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
            "og:image:width": "1200",
            "og:image:height": "700",
            "og:image:alt": "arXiv logo",
            "og:description": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
            "twitter:site": "@arxiv",
            "twitter:card": "summary",
            "twitter:title": "LLaMA: Open and Efficient Foundation Language Models",
            "twitter:description": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art...",
            "twitter:image": "https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png",
            "twitter:image:alt": "arXiv logo"
        }
    },
    "parent_metadata": {
        "id": "4cabadefac9e7c01993a7bfb6ecf3fb2",
        "url": "https://www.notion.so/LLM-Inference-Optimization-Other-Techniques-4cabadefac9e7c01993a7bfb6ecf3fb2",
        "title": "LLM Inference Optimization & Other Techniques",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "[Skip to main content](https://arxiv.org/abs/2302.13971/#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/) > [cs](https://arxiv.org/list/cs/recent) > arXiv:2302.13971 \n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[ ![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg) ](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n## quick links\n  * [Login](https://arxiv.org/login)\n  * [Help Pages](https://info.arxiv.org/help)\n  * [About](https://info.arxiv.org/about)\n\n\n# Computer Science > Computation and Language\n**arXiv:2302.13971** (cs) \n[Submitted on 27 Feb 2023]\n# Title:LLaMA: Open and Efficient Foundation Language Models\nAuthors:[Hugo Touvron](https://arxiv.org/search/cs?searchtype=author&query=Touvron,+H), [Thibaut Lavril](https://arxiv.org/search/cs?searchtype=author&query=Lavril,+T), [Gautier Izacard](https://arxiv.org/search/cs?searchtype=author&query=Izacard,+G), [Xavier Martinet](https://arxiv.org/search/cs?searchtype=author&query=Martinet,+X), [Marie-Anne Lachaux](https://arxiv.org/search/cs?searchtype=author&query=Lachaux,+M), [Timothée Lacroix](https://arxiv.org/search/cs?searchtype=author&query=Lacroix,+T), [Baptiste Rozière](https://arxiv.org/search/cs?searchtype=author&query=Rozi%C3%A8re,+B), [Naman Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal,+N), [Eric Hambro](https://arxiv.org/search/cs?searchtype=author&query=Hambro,+E), [Faisal Azhar](https://arxiv.org/search/cs?searchtype=author&query=Azhar,+F), [Aurelien Rodriguez](https://arxiv.org/search/cs?searchtype=author&query=Rodriguez,+A), [Armand Joulin](https://arxiv.org/search/cs?searchtype=author&query=Joulin,+A), [Edouard Grave](https://arxiv.org/search/cs?searchtype=author&query=Grave,+E), [Guillaume Lample](https://arxiv.org/search/cs?searchtype=author&query=Lample,+G)\nView a PDF of the paper titled LLaMA: Open and Efficient Foundation Language Models, by Hugo Touvron and 13 other authors\n[View PDF](https://arxiv.org/pdf/2302.13971)\n> Abstract:We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community. \nSubjects: |  Computation and Language (cs.CL)  \n---|---  \nCite as: | [arXiv:2302.13971](https://arxiv.org/abs/2302.13971) [cs.CL]  \n(or  [arXiv:2302.13971v1](https://arxiv.org/abs/2302.13971v1) [cs.CL] for this version)   \n<https://doi.org/10.48550/arXiv.2302.13971> Focus to learn more arXiv-issued DOI via DataCite  \n## Submission history\nFrom: Gautier Izacard [[view email](https://arxiv.org/show-email/4f64b855/2302.13971)] **[v1]** Mon, 27 Feb 2023 17:11:15 UTC (364 KB) \nFull-text links:\n## Access Paper:\nView a PDF of the paper titled LLaMA: Open and Efficient Foundation Language Models, by Hugo Touvron and 13 other authors\n  * [View PDF](https://arxiv.org/pdf/2302.13971)\n  * [TeX Source](https://arxiv.org/src/2302.13971)\n  * [Other Formats](https://arxiv.org/format/2302.13971)\n\n\n[ ![license icon](https://arxiv.org/icons/licenses/by-4.0.png) view license ](http://creativecommons.org/licenses/by/4.0/ \"Rights to this article\")\nCurrent browse context: \ncs.CL\n[< prev](https://arxiv.org/prevnext?id=2302.13971&function=prev&context=cs.CL \"previous in cs.CL \\(accesskey p\\)\") |  [next >](https://arxiv.org/prevnext?id=2302.13971&function=next&context=cs.CL \"next in cs.CL \\(accesskey n\\)\")\n[new](https://arxiv.org/list/cs.CL/new) |  [recent](https://arxiv.org/list/cs.CL/recent) | [2023-02](https://arxiv.org/list/cs.CL/2023-02)\nChange to browse by: \n[cs](https://arxiv.org/abs/2302.13971?context=cs)\n### References & Citations\n  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2302.13971)\n  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2302.13971)\n  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:2302.13971)\n\n\n### [ 5 blog links](https://arxiv.org/tb/2302.13971)\n([what is this?](https://info.arxiv.org/help/trackback.html)) \n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citation Loading...\n## BibTeX formatted citation\n×\nloading...\nData provided by: \n### Bookmark\n[ ![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png) ](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2302.13971&description=LLaMA: Open and Efficient Foundation Language Models \"Bookmark on BibSonomy\") [ ![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png) ](https://reddit.com/submit?url=https://arxiv.org/abs/2302.13971&title=LLaMA: Open and Efficient Foundation Language Models \"Bookmark on Reddit\")\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\nConnected Papers Toggle\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\nLitmaps Toggle\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\nscite.ai Toggle\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_\nDagsHub Toggle\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\nGotitPub Toggle\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\nHuggingface Toggle\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\nLinks to Code Toggle\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\nScienceCast Toggle\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\nDemos\n# Demos\nReplicate Toggle\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\nSpaces Toggle\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\nSpaces Toggle\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\nCore recommender toggle\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n\nAbout arXivLabs \n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2302.13971) | [Disable MathJax](javascript:setMathjaxCookie\\(\\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n  * [About](https://info.arxiv.org/about)\n  * [Help](https://info.arxiv.org/help)\n\n\n  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)\n  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)\n\n\n  * [Copyright](https://info.arxiv.org/help/license/index.html)\n  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n\n  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n  * [arXiv Operational Status ](https://status.arxiv.org) Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n\n\n",
    "content_quality_score": 0.8,
    "summary": null,
    "child_urls": [
        "https://arxiv.org/abs/2302.13971/#content",
        "https://info.arxiv.org/about/ourmembers.html",
        "https://info.arxiv.org/about/donate.html",
        "https://arxiv.org/IgnoreMe",
        "https://arxiv.org/",
        "https://arxiv.org/list/cs/recent",
        "https://info.arxiv.org/help",
        "https://arxiv.org/search/advanced",
        "https://arxiv.org/login",
        "https://info.arxiv.org/about",
        "https://arxiv.org/search/cs?searchtype=author&query=Touvron,+H",
        "https://arxiv.org/search/cs?searchtype=author&query=Lavril,+T",
        "https://arxiv.org/search/cs?searchtype=author&query=Izacard,+G",
        "https://arxiv.org/search/cs?searchtype=author&query=Martinet,+X",
        "https://arxiv.org/search/cs?searchtype=author&query=Lachaux,+M",
        "https://arxiv.org/search/cs?searchtype=author&query=Lacroix,+T",
        "https://arxiv.org/search/cs?searchtype=author&query=Rozi%C3%A8re,+B",
        "https://arxiv.org/search/cs?searchtype=author&query=Goyal,+N",
        "https://arxiv.org/search/cs?searchtype=author&query=Hambro,+E",
        "https://arxiv.org/search/cs?searchtype=author&query=Azhar,+F",
        "https://arxiv.org/search/cs?searchtype=author&query=Rodriguez,+A",
        "https://arxiv.org/search/cs?searchtype=author&query=Joulin,+A",
        "https://arxiv.org/search/cs?searchtype=author&query=Grave,+E",
        "https://arxiv.org/search/cs?searchtype=author&query=Lample,+G",
        "https://arxiv.org/pdf/2302.13971",
        "https://arxiv.org/abs/2302.13971",
        "https://arxiv.org/abs/2302.13971v1",
        "https://arxiv.org/show-email/4f64b855/2302.13971",
        "https://arxiv.org/src/2302.13971",
        "https://arxiv.org/format/2302.13971",
        "https://arxiv.org/prevnext?id=2302.13971&function=prev&context=cs.CL",
        "https://arxiv.org/prevnext?id=2302.13971&function=next&context=cs.CL",
        "https://arxiv.org/list/cs.CL/new",
        "https://arxiv.org/list/cs.CL/recent",
        "https://arxiv.org/list/cs.CL/2023-02",
        "https://arxiv.org/abs/2302.13971?context=cs",
        "https://arxiv.org/tb/2302.13971",
        "https://info.arxiv.org/help/trackback.html",
        "https://arxiv.org/static/browse/0.3.4/css/cite.css",
        "https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer",
        "https://info.arxiv.org/labs/index.html",
        "https://arxiv.org/auth/show-endorsers/2302.13971",
        "https://info.arxiv.org/help/mathjax.html",
        "https://info.arxiv.org/help/contact.html",
        "https://info.arxiv.org/help/subscribe",
        "https://info.arxiv.org/help/license/index.html",
        "https://info.arxiv.org/help/policies/privacy_policy.html",
        "https://info.arxiv.org/help/web_accessibility.html",
        "https://status.arxiv.org",
        "https://www.cornell.edu/",
        "https://doi.org/10.48550/arXiv.2302.13971",
        "http://creativecommons.org/licenses/by/4.0/",
        "https://ui.adsabs.harvard.edu/abs/arXiv:2302.13971",
        "https://scholar.google.com/scholar_lookup?arxiv_id=2302.13971",
        "https://api.semanticscholar.org/arXiv:2302.13971",
        "http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2302.13971&description=LLaMA: Open and Efficient Foundation Language Models",
        "https://reddit.com/submit?url=https://arxiv.org/abs/2302.13971&title=LLaMA: Open and Efficient Foundation Language Models",
        "https://www.connectedpapers.com/about",
        "https://www.litmaps.co/",
        "https://www.scite.ai/",
        "https://alphaxiv.org/",
        "https://www.catalyzex.com",
        "https://dagshub.com/",
        "http://gotit.pub/faq",
        "https://huggingface.co/huggingface",
        "https://paperswithcode.com/",
        "https://sciencecast.org/welcome",
        "https://replicate.com/docs/arxiv/about",
        "https://huggingface.co/docs/hub/spaces",
        "https://txyz.ai",
        "https://influencemap.cmlab.dev/",
        "https://core.ac.uk/services/recommender",
        "javascript:setMathjaxCookie()",
        "https://subscribe.sorryapp.com/24846f03/email/new",
        "https://subscribe.sorryapp.com/24846f03/slack/new"
    ]
}