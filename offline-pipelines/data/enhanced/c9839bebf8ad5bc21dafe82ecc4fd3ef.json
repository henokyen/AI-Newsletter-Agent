{
    "id": "c9839bebf8ad5bc21dafe82ecc4fd3ef",
    "metadata": {
        "id": "c9839bebf8ad5bc21dafe82ecc4fd3ef",
        "url": "https://superlinked.com/vectorhub/articles/answering-questions-knowledge-graph-embeddings/",
        "title": "Answering Questions with Knowledge Graph Embeddings | VectorHub by Superlinked",
        "properties": {
            "description": "Knowledge Graph Embedding (KGE) algorithms can improve the performance of LLMs in understanding relationships as a context for Q&As.",
            "keywords": null,
            "author": null,
            "og:title": "Answering Questions with Knowledge Graph Embeddings | VectorHub by Superlinked",
            "og:description": "Knowledge Graph Embedding (KGE) algorithms can improve the performance of LLMs in understanding relationships as a context for Q&As.",
            "og:image": "https://innovative-ants-bf39f838ee.media.strapiapp.com/Opengraph_Vector_Hub_27_2b3e141976.jpg",
            "twitter:card": "summary_large_image",
            "twitter:title": "Answering Questions with Knowledge Graph Embeddings | VectorHub by Superlinked",
            "twitter:description": "Knowledge Graph Embedding (KGE) algorithms can improve the performance of LLMs in understanding relationships as a context for Q&As.",
            "twitter:image": "https://innovative-ants-bf39f838ee.media.strapiapp.com/Opengraph_Vector_Hub_27_2b3e141976.jpg"
        }
    },
    "parent_metadata": {
        "id": "796b7befcca20a81abd374947523473b",
        "url": "https://www.notion.so/Knowledge-Graph-Embeddings-796b7befcca20a81abd374947523473b",
        "title": "Knowledge Graph Embeddings",
        "properties": {
            "Type": [
                "Leaf"
            ]
        }
    },
    "content": "[![VectorHub by Superlinked](https://superlinked.com/vectorhub/_next/static/media/vectorhub-logo.d71e47bd.svg)](https://superlinked.com/vectorhub/)\n[Building Blocks](https://superlinked.com/vectorhub/building-blocks)[Articles](https://superlinked.com/vectorhub/all-articles)[Contributing](https://superlinked.com/vectorhub/contributing/contributing)[VDB Comparison](https://superlinked.com/vector-db-comparison/)\n[---](https://github.com/superlinked/superlinked)\nSearch\nSubscribe\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nMenu\nImprove your vector search[![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow-primary.2ff1db1a.svg)Try Superlinked](https://github.com/superlinked/superlinked)\nHome\nManifesto\nMost Recent\nCompare Vector DBs\nContributeSubscribe\nSearch\nTable of Contents\nKnowledge Graphs and missing edges\nKnowledge Graph Embeddings and how they work\nDemo using DistMult KGE\nBuilding and Training a KGE model\nAnswering questions with our model\nComparing KGE with LLM performance on a large Knowledge Graph\nResults\nDisMult limitations\nKGEs for relational data\nContributors\n  1. [Articles](https://superlinked.com/vectorhub/all-articles)\n/\n  2. [Answering questions knowledge ...](https://superlinked.com/vectorhub/articles/answering-questions-knowledge-graph-embeddings/)\n\n\nPublication Date: January 5, 2024|\n#Knowledge Graphs\n|Update on Github\n# Answering Questions with Knowledge Graph Embeddings\n## Takeaways\n[Watch Summary](https://youtu.be/Hk0i67dtd6I)[Notebook Code](https://colab.research.google.com/github/superlinked/VectorHub/blob/main/docs/assets/use_cases/knowledge_graph_embedding/kge_demo.ipynb)\n  * KGE algorithms predict missing edges by embedding entities in vector space - e.g. DistMult for bilinear scoring\n\n\n  * Demo shows 10x better performance vs LLMs on link prediction tasks using HotpotQA benchmark\n\n\n  * KGE outperforms LLMs specifically for relational queries using WikiKG2 dataset\n\n\n  * PyTorch implementation demonstrates embeddings capturing semantic relationships like professions\n\n\n  * Simple implementation using HuggingFace models and PyTorch achieves strong results\n\n\n  * KGE limited by cold start issues and complex multi-hop reasoning requirements\n\n\nLarge Language Models (LLMs) are everywhere, achieving impressive results in all sorts of language-related tasks - language understanding, sentiment analysis, text completion, and so on. But in some domains, including those involving relational data (often stored in Knowledge Graphs), LLMs don't always perform as well. For use cases that require you to capture semantic relationships - like relation extraction and link prediction, specialized approaches that _embed_ relational data can perform much better than LLMs.\nWe look at how Knowledge Graph Embedding (KGE) algorithms can improve performance on some tasks that LLMs have difficulty with, explore some example code for training and evaluating a KGE model, and use the KGE model to perform Q&A tasks. We also compare KGE and LLM performance on a Q&A task.\nLet's get started.\n## Knowledge Graphs and missing edges\nWe use Knowledge Graphs (KGs) to describe how different entities, like people, places, or more generally \"things,\" relate to each other. For example, a KG can show us how a writer is linked to their books, or how a book is connected to its awards:\n![Knowledge Graph example](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/knowledge_graph_embedding/small_kg.png)\nIn domains where understanding these specific connections is crucial - like recommendation systems, search engines, or information retrieval - KGs specialize in helping computers understand the detailed relationships between things.\nThe problem with KGs is that they are usually incomplete. Edges that should be present are missing. These missing links can result from inaccuracies in the data collection process, or simply reflect that our data source is imperfect. In large open-source knowledge bases, [we can observe a _significant_ amount of incompleteness](https://towardsdatascience.com/neural-graph-databases-cc35c9e1d04f):\n> … in Freebase, 93.8% of people have no place of birth, and [78.5% have no nationality](https://aclanthology.org/P09-1113.pdf), [about 68% of people do not have any profession](https://dl.acm.org/doi/abs/10.1145/2566486.2568032), while, in Wikidata, [about 50% of artists have no date of birth](https://arxiv.org/abs/2207.00143), and only [0.4% of known buildings have information about height](https://dl.acm.org/doi/abs/10.1145/3485447.3511932).\nThe **imperfections of KGs** can lead to negative outcomes. For example, in recommendation systems, KG incompleteness can result in **limited or biased recommendations** ; on Q&A tasks, KG incompleteness can yield **substantively and contextually incomplete or inaccurate answers to queries**.\nFortunately, KGEs can help solve problems that plague KGs.\n## Knowledge Graph Embeddings and how they work\nTrained KGE algorithms can generalize and predict missing edges by calculating the likelihood of connections between entities.\nKGE algorithms do this by taking tangled complex webs of connections between entities and turning them into something AI systems can understand: **vectors**. Embedding entities in a vector space allows KGE algorithms to define a **loss function** that measures the discrepancy between embedding similarity and node similarity in the graph. _If the loss is minimal, similar nodes in the graph have similar embeddings_.\nThe KGE model is **trained** by trying to make the similarities between embedding vectors align with the similarities of corresponding nodes in the graph. The model adjusts its parameters during training to ensure that entities that are similar in the KG have similar embeddings. This ensures that vector representations capture the structural and relational aspects of entities in the graph.\nKGE algorithms vary in the similarity functions they employ, and how they define node similarity within a graph. A **simple approach** is to consider nodes that are connected by an edge as similar. Using this definition, learning node embeddings can be framed as a classification task. In this task, the goal is to determine how likely it is that any given pair of nodes have a specific type of relationship (i.e., share a specific edge), given their embeddings.\n## Demo using DistMult KGE\nFor our KGE model demo, we opted for the DistMult KGE algorithm. It works by representing the likelihood of relationships between entities (i.e., similarity) as a _bilinear_ function. Essentially, DisMult KGE assumes that the score of a given triple (comprised of a head entity _h_ , a relationship _r_ , and a tail entity _t_) can be computed as: _h_ ^T (diag)_r_ _t_.\n![DistMult similarity function](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/knowledge_graph_embedding/distmult.png)\ndiagram source: [dglke](https://dglke.dgl.ai/doc/kg.html)\nThe model parameters are learned (internalizing the intricate relationships within the KG) by _minimizing cross entropy between real and corrupted triplets_.\nIn the following two sections we'll walk you through:\n**1. Building and training a DistMult model** **2. Using the model to answer questions**\n### Building and Training a KGE model\nWe use a subgraph of the Freebase Knowledge Graph, a database of general facts (transferred to Wikidata after Freebase Knowledge Graph's 2014 shutdown). This subgraph contains 14541 different entities, 237 different relation types, and 310116 edges in total.\nYou can load the subgraph as follows:\n```\n\nfrom torch_geometric.datasets import FB15k_237\ntrain_data = FB15k_237(\"./data\", split='train')[0]\n\n```\n\nWe'll use PyTorch Geometric, a library built on top of PyTorch, to construct and train the model. PyTorch Geometric is specifically designed for building machine learning models on graph-structured data.\nThe implementation of the DistMult algorithm lies under the `torch_geometric.nn` package. To create the model, we need to specify the following three parameters:\n  * `num_nodes`: The number of distinct entities in the graph (in our case, 14541)\n  * `num_relations`: The number of distinct relations in the graph (in our case, 237)\n  * `hidden_channels`: The dimensionality of the embedding space (for this, we'll use 64)\n\n```\n\nfrom torch_geometric.nn import DistMult\nmodel = DistMult(  num_nodes=train_data.num_nodes,  num_relations=train_data.num_edge_types,  hidden_channels=64)\n\n```\n\nFor additional configuration of the model, please refer to the [PyTorch Geometric documentation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.DistMult.html).\nThe process of **model training in PyTorch** follows a standard set of steps:\nThe first step is **initialization of an optimizer**. The optimizer is a fundamental part of machine learning model training; it adjusts the parameters of the model to reduce loss. In our demo, we use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer.\n```\n\nimport torch.optim as optim\n# 1. initialize optimizeropt = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)\n\n```\n\nSecond, **creation of a data loader**. The purpose of this loader is to return a batch iterator over the entire dataset. The batch size can be adjusted according to the specific requirements of the model and the capacity of the hardware. The loader not only provides an efficient way to load the data but also shuffles it, to ensure that the model is not biased by the order of the training samples.\n```\n\n# 2. create data loader on the training setloader = model.loader(  head_index=train_data.edge_index[0],  rel_type=train_data.edge_type,  tail_index=train_data.edge_index[1],  batch_size=2000,  shuffle=True,)\n\n```\n\nFinally, **execution of the training loop**. This is where the actual learning takes place. The model processes each batch of data, then we compare the actual output to the expected output (labels). The model parameters are then adjusted to bring the outputs closer to the labels. This process continues until the model's performance on a validation set reaches an acceptable level, or a predefined number of iterations has been completed (we opt for the latter in our example).\n```\n\n# 3. usual torch training loopEPOCHS =20model.train()for e inrange(EPOCHS):  l =[]for batch in loader:    opt.zero_grad()    loss = model.loss(*batch)    l.append(loss.item())    loss.backward()    opt.step()print(f\"Epoch {e} loss {sum(l)/len(l):.4f}\")\n\n```\n\nNow that we have a trained model, we can do **some experiments** to see how well the learned embeddings capture semantic meaning. To do so, we will construct 3 fact triplets and then we'll use the model to score these triplets. The triplets (each consisting of a head entity, a relationship, and a tail entity) are:\n  1. France contains Burgundy (which is true)\n  2. France contains Rio de Janeiro (which is not true)\n  3. France contains Bonnie and Clyde (which makes no sense)\n\n```\n\n# Get node and relation IDsfrance = nodes[\"France\"]rel = edges[\"/location/location/contains\"]burgundy = nodes[\"Burgundy\"]riodj = nodes[\"Rio de Janeiro\"]bnc = nodes[\"Bonnie and Clyde\"]\n# Define tripletshead_entities = torch.tensor([france, france, france], dtype=torch.long)relationships = torch.tensor([rel, rel, rel], dtype=torch.long)tail_entities = torch.tensor([burgundy, riodj, bnc], dtype=torch.long)\n# Score triplets using the modelscores = model(head_entities, relationships, tail_entities)print(scores.tolist())>>>[3.890,2.069,-2.666]\n# Burgundy gets the highest score# Bonnie and Clyde gets the lowest (negative) score\n\n```\n\n**Note:** The code snippet provided serves primarily as an illustrative tool for enhancing comprehension. The process of resolving entity names for the [FB15k_237](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.FB15k_237.html#torch_geometric.datasets.FB15k_237) dataset can indeed be somewhat convoluted.\nHere's a [gist](https://gist.github.com/ricsi98/81138cd51e8fe7e15644805c2371bca0) that extracts ID-node index mappings. With these mappings, resolving entity names based on the IDs becomes feasible.\nTo illustrate, here's an [example](https://query.wikidata.org/#SELECT%20%3Fitem%20%3FitemLabel%20WHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP646%20%22%2Fm%2F0mm1q%22.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22%5BAUTO_LANGUAGE%5D%2Cen%22.%20%7D%0A%7D%0A) of how you can resolve entity names using the Wikidata Query Service.\n### Answering questions with our model\nNext, we'll demo how to apply the trained model to answer questions. To answer the question, \"What is Guy Ritchie's profession?\" we start by finding the embedding vectors of the node \"Guy Ritchie\" and the relation \"profession.\"\n```\n\n# Accessing node and relation embeddingsnode_embeddings = model.node_emb.weight\nrelation_embeddings = model.rel_emb.weight\n\n# Accessing embeddings for specific entities and relationsguy_ritchie = node_embeddings[nodes[\"Guy Ritchie\"]]profession = relation_embeddings[edges[\"/people/person/profession\"]]\n\n```\n\nRemember, the DistMult algorithm models connections as a bilinear function of a (head, relation, tail) triplet, so we can express our question as: <Guy Ritchie, profession, ?>. The model will answer with whichever node maximizes this expression. That is, it will find the tail entity (the profession of Guy Ritchie) that results in the highest score when plugged into the bilinear function.\n```\n\n# Creating embedding for the query based on the chosen relation and entityquery = guy_ritchie * profession\n\n# Calculating scores using vector operationsscores = node_embeddings @ query\n\n# Find the index for the top 5 scoressorted_indices = scores.argsort().tolist()[-5:][::-1]# Get the score for the top 5 indextop_5_scores = scores[sorted_indices]\n>>>[('film producer',3.171),# Correct answer('author',2.923),('film director',2.676),('writer',2.557),('artist',2.522)]\n\n```\n\nImpressively, the model **correctly interprets and infers information that isn't explicitly included in the graph** , and provides the right answer to our question. Our model aptly demonstrates KGE's ability to make up for graph incompleteness.\nFurthermore, the fact that the top five relevant entities identified by the model are all professions suggests that the model has successfully learned and understood the concept of a \"profession\" - that is, the model has **discerned the broader context and implications** of \"profession,\" rather than just recognizing the term itself.\nMoreover, these five professions are all closely related to the film industry, suggesting that the model has _not only_ understood the concept of a profession but _also_ narrowed this category to film industry professions specifically; that is, KGE has managed to **grasp the semantic meaning** of the combination of the two query terms: the head entity (Guy Ritchie) and the relation entity (profession), and therefore was able to link the general concept of a profession to the specific context of the film industry, a testament to its ability to capture and interpret semantic meaning.\nIn sum, the model's performance in this scenario demonstrates its potential for **understanding concepts** , **interpreting context** , and **extracting semantic meaning**.\nHere is the [complete code for this demo](https://github.com/superlinked/VectorHub/blob/main/docs/assets/use_cases/knowledge_graph_embedding/kge_demo.ipynb).\n## Comparing KGE with LLM performance on a large Knowledge Graph\nNext, let's compare the performance of KGE and LLMs on the ogbl-wikikg2 dataset, drawn from Wikidata. This dataset includes 2.5 million unique entities, 535 types of relations, and 17.1 million fact triplets. We'll evaluate their performance using hit rates (ratio of correct answers), following the guidelines provided in [Stanford's Open Graph Benchmark](https://ogb.stanford.edu/docs/linkprop/#ogbl-wikikg2).\nFirst, we create textual representations for each node within the graph by crafting sentences that describe their connections, like this: \"[node] [relation1] [neighbor1], [neighbor2]. [node] [relation2] [neighbor3], [neighbor4]. ...\"\nWe then feed these textual representations into a LLM – specifically, the **BAAI/bge-base-en-v1.5** model available on [HuggingFace](https://huggingface.co/BAAI/bge-base-en-v1.5). The embeddings that result from this process serve as our node embeddings.\nFor queries, we take a similar textual representation approach, creating descriptions of the query but omitting the specific entity in question. With these representations in hand, we utilize dot product similarity to find and rank relevant answers.\nFor the KGE algorithm, we employ DistMult with a 250-dimensional embedding space.\n### Results\nYou can see the results on the Open Graph Benchmark query set in the table below:\nmetric/model| Random| LLM| DistMult  \n---|---|---|---  \nHitRate@1| 0.001| 0.0055| **0.065**  \nHitRate@3| 0.003| 0.0154| **0.150**  \nHitRate@10| 0.010| 0.0436| **0.307**  \nWhile the LLM performs three times better than when the nodes are randomly ordered, it's KGE that really stands out as the superior option, with **hit rates almost ten times higher than the LLM**. In addition, DistMult finds the **correct answer on its first try more frequently** than LLM does in ten attempts. DisMult's performance is all the more remarkable when considering that it outperforms LLM even though we used lower-dimensional (250) embeddings with DisMult than the LLM, which outputs 768-dimensional embeddings.\nOur results unequivocally demonstrate **KGE's clear advantage over LLMs for tasks where relational information is important**.\n### DisMult limitations\nWhile DistMult stands out as a simple but powerful tool for embedding KGs, it does have limitations. It struggles with:\n  1. cold starts: When the graph evolves or changes over time, DistMult can't represent new nodes introduced later on, or can't model the effect of new connections introduced to the graph.\n  2. complex questions: While it excels in straightforward question-answering scenarios, the DistMult model falls short when faced with complex questions that demand a deeper comprehension, extending beyond immediate connections. Other KGE algorithms better suit such tasks.\n\n\n## KGEs for relational data\nBecause LLMs have trouble encoding intricate relation structures, their performance suffers when dealing with relational information. Creating a string representation of a node's connections tends to overload the LLM's input. Instead, their strength lies in processing more focused and specific textual information; LLMs are typically not trained to handle broad and diverse information within a single context. KGE algorithms, on the other hand, are specifically designed to handle relational data, and can be further customized to fit the specific needs of a wide variety of use cases.\n## Contributors\n  * [Richárd Kiss, author](https://www.linkedin.com/in/richard-kiss-3209a1186/)\n  * [Mór Kapronczay, contributor](https://www.linkedin.com/in/m%C3%B3r-kapronczay-49447692/)\n  * [Robert Turner, editor](https://robertturner.co/copyedit)\n\n\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nStay updated with VectorHub\nSubscribe\n![arrow](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nContinue Reading\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/research_agent_3a22caf3b8.png)\n#Superlinked\nSimplifying Complex Research with AI\nLearn how to build an AI research assistant that finds, summarizes, and answers questions about scie...\nMarch 24, 2025\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/Improving_RAG_performance_with_Knowledge_Graphs_19_7e05c0ebd4.png)\n#Knowledge Graphs\n#RAG\nImproving RAG performance with Knowledge Graphs\nUsing knowledge graphs to improve the performance of RAG systems by optimising retrieval design arch...\nJanuary 23, 2024\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/Representation_Learning_on_Graph_Structured_Data_21_c627931550.png)\n#Vector Embedding\n#Knowledge Graphs\nRepresentation Learning on Graph Structured Data\nWe evaluate several approaches to vector representation on articles in a subset of the Cora citation...\nJanuary 17, 2024\n[![Superlinked](https://superlinked.com/vectorhub/_next/static/media/superlinked-logo.9ad3911a.svg)](https://superlinked.com/)\nSubscribe\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nBy subscribing, you agree to our [Terms and Conditions](https://www.superlinked.com/policies/terms-and-conditions).\nAbout\n[Company](https://superlinked.com/)[Careers](https://superlinked.notion.site/Work-at-Superlinked-541f2d74ce714c889a1c6dba6dd0dbf1)[Index](https://superlinked.com/vectorhub/siteindex)\nSupport\nContact Us[Terms of Use](https://superlinked.com/policies/terms-and-conditions)[Privacy Policy](https://superlinked.com/policies/privacy-policy)[Cookie Policy](https://superlinked.com/policies/cookie-policy)\nSocial\n[Github](https://github.com/superlinked/superlinked)[X (Twitter)](https://twitter.com/superlinked)[LinkedIn](https://www.linkedin.com/company/superlinked/)\n",
    "content_quality_score": 0.9,
    "summary": null,
    "child_urls": [
        "https://superlinked.com/vectorhub/",
        "https://superlinked.com/vectorhub/building-blocks",
        "https://superlinked.com/vectorhub/all-articles",
        "https://superlinked.com/vectorhub/contributing/contributing",
        "https://superlinked.com/vector-db-comparison/",
        "https://superlinked.com/vectorhub/articles/answering-questions-knowledge-graph-embeddings/",
        "https://superlinked.com/",
        "https://www.superlinked.com/policies/terms-and-conditions",
        "https://superlinked.com/vectorhub/siteindex",
        "https://superlinked.com/policies/terms-and-conditions",
        "https://superlinked.com/policies/privacy-policy",
        "https://superlinked.com/policies/cookie-policy",
        "https://github.com/superlinked/superlinked",
        "https://youtu.be/Hk0i67dtd6I",
        "https://colab.research.google.com/github/superlinked/VectorHub/blob/main/docs/assets/use_cases/knowledge_graph_embedding/kge_demo.ipynb",
        "https://towardsdatascience.com/neural-graph-databases-cc35c9e1d04f",
        "https://aclanthology.org/P09-1113.pdf",
        "https://dl.acm.org/doi/abs/10.1145/2566486.2568032",
        "https://arxiv.org/abs/2207.00143",
        "https://dl.acm.org/doi/abs/10.1145/3485447.3511932",
        "https://dglke.dgl.ai/doc/kg.html",
        "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.DistMult.html",
        "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html",
        "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.FB15k_237.html#torch_geometric.datasets.FB15k_237",
        "https://gist.github.com/ricsi98/81138cd51e8fe7e15644805c2371bca0",
        "https://query.wikidata.org/#SELECT%20%3Fitem%20%3FitemLabel%20WHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP646%20%22%2Fm%2F0mm1q%22.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22%5BAUTO_LANGUAGE%5D%2Cen%22.%20%7D%0A%7D%0A",
        "https://github.com/superlinked/VectorHub/blob/main/docs/assets/use_cases/knowledge_graph_embedding/kge_demo.ipynb",
        "https://ogb.stanford.edu/docs/linkprop/#ogbl-wikikg2",
        "https://huggingface.co/BAAI/bge-base-en-v1.5",
        "https://www.linkedin.com/in/richard-kiss-3209a1186/",
        "https://www.linkedin.com/in/m%C3%B3r-kapronczay-49447692/",
        "https://robertturner.co/copyedit",
        "https://superlinked.notion.site/Work-at-Superlinked-541f2d74ce714c889a1c6dba6dd0dbf1",
        "mailto:support@superlinked.com",
        "https://twitter.com/superlinked",
        "https://www.linkedin.com/company/superlinked/"
    ]
}