[
  {
    "instruction": "[ Tell 120+K peers about your AI research \u2192 Learn more \ud83d\udca1 ![](https://neptune.ai/wp-content/themes/neptune/img/icon-cancel.svg) ](https://neptune.ai/neurips-2024)\n[ ![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg) ](https://neptune.ai \"neptune.ai\")\n  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-registry/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](https://neptune.ai/blog/ml-model-registry/)\n      * [Walkthrough [2 min]](https://neptune.ai/walkthrough)\n      * [Play with public sandbox](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n      * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](https://neptune.ai/blog/ml-model-registry/)\n      * [Neptune vs WandB](https://neptune.ai/vs/wandb)\n      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n      * [Other comparisons](https://neptune.ai/vs)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune Scale projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-registry/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](https://neptune.ai/blog/ml-model-registry/)\n      * [AI Researcher](https://neptune.ai/product/ai-researcher)\n      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n      * [Academia & Kagglers](https://neptune.ai/research)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](https://neptune.ai/blog/ml-model-registry/)\n      * [Monitor training](https://neptune.ai/product/monitor-training)\n      * [Compare experiments](https://neptune.ai/product/compare-experiments)\n      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)\n      * [Reports](https://neptune.ai/product/reports)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/deepsense-ai)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2025/01/Artera-color-signet.png?fit=129%2C97&ssl=1)Case studyHow Neptune Helps Artera Bring AI Solutions to Market Faster ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/artera)\n[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](https://neptune.ai/customers)\n  * [Documentation](https://docs.neptune.ai/)\n  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-registry/)\n    * [Menu Item](https://neptune.ai/blog/ml-model-registry/)\n      * [Blog](https://neptune.ai/blog)\n      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Grici\u016bnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)\n  * [Pricing](https://neptune.ai/pricing)\n  * [Enterprise](https://neptune.ai/product/enterprise)\n  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-registry/)\n    * [Menu Item](https://neptune.ai/blog/ml-model-registry/)\n      * [About us](https://neptune.ai/about-us)\n      * [Customers](https://neptune.ai/customers)\n      * [Careers](https://neptune.ai/jobs)\n      * [Security](https://security.neptune.ai/)\n      * [Contact us](https://neptune.ai/contact-us)\n\n\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find? \nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/) > [Blog](https://neptune.ai/blog) > [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-registry/)\n    * [Categories](https://neptune.ai/blog/ml-model-registry/)\n      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n      * [MLOps](https://neptune.ai/blog/category/mlops)\n      * [LLMOps](https://neptune.ai/blog/category/llmops)\n      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)\n    * [Categories](https://neptune.ai/blog/ml-model-registry/)\n      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)\n      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)\n      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)\n\n\nSearch in Blog... \n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg) ![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog... \n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-registry/)\n    * [Categories](https://neptune.ai/blog/ml-model-registry/)\n      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n      * [MLOps](https://neptune.ai/blog/category/mlops)\n      * [LLMOps](https://neptune.ai/blog/category/llmops)\n      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)\n    * [Categories](https://neptune.ai/blog/ml-model-registry/)\n      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)\n      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)\n      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)\n\n\n  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-registry/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](https://neptune.ai/blog/ml-model-registry/)\n      * [Walkthrough [2 min]](https://neptune.ai/walkthrough)\n      * [Play with public sandbox](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n      * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](https://neptune.ai/blog/ml-model-registry/)\n      * [Neptune vs WandB](https://neptune.ai/vs/wandb)\n      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n      * [Other comparisons](https://neptune.ai/vs)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune Scale projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-registry/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](https://neptune.ai/blog/ml-model-registry/)\n      * [AI Researcher](https://neptune.ai/product/ai-researcher)\n      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n      * [Academia & Kagglers](https://neptune.ai/research)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](https://neptune.ai/blog/ml-model-registry/)\n      * [Monitor training](https://neptune.ai/product/monitor-training)\n      * [Compare experiments](https://neptune.ai/product/compare-experiments)\n      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)\n      * [Reports](https://neptune.ai/product/reports)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/deepsense-ai)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2025/01/Artera-color-signet.png?fit=129%2C97&ssl=1)Case studyHow Neptune Helps Artera Bring AI Solutions to Market Faster ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/artera)\n[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](https://neptune.ai/customers)\n  * [Documentation](https://docs.neptune.ai/)\n  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-registry/)\n    * [Menu Item](https://neptune.ai/blog/ml-model-registry/)\n      * [Blog](https://neptune.ai/blog)\n      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Grici\u016bnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)\n  * [Pricing](https://neptune.ai/pricing)\n  * [Enterprise](https://neptune.ai/product/enterprise)\n  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-registry/)\n    * [Menu Item](https://neptune.ai/blog/ml-model-registry/)\n      * [About us](https://neptune.ai/about-us)\n      * [Customers](https://neptune.ai/customers)\n      * [Careers](https://neptune.ai/jobs)\n      * [Security](https://security.neptune.ai/)\n      * [Contact us](https://neptune.ai/contact-us)\n\n\n[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)\n[Neptune Blog](https://neptune.ai/blog)\n#  ML Model Registry: The Ultimate Guide \n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Stephen-Oladele.png?fit=560%2C560&ssl=1)\n[ Stephen Oladele  ](https://neptune.ai/blog/author/stephen-oladele)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg) 13 min \n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg) 29th August, 2024 \n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[MLOps](https://neptune.ai/blog/category/mlops)\n![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 3 2'%3E%3C/svg%3E)\nImagine you are the only data scientist on your team, you start working on a machine learning project and perform a series of experiments that produce various ML models (and artifacts) that you \u201ctrack\u201d through non-standard naming conventions. Since the naming conventions you used for your model files were unclear, it took you a while to find the most optimal model you trained. \nWhen you finally did, you decided to either hand the raw model artifacts over to the operations team or, worse, deploy it yourself. When you handed the raw artifacts to your operations team, they requested more information like:\n  * 1 How to use it? \n  * 2 If the model is tested? \n  * 3 The runtime dependencies for the model? \n  * 4 And other crucial operational information \n\n\nBecause all you did was build the model and hand it off, it was perhaps difficult to collaborate with them for a successful deployment.\nNow imagine another scenario. \nYour company is planning on shipping out more ml-based products/features. A data scientist, engineer, and maybe even a product manager joins the team. When you were working alone, although your model was a pain to deploy (if it was deployed), your workflow worked. Now that you have new teammates, and they started to ask you about your model versions, you realize that storing the ML models in files is not that manageable after all. \nThis is the moment when you really feel the pain of not having an efficient way to share your model versions, model artifacts, and model metadata. And the thing is, at this point, there is no easy way to go back in time and set up something proper. \nYour new and improved cross-functional team is asking you about:\n  * Where can we find the best version of this model so we can audit, test, deploy, or reuse it?\n  * How was this model trained?\n  * How can we track the docs for each model to make sure they are compliant and people can know the necessary details about it including the metadata?\n  * How can we review models before they are put to use or even after they have been deployed?\n  * How can we integrate with tools and services that make shipping new projects easier?\n\n\nCan you blame them?\nThey want to understand what is running in production, and how to improve it or roll back to previous versions. It makes total sense. \nSo with all that pretty experience you gained, you start your next project and look for a tool that deals with it. And you find this article about the machine learning model registry. \nSo let\u2019s dive in and tell you all about it!\n## What is model registry?\nAn ML model registry serves as a centralized repository, enabling effective model management and documentation. It allows for clear naming conventions, comprehensive metadata, and improved collaboration between data scientists and operations teams, ensuring smooth deployment and utilization of trained models.\n### How does machine learning model registry work?\nA data scientist can push trained models to the model registry. Once in the registry, your models are ready to be tested, validated, and deployed to production in a workflow that is similar to the one below:\n![Model registry](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/model-registry.png?resize=1500%2C785&ssl=1)Model registry | Author: Harshil Patel\nWhile Model Registry serves as a repository of your machine learning model artifacts, there are however some differences, as a repository is a subset feature of a registry. Let\u2019s understand the distinctions between Model Registry and other similar-sounding terms.\n### Model registry vs model repository\n**Model Repository is a storage location for machine learning models** , while a Model Registry is a more comprehensive system that tracks and manages the full lifecycle of machine learning models.\nHowever, both are often used interchangeably, and the specific definitions may vary depending on the context or the tools and platforms being used. \nIn a nutshell, broad differences on a conceptual level look like this:\nFeature  |  Model Repository  |  Model Registry   \n---|---|---  \nStorage |  Stores ML models in a file format |  Stores machine learning models and associated metadata  \nVersion control |  No |  Yes  \nModel lineage tracking |  No |  Yes  \nAccess control |  No |  Yes  \nOther features |  None |  Can track the performance of models over time, ensure reliability and reproducibility of models, and protect sensitive models from unauthorized access  \n### Model registry vs model store\n**[Model store](https://neptune.ai/blog/mlops-model-stores) is a superset of a Model registry**. Meaning within a model store, you can find the model registry component. The store is a service other services can interface with to retrieve models from the registry. \nIn a model registry, you may store and fetch models (like a docker registry), but in a model store, you can have the full package of logging, discovery, assets, pipelines, metadata, all model information, and even blueprints to use for building new models. Examples of typical model stores are ModelStore or ClearML.\n### Model registry vs experiment tracking\n**Model registry has to integrate with the[Experiment management system](https://neptune.ai/blog/ml-experiment-tracking) (which tracks the experiments) to register ML models** from various experiment runs to make them easier to find and work with. Let\u2019s take a look at some of the key differences between a model registry and an experiment management system.\nParameters  |  Model registry  |  Experiment tracking   \n---|---|---  \nPurpose |  To store trained, production-ready, and retired models in a central repository. |  To track experiment runs of different parameter configurations and combinations.  \nPriority |  To make sure models are discoverable and can be accessed by any user or system. |  To make sure experiments are easier to manage and collaborate on.  \nIntegration |  Integrates with the experiment tracking system to register models from successful experiments including the model and experiment metadata. |  Integrates with the training pipeline to perform experiment runs and track experiment details including the dataset version and metadata.  \nMLOps |  A crucial piece of MLOps and production models. |  Most useful in the model development phase, has an indirect impact on operationalizing the model.  \n## Model registry key features and functionalities\n![Workflow components of an ideal model registry](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/model-registry-4.png?resize=1800%2C942&ssl=1)Workflow components of an ideal model registry | Author: Stephen Oladele\n### Acts as centralized storage for effective collaboration\nModel registry provides a central storage unit that holds ML models (including model artifacts) for easy retrieval by an application (or service). Without the model registry, the model artifacts would be stored in files that are difficult to track and saved to whatever source code repository is established.\nThe centralized storage also enables data teams to have a single view of the status of all ML models, making collaboration easier. Here is an example showing a single view of different models with model artifacts stored in a model registry.\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/app-screenshot/bar-buttons.svg)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button--test-tube.svg) See in the app  ](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/table?viewId=9cb5bc7c-3bce-4c69-8f5c-90d3d9cc682c) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/icon-zoom.svg) Full screen preview \nA list of different model versions and associated metadata tracked in neptune.ai \n### Bridges the gap between experiment and production activities\nModel registry acts as a glue between ML experimentation and Operations, enabling model development, software development, and operational teams to collaborate.\n![A model registry is a central component of MLOps](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/model-registry-1.png?resize=1800%2C942&ssl=1)A model registry is a central component of MLOps | Modified based on: [YouTube](https://youtu.be/WrieKPgXZyo?t=643)\n### Providing a central UI \nModel registry provides teams with visibility over their ML models. With a central interface, teams can: \n  * Search for models,\n  * View the status of models (if they are being staged, deployed, or retired),\n  * Approve or disapprove models across different stages,\n  * And view the necessary documentation. \n\n\nThis makes model discovery easier for everyone on the team. If a model needs to be deployed, the operations teams can easily:\n  * Search for it,\n  * Look up the validation results and other metrics,\n  * Package the model (if needed),\n  * And move it from the staging environment to the production environment. \n\n\nThis improves the way cross-functional teams collaborate on ML projects.\n### Provides an interface for downstream systems to consume models\nModel registries provide interfaces that enable downstream services to consume the model through API integration. The integration can also track offline and online evaluation metrics for the models. \nThis makes it easy to build an automated setup with CI/CD/CT with ML pipelines. The downstream service could be either a model user, an automated job, or a REST serving that can consume the most stable\u2014or any\u2014version of the model. \n### Enables model versioning by tracking different versions of a model\nModel registry enables [model versioning](https://neptune.ai/blog/top-model-versioning-tools) by tracking the different versions of a model as they are developed and improved. This allows you to compare different versions of the model, track its performance, and select the best version for deployment.\nHere\u2019s how it typically works:\n  * **Model Registration:** When a new model is developed or trained, it is registered in the model registry.\n  * **Version Control:** The model registry maintains a history of all registered models and their versions. \n  * **Model Comparison:** The model registry allows users to compare performance metrics, model architectures, hyperparameters, and other relevant information in different versions of a model.\n  * **Model Tracking:** As new versions of the model are developed or trained, they are registered in the model registry as well, incrementing the version number.\n  * **Retention and Archiving:** The model registry typically retains older versions of the models, ensuring a complete history and traceability.\n\n\nBy enabling model versioning, the model registry ensures that different iterations of a model can be stored, tracked, compared, and accessed conveniently.\n### Integrates with experiment management systems or training pipelines\nModel registry integrates with systems that output the trained ML models. They could be the raw model artifacts (model weights, configuration, and metadata) or models that have been serialized into a file (e.g., an ONNX file) for compatibility with the production environment or containerized (using Docker) to be exported to the production environment.\nModel registries:\n  * Register the model, \n  * Assign a version to it,\n  * Note the version of the dataset the model was trained on,\n  * Add annotations and tags,\n  * Retrieve the parameters, validation results (including metrics and visualizations), and other relevant model metadata on the model from the experiment management system.\n\n\nTo make collaboration easier, the registry also includes details such as:\n  * The model owner or developer, \n  * Experiment run id the model was trained under, \n  * Versioned model source code, \n  * Environment runtime dependencies used to train the model (and the versions),\n  * Comments and model change history, \n  * And the model documentation.\n\n\n### Integrates with the staging environment for your trained ML models\nModel Registry provides the functionality for integrating with the staging environment for running all types of checks and balances on the model. These checks can include integration testing (with other applications) and other QA tests before the model can be promoted to the production environment. \nSharing and collaboration should be enabled for ML models in this environment so that deployment engineers can work with data scientists to test models and ensure they are good to deploy.\nIn the staging environment, the model reviewers should also be able to perform fairness checks on the model to ensure it:\n  * Outputs explainable results, \n  * Complies with regulatory requirements, \n  * And provides useful business benefits.\n\n\nGenerally, the governance and approval workflows should be configured in this environment. There should also be access level control and secure authorization to models in this environment, especially models trained on data with sensitive information.\n### Integrate with model delivery (CI/CD) tools and services for automation\nAutomation is a critical part of building any scalable software. In machine learning, building automated pipelines will allow you to spend more time building new products rather than maintaining old models. \nA Model registry integrates with pipeline automation tools and provides custom APIs that can allow you to plug into custom workflow tools. For example, using webhooks to trigger downstream actions based on predefined events in the registry. \nYou should also be able to configure model promotion schemes through different environments like development (training), staging (testing), and production (serving). Performance is a crucial requirement for building automated pipelines. Model registries should be highly available for automated jobs that are event or schedule-based to enable continuous training and delivery of the model.\n### Integrate with model deployment tools\nEventually, ML models have to be deployed, and the more efficient the deployment process, the better. Model Registries:\n  * Integrate with downstream services and REST serving services that can consume the model and serve it in the production environment.\n  * Collect real-time (or aggregated) metrics on the production model to log performance details of the model. This will be helpful for comparison between models (deployed and staged), as well as auditing the production model for review\n\n\n## Why do you need a model registry?\nModel registry enables machine learning operationsand fits well with the pillars of MLOps:\n  * 1 Model Deployment \n  * 2 Model Management \n  * 3 Model Governance \n  * 4 Model Security \n\n\n### ML model registry enables faster deployment of your models\nAs we discussed earlier, one of the ways a model registry enables collaboration is by bridging the gap between experimentation and production activities. This results in a faster rollout of your production ML models. \nIn addition, model registries store trained models for fast and easy retrieval by any integrated application or one of the [model deployment tools](https://neptune.ai/blog/best-ml-model-deployment-tools), which is ultimately what you want in an ideal automation setup. \nWith a model registry, software engineers and reviewers can easily identify and select only the best version of the trained ML models (based on the evaluation metrics), so the model can be tested, reviewed, and released to production. This makes it a good unifying component for both training and deployment pipelines, as there is less friction in the hand-off of production-ready models from experiment to production environments.\n### ML model registry simplifies model lifecycle management\nWhen you work in a large organization with lots of experiments running, many ML models, and cross-functional teams, managing the lifecycle of these models is often a challenging process. \nModel registry helps tackle this challenge by simplifying the management of your model lifecycle. With the registry, you can:\n  * Register, track, and version your trained, deployed, and retired models in a central repository that is organized and searchable.\n  * Store the metadata for your trained models, as well as their runtime dependencies so the deployment process is eased.\n  * Build automated pipelines that make CI/CD and training of your production model possible.\n  * Compare models running in production (champion models) to freshly trained models (or challenger models) in the staging environment.\n\n\nThe registry can also track and store online and offline evaluation metrics for the models. With this functionality, you can easily look up models that are in production to detect a drop in the performance of the model (or [concept drift](https://en.wikipedia.org/wiki/Concept_drift)). \nYou can also compare their online and offline performance to see which of the production models need to be reviewed, maintained, or archived.\nNot only can you track evaluation metrics for the model both in production and training, but you can also track the system metrics to understand which models are consuming the most application resources (CPU, memory, and GPU usage). \n### ML model registry enables model governance\nOne thing that the model registry does really well is centralizing ML models and organizing their relevant details. With the registry, you have a central source of truth for your models throughout different stages of their lifecycle, including: \n  * Development, \n  * Validation,\n  * Deployment,\n  * And monitoring.\n\n\nThis helps create visibility and model discovery which is very crucial for ML models that require thorough regulatory complaint processes in specific industries such as health, finance, and law.\nA user in charge of ensuring legal compliance should be able to easily review the models in the registry and understand: \n  * How the model was trained,\n  * What version of data the model is trained on,\n  * The conditions a model performs best and produces consistent results at, being well-informed of the model\u2019s capabilities and limitations,\n  * Exposure to any compliance risk in case of regulated industries like financial institutions\n\n\nA standard model registry will also enforce the documentation and reporting of models, ensuring results are repeatable and can be reproduced by any auditing user. \n**Review** , **Approve** , **Release** , and **Rollback** are all steps in the model launch process that the registry may help with. These choices are based on a variety of factors, including offline performance, bias, fairness measures, and the results of online experiments.\n### ML model registry improves model security\nML models, as well as the underlying packages used to build them, must be scanned for vulnerabilities, especially when a large number of packages are used to develop and deploy the models. \nA model registry can manage specific versions of the packages, and you can scan and remove security vulnerabilities that may pose a threat to the system.\nModels are likewise vulnerable to adversarial attacks, and as a result, they must be maintained and secured. In some cases, the[ least privilege access security concept](https://en.wikipedia.org/wiki/Principle_of_least_privilege) must be employed so that only authorized users have access to specified model information, data privacy, and protecting PII and other resources.\n## Where does a model registry fit in the MLOps stack?\nIf you want to run machine learning projects efficiently and at scale, you would most likely need to add a model registry to your MLOps stack. Depending on what[ level of implementation](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning) you are in your MLOps stack, your needs and requirements for a model registry would differ. Where does it fit? \nWell, recall we learned earlier that the model registry sits between machine learning model development and deployment. \n### Model registry in MLOps level 0\nIf you are at level 0 implementation of MLOps, your workflow with a model registry could look like this: \n![MLOps level 0 workflow with a model registry](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/model-registry-3.png?resize=1800%2C942&ssl=1)MLOps level 0 workflow with a model registry | Modified based on: [Source](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#mlops_level_0_manual_process)\nThe output from the experimentation step is fed into the model registry. This involves a manual process where the data scientist prepares the model artifact and metadata and could also package them (serialization, containerization) before registering them. \nThe operations team can push the packaged model to the staging environment for testing before deploying it to a prediction service engine that can integrate with other applications.\n### Model registry in MLOps level 1\nAs opposed to level 0 (where the workflow is a manual process), the goal of the workflow in [level 1](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#mlops_level_1_ml_pipeline_automation) is to perform continuous training of the model by automating the ML pipeline. This is one process a model registry enables well because of its ability to integrate with the pipeline. \nAt this level, the entire pipeline is deployed, and when ML models are trained on the provided dataset, the output (trained model and metadata) is fed into the model registry where it can be staged, and if it passes the necessary tests and checks, it can be fed to the continuous delivery pipeline for release.\n[![MLOps level 1 workflow with a model registry](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/model-registry-1-1.png?resize=1920%2C1120&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/model-registry-1-1.png?ssl=1)MLOps level 1 workflow with a model registry | Modified based on: [Source](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#mlops_level_1_ml_pipeline_automation)\n### Model registry in MLOps level 2\nThe role of the model registry in level 2 of the MLOps workflow is also the same as that of level 1\u2014the automated pipeline delivers the trained model to the model registry where it is staged, may be passed through QA checks, and sent to the continuous delivery pipeline:\n[![Stages of the CI/CD automated ML pipeline with a model registry](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/model-registry-2.png?resize=1800%2C942&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/model-registry-2.png?ssl=1)Stages of the CI/CD automated ML pipeline with a model registry |Modified based on: [Source](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#mlops_level_2_cicd_pipeline_automation)\nThe model registry serves as a crucial component in any automated pipeline because event triggers can be integrated with it to promote models with good metrics upon re-training on fresh data or archive models.\n## Setting up machine learning model registry: build, maintain, or buy?\nSetting up a model registry for your MLOps workflow will require you to decide on either **Building** one, **Maintaining** one, or **Buying** one. Let\u2019s take a close look at each one of these decisions and the factors to consider before making a choice.\n### Building a model registry solution\nLike any software solution, if you understand the key functionalities and requirements, you can build a system yourself. This is the case with a model registry. You may want to set up the following:\n  * Object storage for models and artifacts.\n  * Database for logging model details.\n  * API integration for both receiving models, promoting models across various environments, and collecting model information from the different environments.\n  * User interface (UI) for ML teams to interact with a visual workflow. \n\n\nWhile building the solution yourself might seem ideal, you should consider the following factors:\n  * **Incentive** : What\u2019s the incentive to build out your solution? Is it for customization or for owning a proprietary license to the solution?\n  * **Human resources** : Do you have the talents and skills to build out your solution?\n  * **Time** : How long would it take you to build out a solution and is it worth the wait?\n  * **Operations** : When the solution is eventually built out, who would maintain its operations?\n  * **Cost** : What would it cost you to build a solution, including the maintenance of the solution?\n\n\n### Maintaining a self-hosted model registry\nAnother option to consider\u2014if you do not want to build out a solution\u2014is to maintain an existing solution yourself. In this scenario, the solution has been built out already, but you might have to manage some features, such as object storage and the database. Most of these existing solutions are open-source solutions.\nThe following are the factors to consider:\n  * **Type of solution:** Are you going to opt for an open-source solution with no license cost or a closed-source solution with license cost?\n  * **Operations** : Who is going to manage the solution? Does the solution support consistent maintenance and software updates?\n  * **Cost** : What is the cost of operating the solution in terms of the infrastructure to host it and the running cost? \n  * **Features** : What features have already been implemented, and what features do you have to build and manage yourself? Is it worth adopting compared to building out your solution?\n  * **Support** : What type of support is available in case things break during operations? Is there a community or dedicated customer support channel? For open-source solutions, while you might have a community, you will likely lack the necessary developer support required to fix things.\n  * **Accessibility** : How easy is it to get started with the solution? Is the documentation comprehensive enough? Can everyone from the model reviewers to the model developers and software engineers intuitively use the solution?\n\n\n### Purchase the license to a fully-managed solution\nThe final option to consider is subscribing to a fully managed solution where the operations and management of the registry are handled by the solution vendor. In this case, you do not have to worry about building or maintaining a solution. You just have to ensure your systems and services can integrate with the registry.\nHere are the factors to consider: \n  * **Industry type** : What type of industry is the model built for? What sensitive information have the models learned? Are there data privacy compliance measures? Is the model only allowed to stay on-premise?\n  * **Features** : Are the key features and functionalities of any model registry available in this solution? What extra features are available, and how relevant are they to your workflow?\n  * **Cost** : What\u2019s the cost of purchasing a license, and do the features justify the cost?\n  * **Security** : How secure is the platform hosting the solution? Is it resistant to third-party attacks?\n  * **Performance** : Is the registry highly performant? For situations where models are too large, can the registry provide models for services to consume at low latency?\n  * **Availability** : What\u2019s the uptime of the solution, and does it meet your required service level agreement (SLA)?\n  * **Support** : What level of support is available in case things go south?\n  * **Accessibility** : How easy is it to get started with the solution? Are the documentation and learning support decent enough? What\u2019s the learning curve in terms of usage?\n\n\nThat being said, there are lots of options available out there, and it totally depends on the team/organization\u2019s requirements to choose. \nNow, let\u2019s take a look at some of the model registry solutions on the market.\n## ML model registry solutions out there\nHere are the most popular ones available out there that offer a host of Model Registry features.\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-horizontal-dots.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down--white.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-right--blue.svg) Expand all\nAzure ML Studio \n![](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/10/azure-machine-learning-service-logo-445C459FD8-seeklogo.com_.png?fit=280%2C300&ssl=1)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-link-external.svg) Website  ](https://azure.microsoft.com/en-us/free/machine-learning/search/?ef_id=_k_CjwKCAiA9dGqBhAqEiwAmRpTC5UcF7QWOTePzATWywK5W8GvxuLc69PCDaad9oTX29NuhaDTjTibLxoCysAQAvD_BwE_k_&OCID=AIDcmm4rphvbww_SEM__k_CjwKCAiA9dGqBhAqEiwAmRpTC5UcF7QWOTePzATWywK5W8GvxuLc69PCDaad9oTX29NuhaDTjTibLxoCysAQAvD_BwE_k_&gad_source=1&gclid=CjwKCAiA9dGqBhAqEiwAmRpTC5UcF7QWOTePzATWywK5W8GvxuLc69PCDaad9oTX29NuhaDTjTibLxoCysAQAvD_BwE)\nMLflow \n![](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlFlow-integration-with-Neptune.png?fit=151%2C86&ssl=1)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-link-external.svg) Website  ](https://mlflow.org/)\nVerta.ai \n![](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/09/Verta-logo.jpg?fit=1200%2C627&ssl=1)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-link-external.svg) Website  ](https://www.verta.ai/)\nAWS Sagemaker \n![](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/sagemaker-2.png?fit=301%2C301&ssl=1)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-link-external.svg) Website  ](https://aws.amazon.com/sagemaker/)\nneptune.ai \n![](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/09/neptune_signet_smaller.png?fit=140%2C140&ssl=1)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-link-external.svg) Website  ](https://neptune.ai/)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-horizontal-dots.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down--white.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-right--blue.svg) Expand all\nAzure ML Studio \n![](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/10/azure-machine-learning-service-logo-445C459FD8-seeklogo.com_.png?fit=280%2C300&ssl=1)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-link-external.svg) Website  ](https://azure.microsoft.com/en-us/free/machine-learning/search/?ef_id=_k_CjwKCAiA9dGqBhAqEiwAmRpTC5UcF7QWOTePzATWywK5W8GvxuLc69PCDaad9oTX29NuhaDTjTibLxoCysAQAvD_BwE_k_&OCID=AIDcmm4rphvbww_SEM__k_CjwKCAiA9dGqBhAqEiwAmRpTC5UcF7QWOTePzATWywK5W8GvxuLc69PCDaad9oTX29NuhaDTjTibLxoCysAQAvD_BwE_k_&gad_source=1&gclid=CjwKCAiA9dGqBhAqEiwAmRpTC5UcF7QWOTePzATWywK5W8GvxuLc69PCDaad9oTX29NuhaDTjTibLxoCysAQAvD_BwE)\nMLflow \n![](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/mlFlow-integration-with-Neptune.png?fit=151%2C86&ssl=1)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-link-external.svg) Website  ](https://mlflow.org/)\nVerta.ai \n![](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/09/Verta-logo.jpg?fit=1200%2C627&ssl=1)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-link-external.svg) Website  ](https://www.verta.ai/)\nAWS Sagemaker \n![](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/sagemaker-2.png?fit=301%2C301&ssl=1)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-link-external.svg) Website  ](https://aws.amazon.com/sagemaker/)\nneptune.ai \n![](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/09/neptune_signet_smaller.png?fit=140%2C140&ssl=1)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-link-external.svg) Website  ](https://neptune.ai/)\nModel versioning \n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-cancel.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\nCI/CD/CT compatibility \n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-cancel.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-cancel.svg)\nModel packaging \n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-cancel.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-cancel.svg)\nModel searching \n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\nAccess control, model review, and promoting models \n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-cancel.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\nModel lineage and evaluation history \n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-cancel.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-checkmark.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/big-table/icon-cancel.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down--white.svg)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-right--blue.svg) Expand all\n## MLflow model registry\nType: Open-source/Managed\nOptions: Managed (self-hosted), fully-managed offering.\n![MLflow model registry dashboard](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/07/ml-model-registry-the-ultimate-guide-12.png?resize=900%2C667&ssl=1)MLflow model registry dashboard | [Source](https://www.mlflow.org/docs/latest/model-registry.html#ui-workflow)\nThe [MLflow ](https://www.mlflow.org/docs/latest/model-registry.html#mlflow-model-registry)Model Registry component is a centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of an MLflow Model. It provides model lineage (which MLflow experiment and run produced the model), model versioning, stage transitions (for example, from staging to production), and annotations. The model registry component was one of the clamored needs of MLflow users in 2019. \nThe MLflow Model Registry is one of the few open-source model registries available in the market today. You can decide to [manage this](https://www.mlflow.org/docs/latest/quickstart.html) on your infrastructure or use a [fully-managed implementation](https://docs.databricks.com/mlflow/index.html) on a platform like Databricks.\n### MLflow model registry features\nMLflow provides \n  * **Annotation and description tools** for tagging models, providing documentation and model information such as the date the model was registered, modification history of the registered model, the model owner, stage, version, and so on.\n  * **Model versioning** to automatically keep track of versions for registered models when updated.\n  * An **API integration** to serve ML models as RESTful APIs for online testing, dashboard updates, etc.\n  * **CI/CD workflow integration** to record stage transitions, request, review, and approve changes as part of CI/CD pipelines for better control and governance.\n  * A **model stage feature** to assign preset or custom stages to each model version, like \u201cStaging\u201d and \u201cProduction\u201d to represent the lifecycle of a model.\n  * **Configuration for promotion schemes** to easily move models across different stages.\n\n\nMLflow stores:\n  * The model artifacts,\n  * Metadata,\n  * Parameters,\n  * Metrics.\n\n\nThe pricing will depend on the option you are opting for\u2014a self-hosted, open-source solution or a fully-managed offering.\nYou can learn more about the workflow [here](https://www.mlflow.org/docs/latest/model-registry.html#model-registry-workflows) and get started with MLflow Model Registry [here](https://www.mlflow.org/docs/latest/quickstart.html).\n## Amazon Sagemaker model registry \nType: Bundled with SageMaker\u2019s free-tier and on-demand pricing.\nOptions: Fully managed offering\n![Amazon SageMaker model registry](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/07/imageml-model-registry-the-ultimate-guide-13.png?resize=444%2C402&ssl=1)Amazon SageMaker model registry | [Source](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-details.html)\nAmazon SageMaker is a fully managed service that developers can use for every step of ML development, including model registry. The model registry is part of the suite of MLOps offerings in SageMaker that helps users build and operationalize machine learning solutions by automating and standardizing MLOps practices across their organization.\n### AWS Sagemaker model registry features\nWith the SageMaker Model Registry you can do the following:\n  * Catalog models for production.\n  * Manage model versions.\n  * Associate metadata, such as training metrics, with a model.\n  * Manage the approval status of a model.\n  * Deploy models to production.\n  * Automate model deployment with CI/CD.\n\n\nYou can make a model group to keep track of all the ML models you\u2019ve trained to solve a specific problem. Each model you train can then be registered, and the model registry will add it to the model group as a new model version. A typical workflow might look like the following:\n  * Create a model group.\n  * Create an ML pipeline with [SageMaker Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-build.html) that trains a model.\n  * For each run of the ML pipeline, create a model version that you register in the model group you created in the first step.\n\n\nThe cost of using the Model Registry is bundled with the SageMaker [pricing tiers](https://aws.amazon.com/sagemaker/pricing/). You can learn more about the SageMaker Model Registry in the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html).\n## Verta.ai model registry\nType: Proprietary, with Open-source, SaaS, and Enterprise offerings.\nOptions: Fully managed offering.\n![Verta.ai model registry dashboard ](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/07/ml-model-registry-the-ultimate-guide-14.png?resize=1572%2C1634&ssl=1)Verta.ai model registry dashboard | [Source](https://blog.verta.ai/introducing-verta-model-registry)\nThe Verta.ai Model Registry helps you manage your ML models in one place. It provides features that enable you to package, validate, and reliably promote release-ready models and apply safe release and governance practices.\n### Verta.ai model registry features\n  * It provides a unified hub to publish release-ready models by allowing you to: \n    * Connect to an experiment management system for end-to-end information tracking.\n    * Publish all the model metadata, documentation, and model artifacts in one central repository.\n    * Select the best-fit models from model experiments and stage them for release.\n    * Record state transitions and manage the release lifecycle from development, staging, and production to archive.\n  * It enables model validation and CI/CD automation by allowing you to: \n    * Integrate with existing CI/CD pipelines like Jenkins, Chef, and so on.\n    * Use webhooks to trigger downstream actions for model validation and deployment.\n    * Automatically track model versions and tagged releases.\n  * Set up granular access control editors, reviewers, and collaborators.\n  * Access detailed audit logs for compliance.\n  * Release models once they pass basic security and fairness checks.\n\n\nYou can learn more about the open-source version and pricing tiers available on this [page](https://www.verta.ai/pricing). You can learn more about the Verta Model Registry on this [page](https://blog.verta.ai/introducing-verta-model-registry).\n## neptune.ai\nType: Proprietary, with free and paid offerings.\nOptions: Managed (self-hosted), fully-managed offering.\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/app-screenshot/bar-buttons.svg)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button--test-tube.svg) See in the app  ](https://app.neptune.ai/o/showcase/org/e2e-tracking/runs/table?viewId=9cb5bc7c-3bce-4c69-8f5c-90d3d9cc682c) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/icon-zoom.svg) Full screen preview \nModel versions tracked in Neptune \n[Neptune](https://neptune.ai/) is primarily an experiment tracker, but it provides model registry functionality to some extent. \nIt\u2019s a lightweight solution that fits well into any workflow and lets you track, compare, and share all kinds of metadata about your experiments and models. \nOn the model registry front, it will prove to be the most useful in the model handover phase. Since you can store your model versions in Neptune and attach any metadata or artifacts to them \u2013 it can serve as a connection between the development and deployment phases. ML Engineers can access model artifacts in Neptune instantly, and they can get all the information about how the model was trained, what data was used, who ran the experiment, etc. \n[ ![](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/05/Resources-model-registry-FI-12.22.jpg?fit=200%2C105&ssl=1) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--resource.svg) Learn more  How to Track Your Model Metadata in Neptune  Read docs  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://docs.neptune.ai/model_registry/)\n### Neptune features\nNeptune\u2019s lets you:\n  * Keep track of your experiments, model versions and production-ready models;\n  * Attach any metadata or artifacts to models;\n  * Save hash, location, and other model artifact metadata (by the way, you don\u2019t have to upload the model to Neptune, you can keep track of the model reference to local or S3-compatible storage);\n  * Use tags to highlight the stage of the model (e.g. production or staging);\n  * Access your models via web app and API.\n\n![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n##  **Editor\u2019s note**\nDo you feel like experimenting with neptune.ai?\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Create a free account](https://neptune.ai/register) right away and give it a go\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Try it out first](https://docs.neptune.ai/usage/quickstart/) and learn how it works (zero setup, no registration)\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[See the docs](https://docs.neptune.ai/) or watch a short [product demo (2 min)](https://neptune.ai/walkthrough)\n\n\n## You\u2019ve reached the end!\nBy now, we\u2019ve established the fact that model registries enable the successful operationalization of machine learning projects, and it does so by:\n  * helping you speed up your roll-out process\n  * making your ML experiments easy to manage\n  * making collaboration easier\n  * creating a seamless hand-off for your team\n  * increasing security and governance\n\n\nThe ideal next step for you is to pick up a solution and see if it improves your MLOps workflow. \nLet this article be your guide to making a concrete decision based on your level of MLOps implementation and the factors worth taking into account. \nHere are some additional resources you might want to read for the same.\n  * [Practitioners guide to MLOps: A framework for continuous delivery and automation of machine learning](https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf)\n  * [MLflow Model Registry \u2013 Databricks](https://databricks.com/product/mlflow-model-registry)\n  * [MLflow Model Registry \u2014 MLflow 1.21.0 documentation](https://www.mlflow.org/docs/latest/model-registry.html#mlflow-model-registry)\n  * [neptune.ai\u2013 Model metadata documentation](https://docs.neptune.ai/model_registry/)\n  * [ML Pipelines on Google Cloud ](https://www.coursera.org/learn/ml-pipelines-google-cloud/home/welcome)\n  * [MLOps: Continuous delivery and automation pipelines in machine learning (google.com)](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#mlops_level_2_cicd_pipeline_automation)\n  * [Model Registry Makes MLOps Work \u2013 Here\u2019s Why \u2013 neptune.ai](https://neptune.ai/blog/model-registry-makes-mlops-work)\n  * [MLOps Model Stores: Definition, Functionality, Tools Review \u2013 neptune.ai](https://neptune.ai/blog/mlops-model-stores)\n  * [Introducing Verta Model Registry](https://blog.verta.ai/introducing-verta-model-registry)\n  * [Verta Model Registry | Manage all your AI\u2011ML models in one place](https://www.verta.ai/platform/model-registry)\n  * [Top 3 Reasons You Need a Model Registry (verta.ai)](https://blog.verta.ai/top-3-reasons-you-need-a-model-registry)\n  * [Simplifying MLOps with Model Registry](https://youtu.be/WrieKPgXZyo)\n\n\nThat\u2019s it for now, stay tuned for more! Adios!\n##  Was the article useful? \n![yes](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--yes.svg) Yes  ![no](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--no.svg) No \n![](https://neptune.ai/wp-content/themes/neptune/img/icon-bulb.svg) Suggest changes \nYour email Your message (optional)\nThis site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply. \nI am familiar with the [Privacy Policy](https://neptune.staginglab.eu/privacy-policy)*\nSubmit\n\u0394\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/i-box/header-icon.svg)\n### **More about** ML Model Registry: The Ultimate Guide \n####  Check out our  **product resources** and  **related articles** below: \n[ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  MLOps Landscape in 2024: Top Tools and Platforms  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/mlops-tools-platforms-landscape) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-bulb.svg) Product resource  How Elevatus Can Now Find Any Information About a Model in a Minute  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/customers/elevatus)\n###  Explore more content topics: \n[ Computer Vision ](https://neptune.ai/blog/category/computer-vision) [ General ](https://neptune.ai/blog/category/general) [ LLMOps ](https://neptune.ai/blog/category/llmops) [ ML Model Development ](https://neptune.ai/blog/category/machine-learning-model-development) [ ML Tools ](https://neptune.ai/blog/category/machine-learning-tools) [ MLOps ](https://neptune.ai/blog/category/mlops) [ Natural Language Processing ](https://neptune.ai/blog/category/natural-language-processing) [ Paper Reflections ](https://neptune.ai/blog/category/paper-reflections) [ Product Updates ](https://neptune.ai/blog/category/product-updates) [ Reinforcement Learning ](https://neptune.ai/blog/category/reinforcement-learning) [ Tabular Data ](https://neptune.ai/blog/category/tabular-data) [ Time Series ](https://neptune.ai/blog/category/time-series-forecasting)\nAbout neptune.ai ![chevron](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1920 1080'%3E%3C/svg%3E) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-play.svg)\nNeptune is the experiment tracker purpose-built for foundation model training. \nIt lets you monitor and visualize thousands of per-layer metrics\u2014losses, gradients, and activations\u2014at any scale. Drill down into logs and debug training issues fast. Keep your model training stable while reducing wasted GPU cycles. \n[ Play with a live project  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://demo.neptune.ai/) [ See Docs  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://docs.neptune.ai/)\nTable of contents\n![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-small.svg)\n  1. What is model registry?\n  2. Model registry key features and functionalities\n  3. Why do you need a model registry?\n  4. Where does a model registry fit in the MLOps stack?\n  5. Setting up machine learning model registry: build, maintain, or buy?\n  6. ML model registry solutions out there\n  7. MLflow model registry\n  8. Amazon Sagemaker model registry \n  9. Verta.ai model registry\n  10. neptune.ai\n  11. You\u2019ve reached the end!\n\n\n![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-cancel.svg) ![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-more.svg)\nCheck also: [Building a Machine Learning Platform [Definitive Guide]](https://neptune.ai/blog/ml-platform-guide)\n##  **Monitor your model training at scale**\nJoin 60,000+ researchers and practitioners who use Neptune to debug training failures, spot anomalies, and compare experiments.\n[ Try for free  ](https://app.neptune.ai/register) [ Get early access to Neptune Scale  ](https://neptune.ai/early-access)\n[ Take an interactive product tour  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)\n![](https://neptune.ai/wp-content/themes/neptune/img/footer-post-illustration.svg)\n#### Newsletter\nTop articles, case studies, events (and more) in your inbox every month.\nGet Newsletter\n  * [Product](https://neptune.ai/blog/ml-model-registry/)\n    * [Walkthrough](https://neptune.ai/walkthrough)\n    * [Documentation](https://docs.neptune.ai/)\n    * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [Pricing](https://neptune.ai/pricing)\n    * [Service status](https://status.neptune.ai/)\n\n\n  * [Solutions](https://neptune.ai/blog/ml-model-registry/)\n    * [AI Researcher](https://neptune.ai/product/ai-researcher)\n    * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n    * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n    * [Enterprise](https://neptune.ai/product/enterprise)\n    * [Academic Research](https://neptune.ai/research)\n\n\n  * [Compare](https://neptune.ai/blog/ml-model-registry/)\n    * [Neptune vs Weights & Biases](https://neptune.ai/vs/wandb)\n    * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n    * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n    * [Other comparisons](https://neptune.ai/vs)\n    * [ML experiment tracking tools](https://neptune.ai/blog/best-ml-experiment-tracking-tools)\n\n\n  * [Community](https://neptune.ai/blog/ml-model-registry/)\n    * [Blog](https://neptune.ai/blog)\n    * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n    * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n    * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n    * [How to Build an Experiment Tracker](https://neptune.ai/blog/build-experiment-tracking-tool)\n\n\n  * [Company](https://neptune.ai/blog/ml-model-registry/)\n    * [Customers](https://neptune.ai/customers)\n    * [About us](https://neptune.ai/about-us)\n    * [Careers](https://neptune.ai/jobs)\n    * [Security portal and SOC 2](https://security.neptune.ai/)\n    * [Contact us](https://neptune.ai/contact-us)\n\n\n[ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-linked-in.svg) ](https://www.linkedin.com/company/neptuneai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-twitter.svg) ](https://twitter.com/neptune_ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-github.svg) ](https://github.com/neptune-ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-facebook.svg) ](https://www.facebook.com/neptuneAI) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-youtube.svg) ](https://www.youtube.com/channel/UCvOJU-ubyUqxGSDRN7xK4Ng) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-spotify.svg) ](https://open.spotify.com/show/4kGi82i4wTYgHbWmVMri5x) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-apple-podcast.svg) ](https://podcasts.apple.com/us/podcast/mlops-live/id1634179447)\n[ Terms of Service ](https://neptune.ai/terms-of-service) [ Privacy Policy ](https://neptune.ai/privacy-policy) [ SLA ](https://neptune.ai/service-level-agreement)\nCopyright \u00a9 2025 Neptune Labs. All rights reserved.\n![](https://neptune.ai/wp-content/themes/neptune/img/footer-stars--414w.png) ![](https://pixel.wp.com/g.gif?v=ext&blog=211928962&post=6285&tz=0&srv=neptune.ai&hp=atomic&ac=2&amp=0&j=1%3A14.5-a.9&host=neptune.ai&ref=&fcp=148&rand=0.8779923549144177)\n![](https://bat.bing.com/action/0?ti=97011748&tm=gtm002&Ver=2&mid=d54aefce-c4b5-45b8-979e-d844a2c6f4bb&bo=1&sid=37ef08200d8e11f0a36ed785bff3ae58&vid=37ef08800d8e11f089240dd13df19027&vids=1&msclkid=N&pi=918639831&lg=en-US&sw=1080&sh=600&sc=30&tl=ML%20Model%20Registry%3A%20The%20Ultimate%20Guide&p=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&r=&lt=235&evt=pageLoad&sv=1&cdb=AQAQ&rn=168145)\n![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=951179ee-3d54-4aa9-ab43-7e8ef5047752&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=a924f868-6ceb-476b-8130-4fe9d10222db&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=951179ee-3d54-4aa9-ab43-7e8ef5047752&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=a924f868-6ceb-476b-8130-4fe9d10222db&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31) ![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=b553bd0a-8564-4e92-bbac-fe2365c53d2b&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=a924f868-6ceb-476b-8130-4fe9d10222db&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=b553bd0a-8564-4e92-bbac-fe2365c53d2b&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=a924f868-6ceb-476b-8130-4fe9d10222db&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=53bc47f9-09fd-46eb-9a7f-cead4e27896f&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=a924f868-6ceb-476b-8130-4fe9d10222db&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=53bc47f9-09fd-46eb-9a7f-cead4e27896f&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=a924f868-6ceb-476b-8130-4fe9d10222db&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=0cd91c53-40c5-43b5-87dd-25f7efa0c079&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=a924f868-6ceb-476b-8130-4fe9d10222db&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=0cd91c53-40c5-43b5-87dd-25f7efa0c079&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=a924f868-6ceb-476b-8130-4fe9d10222db&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=49ab8153-78c2-4bb6-b721-2e05eb5b7530&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=a924f868-6ceb-476b-8130-4fe9d10222db&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=49ab8153-78c2-4bb6-b721-2e05eb5b7530&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=a924f868-6ceb-476b-8130-4fe9d10222db&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=65c1cf85-a249-4a86-b989-77dacea438d8&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=a924f868-6ceb-476b-8130-4fe9d10222db&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=65c1cf85-a249-4a86-b989-77dacea438d8&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=a924f868-6ceb-476b-8130-4fe9d10222db&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-registry&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe article discusses the importance of a Machine Learning (ML) model registry, which serves as a centralized repository for managing model versions, metadata, and collaboration between teams. It highlights key features such as model versioning, integration with CI/CD tools, and governance, ultimately improving deployment speed and lifecycle management in MLOps.\n```"
  },
  {
    "instruction": "Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters\n[Lightning AI Studios: Never set up a local environment again \u2192](https://lightning.ai)\n[\u2190 Back to blog](https://lightning.ai/blog)\n#  Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters\nPosted on April 12, 2023 by [Sebastian Raschka](https://lightning.ai/pages/author/sebastian-raschka/ \"Posts by Sebastian Raschka\") - [Articles](https://lightning.ai/pages/category/community/article/), [Tutorials](https://lightning.ai/pages/category/community/tutorial/)\n#### Table of Contents\n  * [Finetuning Large Language Models](https://lightning.ai/pages/community/article/understanding-llama-adapters/#finetuning-large-language-models)\n  * [Parameter-Efficient Finetuning](https://lightning.ai/pages/community/article/understanding-llama-adapters/#parameter-efficient-finetuning)\n  * [Prompt Tuning And Prefix Tuning](https://lightning.ai/pages/community/article/understanding-llama-adapters/#prompt-tuning-and-prefix-tuning)\n  * [Adapters](https://lightning.ai/pages/community/article/understanding-llama-adapters/#adapters)\n  * [Extending Prefix Tuning and Adapters: LLaMA-Adapter](https://lightning.ai/pages/community/article/understanding-llama-adapters/#extending-prefix-tuning-and-adapters-llama-adapter)\n  * [Conclusion](https://lightning.ai/pages/community/article/understanding-llama-adapters/#conclusion)\n\n\n### Key takeaway\nLearn how popular parameter-efficient finetuning methods for LLM work: prefix tuning, adapters, and LLaMA-Adapter.\nIn the rapidly evolving field of artificial intelligence, utilizing large language models in an efficient and effective manner has become increasingly important.\nParameter-efficient finetuning stands at the forefront of this pursuit, allowing researchers and practitioners to reuse pretrained models while minimizing their computational and resource footprints. It also allows us to train AI models on a broader range of hardware, including devices with limited computational power, such as laptops, smartphones, and IoT devices. Lastly, with the increasing focus on environmental sustainability, parameter-efficient finetuning reduces the energy consumption and carbon footprint associated with training large-scale AI models.\nThis article explains the broad concept of finetuning and discusses popular parameter-efficient alternatives like prefix tuning and adapters. Finally, we will look at the recent LLaMA-Adapter method and see how we can use it in practice.\n## Finetuning Large Language Models\nSince GPT-2 ([Radford et al.](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)) and GPT-3 ([Brown et al.](https://arxiv.org/abs/2005.14165)), we have seen that generative large language models (LLMs) pretrained on a general text corpus are capable of in-context learning, which doesn\u2019t require us to further train or finetune pretrained LLMs if we want to perform specific or new tasks that the LLM wasn\u2019t explicitly trained on. Instead, we can directly provide a few examples of a target task via the input prompt, as illustrated in the example below.\n![In-context learning example.](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/in-context.png)\nIn-context learning is a valuable and user-friendly method for situations where direct access to the large language model (LLM) is limited, such as when interacting with the LLM through an API or user interface. However, if we have access to the LLM, adapting and finetuning it on a target task using data from a target domain usually leads to superior results. So, how can we adapt a model to a target task? There are three conventional approaches outlined in the figure below.\n![the three classic finetuning approaches](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/classic-flowchart.png)\nThese methods above are compatible with generative (decoder-style) models such as GPT and embedding-focused (encoder-style) models such as BERT. In contrast to these three approaches, in-context learning only applies to generative models. It\u2019s also worth highlighting that when we finetune generative models, we work with and build on the embeddings they create instead of the generated output texts.\n**Feature-based approach**\nIn the feature-based approach, we load a pretrained LLM and apply it to our target dataset. Here, we are particularly interested in generating the output embeddings for the training set, which we can use as input features to train a classification model. While this approach is particularly common for embedding-focused like BERT, we can also extract embeddings from generative GPT-style models (you can find an example in our blog post [here](https://lightning.ai/pages/blog/gradient-accumulation/)).\nThe classification model can then be a logistic regression model, a random forest, or XGBoost \u2014 whatever our hearts desire. (However, based on my experience, linear classifiers like logistic regression perform best here.)\nConceptually, we can illustrate the feature-based approach with the following code:\n```\n\nmodel = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n# ...\n# tokenize dataset\n# ...\n# generate embeddings\n@torch.inference_mode()\ndef get_output_embeddings(batch):\n  output = model(\n    batch[\"input_ids\"],\n    attention_mask=batch[\"attention_mask\"]\n  ).last_hidden_state[:, 0]\nreturn {\"features\": output}\ndataset_features = dataset_tokenized.map(\n get_output_embeddings, batched=True, batch_size=10)\nX_train = np.array(dataset_features[\"train\"][\"features\"])\ny_train = np.array(dataset_features[\"train\"][\"label\"])\nX_val = np.array(dataset_features[\"validation\"][\"features\"])\ny_val = np.array(dataset_features[\"validation\"][\"label\"])\nX_test = np.array(dataset_features[\"test\"][\"features\"])\ny_test = np.array(dataset_features[\"test\"][\"label\"])\n# train classifier\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\nprint(\"Training accuracy\", clf.score(X_train, y_train))\nprint(\"Validation accuracy\", clf.score(X_val, y_val))\nprint(\"test accuracy\", clf.score(X_test, y_test))\n\nExpandCopy\n\n```\n\n(Interested readers can find the full code example [here](https://github.com/rasbt/blog-finetuning-llama-adapters/blob/main/three-conventional-methods/1_distilbert-feature-extractor.ipynb).)\n**Finetuning I \u2014 Updating The Output Layers**\nA popular approach related to the feature-based approach described above is finetuning the output layers (we will refer to this approach as  _finetuning I_). Similar to the feature-based approach, we keep the parameters of the pretrained LLM frozen. We only train the newly added output layers, analogous to training a logistic regression classifier or small multilayer perceptron on the embedded features.\nIn code, this would look as follows:\n```\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n  \"distilbert-base-uncased\",\n   num_labels=2 # suppose target task is a binary classification task\n) \n# freeze all layers\nfor param in model.parameters():\n  param.requires_grad = False\n# then unfreeze the two last layers (output layers)\nfor param in model.pre_classifier.parameters():\n  param.requires_grad = True\nfor param in model.classifier.parameters():\n  param.requires_grad = True\n# finetune model\nlightning_model = CustomLightningModule(model)\ntrainer = L.Trainer(\n  max_epochs=3,\n  ...\n)\ntrainer.fit(\n model=lightning_model,\n train_dataloaders=train_loader,\n val_dataloaders=val_loader)\n# evaluate model\ntrainer.test(lightning_model, dataloaders=test_loader)\n\nExpandCopy\n\n```\n\n(Interested readers can find the complete code example [here](https://github.com/rasbt/blog-finetuning-llama-adapters/blob/main/three-conventional-methods/2_finetune-last-layers.ipynb).)\nIn theory, this approach should perform similarly well, in terms of modeling performance and speed, as the feature-based approach since we use the same frozen backbone model. However, since the feature-based approach makes it slightly easier to pre-compute and store the embedded features for the training dataset, the feature-based approach may be more convenient for specific practical scenarios.\n**Finetuning II \u2014 Updating All Layers**\nWhile the original BERT paper ([Devlin et al.](https://arxiv.org/abs/1810.04805)) reported that finetuning only the output layer can result in modeling performance comparable to finetuning all layers, which is substantially more expensive since more parameters are involved. For instance, a BERT base model has approximately 110 million parameters. However, the final layer of a BERT base model for binary classification consists of merely 1,500 parameters. Furthermore, the last two layers of a BERT base model account for 60,000 parameters \u2014 that\u2019s only around 0.6% of the total model size.\nOur mileage will vary based on how similar our target task and target domain is to the dataset the model was pretrained on. But in practice, finetuning all layers almost always results in superior modeling performance.\nSo, when optimizing the modeling performance, the gold standard for using pretrained LLMs is to update all layers (here referred to as finetuning II). Conceptually finetuning II is very similar to finetuning I. The only difference is that we do not freeze the parameters of the pretrained LLM but finetune them as well:\n```\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n  \"distilbert-base-uncased\",\n   num_labels=2 # suppose target task is a binary classification task\n) \n# don't freeze layers\n# for param in model.parameters():\n#  param.requires_grad = False\n# finetune model\nlightning_model = LightningModel(model)\ntrainer = L.Trainer(\n  max_epochs=3,\n  ...\n)\ntrainer.fit(\n model=lightning_model,\n train_dataloaders=train_loader,\n val_dataloaders=val_loader)\n# evaluate model\ntrainer.test(lightning_model, dataloaders=test_loader)\n\nExpandCopy\n\n```\n\n(Interested readers can find the complete code example here.)\nIf you are curious about some real-world results, the code snippets above were used to train a movie review classifier using a pretrained DistilBERT base model (you can access the code notebooks here): Feature-based approach with logistic regression: 83% test accuracy Finetuning I, updating the last 2 layers: 87% accuracy Finetuning II, updating all layers: 92% accuracy. These results are consistent with the general rule of thumb that finetuning more layers often results in better performance, but it comes with increased cost.\n![finetuning performance trade-offs](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/classic-performance.png)\n## **Parameter-Efficient Finetuning**\nIn the previous sections, we learned that finetuning more layers usually leads to better results. Now, the experiments above are based on a DistilBERT model, which is relatively small. What if we want to finetune larger models that only barely fit into GPU memory, for example, the latest generative LLMs? We can use the feature-based or finetuning I approach above, of course. But suppose we want to get a similar modeling quality as finetuning II?\nOver the years, researchers developed several techniques ([Lialin et al.](https://arxiv.org/abs/2303.15647)) to finetune LLM with high modeling performance while only requiring the training of only a small number of parameters. These methods are usually referred to as parameter-efficient finetuning techniques (PEFT).\nSome of the most widely used PEFT techniques are summarized in the figure below.\n![popular LLM finetuning methods](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/popular-methods.png)\nOne PEFT technique that recently made big waves is LLaMA-Adapter, which was proposed for Meta\u2019s popular LLaMA model ([Touvron et al.](https://arxiv.org/abs/2302.13971)) \u2014 however, while LLaMA-Adapter was proposed in the context of LLaMA, the idea is model-agnostic.\nTo understand how LLaMA-Adapter works, we have to take a (small) step back and review two related techniques called  _prefix tuning_ and  _adapters_ \u2014 LLaMA-Adapter ([Zhang et al.](https://arxiv.org/abs/2303.16199)) combines and extends both of these ideas.\nSo, in the remainder of this article, we will discuss the various concepts of prompt modifications to understand prefix tuning and adapter methods before we take a closer look at LLaMA-Adapter. (And we will save low-rank adaptation for a future article.)\n## **Prompt Tuning And Prefix Tuning**\nThe original concept of prompt tuning refers to techniques that vary the input prompt to achieve better modeling results. For example, suppose we are interested in translating an English sentence into German. We can ask the model in various different ways, as illustrated below.\n![an example of hard-prompting](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/hard-prompting.png)\nNow, this concept illustrated above is referred to as  _hard_ prompt tuning since we directly change the discrete input tokens, which are not differentiable. \nIn contrast to  _hard_ prompt tuning,  _soft_ prompt tuning concatenates the embeddings of the input tokens with a trainable tensor that can be optimized via backpropagation to improve the modeling performance on a target task. \nA specific flavor of prompt tuning is prefix tuning ([Li and Liang](https://arxiv.org/abs/2101.00190)). The idea in prefix tuning is to add a trainable tensor to each transformer block instead of only the input embeddings, as in  _soft_ prompt tuning. The following figure illustrates the difference between a regular transformer block and a transformer block modified with a prefix.\n![prefix-tuning for LLMs](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/prefix-tuning.png)\nNote that in the figure above, the \u201cfully connected layers\u201d refer to a small multilayer perceptron (two fully connected layers with a nonlinear activation function in-between). These fully connected layers embed the soft prompt in a feature space with the same dimensionality as the transformer-block input to ensure compatibility for concatenation. Using (Python) pseudo-code, we can illustrate the difference between a regular transformer block and a prefix-modified transformer block as follows:\n![transformer blog with prefix code](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/prefix-code.png)\nAccording to the original [prefix tuning](https://arxiv.org/abs/2101.00190) paper, prefix tuning achieves comparable modeling performance to finetuning all layers while only requiring the training of 0.1% of the parameters \u2014 the experiments were based on GPT-2 models. Moreover, in many cases, prefix tuning even outperformed the finetuning of all layers, which is likely because fewer parameters are involved, which helps reduce overfitting on smaller target datasets.\nLastly, to clarify the use of soft prompts during inference: after learning a soft prompt, we have to supply it as a prefix when performing the specific task we finetuned the model on. This allows the model to tailor its responses to that particular task. Moreover, we can have multiple soft prompts, each corresponding to a different task, and provide the appropriate prefix during inference to achieve optimal results for a particular task.\n## **Adapters**\nThe original  _adapter_ method ([Houlsby et al.](https://arxiv.org/abs/1902.00751)) is somewhat related to the aforementioned  _prefix tuning_ as they also add additional parameters to each transformer block. However, instead of prepending prefixes to the input embeddings, the adapter method adds adapter layers in two places, as illustrated in the figure below.\n![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/adapter-outline.png)\nAnd for readers who prefer (Python) pseudo-code, the adapter layer can be written as follows:\n![LLM Adapter Code](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/adapter.png)\nNote that the fully connected layers of the adapters are usually relatively small and have a bottleneck structure similar to autoencoders. Each adapter block\u2019s first fully connected layer projects the input down onto a low-dimensional representation. The second fully connected layer projects the input back into the input dimension. How is this parameter efficient? For example, assume the first fully connected layer projects a 1024-dimensional input down to 24 dimensions, and the second fully connected layer projects it back into 1024 dimensions. This means we introduced 1,024 x 24 + 24 x 1,024 = 49,152 weight parameters. In contrast, a single fully connected layer that reprojects a 1024-dimensional input into a 1,024-dimensional space would have 1,024 x 1024 = 1,048,576 parameters.\nAccording to the original [adapter paper](https://arxiv.org/abs/1902.00751), a BERT model trained with the adapter method reaches a modeling performance comparable to a fully finetuned BERT model while only requiring the training of 3.6% of the parameters.\nNow, the question is how the adapter method compares to prefix tuning. Based on the original [prefix tuning paper](https://arxiv.org/abs/2101.00190), the adapter method performed slightly worse than the prefix tuning method when 0.1% of the total number of model parameters were tuned. However, when the adapter method is used to tune 3% of the model parameters, the method ties with prefix tuning of 0.1% of the model parameters. So, we may conclude that the prefix tuning method is the more efficient of the two.\n## **Extending Prefix Tuning and Adapters: LLaMA-Adapter**\nExtending the ideas of prefix tuning and the original adapter method, researchers recently proposed LLaMA-Adapter ([Zhang et al.](https://arxiv.org/abs/2303.16199)), a parameter-efficient finetuning method for [LLaMA](https://github.com/facebookresearch/llama) (LLaMA is a popular GPT-alternative by Meta).\nLike  _prefix tuning_ , the LLaMA-Adapter method prepends tunable prompt tensors to the embedded inputs. It\u2019s worth noting that in the LLaMA-Adapter method, the prefix is learned and maintained within an embedding table rather than being provided externally. Each transformer block in the model has its own distinct learned prefix, allowing for more tailored adaptation across different model layers.\nIn addition, LLaMA-Adapter introduces a zero-initialized attention mechanism coupled with gating. The motivation behind this so-called  _zero-init_ attention and gating is that adapters and prefix tuning could potentially disrupt the linguistic knowledge of the pretrained LLM by incorporating randomly initialized tensors (prefix prompts or adapter layers), resulting in unstable finetuning and high loss values during initial training phases.\nAnother difference compared to prefix tuning and the original adapter method is that LLaMA-Adapter adds the learnable adaption prompts only to the  _L_ topmost transformer layers instead of all transformer layers. The authors argue that this approach enables more effective tuning of language representations focusing on higher-level semantic information.\nWhile the basic idea of the LLaMA adapter method is related to prefix tuning (prepending tunable soft prompts), there are some additional, subtle differences in how this is implemented. For instance, only a self-attention input\u2019s key and value sequences are modified via the tunable soft prompt. Then, depending on the gating factor (which is set to zero at the beginning of the training), the prefix-modified attention is either used or not. This concept is illustrated in the visualization below.\n![llama-adapter outline](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/llama-adapter.png)\nIn pseudo-code, we may express this as follows:\n![llama-adapter pseudo-code](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/llama-adapter-code-1.png)\nIn short, the differences between LLaMA-Adapter and regular prefix tuning are that LLaMA-Adapter only modifies the top (i.e., the first few) transformer blocks and introduces a gating mechanism to stabilize the training. While the researchers specifically experiment with LLaMA, their proposed Adapter method is a general method that can also be applied to other types of LLMs (like GPT).\nUsing the LLaMA-Adapter approach, the researchers were able to finetune a 7 billion parameter LLaMA model in only 1 hour (using eight A100 GPUs) on a dataset consisting of 52k instruction pairs. Furthermore, the finetuned LLaMA-Adapter model outperformed all other models compared in this study on question-answering tasks, while only 1.2 M parameters (the adapter layers) needed to be finetuned.\nIf you want to check out the LLaMA-Adapter method, you can find the original implementation on top of the GPL-licensed LLaMA code [here](https://github.com/ZrrSkywalker/LLaMA-Adapter).\nAlternatively, if your use cases are incompatible with the GPL license, which requires you to open source all derivative works under a similar license, check out the [Lit-LLaMA GitHub repository](https://github.com/Lightning-AI/lit-llama). Lit-LLaMA is a readable implementation of LLaMA on top of the Apache-licensed nanoGPT code, which has less restrictive licensing terms.\nSpecifically, if you are interested in finetuning a LLaMA model using the LLaMA-Apapter method, you can run the\n```\n\npython finetune_adapter.py\n\nExpandCopy\n\n```\n\nscript from the [Lit-LLaMA GitHub repository](https://github.com/Lightning-AI/lit-llama).\n## **Conclusion**\nFinetuning pre-trained large language models (LLMs) is an effective method to tailor these models to suit specific business requirements and align them with target domain data. This process involves adjusting the model parameters using a smaller dataset relevant to the desired domain, which enables the model to learn domain-specific knowledge and vocabulary.\nHowever, as LLMs are \u201clarge,\u201d updating multiple layers in a transformer model can be very expensive, so researchers started developing parameter-efficient alternatives.\nIn this article, we discussed several parameter-efficient alternatives to the conventional LLM finetuning mechanism. In particular, we covered prepending tunable soft prompts via prefix tuning and inserting additional adapter layers.\nFinally, we discussed the recent and popular LLaMA-Adapter method that prepends tunable soft prompts and introduces an additional gating mechanism to stabilize the training.\nIf you want to try this out in practice, check out [the Lit-LLaMA repository](https://github.com/Lightning-AI/lit-llama) \u2014 questions and suggestions for additional parameter-efficient finetuning methods are very welcome! (Preferably via the \ud83e\udd99[lit-llama channel on Discord](https://discord.com/invite/XncpTy7DSt)) \n**Acknowledgments**\nI want to thank Carlos Mocholi, Luca Antiga, and Adrian Waelchli for the constructive feedback to improve the clarity of this article.\n#### Table of Contents\n  * [Finetuning Large Language Models](https://lightning.ai/pages/community/article/understanding-llama-adapters/#finetuning-large-language-models)\n  * [Parameter-Efficient Finetuning](https://lightning.ai/pages/community/article/understanding-llama-adapters/#parameter-efficient-finetuning)\n  * [Prompt Tuning And Prefix Tuning](https://lightning.ai/pages/community/article/understanding-llama-adapters/#prompt-tuning-and-prefix-tuning)\n  * [Adapters](https://lightning.ai/pages/community/article/understanding-llama-adapters/#adapters)\n  * [Extending Prefix Tuning and Adapters: LLaMA-Adapter](https://lightning.ai/pages/community/article/understanding-llama-adapters/#extending-prefix-tuning-and-adapters-llama-adapter)\n  * [Conclusion](https://lightning.ai/pages/community/article/understanding-llama-adapters/#conclusion)\n\n\n#### Related Content\n[](https://lightning.ai/pages/community/announcements/lightning-ai-joins-ai-alliance-to-advance-open-safe-responsible-ai/)\n#####  Lightning AI Joins AI Alliance To Advance Open, Safe, Responsible AI\n[Read More](https://lightning.ai/pages/community/announcements/lightning-ai-joins-ai-alliance-to-advance-open-safe-responsible-ai/)\n[](https://lightning.ai/pages/community/tutorial/doubling-neural-network-finetuning-efficiency-with-16-bit-precision-techniques/)\n![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/bf16-featured-300x132.png)\n#####  Doubling Neural Network Finetuning Efficiency with 16-bit Precision Techniques\n[Read More](https://lightning.ai/pages/community/tutorial/doubling-neural-network-finetuning-efficiency-with-16-bit-precision-techniques/)\n[](https://lightning.ai/pages/community/lora-insights/)\n![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage7-300x177.png)\n#####  Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments\n[Read More](https://lightning.ai/pages/community/lora-insights/)\n![](https://t.co/1/i/adsct?=&bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%22%22%3A%22%22%7D&event_id=205f78f0-fe18-4c1e-95d2-bbc67046c355&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=bf4e31f2-1b6d-4b60-9bfb-4bc9b580a5db&tw_document_href=https%3A%2F%2Flightning.ai%2Fpages%2Fcommunity%2Farticle%2Funderstanding-llama-adapters%2F&tw_iframe_status=0&txn_id=p06ii&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?=&bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%22%22%3A%22%22%7D&event_id=205f78f0-fe18-4c1e-95d2-bbc67046c355&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=bf4e31f2-1b6d-4b60-9bfb-4bc9b580a5db&tw_document_href=https%3A%2F%2Flightning.ai%2Fpages%2Fcommunity%2Farticle%2Funderstanding-llama-adapters%2F&tw_iframe_status=0&txn_id=p06ii&type=javascript&version=2.3.31)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis article explores parameter-efficient finetuning methods for large language models (LLMs), focusing on prefix tuning, adapters, and the LLaMA-Adapter. These techniques minimize resource usage while enhancing model performance, crucial for sustainable AI development. LLaMA-Adapter combines prefix tuning and adapters, optimizing training efficiency and effectiveness.\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\n# \n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png)](https://huggingface.co/google)\n[google](https://huggingface.co/google)\n/\n[paligemma-3b-mix-448](https://huggingface.co/google/paligemma-3b-mix-448)\nlike 107\nFollow\n![](https://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png) Google 10.7k\n[ Image-Text-to-Text ](https://huggingface.co/models?pipeline_tag=image-text-to-text)[ Transformers ](https://huggingface.co/models?library=transformers)[ Safetensors ](https://huggingface.co/models?library=safetensors)[ paligemma ](https://huggingface.co/models?other=paligemma)[ text-generation-inference ](https://huggingface.co/models?other=text-generation-inference)\narxiv: 20 papers\nLicense: gemma\n[ Model card ](https://huggingface.co/google/paligemma-3b-mix-448)[ Files Files and versions ](https://huggingface.co/google/paligemma-3b-mix-448/tree/main)[ Community 10 ](https://huggingface.co/google/paligemma-3b-mix-448/discussions)\nTrain \nDeploy \nUse this model \n##  Access PaliGemma on Hugging Face\nThis repository is publicly accessible, but you have to accept the conditions to access its files and content.\nTo access PaliGemma on Hugging Face, you\u2019re required to review and agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged-in to Hugging Face and click below. Requests are processed immediately.\n[Log in](https://huggingface.co/login?next=%2Fgoogle%2Fpaligemma-3b-mix-448) or [Sign Up](https://huggingface.co/join?next=%2Fgoogle%2Fpaligemma-3b-mix-448) to review the conditions and access this model content.\n  * [PaliGemma model card](https://huggingface.co/google/paligemma-3b-mix-448/#paligemma-model-card \"PaliGemma model card\")\n    * [Model information](https://huggingface.co/google/paligemma-3b-mix-448/#model-information \"Model information\")\n      * [Model summary](https://huggingface.co/google/paligemma-3b-mix-448/#model-summary \"Model summary\")\n      * [Model data](https://huggingface.co/google/paligemma-3b-mix-448/#model-data \"Model data\")\n    * [How to Use](https://huggingface.co/google/paligemma-3b-mix-448/#how-to-use \"How to Use\")\n    * [Use in Transformers](https://huggingface.co/google/paligemma-3b-mix-448/#use-in-transformers \"Use in Transformers\")\n      * [Running the default precision (`float32`) on CPU](https://huggingface.co/google/paligemma-3b-mix-448/#running-the-default-precision-float32-on-cpu \"Running the default precision \\(<code>float32</code>\\) on CPU\")\n      * [Running other precisions on CUDA](https://huggingface.co/google/paligemma-3b-mix-448/#running-other-precisions-on-cuda \"Running other precisions on CUDA\")\n      * [Loading in 4-bit / 8-bit](https://huggingface.co/google/paligemma-3b-mix-448/#loading-in-4-bit--8-bit \"Loading in 4-bit / 8-bit\")\n    * [Implementation information](https://huggingface.co/google/paligemma-3b-mix-448/#implementation-information \"Implementation information\")\n      * [Hardware](https://huggingface.co/google/paligemma-3b-mix-448/#hardware \"Hardware\")\n      * [Software](https://huggingface.co/google/paligemma-3b-mix-448/#software \"Software\")\n    * [Evaluation information](https://huggingface.co/google/paligemma-3b-mix-448/#evaluation-information \"Evaluation information\")\n      * [Benchmark results](https://huggingface.co/google/paligemma-3b-mix-448/#benchmark-results \"Benchmark results\")\n    * [Ethics and safety](https://huggingface.co/google/paligemma-3b-mix-448/#ethics-and-safety \"Ethics and safety\")\n      * [Evaluation approach](https://huggingface.co/google/paligemma-3b-mix-448/#evaluation-approach \"Evaluation approach\")\n      * [Evaluation results](https://huggingface.co/google/paligemma-3b-mix-448/#evaluation-results \"Evaluation results\")\n    * [Usage and limitations](https://huggingface.co/google/paligemma-3b-mix-448/#usage-and-limitations \"Usage and limitations\")\n      * [Intended usage](https://huggingface.co/google/paligemma-3b-mix-448/#intended-usage \"Intended usage\")\n      * [Ethical considerations and risks](https://huggingface.co/google/paligemma-3b-mix-448/#ethical-considerations-and-risks \"Ethical considerations and risks\")\n      * [Limitations](https://huggingface.co/google/paligemma-3b-mix-448/#limitations \"Limitations\")\n\n\n#  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#paligemma-model-card) PaliGemma model card \n**Model page:** [PaliGemma](https://ai.google.dev/gemma/docs/paligemma)\nTransformers PaliGemma 3B weights, fine-tuned with 448*448 input images and 512 token input/output text sequences on a mixture of downstream academic datasets. The models are available in float32, bfloat16 and float16 format for research purposes only.\n**Resources and technical documentation:**\n  * [Responsible Generative AI Toolkit](https://ai.google.dev/responsible)\n  * [PaliGemma on Kaggle](https://www.kaggle.com/models/google/paligemma)\n  * [PaliGemma on Vertex Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/363)\n\n\n**Terms of Use:** [Terms](https://www.kaggle.com/models/google/paligemma/license/consent/verify/huggingface?returnModelRepoId=google/paligemma-3b-mix-448)\n**Authors:** Google\n##  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#model-information) Model information \n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#model-summary) Model summary \n####  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#description) Description \nPaliGemma is a versatile and lightweight vision-language model (VLM) inspired by [PaLI-3](https://arxiv.org/abs/2310.09199) and based on open components such as the [SigLIP vision model](https://arxiv.org/abs/2303.15343) and the [Gemma language model](https://arxiv.org/abs/2403.08295). It takes both image and text as input and generates text as output, supporting multiple languages. It is designed for class-leading fine-tune performance on a wide range of vision-language tasks such as image and short video caption, visual question answering, text reading, object detection and object segmentation.\n####  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#model-architecture) Model architecture \nPaliGemma is the composition of a [Transformer decoder](https://arxiv.org/abs/1706.03762) and a [Vision Transformer image encoder](https://arxiv.org/abs/2010.11929), with a total of 3 billion params. The text decoder is initialized from [Gemma-2B](https://www.kaggle.com/models/google/gemma). The image encoder is initialized from [SigLIP-So400m/14](https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/SigLIP_demo.ipynb). PaliGemma is trained following the PaLI-3 recipes.\n####  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#inputs-and-outputs) Inputs and outputs \n  * **Input:** Image and text string, such as a prompt to caption the image, or a question.\n  * **Output:** Generated text in response to the input, such as a caption of the image, an answer to a question, a list of object bounding box coordinates, or segmentation codewords.\n\n\n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#model-data) Model data \n####  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#pre-train-datasets) Pre-train datasets \nPaliGemma is pre-trained on the following mixture of datasets:\n  * **WebLI:** [WebLI (Web Language Image)](https://arxiv.org/abs/2209.06794) is a web-scale multilingual image-text dataset built from the public web. A wide range of WebLI splits are used to acquire versatile model capabilities, such as visual semantic understanding, object localization, visually-situated text understanding, multilinguality, etc.\n  * **CC3M-35L:** Curated English image-alt_text pairs from webpages ([Sharma et al., 2018](https://aclanthology.org/P18-1238/)). We used the [Google Cloud Translation API](https://cloud.google.com/translate) to translate into 34 additional languages.\n  * **VQ\u00b2A-CC3M-35L/VQG-CC3M-35L:** A subset of VQ2A-CC3M ([Changpinyo et al., 2022a](https://aclanthology.org/2022.naacl-main.142/)), translated into the same additional 34 languages as CC3M-35L, using the [Google Cloud Translation API](https://cloud.google.com/translate).\n  * **OpenImages:** Detection and object-aware questions and answers ([Piergiovanni et al. 2022](https://arxiv.org/abs/2209.04372)) generated by handcrafted rules on the [OpenImages dataset](https://storage.googleapis.com/openimages/web/factsfigures_v7.html).\n  * **WIT:** Images and texts collected from Wikipedia ([Srinivasan et al., 2021](https://arxiv.org/abs/2103.01913)).\n\n\n####  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#data-responsibility-filtering) Data responsibility filtering \nThe following filters are applied to WebLI, with the goal of training PaliGemma on clean data:\n  * **Pornographic image filtering:** This filter removes images deemed to be of pornographic nature.\n  * **Text safety filtering:** We identify and filter out images that are paired with unsafe text. Unsafe text is any text deemed to contain or be about CSAI, pornography, vulgarities, or otherwise offensive.\n  * **Text toxicity filtering:** We further use the [Perspective API](https://perspectiveapi.com/) to identify and filter out images that are paired with text deemed insulting, obscene, hateful or otherwise toxic.\n  * **Text personal information filtering:** We filtered certain personal information and other sensitive data using [Cloud Data Loss Prevention (DLP) API](https://cloud.google.com/security/products/dlp) to protect the privacy of individuals. Identifiers such as social security numbers and [other sensitive information types](https://cloud.google.com/sensitive-data-protection/docs/high-sensitivity-infotypes-reference?_gl=1*jg604m*_ga*ODk5MzA3ODQyLjE3MTAzMzQ3NTk.*_ga_WH2QY8WWF5*MTcxMDUxNTkxMS4yLjEuMTcxMDUxNjA2NC4wLjAuMA..&_ga=2.172110058.-899307842.1710334759) were removed.\n  * **Additional methods:** Filtering based on content quality and safety in line with our policies and practices.\n\n\n##  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#how-to-use) How to Use \nPaliGemma is a single-turn vision language model not meant for conversational use, and it works best when fine-tuning to a specific use case. \nYou can configure which task the model will solve by conditioning it with task prefixes, such as \u201cdetect\u201d or \u201csegment\u201d. The pretrained models were trained in this fashion to imbue them with a rich set of capabilities (question answering, captioning, segmentation, etc.). However, they are not designed to be used directly, but to be transferred (by fine-tuning) to specific tasks using a similar prompt structure. For interactive testing, you can use the \"mix\" family of models, which have been fine-tuned on a mixture of tasks. To see this model in action, check [this Space that uses the Transformers codebase](https://huggingface.co/spaces/big-vision/paligemma-hf).\nPlease, refer to the [usage and limitations section](https://huggingface.co/google/paligemma-3b-mix-448/#usage-and-limitations) for intended use cases, or visit the [blog post](https://huggingface.co/blog/paligemma-google-vlm) for additional details and examples.\n##  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#use-in-transformers) Use in Transformers \nThe following snippets use model `google/paligemma-3b-mix-224` for reference purposes. The model in this repo you are now browsing may have been trained for other tasks, please make sure you use appropriate inputs for the task at hand.\n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#running-the-default-precision-float32-on-cpu) Running the default precision (`float32`) on CPU \n```\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/paligemma-3b-mix-224\"\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in Spanish\nprompt = \"caption es\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\n  generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n  generation = generation[0][input_len:]\n  decoded = processor.decode(generation, skip_special_tokens=True)\n  print(decoded)\n\n```\n\nOutput: `Un auto azul estacionado frente a un edificio.`\n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#running-other-precisions-on-cuda) Running other precisions on CUDA \nFor convenience, the repos contain revisions of the weights already converted to `bfloat16` and `float16`, so you can use them to reduce the download size and avoid casting on your local computer.\nThis is how you'd run `bfloat16` on an nvidia CUDA card.\n```\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/paligemma-3b-mix-224\"\ndevice = \"cuda:0\"\ndtype = torch.bfloat16\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\n  model_id,\n  torch_dtype=dtype,\n  device_map=device,\n  revision=\"bfloat16\",\n).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in Spanish\nprompt = \"caption es\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\n  generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n  generation = generation[0][input_len:]\n  decoded = processor.decode(generation, skip_special_tokens=True)\n  print(decoded)\n\n```\n\n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#loading-in-4-bit--8-bit) Loading in 4-bit / 8-bit \nYou need to install `bitsandbytes` to automatically run inference using 8-bit or 4-bit precision:\n```\npip install bitsandbytes accelerate\n\n```\n\n```\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/paligemma-3b-mix-224\"\ndevice = \"cuda:0\"\ndtype = torch.bfloat16\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\n  model_id, quantization_config=quantization_config\n).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in Spanish\nprompt = \"caption es\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\n  generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n  generation = generation[0][input_len:]\n  decoded = processor.decode(generation, skip_special_tokens=True)\n  print(decoded)\n\n```\n\n##  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#implementation-information) Implementation information \n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#hardware) Hardware \nPaliGemma was trained using the latest generation of Tensor Processing Unit (TPU) hardware (TPUv5e).\n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#software) Software \nTraining was done using [JAX](https://github.com/google/jax), [Flax](https://github.com/google/flax), [TFDS](https://github.com/tensorflow/datasets) and [`big_vision`](https://github.com/google-research/big_vision).\nJAX allows researchers to take advantage of the latest generation of hardware, including TPUs, for faster and more efficient training of large models.\nTFDS is used to access datasets and Flax is used for model architecture. The PaliGemma fine-tune code and inference code are released in the `big_vision` GitHub repository.\n##  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#evaluation-information) Evaluation information \n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#benchmark-results) Benchmark results \nIn order to verify the transferability of PaliGemma to a wide variety of academic tasks, we fine-tune the pretrained models on each task. Additionally we train the mix model with a mixture of the transfer tasks. We report results on different resolutions to provide an impression of which tasks benefit from increased resolution. Importantly, none of these tasks or datasets are part of the pretraining data mixture, and their images are explicitly removed from the web-scale pre-training data.\n####  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#single-task-fine-tune-on-single-task) Single task (fine-tune on single task) \nBenchmark(train split) | Metric(split) | pt-224 | pt-448 | pt-896  \n---|---|---|---|---  \nCaptioning  \n[COCO captions](https://cocodataset.org/#home)(train+restval)  | CIDEr (val) | 141.92 | 144.60  \n[NoCaps](https://nocaps.org/)(Eval of COCOcaptions transfer)  | CIDEr (val) | 121.72 | 123.58  \n[COCO-35L](https://arxiv.org/pdf/2205.12522)(train)  | CIDEr dev(en/avg-34/avg) |  139.2 115.8 116.4  |  141.2 118.0 118.6   \n[XM3600](https://arxiv.org/pdf/2205.12522)(Eval of COCO-35L transfer)  | CIDEr dev(en/avg-34/avg) |  78.1 41.3 42.4  |  80.0 41.9 42.9   \n[TextCaps](https://textvqa.org/textcaps/)(train)  | CIDEr (val) | 127.48 | 153.94  \n[SciCap](https://arxiv.org/abs/2110.11624)(first sentence, no subfigure)(train+val)  | CIDEr/BLEU-4(test) |  162.25 0.192  |  181.49 0.211   \n[Screen2words](https://arxiv.org/abs/2108.03353)(train+dev)  | CIDEr (test) | 117.57 | 119.59  \n[Widget Captioning](https://arxiv.org/abs/2010.04295)(train+dev)  | CIDEr (test) | 136.07 | 148.36  \nQuestion answering  \n[VQAv2](https://visualqa.org/index.html)(train+validation)  | Accuracy(Test server - std) | 83.19 | 85.64  \n[MMVP](https://arxiv.org/abs/2401.06209)(Eval of VQAv2 transfer)  | Paired Accuracy | 47.33 | 45.33  \n[POPE](https://arxiv.org/abs/2305.10355)(Eval of VQAv2 transfer)  | Accuracy(random/popular/adversarial) |  87.80 85.87 84.27  |  88.23 86.77 85.90   \n[OKVQA](https://okvqa.allenai.org/)(train)  | Accuracy (val) | 63.54 | 63.15  \n[A-OKVQA](https://allenai.org/project/a-okvqa/home) (MC)(train+val)  | Accuracy(Test server) | 76.37 | 76.90  \n[A-OKVQA](https://allenai.org/project/a-okvqa/home) (DA)(train+val)  | Accuracy(Test server) | 61.85 | 63.22  \n[GQA](https://cs.stanford.edu/people/dorarad/gqa/about.html)(train_balanced+val_balanced)  | Accuracy(testdev balanced) | 65.61 | 67.03  \n[xGQA](https://aclanthology.org/2022.findings-acl.196/)(Eval of GQA transfer)  | Mean Accuracy(bn, de, en, id,ko, pt, ru, zh) | 58.37 | 59.07  \n[NLVR2](https://lil.nlp.cornell.edu/nlvr/)(train+dev)  | Accuracy (test) | 90.02 | 88.93  \n[MaRVL](https://marvl-challenge.github.io/)(Eval of NLVR2 transfer)  | Mean Accuracy(test)(id, sw, ta, tr, zh) | 80.57 | 76.78  \n[AI2D](https://allenai.org/data/diagrams)(train)  | Accuracy (test) | 72.12 | 73.28  \n[ScienceQA](https://scienceqa.github.io/)(Img subset, no CoT)(train+val)  | Accuracy (test) | 95.39 | 95.93  \n[RSVQA-LR](https://zenodo.org/records/6344334) (Non numeric)(train+val)  | Mean Accuracy(test) | 92.65 | 93.11  \n[RSVQA-HR](https://zenodo.org/records/6344367) (Non numeric)(train+val)  | Mean Accuracy(test/test2) |  92.61 90.58  |  92.79 90.54   \n[ChartQA](https://arxiv.org/abs/2203.10244)(human+aug)x(train+val)  | Mean RelaxedAccuracy(test_human,test_aug) | 57.08 | 71.36  \n[VizWiz VQA](https://vizwiz.org/tasks-and-datasets/vqa/)(train+val)  | Accuracy(Test server - std) |  73.7  |  75.52   \n[TallyQA](https://arxiv.org/abs/1810.12440)(train)  | Accuracy(test_simple/test_complex) |  81.72 69.56  |  84.86 72.27   \n[OCR-VQA](https://ocr-vqa.github.io/)(train+val)  | Accuracy (test) | 72.32 | 74.61 | 74.93  \n[TextVQA](https://textvqa.org/)(train+val)  | Accuracy(Test server - std) | 55.47 | 73.15 | 76.48  \n[DocVQA](https://www.docvqa.org/)(train+val)  | ANLS (Test server) | 43.74 | 78.02 | 84.77  \n[Infographic VQA](https://openaccess.thecvf.com/content/WACV2022/papers/Mathew_InfographicVQA_WACV_2022_paper.pdf)(train+val)  | ANLS (Test server) | 28.46 | 40.47 | 47.75  \n[SceneText VQA](https://arxiv.org/abs/1905.13648)(train+val)  | ANLS (Test server) | 63.29 | 81.82 | 84.40  \nSegmentation  \n[RefCOCO](https://arxiv.org/abs/1608.00272)(combined refcoco, refcoco+,refcocog excluding valand test images)  | MIoU(validation)refcoco/refcoco+/refcocog |  73.40 68.32 67.65  |  75.57 69.76 70.17  |  76.94 72.18 72.22   \nVideo tasks (Caption/QA)  \nMSR-VTT (Captioning) | CIDEr (test) | 70.54  \nMSR-VTT (QA) | Accuracy (test) | 50.09  \nActivityNet (Captioning) | CIDEr (test) | 34.62  \nActivityNet (QA) | Accuracy (test) | 50.78  \nVATEX (Captioning) | CIDEr (test) | 79.73  \nMSVD (QA) | Accuracy (test) | 60.22  \n####  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#mix-model-fine-tune-on-mixture-of-transfer-tasks) Mix model (fine-tune on mixture of transfer tasks) \nBenchmark | Metric (split) | mix-224 | mix-448  \n---|---|---|---  \n[MMVP](https://arxiv.org/abs/2401.06209) | Paired Accuracy | 46.00 | 45.33  \n[POPE](https://arxiv.org/abs/2305.10355) | Accuracy(random/popular/adversarial) |  88.00 86.63 85.67  |  89.37 88.40 87.47   \n##  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#ethics-and-safety) Ethics and safety \n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#evaluation-approach) Evaluation approach \nOur evaluation methods include structured evaluations and internal red-teaming testing of relevant content policies. Red-teaming was conducted by a number of different teams, each with different goals and human evaluation metrics. These models were evaluated against a number of different categories relevant to ethics and safety, including:\n  * Human evaluation on prompts covering child safety, content safety and representational harms. See the [Gemma model card](https://ai.google.dev/gemma/docs/model_card#evaluation_approach) for more details on evaluation approach, but with image captioning and visual question answering setups.\n  * Image-to-Text benchmark evaluation: Benchmark against relevant academic datasets such as FairFace Dataset ([Karkkainen et al., 2021](https://arxiv.org/abs/1908.04913)).\n\n\n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#evaluation-results) Evaluation results \n  * The human evaluation results of ethics and safety evaluations are within acceptable thresholds for meeting [internal policies](https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11) for categories such as child safety, content safety and representational harms.\n  * On top of robust internal evaluations, we also use the Perspective API (threshold of 0.8) to measure toxicity, profanity, and other potential issues in the generated captions for images sourced from the FairFace dataset. We report the maximum and median values observed across subgroups for each of the perceived gender, ethnicity, and age attributes.\n\nMetric | Perceivedgender | Ethnicity | Age group  \n---|---|---|---  \nMaximum | Median | Maximum | Median | Maximum | Median  \nToxicity | 0.04% | 0.03% | 0.08% | 0.00% | 0.09% | 0.00%  \nIdentity Attack | 0.00% | 0.00% | 0.00% | 0.00% | 0.00% | 0.00%  \nInsult | 0.06% | 0.04% | 0.09% | 0.07% | 0.16% | 0.00%  \nThreat | 0.06% | 0.05% | 0.14% | 0.05% | 0.17% | 0.00%  \nProfanity | 0.00% | 0.00% | 0.00% | 0.00% | 0.00% | 0.00%  \n##  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#usage-and-limitations) Usage and limitations \n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#intended-usage) Intended usage \nOpen Vision Language Models (VLMs) have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development.\nFine-tune on specific vision-language task:\n  * The pre-trained models can be fine-tuned on a wide range of vision-language tasks such as: image captioning, short video caption, visual question answering, text reading, object detection and object segmentation.\n  * The pre-trained models can be fine-tuned for specific domains such as remote sensing question answering, visual questions from people who are blind, science question answering, describe UI element functionalities.\n  * The pre-trained models can be fine-tuned for tasks with non-textual outputs such as bounding boxes or segmentation masks.\n\n\nVision-language research:\n  * The pre-trained models and fine-tuned models can serve as a foundation for researchers to experiment with VLM techniques, develop algorithms, and contribute to the advancement of the field.\n\n\n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#ethical-considerations-and-risks) Ethical considerations and risks \nThe development of vision-language models (VLMs) raises several ethical concerns. In creating an open model, we have carefully considered the following:\n  * Bias and Fairness\n    * VLMs trained on large-scale, real-world image-text data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card.\n  * Misinformation and Misuse\n    * VLMs can be misused to generate text that is false, misleading, or harmful.\n    * Guidelines are provided for responsible use with the model, see the [Responsible Generative AI Toolkit](https://ai.google.dev/responsible).\n  * Transparency and Accountability\n    * This model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes.\n    * A responsibly developed open model offers the opportunity to share innovation by making VLM technology accessible to developers and researchers across the AI ecosystem.\n\n\nRisks identified and mitigations:\n  * **Perpetuation of biases:** It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases.\n  * **Generation of harmful content:** Mechanisms and guidelines for content safety are essential. Developers are encouraged to exercise caution and implement appropriate content safety safeguards based on their specific product policies and application use cases.\n  * **Misuse for malicious purposes:** Technical limitations and developer and end-user education can help mitigate against malicious applications of LLMs. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in the [Gemma Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy).\n  * **Privacy violations:** Models were trained on data filtered to remove certain personal information and sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques.\n\n\n###  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#limitations) Limitations \n  * Most limitations inherited from the underlying Gemma model still apply:\n    * VLMs are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging.\n    * Natural language is inherently complex. VLMs might struggle to grasp subtle nuances, sarcasm, or figurative language.\n    * VLMs generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements.\n    * VLMs rely on statistical patterns in language and images. They might lack the ability to apply common sense reasoning in certain situations.\n  * PaliGemma was designed first and foremost to serve as a general pre-trained model for transfer to specialized tasks. Hence, its \"out of the box\" or \"zero-shot\" performance might lag behind models designed specifically for that.\n  * PaliGemma is not a multi-turn chatbot. It is designed for a single round of image and text input.\n##  [ ](https://huggingface.co/google/paligemma-3b-mix-448/#citation) Citation \n```\n@article{beyer2024paligemma,\n  title={{PaliGemma: A versatile 3B VLM for transfer}},\n  author={Lucas Beyer* and Andreas Steiner* and Andr\u00e9 Susano Pinto* and Alexander Kolesnikov* and Xiao Wang* and Daniel Salz and Maxim Neumann and Ibrahim Alabdulmohsin and Michael Tschannen and Emanuele Bugliarello and Thomas Unterthiner and Daniel Keysers and Skanda Koppula and Fangyu Liu and Adam Grycner and Alexey Gritsenko and Neil Houlsby and Manoj Kumar and Keran Rong and Julian Eisenschlos and Rishabh Kabra and Matthias Bauer and Matko Bo\u0161njak and Xi Chen and Matthias Minderer and Paul Voigtlaender and Ioana Bica and Ivana Balazevic and Joan Puigcerver and Pinelopi Papalampidi and Olivier Henaff and Xi Xiong and Radu Soricut and Jeremiah Harmsen and Xiaohua Zhai*},\n  year={2024},\n  journal={arXiv preprint arXiv:2407.07726}\n}\n\n```\n\n\n\nFind the paper [here](https://arxiv.org/abs/2407.07726). \n\nDownloads last month\n    6,139 \nSafetensors[](https://huggingface.co/docs/safetensors)\nModel size\n2.92B params\nTensor type\nF32 \n\u00b7\nInference Providers [NEW](https://huggingface.co/blog/inference-providers)\n[ Image-Text-to-Text](https://huggingface.co/tasks/image-text-to-text \"Learn more about image-text-to-text\")\nThis model isn't deployed by any Inference Provider. [\ud83d\ude4b Ask for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=google/paligemma-3b-mix-448&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bgoogle%2Fpaligemma-3b-mix-448%5D\\(%2Fgoogle%2Fpaligemma-3b-mix-448\\)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A\\(optional\\)%20Which%20providers%20are%20you%20interested%20in%3F%20\\(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6\\)%0A)\n##  Model tree for google/paligemma-3b-mix-448 [](https://huggingface.co/docs/hub/model-cards#specifying-a-base-model)\nAdapters\n[28 models](https://huggingface.co/models?other=base_model:adapter:google/paligemma-3b-mix-448)\nFinetunes\n[15 models](https://huggingface.co/models?other=base_model:finetune:google/paligemma-3b-mix-448)\nMerges\n[7 models](https://huggingface.co/models?other=base_model:merge:google/paligemma-3b-mix-448)\n##  Spaces using google/paligemma-3b-mix-448 12\n[\ud83e\udd17 big-vision/paligemma-hf](https://huggingface.co/spaces/big-vision/paligemma-hf)[\ud83d\ude80 Justinrune/LLaMA-Factory](https://huggingface.co/spaces/Justinrune/LLaMA-Factory)[\ud83d\udc28 merve/paligemma-tracking](https://huggingface.co/spaces/merve/paligemma-tracking)[\ud83e\udd80 kenken999/fastapi_django_main_live](https://huggingface.co/spaces/kenken999/fastapi_django_main_live)[\ud83d\ude3b Saee/vQA-exploration](https://huggingface.co/spaces/Saee/vQA-exploration)[\ud83d\ude80 taufiqdp/paligemma](https://huggingface.co/spaces/taufiqdp/paligemma)[\ud83d\ude3b gabrielaltay/vlmqa](https://huggingface.co/spaces/gabrielaltay/vlmqa)[\ud83d\udd25 NSTiwari/PaliGemma-ZeroShotDetection-Video](https://huggingface.co/spaces/NSTiwari/PaliGemma-ZeroShotDetection-Video)[\ud83e\udd17 hermanhelf/paligemma-hf](https://huggingface.co/spaces/hermanhelf/paligemma-hf)[\ud83d\ude80 HUANG-Stephanie/cvquest-colpali](https://huggingface.co/spaces/HUANG-Stephanie/cvquest-colpali)[\u26a1 sprakhil/OCRQuest-2.0](https://huggingface.co/spaces/sprakhil/OCRQuest-2.0)[\ud83d\ude80 msun415/Llamole](https://huggingface.co/spaces/msun415/Llamole) + 7 Spaces\n##  Collections including google/paligemma-3b-mix-448\n#### [PaliGemma Release Collection  Pretrained and mix checkpoints for PaliGemma \u2022  16 items \u2022  Updated 3 days ago \u2022 145](https://huggingface.co/collections/google/paligemma-release-6643a9ffbf57de2ae0448dda)\n#### [Google's Gemma models family Collection  264 items \u2022  Updated 3 days ago \u2022 129](https://huggingface.co/collections/google/googles-gemma-models-family-675bfd70e574a62dd0e406bd)\nSystem theme \nCompany\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/) [](https://huggingface.co/)\nWebsite\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)\n",
    "answer": "```markdown\n# TL;DR Summary of PaliGemma Model\n\nPaliGemma is a 3B parameter vision-language model by Google, designed for tasks like image captioning and visual question answering. It uses a Transformer architecture and is trained on diverse datasets. Ethical considerations include bias mitigation and content safety. Fine-tuning is recommended for specific applications.\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\nAmazon SageMaker documentation\nDeploy models to Amazon SageMaker\n# Amazon SageMaker\n\ud83c\udfe1 View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersDistilabelEvaluateGradioHubHub Python LibraryHugging Face Generative AI Services (HUGS)Huggingface.jsInference API (serverless)Inference Endpoints (dedicated)LeaderboardsLightevalOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jssmolagentstimm\nSearch documentation\n`\u2318K`\nmain EN [ 365](https://github.com/huggingface/hub-docs)\n[Hugging Face on Amazon SageMaker ](https://huggingface.co/docs/sagemaker/en/index)[Get started ](https://huggingface.co/docs/sagemaker/en/getting-started)[Run training on Amazon SageMaker ](https://huggingface.co/docs/sagemaker/en/train)[Deploy models to Amazon SageMaker ](https://huggingface.co/docs/sagemaker/en/inference)[Reference ](https://huggingface.co/docs/sagemaker/en/reference)\n![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\nJoin the Hugging Face community\nand get access to the augmented documentation experience \nCollaborate on models, datasets and Spaces \nFaster examples with accelerated inference \nSwitch between documentation themes \n[Sign Up](https://huggingface.co/join)\nto get started\n# [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-models-to-amazon-sagemaker) Deploy models to Amazon SageMaker\nDeploying a \ud83e\udd17 Transformers models in SageMaker for inference is as easy as:\nCopied\n```\nfrom sagemaker.huggingface import HuggingFaceModel\n# create Hugging Face Model Class and deploy it as SageMaker endpoint\nhuggingface_model = HuggingFaceModel(...).deploy()\n```\n\nThis guide will show you how to deploy models with zero-code using the [Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit). The Inference Toolkit builds on top of the [`pipeline` feature](https://huggingface.co/docs/transformers/main_classes/pipelines) from \ud83e\udd17 Transformers. Learn how to:\n  * [Install and setup the Inference Toolkit](https://huggingface.co/docs/sagemaker/en/inference/#installation-and-setup).\n  * [Deploy a \ud83e\udd17 Transformers model trained in SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-transformer-model-trained-in-sagemaker).\n  * [Deploy a \ud83e\udd17 Transformers model from the Hugging Face [model Hub](https://huggingface.co/models)](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-model-from-the-hub).\n  * [Run a Batch Transform Job using \ud83e\udd17 Transformers and Amazon SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#run-batch-transform-with-transformers-and-sagemaker).\n  * [Create a custom inference module](https://huggingface.co/docs/sagemaker/en/inference/#user-defined-code-and-modules).\n\n\n## [](https://huggingface.co/docs/sagemaker/en/inference/#installation-and-setup) Installation and setup\nBefore deploying a \ud83e\udd17 Transformers model to SageMaker, you need to sign up for an AWS account. If you don\u2019t have an AWS account yet, learn more [here](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html).\nOnce you have an AWS account, get started using one of the following:\n  * [SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html)\n  * [SageMaker notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)\n  * Local environment\n\n\nTo start training locally, you need to setup an appropriate [IAM role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\nUpgrade to the latest `sagemaker` version.\nCopied\n```\npip install sagemaker --upgrade\n```\n\n**SageMaker environment**\nSetup your SageMaker environment as shown below:\nCopied\n```\nimport sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n```\n\n_Note: The execution role is only available when running a notebook within SageMaker. If you run`get_execution_role` in a notebook not on SageMaker, expect a `region` error._\n**Local environment**\nSetup your local environment as shown below:\nCopied\n```\nimport sagemaker\nimport boto3\niam_client = boto3.client('iam')\nrole = iam_client.get_role(RoleName='role-name-of-your-iam-role-with-right-permissions')['Role']['Arn']\nsess = sagemaker.Session()\n```\n\n## [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a--transformers-model-trained-in-sagemaker) Deploy a \ud83e\udd17 Transformers model trained in SageMaker\nThere are two ways to deploy your Hugging Face model trained in SageMaker:\n  * Deploy it after your training has finished.\n  * Deploy your saved model at a later time from S3 with the `model_data`.\n\n\n\ud83d\udcd3 Open the [deploy_transformer_model_from_s3.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb) for an example of how to deploy a model from S3 to SageMaker for inference.\n### [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-after-training) Deploy after training\nTo deploy your model directly after training, ensure all required files are saved in your training script, including the tokenizer and the model.\nIf you use the Hugging Face `Trainer`, you can pass your tokenizer as an argument to the `Trainer`. It will be automatically saved when you call `trainer.save_model()`.\nCopied\n```\nfrom sagemaker.huggingface import HuggingFace\n############ pseudo code start ############\n# create Hugging Face Estimator for training\nhuggingface_estimator = HuggingFace(....)\n# start the train job with our uploaded datasets as input\nhuggingface_estimator.fit(...)\n############ pseudo code end ############\n# deploy model to SageMaker Inference\npredictor = hf_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")\n# example request: you always need to define \"inputs\"\ndata = {\n  \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n}\n# request\npredictor.predict(data)\n```\n\nAfter you run your request you can delete the endpoint as shown:\nCopied\n```\n# delete endpoint\npredictor.delete_endpoint()\n```\n\n### [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-with-modeldata) Deploy with model_data\nIf you\u2019ve already trained your model and want to deploy it at a later time, use the `model_data` argument to specify the location of your tokenizer and model weights.\nCopied\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n  model_data=\"s3://models/my-bert-model/model.tar.gz\", # path to your trained SageMaker model\n  role=role,                      # IAM role with permissions to create an endpoint\n  transformers_version=\"4.26\",              # Transformers version used\n  pytorch_version=\"1.13\",                # PyTorch version used\n  py_version='py39',                  # Python version used\n)\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n  initial_instance_count=1,\n  instance_type=\"ml.m5.xlarge\"\n)\n# example request: you always need to define \"inputs\"\ndata = {\n  \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n}\n# request\npredictor.predict(data)\n```\n\nAfter you run our request, you can delete the endpoint again with:\nCopied\n```\n# delete endpoint\npredictor.delete_endpoint()\n```\n\n### [](https://huggingface.co/docs/sagemaker/en/inference/#create-a-model-artifact-for-deployment) Create a model artifact for deployment\nFor later deployment, you can create a `model.tar.gz` file that contains all the required files, such as:\n  * `pytorch_model.bin`\n  * `tf_model.h5`\n  * `tokenizer.json`\n  * `tokenizer_config.json`\n\n\nFor example, your file should look like this:\nCopied\n```\nmodel.tar.gz/\n|- pytorch_model.bin\n|- vocab.txt\n|- tokenizer_config.json\n|- config.json\n|- special_tokens_map.json\n```\n\nCreate your own `model.tar.gz` from a model from the \ud83e\udd17 Hub:\n  1. Download a model:\n\n\nCopied\n```\ngit lfs install\ngit clone git@hf.co:{repository}\n```\n\n  1. Create a `tar` file:\n\n\nCopied\n```\ncd {repository}\ntar zcvf model.tar.gz *\n```\n\n  1. Upload `model.tar.gz` to S3:\n\n\nCopied\n```\naws s3 cp model.tar.gz <s3://{my-s3-path}>\n```\n\nNow you can provide the S3 URI to the `model_data` argument to deploy your model later.\n## [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-model-from-the--hub) Deploy a model from the \ud83e\udd17 Hub\nTo deploy a model directly from the \ud83e\udd17 Hub to SageMaker, define two environment variables when you create a `HuggingFaceModel`:\n  * `HF_MODEL_ID` defines the model ID which is automatically loaded from [huggingface.co/models](http://huggingface.co/models) when you create a SageMaker endpoint. Access 10,000+ models on he \ud83e\udd17 Hub through this environment variable.\n  * `HF_TASK` defines the task for the \ud83e\udd17 Transformers `pipeline`. A complete list of tasks can be found [here](https://huggingface.co/docs/transformers/main_classes/pipelines).\n\n\n> \u26a0\ufe0f ** Pipelines are not optimized for parallelism (multi-threading) and tend to consume a lot of RAM. For example, on a GPU-based instance, the pipeline operates on a single vCPU. When this vCPU becomes saturated with the inference requests preprocessing, it can create a bottleneck, preventing the GPU from being fully utilized for model inference. Learn more [here](https://huggingface.co/docs/transformers/en/pipeline_webserver#using-pipelines-for-a-webserver)\nCopied\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n# Hub model configuration <https://huggingface.co/models>\nhub = {\n 'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models\n 'HF_TASK':'question-answering'              # NLP task you want to use for predictions\n}\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n  env=hub,                        # configuration for loading model from Hub\n  role=role,                       # IAM role with permissions to create an endpoint\n  transformers_version=\"4.26\",               # Transformers version used\n  pytorch_version=\"1.13\",                 # PyTorch version used\n  py_version='py39',                   # Python version used\n)\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n  initial_instance_count=1,\n  instance_type=\"ml.m5.xlarge\"\n)\n# example request: you always need to define \"inputs\"\ndata = {\n\"inputs\": {\n\t\"question\": \"What is used for inference?\",\n\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n\t}\n}\n# request\npredictor.predict(data)\n```\n\nAfter you run our request, you can delete the endpoint again with:\nCopied\n```\n# delete endpoint\npredictor.delete_endpoint()\n```\n\n\ud83d\udcd3 Open the [deploy_transformer_model_from_hf_hub.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb) for an example of how to deploy a model from the \ud83e\udd17 Hub to SageMaker for inference.\n## [](https://huggingface.co/docs/sagemaker/en/inference/#run-batch-transform-with--transformers-and-sagemaker) Run batch transform with \ud83e\udd17 Transformers and SageMaker\nAfter training a model, you can use [SageMaker batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html) to perform inference with the model. Batch transform accepts your inference data as an S3 URI and then SageMaker will take care of downloading the data, running the prediction, and uploading the results to S3. For more details about batch transform, take a look [here](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html).\n\u26a0\ufe0f The Hugging Face Inference DLC currently only supports `.jsonl` for batch transform due to the complex structure of textual data.\n_Note: Make sure your`inputs` fit the `max_length` of the model during preprocessing._\nIf you trained a model using the Hugging Face Estimator, call the `transformer()` method to create a transform job for a model based on the training job (see [here](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform) for more details):\nCopied\n```\nbatch_job = huggingface_estimator.transformer(\n  instance_count=1,\n  instance_type='ml.p3.2xlarge',\n  strategy='SingleRecord')\n\nbatch_job.transform(\n  data='s3://s3-uri-to-batch-data',\n  content_type='application/json',  \n  split_type='Line')\n```\n\nIf you want to run your batch transform job later or with a model from the \ud83e\udd17 Hub, create a `HuggingFaceModel` instance and then call the `transformer()` method:\nCopied\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n# Hub model configuration <https://huggingface.co/models>\nhub = {\n\t'HF_MODEL_ID':'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n\t'HF_TASK':'text-classification'\n}\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n  env=hub,                        # configuration for loading model from Hub\n  role=role,                       # IAM role with permissions to create an endpoint\n  transformers_version=\"4.26\",               # Transformers version used\n  pytorch_version=\"1.13\",                 # PyTorch version used\n  py_version='py39',                   # Python version used\n)\n# create transformer to run a batch job\nbatch_job = huggingface_model.transformer(\n  instance_count=1,\n  instance_type='ml.p3.2xlarge',\n  strategy='SingleRecord'\n)\n# starts batch transform job and uses S3 data as input\nbatch_job.transform(\n  data='s3://sagemaker-s3-demo-test/samples/input.jsonl',\n  content_type='application/json',  \n  split_type='Line'\n)\n```\n\nThe `input.jsonl` looks like this:\nCopied\n```\n{\"inputs\":\"this movie is terrible\"}\n{\"inputs\":\"this movie is amazing\"}\n{\"inputs\":\"SageMaker is pretty cool\"}\n{\"inputs\":\"SageMaker is pretty cool\"}\n{\"inputs\":\"this movie is terrible\"}\n{\"inputs\":\"this movie is amazing\"}\n```\n\n\ud83d\udcd3 Open the [sagemaker-notebook.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb) for an example of how to run a batch transform job for inference.\n## [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-an-llm-to-sagemaker-using-tgi) Deploy an LLM to SageMaker using TGI\nIf you are interested in using a high-performance serving container for LLMs, you can use the Hugging Face TGI container. This utilizes the [Text Generation Inference](https://github.com/huggingface/text-generation-inference) library. A list of compatible models can be found [here](https://huggingface.co/docs/text-generation-inference/supported_models#supported-models).\nFirst, make sure that the latest version of SageMaker SDK is installed:\nCopied\n```\npip install sagemaker>=2.231.0\n```\n\nThen, we import the SageMaker Python SDK and instantiate a sagemaker_session to find the current region and execution role.\nCopied\n```\nimport sagemaker\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\nimport time\nsagemaker_session = sagemaker.Session()\nregion = sagemaker_session.boto_region_name\nrole = sagemaker.get_execution_role()\n```\n\nNext we retrieve the LLM image URI. We use the helper function get_huggingface_llm_image_uri() to generate the appropriate image URI for the Hugging Face Large Language Model (LLM) inference. The function takes a required parameter backend and several optional parameters. The backend specifies the type of backend to use for the model: \u201chuggingface\u201d refers to using Hugging Face TGI backend.\nCopied\n```\nimage_uri = get_huggingface_llm_image_uri(\n backend=\"huggingface\",\n region=region\n)\n```\n\nNow that we have the image uri, the next step is to configure the model object. We specify a unique name, the image_uri for the managed TGI container, and the execution role for the endpoint. Additionally, we specify a number of environment variables including the `HF_MODEL_ID` which corresponds to the model from the HuggingFace Hub that will be deployed, and the `HF_TASK` which configures the inference task to be performed by the model.\nYou should also define `SM_NUM_GPUS`, which specifies the tensor parallelism degree of the model. Tensor parallelism can be used to split the model across multiple GPUs, which is necessary when working with LLMs that are too big for a single GPU. To learn more about tensor parallelism with inference, see our previous blog post. Here, you should set `SM_NUM_GPUS` to the number of available GPUs on your selected instance type. For example, in this tutorial, we set `SM_NUM_GPUS` to 4 because our selected instance type ml.g4dn.12xlarge has 4 available GPUs.\nNote that you can optionally reduce the memory and computational footprint of the model by setting the `HF_MODEL_QUANTIZE` environment variable to `true`, but this lower weight precision could affect the quality of the output for some models.\nCopied\n```\nmodel_name = \"llama-3-1-8b-instruct\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\nhub = {\n  'HF_MODEL_ID':'meta-llama/Llama-3.1-8B-Instruct',\n  'SM_NUM_GPUS':'1',\n\t'HUGGING_FACE_HUB_TOKEN': '<REPLACE WITH YOUR TOKEN>',\n}\nassert hub['HUGGING_FACE_HUB_TOKEN'] != '<REPLACE WITH YOUR TOKEN>', \"You have to provide a token.\"\n\nmodel = HuggingFaceModel(\n  name=model_name,\n  env=hub,\n  role=role,\n  image_uri=image_uri\n)\n```\n\nNext, we invoke the deploy method to deploy the model.\nCopied\n```\npredictor = model.deploy(\n initial_instance_count=1,\n instance_type=\"ml.g5.2xlarge\",\n endpoint_name=model_name\n)\n```\n\nOnce the model is deployed, we can invoke it to generate text. We pass an input prompt and run the predict method to generate a text response from the LLM running in the TGI container.\nCopied\n```\ninput_data = {\n \"inputs\": \"The diamondback terrapin was the first reptile to\",\n \"parameters\": {\n  \"do_sample\": True,\n  \"max_new_tokens\": 100,\n  \"temperature\": 0.7,\n  \"watermark\": True\n }\n}\npredictor.predict(input_data)\n```\n\nWe receive the following auto-generated text response:\nCopied\n```\n[{'generated_text': 'The diamondback terrapin was the first reptile to make the list, followed by the American alligator, the American crocodile, and the American box turtle. The polecat, a ferret-like animal, and the skunk rounded out the list, both having gained their slots because they have proven to be particularly dangerous to humans.\\n\\nCalifornians also seemed to appreciate the new list, judging by the comments left after the election.\\n\\n\u201cThis is fantastic,\u201d one commenter declared.\\n\\n\u201cCalifornia is a very'}]\n```\n\nOnce we are done experimenting, we delete the endpoint and the model resources.\nCopied\n```\npredictor.delete_model()\npredictor.delete_endpoint()\n```\n\n## [](https://huggingface.co/docs/sagemaker/en/inference/#user-defined-code-and-modules) User defined code and modules\nThe Hugging Face Inference Toolkit allows the user to override the default methods of the `HuggingFaceHandlerService`. You will need to create a folder named `code/` with an `inference.py` file in it. See [here](https://huggingface.co/docs/sagemaker/en/inference/#create-a-model-artifact-for-deployment) for more details on how to archive your model artifacts. For example:\nCopied\n```\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n |- inference.py\n |- requirements.txt \n```\n\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:\n  * `model_fn(model_dir)` overrides the default method for loading a model. The return value `model` will be used in `predict` for predictions. `predict` receives argument the `model_dir`, the path to your unzipped `model.tar.gz`.\n  * `transform_fn(model, data, content_type, accept_type)` overrides the default transform function with your custom implementation. You will need to implement your own `preprocess`, `predict` and `postprocess` steps in the `transform_fn`. This method can\u2019t be combined with `input_fn`, `predict_fn` or `output_fn` mentioned below.\n  * `input_fn(input_data, content_type)` overrides the default method for preprocessing. The return value `data` will be used in `predict` for predictions. The inputs are:\n    * `input_data` is the raw body of your request.\n    * `content_type` is the content type from the request header.\n  * `predict_fn(processed_data, model)` overrides the default method for predictions. The return value `predictions` will be used in `postprocess`. The input is `processed_data`, the result from `preprocess`.\n  * `output_fn(prediction, accept)` overrides the default method for postprocessing. The return value `result` will be the response of your request (e.g.`JSON`). The inputs are:\n    * `predictions` is the result from `predict`.\n    * `accept` is the return accept type from the HTTP Request, e.g. `application/json`.\n\n\nHere is an example of a custom inference module with `model_fn`, `input_fn`, `predict_fn`, and `output_fn`:\nCopied\n```\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder\ndef model_fn(model_dir):\n  # implement custom code to load the model\n  loaded_model = ...\n  \n  return loaded_model \ndef input_fn(input_data, content_type):\n  # decode the input data (e.g. JSON string -> dict)\n  data = decoder_encoder.decode(input_data, content_type)\n  return data\ndef predict_fn(data, model):\n  # call your custom model with the data\n  outputs = model(data , ... )\n  return predictions\ndef output_fn(prediction, accept):\n  # convert the model output to the desired output format (e.g. dict -> JSON string)\n  response = decoder_encoder.encode(prediction, accept)\n  return response\n```\n\nCustomize your inference module with only `model_fn` and `transform_fn`:\nCopied\n```\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder\ndef model_fn(model_dir):\n  # implement custom code to load the model\n  loaded_model = ...\n  \n  return loaded_model \ndef transform_fn(model, input_data, content_type, accept):\n   # decode the input data (e.g. JSON string -> dict)\n  data = decoder_encoder.decode(input_data, content_type)\n  # call your custom model with the data\n  outputs = model(data , ... ) \n  # convert the model output to the desired output format (e.g. dict -> JSON string)\n  response = decoder_encoder.encode(output, accept)\n  return response\n```\n\n[< > Update on GitHub](https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md)\n[\u2190Run training on Amazon SageMaker](https://huggingface.co/docs/sagemaker/en/train) [Reference\u2192](https://huggingface.co/docs/sagemaker/en/reference)\n[Deploy models to Amazon SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#deploy-models-to-amazon-sagemaker) [Installation and setup](https://huggingface.co/docs/sagemaker/en/inference/#installation-and-setup) [Deploy a \ud83e\udd17 Transformers model trained in SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a--transformers-model-trained-in-sagemaker) [Deploy after training](https://huggingface.co/docs/sagemaker/en/inference/#deploy-after-training) [Deploy with model_data](https://huggingface.co/docs/sagemaker/en/inference/#deploy-with-modeldata) [Create a model artifact for deployment](https://huggingface.co/docs/sagemaker/en/inference/#create-a-model-artifact-for-deployment) [Deploy a model from the \ud83e\udd17 Hub](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-model-from-the--hub) [Run batch transform with \ud83e\udd17 Transformers and SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#run-batch-transform-with--transformers-and-sagemaker) [Deploy an LLM to SageMaker using TGI](https://huggingface.co/docs/sagemaker/en/inference/#deploy-an-llm-to-sagemaker-using-tgi) [User defined code and modules](https://huggingface.co/docs/sagemaker/en/inference/#user-defined-code-and-modules)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis document provides a comprehensive guide on deploying Hugging Face models to Amazon SageMaker, covering installation, model deployment, batch transforms, and custom inference modules. Key features include zero-code deployment, support for LLMs using TGI, and integration with the Hugging Face Hub.\n```"
  },
  {
    "instruction": "[Jump to Content](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN/#page-content)\n[ Research ](https://research.google/ \"Google Research\")\n[ Research ](https://research.google/ \"Google Research\")\n  * Who we are \nBack to Who we are menu\n## Defining the technology of today and tomorrow.\n    * ##  Philosophy \nWe strive to create an environment conducive to many different types of research across many different time scales and levels of risk.\n[ Learn more about our Philosophy Learn more ](https://research.google/philosophy/)\n[ Philosophy ](https://research.google/philosophy/)\n    * ##  People \nOur researchers drive advancements in computer science through both fundamental and applied research.\n[ Learn more about our People Learn more ](https://research.google/people/)\n[ People ](https://research.google/people/)\n  * Research areas \nBack to Research areas menu\n    * ## Research areas\n      * [ Explore all research areas ](https://research.google/research-areas/)\nResearch areas \nBack to Research areas menu\n      * [ Explore all research areas ](https://research.google/research-areas/)\n    * ## Foundational ML & Algorithms\n      * [ Algorithms & Theory ](https://research.google/research-areas/algorithms-and-theory/)\n      * [ Data Management ](https://research.google/research-areas/data-management/)\n      * [ Data Mining & Modeling ](https://research.google/research-areas/data-mining-and-modeling/)\n      * [ Information Retrieval & the Web ](https://research.google/research-areas/information-retrieval-and-the-web/)\n      * [ Machine Intelligence ](https://research.google/research-areas/machine-intelligence/)\n      * [ Machine Perception ](https://research.google/research-areas/machine-perception/)\n      * [ Machine Translation ](https://research.google/research-areas/machine-translation/)\n      * [ Natural Language Processing ](https://research.google/research-areas/natural-language-processing/)\n      * [ Speech Processing ](https://research.google/research-areas/speech-processing/)\nFoundational ML & Algorithms \nBack to Foundational ML & Algorithms menu\n      * [ Algorithms & Theory ](https://research.google/research-areas/algorithms-and-theory/)\n      * [ Data Management ](https://research.google/research-areas/data-management/)\n      * [ Data Mining & Modeling ](https://research.google/research-areas/data-mining-and-modeling/)\n      * [ Information Retrieval & the Web ](https://research.google/research-areas/information-retrieval-and-the-web/)\n      * [ Machine Intelligence ](https://research.google/research-areas/machine-intelligence/)\n      * [ Machine Perception ](https://research.google/research-areas/machine-perception/)\n      * [ Machine Translation ](https://research.google/research-areas/machine-translation/)\n      * [ Natural Language Processing ](https://research.google/research-areas/natural-language-processing/)\n      * [ Speech Processing ](https://research.google/research-areas/speech-processing/)\n    * ## Computing Systems & Quantum AI\n      * [ Distributed Systems & Parallel Computing ](https://research.google/research-areas/distributed-systems-and-parallel-computing/)\n      * [ Hardware & Architecture ](https://research.google/research-areas/hardware-and-architecture/)\n      * [ Mobile Systems ](https://research.google/research-areas/mobile-systems/)\n      * [ Networking ](https://research.google/research-areas/networking/)\n      * [ Quantum Computing ](https://research.google/research-areas/quantum-computing/)\n      * [ Robotics ](https://research.google/research-areas/robotics/)\n      * [ Security, Privacy, & Abuse Prevention ](https://research.google/research-areas/security-privacy-and-abuse-prevention/)\n      * [ Software Engineering ](https://research.google/research-areas/software-engineering/)\n      * [ Software Systems ](https://research.google/research-areas/software-systems/)\nComputing Systems & Quantum AI \nBack to Computing Systems & Quantum AI menu\n      * [ Distributed Systems & Parallel Computing ](https://research.google/research-areas/distributed-systems-and-parallel-computing/)\n      * [ Hardware & Architecture ](https://research.google/research-areas/hardware-and-architecture/)\n      * [ Mobile Systems ](https://research.google/research-areas/mobile-systems/)\n      * [ Networking ](https://research.google/research-areas/networking/)\n      * [ Quantum Computing ](https://research.google/research-areas/quantum-computing/)\n      * [ Robotics ](https://research.google/research-areas/robotics/)\n      * [ Security, Privacy, & Abuse Prevention ](https://research.google/research-areas/security-privacy-and-abuse-prevention/)\n      * [ Software Engineering ](https://research.google/research-areas/software-engineering/)\n      * [ Software Systems ](https://research.google/research-areas/software-systems/)\n    * ## Science, AI & Society\n      * [ Climate & Sustainability ](https://research.google/research-areas/climate-and-sustainability/)\n      * [ Economics & Electronic Commerce ](https://research.google/research-areas/economics-and-electronic-commerce/)\n      * [ Education Innovation ](https://research.google/research-areas/education-innovation/)\n      * [ General Science ](https://research.google/research-areas/general-science/)\n      * [ Health & Bioscience ](https://research.google/research-areas/health-bioscience/)\n      * [ Human-Computer Interaction and Visualization ](https://research.google/research-areas/human-computer-interaction-and-visualization/)\nScience, AI & Society \nBack to Science, AI & Society menu\n      * [ Climate & Sustainability ](https://research.google/research-areas/climate-and-sustainability/)\n      * [ Economics & Electronic Commerce ](https://research.google/research-areas/economics-and-electronic-commerce/)\n      * [ Education Innovation ](https://research.google/research-areas/education-innovation/)\n      * [ General Science ](https://research.google/research-areas/general-science/)\n      * [ Health & Bioscience ](https://research.google/research-areas/health-bioscience/)\n      * [ Human-Computer Interaction and Visualization ](https://research.google/research-areas/human-computer-interaction-and-visualization/)\n  * Our work \nBack to Our work menu\n    * ##  Projects \nWe regularly open-source projects with the broader research community and apply our developments to Google products.\n[ Learn more about our Projects Learn more ](https://research.google/resources/our-projects/)\n[ Projects ](https://research.google/resources/our-projects/)\n    * ##  Publications \nPublishing our work allows us to share ideas and work collaboratively to advance the field of computer science.\n[ Learn more about our Publications Learn more ](https://research.google/pubs/)\n[ Publications ](https://research.google/pubs/)\n    * ##  Resources \nWe make products, tools, and datasets available to everyone with the goal of building a more collaborative ecosystem.\n[ Learn more about our Resources Learn more ](https://research.google/resources/)\n[ Resources ](https://research.google/resources/)\n  * Programs & events \nBack to Programs & events menu\n## Shaping the future, together.\n[ Collaborate with us ](https://research.google/programs-and-events/)\n    * ##  Student programs \nSupporting the next generation of researchers through a wide range of programming.\n[ Learn more about our Student programs Learn more ](https://research.google/programs-and-events/student-engagement/)\n[ Student programs ](https://research.google/programs-and-events/student-engagement/)\n    * ##  Faculty programs \nParticipating in the academic research community through meaningful engagement with university faculty.\n[ Learn more about our Faculty programs Learn more ](https://research.google/programs-and-events/faculty-engagement/)\n[ Faculty programs ](https://research.google/programs-and-events/faculty-engagement/)\n    * ##  Conferences & events \nConnecting with the broader research community through events is essential for creating progress in every aspect of our work.\n[ Learn more about our Conferences & events Learn more ](https://research.google/conferences-and-events/)\n[ Conferences & events ](https://research.google/conferences-and-events/)\n[ Collaborate with us ](https://research.google/programs-and-events/)\n  * [ Careers  ](https://research.google/careers/)\n  * [ Blog  ](https://research.google/blog/)\n\n\nSearch\n![](https://storage.googleapis.com/gweb-research2023-media/original_images/cca912e7fbe652676302383247087e22-Screen20Shot202022-11-0820at208.53.4920AM.png)\n  1. [Home](https://research.google/)\n  2. [Blog](https://research.google/blog/)\n\n\n# ReAct: Synergizing Reasoning and Acting in Language Models\nNovember 8, 2022\nPosted by Shunyu Yao, Student Researcher, and Yuan Cao, Research Scientist, Google Research, Brain Team\n## Quick links\n  * Share\n    * [ ](https://twitter.com/intent/tweet?text=https%3A//research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/ \"Share on Twitter\")\n    * [ ](https://www.facebook.com/sharer/sharer.php?u=https%3A//research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/ \"Share on Facebook\")\n    * [ ](https://www.linkedin.com/shareArticle?url=https%3A//research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/&mini=true \"Share on LinkedIn\")\n    *     * Copy link\n\u00d7 \n\n\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuuYg9Pduep9GkUfjloNVOiy3qjpPbT017GKlgGEGMaLNu_TCheEeJ7r8Qok6-0BK3KMfLvsN2vSgFQ8xOvnHM9CAb4Ix4I62bcN2oXFWfqAJzGAGbVqbeCyVktu3h9Dyf5ameRe54LEr32Emp0nG52iofpNOTXCxMY12K7fvmDZNPPmfJaT5zo1OBQA/s16000/Screen%20Shot%202022-11-08%20at%208.53.49%20AM.png)\nRecent advances have expanded the applicability of language models (LM) to downstream tasks. On one hand, existing language models that are properly prompted, via [chain-of-thought](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html), demonstrate emergent capabilities that carry out self-conditioned reasoning traces to derive answers from questions, excelling at various arithmetic, commonsense, and symbolic reasoning tasks. However, with chain-of-thought prompting, a model is not grounded in the external world and uses its own internal representations to generate reasoning traces, limiting its ability to reactively explore and reason or update its knowledge. On the other hand, recent work uses pre-trained language models for planning and acting in various interactive environments (e.g., [text games](https://arxiv.org/pdf/2010.02903.pdf), [web navigation](https://arxiv.org/pdf/2112.09332.pdf), [embodied tasks](https://arxiv.org/pdf/2201.07207.pdf), [robotics](https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html)), with a focus on mapping text contexts to text actions via the language model\u2019s internal knowledge. However, they do not reason abstractly about high-level goals or maintain a working memory to support acting over long horizons. \nIn \u201c[ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629.pdf)\u201d, we propose a general paradigm that combines reasoning and acting advances to enable language models to solve various language reasoning and decision making tasks. We demonstrate that the _Reason+Act_(ReAct) paradigm systematically outperforms reasoning and acting only paradigms, when prompting bigger language models and fine-tuning smaller language models. The tight integration of reasoning and acting also presents human-aligned task-solving trajectories that improve interpretability, diagnosability, and controllability.. \n## Model Overview \nReAct enables language models to generate both verbal reasoning traces and text actions in an interleaved manner. While actions lead to observation feedback from an external environment (\u201cEnv\u201d in the figure below), reasoning traces do not affect the external environment. Instead, they affect the internal state of the model by reasoning over the context and updating it with useful information to support future reasoning and acting. \n[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuuYg9Pduep9GkUfjloNVOiy3qjpPbT017GKlgGEGMaLNu_TCheEeJ7r8Qok6-0BK3KMfLvsN2vSgFQ8xOvnHM9CAb4Ix4I62bcN2oXFWfqAJzGAGbVqbeCyVktu3h9Dyf5ameRe54LEr32Emp0nG52iofpNOTXCxMY12K7fvmDZNPPmfJaT5zo1OBQA/s16000/Screen%20Shot%202022-11-08%20at%208.53.49%20AM.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuuYg9Pduep9GkUfjloNVOiy3qjpPbT017GKlgGEGMaLNu_TCheEeJ7r8Qok6-0BK3KMfLvsN2vSgFQ8xOvnHM9CAb4Ix4I62bcN2oXFWfqAJzGAGbVqbeCyVktu3h9Dyf5ameRe54LEr32Emp0nG52iofpNOTXCxMY12K7fvmDZNPPmfJaT5zo1OBQA/s595/Screen%20Shot%202022-11-08%20at%208.53.49%20AM.png)  \n---  \nPrevious methods prompt language models (LM) to either generate self-conditioned reasoning traces or task-specific actions. We propose ReAct, a new paradigm that combines reasoning and acting advances in language models.  \n## ReAct Prompting\nWe focus on the setup where a frozen language model, [PaLM-540B](https://arxiv.org/pdf/2204.02311.pdf), is prompted with few-shot in-context examples to generate both domain-specific actions (e.g., \u201csearch\u201d in question answering, and \u201cgo to\u201d in room navigation), and free-form language reasoning traces (e.g., \u201cNow I need to find a cup, and put it on the table\u201d) for task solving. \nFor tasks where reasoning is of primary importance, we alternate the generation of reasoning traces and actions so that the task-solving trajectory consists of multiple reasoning-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions, reasoning traces only need to appear sparsely in the most relevant positions of a trajectory, so we write prompts with sparse reasoning and let the language model decide the asynchronous occurrence of reasoning traces and actions for itself. \nAs shown below, there are various types of useful reasoning traces, e.g., decomposing task goals to create action plans, injecting commonsense knowledge relevant to task solving, extracting important parts from observations, tracking task progress while maintaining plan execution, handling exceptions by adjusting action plans, and so on. \nThe synergy between reasoning and acting allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interacting with the external environments (e.g., Wikipedia) to incorporate additional information into reasoning (act to reason). \n## ReAct Fine-tuning \nWe also explore fine-tuning smaller language models using ReAct-format trajectories. To reduce the need for large-scale human annotation, we use the ReAct prompted PaLM-540B model to generate trajectories, and use trajectories with task success to fine-tune smaller language models (PaLM-8/62B). \n[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoAazr9qsoobs5Nkp7_uxjml4AEWA9iwUfoNfJpcJEnj2ZOdrTXptaf9R2CyRK7Qif64zcPbywR6AeIOaeZs19vQ7OH6n-6vEyh1exiHXC965OSoNX4bsGjuIZ3Po9CuJb-LhDYyYTQr1rZum-FZ285gi11jsuiAG58C8MzifUPj8VCC_-2N3k3Fsosg/s16000/HotPotQA.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoAazr9qsoobs5Nkp7_uxjml4AEWA9iwUfoNfJpcJEnj2ZOdrTXptaf9R2CyRK7Qif64zcPbywR6AeIOaeZs19vQ7OH6n-6vEyh1exiHXC965OSoNX4bsGjuIZ3Po9CuJb-LhDYyYTQr1rZum-FZ285gi11jsuiAG58C8MzifUPj8VCC_-2N3k3Fsosg/s776/HotPotQA.png)  \n---  \nComparison of four prompting methods, (a) Standard, (b) Chain of thought (CoT, Reason Only), (c) Act-only, and (d) ReAct, solving a [HotpotQA](https://arxiv.org/abs/1809.09600) question. In-context examples are omitted, and only the task trajectory is shown. ReAct is able to retrieve information to support reasoning, while also using reasoning to target what to retrieve next, demonstrating a synergy of reasoning and acting.  \n## Results \nWe conduct empirical evaluations of ReAct and state-of-the-art baselines across four different benchmarks: question answering (HotPotQA), fact verification ([Fever](https://arxiv.org/abs/1803.05355)), text-based game ([ALFWorld](https://arxiv.org/abs/2010.03768)), and web page navigation ([WebShop](https://arxiv.org/abs/2207.01206)). For HotPotQA and Fever, with access to a [Wikipedia API](https://en.wikipedia.org/api/rest_v1/) with which the model can interact, ReAct outperforms vanilla action generation models while being competitive with chain of thought reasoning (CoT) performance. The approach with the best results is a combination of ReAct and CoT that uses both internal knowledge and externally obtained information during reasoning. \n**HotpotQA (exact match, 6-shot)** | **FEVER (accuracy, 3-shot)**  \n---|---  \nStandard  | 28.7  | 57.1   \nReason-only (CoT)  | 29.4  | 56.3   \nAct-only  | 25.7  | 58.9   \nReAct  | 27.4  | 60.9   \nBest ReAct + CoT Method  | **35.1** | **64.6**  \nSupervised SoTA  | 67.5 (using ~140k samples)  | 89.5 (using ~90k samples)   \nPaLM-540B prompting results on HotpotQA and Fever.  \n---  \nOn ALFWorld and WebShop, ReAct with both one-shot and two-shot prompting outperforms imitation and reinforcement learning methods trained with ~105 task instances, with an absolute improvement of 34% and 10% in success rates, respectively, over existing baselines. \n**AlfWorld (2-shot)** | **WebShop (1-shot)**  \n---|---  \nAct-only  | 45  | 30.1   \nReAct  | **71** | **40**  \nImitation Learning Baselines  | 37 (using ~100k samples)  | 29.1 (using ~90k samples)   \nPaLM-540B prompting task success rate results on AlfWorld and WebShop.  \n---  \n[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_0lCKXSvFq4yyjM5PGdM27OF6LWco9qFGQS1dwa3DtEF8AnAuXg9Q_nPDVyAArYwl9sGsB000-iuKJuSsNjo--fi1ZCJbrj-KwsZ6M569nWg-h2xRGHkdvQobUY9RiIr4MYkathIFyiAHZSnHAwVUfeijU-tCLyaHRgqXQah1XObtE71a00IbGdywVw/s16000/image1.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_0lCKXSvFq4yyjM5PGdM27OF6LWco9qFGQS1dwa3DtEF8AnAuXg9Q_nPDVyAArYwl9sGsB000-iuKJuSsNjo--fi1ZCJbrj-KwsZ6M569nWg-h2xRGHkdvQobUY9RiIr4MYkathIFyiAHZSnHAwVUfeijU-tCLyaHRgqXQah1XObtE71a00IbGdywVw/s839/image1.png)  \n---  \nScaling results for prompting and fine-tuning on HotPotQA with ReAct and different baselines. ReAct consistently achieves best fine-tuning performances.  \n[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgP1HCCuyIgO9D3UQKQSKFAth_Xbtqke0UO0rVbAHYA3tmbGjC6wt_du2bEm12RxFx4uWQs1LxpqaFgmHExL8QRfnPJXHVgmy-TRU3yvsDpHa-oxiX8AzmaWsm92y0J2hxdJdsjxmvFqUyYIdLIfhlr2JOIQzuaXml5YXlrF7MxC22B6thYBl72mNMKvg/s16000/image6.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgP1HCCuyIgO9D3UQKQSKFAth_Xbtqke0UO0rVbAHYA3tmbGjC6wt_du2bEm12RxFx4uWQs1LxpqaFgmHExL8QRfnPJXHVgmy-TRU3yvsDpHa-oxiX8AzmaWsm92y0J2hxdJdsjxmvFqUyYIdLIfhlr2JOIQzuaXml5YXlrF7MxC22B6thYBl72mNMKvg/s1212/image6.png)  \n---  \n[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi41aji28YNe7jqjXOC0-bdWL6nFc6jlrVXOyVD7v15lYMEJ1JNzV-Q9V1Fh-GpX5iW_gH6CWnnvGyECHQkZF33H9E3RI-GTRKA7ZhaSPjyN2rbniob0_biOcP89qZYtGMpQiodO52CJ5iauN11aitR5brKbYIdB349vFMMwqirnZ2TdufpyHz9QbOyDA/s16000/image2.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi41aji28YNe7jqjXOC0-bdWL6nFc6jlrVXOyVD7v15lYMEJ1JNzV-Q9V1Fh-GpX5iW_gH6CWnnvGyECHQkZF33H9E3RI-GTRKA7ZhaSPjyN2rbniob0_biOcP89qZYtGMpQiodO52CJ5iauN11aitR5brKbYIdB349vFMMwqirnZ2TdufpyHz9QbOyDA/s1216/image2.png)  \nA comparison of the ReAct (**top**) and CoT (**bottom**) reasoning trajectories on an example from Fever (observation for ReAct is omitted to reduce space). In this case ReAct provided the right answer, and it can be seen that the reasoning trajectory of ReAct is more grounded on facts and knowledge, in contrast to CoT\u2019s hallucination behavior.  \n---  \nWe also explore human-in-the-loop interactions with ReAct by allowing a human inspector to edit ReAct\u2019s reasoning traces. We demonstrate that by simply replacing a hallucinating sentence with inspector hints, ReAct can change its behavior to align with inspector edits and successfully complete a task. Solving tasks becomes significantly easier when using ReAct as it only requires the manual editing of a few thoughts, which enables new forms of human-machine collaboration. \n[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgORrqQ_PMp1JiljcjCXK3BqVHFR5kJ1mUxISgURlkRa6RH2fCaP3HT6rALL453TM_wD3wyKhJrfAlqlgG6jEU-RsvQsNfb02PNzqgvDLwK1XyZPaaFyc9dGRzkQzLcGGWitXzf2Mthf3YymP-0w09-pxMJxrCScFIfKxDAyFUWQCV7tR8YGGeuiNqiKA/s16000/AlfWorld.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgORrqQ_PMp1JiljcjCXK3BqVHFR5kJ1mUxISgURlkRa6RH2fCaP3HT6rALL453TM_wD3wyKhJrfAlqlgG6jEU-RsvQsNfb02PNzqgvDLwK1XyZPaaFyc9dGRzkQzLcGGWitXzf2Mthf3YymP-0w09-pxMJxrCScFIfKxDAyFUWQCV7tR8YGGeuiNqiKA/s790/AlfWorld.png)  \n---  \nA human-in-the-loop behavior correction example with ReAct on AlfWorld. (a) ReAct trajectory fails due to a hallucinating reasoning trace (Act 17). (b) A human inspector edits two reasoning traces (Act 17, 23), ReAct then produces desirable reasoning traces and actions to complete the task.  \n## Conclusion\nWe present ReAct, a simple yet effective method for synergizing reasoning and acting in language models. Through various experiments that focus on multi-hop question-answering, fact checking, and interactive decision-making tasks, we show that ReAct leads to superior performance with interpretable decision traces. \nReAct demonstrates the feasibility of jointly modeling thought, actions and feedback from the environment within a language model, making it a versatile agent that is capable of solving tasks that require interactions with the environment. We plan to further extend this line of research and leverage the strong potential of the language model for tackling broader embodied tasks, via approaches like massive multitask training and coupling ReAct with equally strong reward models. \n## Acknowledgements\n_We would like to thank Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran and Karthik Narasimhan for their great contribution in this work. We would also like to thank Google\u2019s Brain team and the Princeton NLP Group for their joint support and feedback, including project scoping, advising and insightful discussions._\n## Quick links\n  * Share\n    * [ ](https://twitter.com/intent/tweet?text=https%3A//research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/ \"Share on Twitter\")\n    * [ ](https://www.facebook.com/sharer/sharer.php?u=https%3A//research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/ \"Share on Facebook\")\n    * [ ](https://www.linkedin.com/shareArticle?url=https%3A//research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/&mini=true \"Share on LinkedIn\")\n    *     * Copy link\n\u00d7 \n\n\nFollow us \n  * [ ](https://twitter.com/GoogleAI \"Follow us on x\")\n  * [ ](https://www.linkedin.com/showcase/googleresearch/ \"Follow us on linkedin\")\n  * [ ](https://www.youtube.com/c/GoogleResearch \"Follow us on youtube\")\n  * [ ](https://github.com/google-research \"Follow us on github\")\n\n\n[ ](https://www.google.com \"Google\")\n  * [ About Google ](https://about.google/)\n  * [ Google Products ](https://about.google/intl/en/products/)\n  * [ Privacy ](https://policies.google.com/privacy)\n  * [ Terms ](https://policies.google.com/terms)\n\n\n  * [ Help ](https://support.google.com/?hl=en)\n  * Submit feedback \n\n\n",
    "answer": "```markdown\n# TL;DR: ReAct - Synergizing Reasoning and Acting in Language Models\n\nReAct combines reasoning and acting in language models, outperforming traditional methods in tasks like question answering and decision-making. It enables models to generate reasoning traces and actions interleaved, improving interpretability and task success. Future work aims to enhance embodied task capabilities.\n\n**Tags:** Generative AI, LLMs, Language Models, Google Research\n```"
  },
  {
    "instruction": "[Skip to main content](https://arxiv.org/abs/2303.17651?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN/#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/) > [cs](https://arxiv.org/list/cs/recent) > arXiv:2303.17651 \n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[ ![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg) ](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n## quick links\n  * [Login](https://arxiv.org/login)\n  * [Help Pages](https://info.arxiv.org/help)\n  * [About](https://info.arxiv.org/about)\n\n\n# Computer Science > Computation and Language\n**arXiv:2303.17651** (cs) \n[Submitted on 30 Mar 2023 ([v1](https://arxiv.org/abs/2303.17651v1)), last revised 25 May 2023 (this version, v2)]\n# Title:Self-Refine: Iterative Refinement with Self-Feedback\nAuthors:[Aman Madaan](https://arxiv.org/search/cs?searchtype=author&query=Madaan,+A), [Niket Tandon](https://arxiv.org/search/cs?searchtype=author&query=Tandon,+N), [Prakhar Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta,+P), [Skyler Hallinan](https://arxiv.org/search/cs?searchtype=author&query=Hallinan,+S), [Luyu Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao,+L), [Sarah Wiegreffe](https://arxiv.org/search/cs?searchtype=author&query=Wiegreffe,+S), [Uri Alon](https://arxiv.org/search/cs?searchtype=author&query=Alon,+U), [Nouha Dziri](https://arxiv.org/search/cs?searchtype=author&query=Dziri,+N), [Shrimai Prabhumoye](https://arxiv.org/search/cs?searchtype=author&query=Prabhumoye,+S), [Yiming Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+Y), [Shashank Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta,+S), [Bodhisattwa Prasad Majumder](https://arxiv.org/search/cs?searchtype=author&query=Majumder,+B+P), [Katherine Hermann](https://arxiv.org/search/cs?searchtype=author&query=Hermann,+K), [Sean Welleck](https://arxiv.org/search/cs?searchtype=author&query=Welleck,+S), [Amir Yazdanbakhsh](https://arxiv.org/search/cs?searchtype=author&query=Yazdanbakhsh,+A), [Peter Clark](https://arxiv.org/search/cs?searchtype=author&query=Clark,+P)\nView a PDF of the paper titled Self-Refine: Iterative Refinement with Self-Feedback, by Aman Madaan and 15 other authors\n[View PDF](https://arxiv.org/pdf/2303.17651)\n> Abstract:Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach. \nComments: | Code, data, and demo at [this https URL](https://selfrefine.info/)  \n---|---  \nSubjects: |  Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)  \nCite as: | [arXiv:2303.17651](https://arxiv.org/abs/2303.17651) [cs.CL]  \n(or  [arXiv:2303.17651v2](https://arxiv.org/abs/2303.17651v2) [cs.CL] for this version)   \n<https://doi.org/10.48550/arXiv.2303.17651> Focus to learn more arXiv-issued DOI via DataCite  \n## Submission history\nFrom: Uri Alon [[view email](https://arxiv.org/show-email/21392df0/2303.17651)] **[[v1]](https://arxiv.org/abs/2303.17651v1)** Thu, 30 Mar 2023 18:30:01 UTC (15,993 KB) **[v2]** Thu, 25 May 2023 19:13:47 UTC (1,505 KB) \nFull-text links:\n## Access Paper:\nView a PDF of the paper titled Self-Refine: Iterative Refinement with Self-Feedback, by Aman Madaan and 15 other authors\n  * [View PDF](https://arxiv.org/pdf/2303.17651)\n  * [TeX Source](https://arxiv.org/src/2303.17651)\n  * [Other Formats](https://arxiv.org/format/2303.17651)\n\n\n[ ![license icon](https://arxiv.org/icons/licenses/by-4.0.png) view license ](http://creativecommons.org/licenses/by/4.0/ \"Rights to this article\")\nCurrent browse context: \ncs.CL\n[< prev](https://arxiv.org/prevnext?id=2303.17651&function=prev&context=cs.CL \"previous in cs.CL \\(accesskey p\\)\") |  [next >](https://arxiv.org/prevnext?id=2303.17651&function=next&context=cs.CL \"next in cs.CL \\(accesskey n\\)\")\n[new](https://arxiv.org/list/cs.CL/new) |  [recent](https://arxiv.org/list/cs.CL/recent) | [2023-03](https://arxiv.org/list/cs.CL/2023-03)\nChange to browse by: \n[cs](https://arxiv.org/abs/2303.17651?context=cs) [cs.AI](https://arxiv.org/abs/2303.17651?context=cs.AI) [cs.LG](https://arxiv.org/abs/2303.17651?context=cs.LG)\n### References & Citations\n  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2303.17651)\n  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2303.17651)\n  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:2303.17651)\n\n\n### [ 1 blog link](https://arxiv.org/tb/2303.17651)\n([what is this?](https://info.arxiv.org/help/trackback.html)) \n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citation Loading...\n## BibTeX formatted citation\n\u00d7\nloading...\nData provided by: \n### Bookmark\n[ ![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png) ](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2303.17651&description=Self-Refine: Iterative Refinement with Self-Feedback \"Bookmark on BibSonomy\") [ ![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png) ](https://reddit.com/submit?url=https://arxiv.org/abs/2303.17651&title=Self-Refine: Iterative Refinement with Self-Feedback \"Bookmark on Reddit\")\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\nConnected Papers Toggle\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\nLitmaps Toggle\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\nscite.ai Toggle\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_\nDagsHub Toggle\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\nGotitPub Toggle\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\nHuggingface Toggle\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\nLinks to Code Toggle\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\nScienceCast Toggle\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\nDemos\n# Demos\nReplicate Toggle\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\nSpaces Toggle\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\nSpaces Toggle\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\nCore recommender toggle\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n\nAbout arXivLabs \n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2303.17651) | [Disable MathJax](javascript:setMathjaxCookie\\(\\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n  * [About](https://info.arxiv.org/about)\n  * [Help](https://info.arxiv.org/help)\n\n\n  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)\n  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)\n\n\n  * [Copyright](https://info.arxiv.org/help/license/index.html)\n  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n\n  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n  * [arXiv Operational Status ](https://status.arxiv.org) Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n\n\n",
    "answer": "```markdown\n# TL;DR Summary\n\n**Title:** Self-Refine: Iterative Refinement with Self-Feedback  \n**Authors:** Aman Madaan et al.  \n**Key Insight:** The Self-Refine method enhances LLM outputs through iterative self-feedback, improving performance by ~20% across diverse tasks without requiring additional training. This demonstrates the potential for real-time refinement in models like GPT-4.\n\n**Tags:** Generative AI, LLMs, Self-Feedback, Machine Learning, Artificial Intelligence\n```"
  },
  {
    "instruction": "[Chip Huyen](https://huyenchip.com/)\n[Blog](https://huyenchip.com/blog/) [Books](https://huyenchip.com/books/) [Events](https://huyenchip.com/events/)\nAI Guide\n  * [AI Roadmap](https://huyenchip.com/mlops/)\n  * [Llama Police](https://huyenchip.com/llama-police)\n  * [ML Interviews](https://huyenchip.com/ml-interviews-book/)\n\n\n[List 100](https://huyenchip.com/list-100/) [VN](https://huyenchip.com/vn/)\n  * Jan 16, 2025\n##  [Common pitfalls when building generative AI applications](https://huyenchip.com/2025/01/16/ai-engineering-pitfalls.html)\n  * Jan 7, 2025\n##  [Agents](https://huyenchip.com/2025/01/07/agents.html)\n  * Jul 25, 2024\n##  [Building A Generative AI Platform](https://huyenchip.com/2024/07/25/genai-platform.html)\n  * Apr 17, 2024\n##  [Measuring personal growth](https://huyenchip.com/2024/04/17/personal-growth.html)\n  * Mar 14, 2024\n##  [What I learned from looking at 900 most popular open source AI tools](https://huyenchip.com/2024/03/14/ai-oss.html)\n  * Feb 28, 2024\n##  [Predictive Human Preference: From Model Ranking to Model Routing](https://huyenchip.com/2024/02/28/predictive-human-preference.html)\n  * Jan 16, 2024\n##  [Generation configurations: temperature, top-k, top-p, and test time compute](https://huyenchip.com/2024/01/16/sampling.html)\n  * Oct 10, 2023\n##  [Multimodality and Large Multimodal Models (LMMs)](https://huyenchip.com/2023/10/10/multimodal.html)\n  * Aug 16, 2023\n##  [Open challenges in LLM research](https://huyenchip.com/2023/08/16/llm-research-open-challenges.html)\n  * Jun 7, 2023\n##  [Generative AI Strategy](https://huyenchip.com/2023/06/07/generative-ai-strategy.html)\n  * May 2, 2023\n##  [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)\n  * Apr 11, 2023\n##  [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html)\n  * Jan 24, 2023\n##  [What we look for in a resume](https://huyenchip.com/2023/01/24/what-we-look-for-in-a-candidate.html)\n  * Jan 8, 2023\n##  [Self-serve feature platforms: architectures and APIs](https://huyenchip.com/2023/01/08/self-serve-feature-platforms.html)\n  * Dec 27, 2022\n##  [Books that made me think (as an engineer)](https://huyenchip.com/2022/12/27/books-for-every-engineer.html)\n  * Aug 3, 2022\n##  [Introduction to streaming for data scientists](https://huyenchip.com/2022/08/03/stream-processing-for-data-scientists.html)\n  * Feb 7, 2022\n##  [Data Distribution Shifts and Monitoring](https://huyenchip.com/2022/02/07/data-distribution-shifts-and-monitoring.html)\n  * Jan 2, 2022\n##  [Real-time machine learning: challenges and solutions](https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html)\n  * Sep 13, 2021\n##  [Why data scientists shouldn\u2019t need to know Kubernetes](https://huyenchip.com/2021/09/13/data-science-infrastructure.html)\n  * Sep 7, 2021\n##  [A friendly introduction to machine learning compilers and optimizers](https://huyenchip.com/2021/09/07/a-friendly-introduction-to-machine-learning-compilers-and-optimizers.html)\n  * Feb 27, 2021\n##  [7 reasons not to join a startup and 1 reason to](https://huyenchip.com/2021/02/27/why-not-join-a-startup.html)\n  * Dec 30, 2020\n##  [Machine Learning Tools Landscape v2 (+84 new tools)](https://huyenchip.com/2020/12/30/mlops-v2.html)\n  * Dec 27, 2020\n##  [Machine learning is going real-time](https://huyenchip.com/2020/12/27/real-time-machine-learning.html)\n  * Oct 27, 2020\n##  [Course announcement - Machine Learning Systems Design at Stanford!](https://huyenchip.com/2020/10/27/ml-systems-design-stanford.html)\n  * Jun 22, 2020\n##  [What I learned from looking at 200 machine learning tools](https://huyenchip.com/2020/06/22/mlops.html)\n  * Jan 18, 2020\n##  [Analysis of compensation, level, and experience details of 19k tech workers](https://huyenchip.com/2020/01/18/tech-workers-19k-compensation-details.html)\n  * Dec 28, 2019\n##  [The books that shaped my decade](https://huyenchip.com/2019/12/28/books-that-shaped-my-decade.html)\n  * Dec 23, 2019\n##  [Four lessons I learned after my first full-time job after college](https://huyenchip.com/2019/12/23/leaving-nvidia-lessons.html)\n  * Dec 18, 2019\n##  [Key trends from NeurIPS 2019](https://huyenchip.com/2019/12/18/key-trends-neurips-2019.html)\n  * Aug 21, 2019\n##  [What Glassdoor interview reviews reveal about tech hiring cultures](https://huyenchip.com/2019/08/21/glassdoor-interview-reviews-tech-hiring-cultures.html)\n  * Aug 5, 2019\n##  [Free online machine learning curriculum](https://huyenchip.com/2019/08/05/free-online-machine-learning-curriculum.html)\n  * Jul 21, 2019\n##  [Update on Machine Learning Interviews Book](https://huyenchip.com/2019/07/21/machine-learning-interviews.html)\n  * May 12, 2019\n##  [Top 8 trends from ICLR 2019](https://huyenchip.com/2019/05/12/top-8-trends-from-iclr-2019.html)\n  * Mar 11, 2019\n##  [A simple reason why there aren't more women in tech - we're okay with misogyny](https://huyenchip.com/2019/03/11/silicon-valley-misogyny.html)\n  * Nov 16, 2018\n##  [How to build meaningful relationships after college](https://huyenchip.com/2018/11/16/building-meaningful-relationships.html)\n  * Oct 8, 2018\n##  [Career advice for recent Computer Science graduates](https://huyenchip.com/2018/10/08/career-advice-recent-cs-graduates.html)\n  * Oct 4, 2018\n##  [SOTAWHAT - A script to keep track of state-of-the-art AI research](https://huyenchip.com/2018/10/04/sotawhat.html)\n  * Mar 30, 2018\n##  [A survivor's guide to Artificial Intelligence courses at Stanford (Updated Feb 2020)](https://huyenchip.com/2018/03/30/guide-to-Artificial-Intelligence-Stanford.html)\n  * Jul 28, 2017\n##  [Confession of a so-called AI expert](https://huyenchip.com/2017/07/28/confession.html)\n\n\nsubscribe [via RSS](https://huyenchip.com/feed.xml)\n  * hi@[thiswebsite]\n  * [chiphuyen](https://github.com/chiphuyen)\n  * [chipro](https://twitter.com/chipro)\n  * [chipiscrazy](https://facebook.com/chipiscrazy)\n  * [huyenchip19](https://instagram.com/huyenchip19)\n  * [chiphuyen](https://www.linkedin.com/in/chiphuyen)\n\n\nI help companies deploy machine learning into production. I write about AI applications, tooling, and best practices. \n![](https://huyenchip.com/blog/)\n",
    "answer": "```markdown\n## TL;DR Summary\n\nChip Huyen's blog covers key insights on generative AI, LLMs, and machine learning best practices. Notable topics include pitfalls in generative AI applications, building LLM platforms, and the importance of RLHF. The blog serves as a resource for deploying ML in production effectively.\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\nAmazon SageMaker documentation\nDeploy models to Amazon SageMaker\n# Amazon SageMaker\n\ud83c\udfe1 View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersDistilabelEvaluateGradioHubHub Python LibraryHugging Face Generative AI Services (HUGS)Huggingface.jsInference API (serverless)Inference Endpoints (dedicated)LeaderboardsLightevalOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jssmolagentstimm\nSearch documentation\n`\u2318K`\nmain EN [ 365](https://github.com/huggingface/hub-docs)\n[Hugging Face on Amazon SageMaker ](https://huggingface.co/docs/sagemaker/en/index)[Get started ](https://huggingface.co/docs/sagemaker/en/getting-started)[Run training on Amazon SageMaker ](https://huggingface.co/docs/sagemaker/en/train)[Deploy models to Amazon SageMaker ](https://huggingface.co/docs/sagemaker/en/inference)[Reference ](https://huggingface.co/docs/sagemaker/en/reference)\n![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\nJoin the Hugging Face community\nand get access to the augmented documentation experience \nCollaborate on models, datasets and Spaces \nFaster examples with accelerated inference \nSwitch between documentation themes \n[Sign Up](https://huggingface.co/join)\nto get started\n# [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-models-to-amazon-sagemaker) Deploy models to Amazon SageMaker\nDeploying a \ud83e\udd17 Transformers models in SageMaker for inference is as easy as:\nCopied\n```\nfrom sagemaker.huggingface import HuggingFaceModel\n# create Hugging Face Model Class and deploy it as SageMaker endpoint\nhuggingface_model = HuggingFaceModel(...).deploy()\n```\n\nThis guide will show you how to deploy models with zero-code using the [Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit). The Inference Toolkit builds on top of the [`pipeline` feature](https://huggingface.co/docs/transformers/main_classes/pipelines) from \ud83e\udd17 Transformers. Learn how to:\n  * [Install and setup the Inference Toolkit](https://huggingface.co/docs/sagemaker/en/inference/#installation-and-setup).\n  * [Deploy a \ud83e\udd17 Transformers model trained in SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-transformer-model-trained-in-sagemaker).\n  * [Deploy a \ud83e\udd17 Transformers model from the Hugging Face [model Hub](https://huggingface.co/models)](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-model-from-the-hub).\n  * [Run a Batch Transform Job using \ud83e\udd17 Transformers and Amazon SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#run-batch-transform-with-transformers-and-sagemaker).\n  * [Create a custom inference module](https://huggingface.co/docs/sagemaker/en/inference/#user-defined-code-and-modules).\n\n\n## [](https://huggingface.co/docs/sagemaker/en/inference/#installation-and-setup) Installation and setup\nBefore deploying a \ud83e\udd17 Transformers model to SageMaker, you need to sign up for an AWS account. If you don\u2019t have an AWS account yet, learn more [here](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html).\nOnce you have an AWS account, get started using one of the following:\n  * [SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html)\n  * [SageMaker notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)\n  * Local environment\n\n\nTo start training locally, you need to setup an appropriate [IAM role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\nUpgrade to the latest `sagemaker` version.\nCopied\n```\npip install sagemaker --upgrade\n```\n\n**SageMaker environment**\nSetup your SageMaker environment as shown below:\nCopied\n```\nimport sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n```\n\n_Note: The execution role is only available when running a notebook within SageMaker. If you run`get_execution_role` in a notebook not on SageMaker, expect a `region` error._\n**Local environment**\nSetup your local environment as shown below:\nCopied\n```\nimport sagemaker\nimport boto3\niam_client = boto3.client('iam')\nrole = iam_client.get_role(RoleName='role-name-of-your-iam-role-with-right-permissions')['Role']['Arn']\nsess = sagemaker.Session()\n```\n\n## [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a--transformers-model-trained-in-sagemaker) Deploy a \ud83e\udd17 Transformers model trained in SageMaker\nThere are two ways to deploy your Hugging Face model trained in SageMaker:\n  * Deploy it after your training has finished.\n  * Deploy your saved model at a later time from S3 with the `model_data`.\n\n\n\ud83d\udcd3 Open the [deploy_transformer_model_from_s3.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb) for an example of how to deploy a model from S3 to SageMaker for inference.\n### [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-after-training) Deploy after training\nTo deploy your model directly after training, ensure all required files are saved in your training script, including the tokenizer and the model.\nIf you use the Hugging Face `Trainer`, you can pass your tokenizer as an argument to the `Trainer`. It will be automatically saved when you call `trainer.save_model()`.\nCopied\n```\nfrom sagemaker.huggingface import HuggingFace\n############ pseudo code start ############\n# create Hugging Face Estimator for training\nhuggingface_estimator = HuggingFace(....)\n# start the train job with our uploaded datasets as input\nhuggingface_estimator.fit(...)\n############ pseudo code end ############\n# deploy model to SageMaker Inference\npredictor = hf_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")\n# example request: you always need to define \"inputs\"\ndata = {\n  \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n}\n# request\npredictor.predict(data)\n```\n\nAfter you run your request you can delete the endpoint as shown:\nCopied\n```\n# delete endpoint\npredictor.delete_endpoint()\n```\n\n### [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-with-modeldata) Deploy with model_data\nIf you\u2019ve already trained your model and want to deploy it at a later time, use the `model_data` argument to specify the location of your tokenizer and model weights.\nCopied\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n  model_data=\"s3://models/my-bert-model/model.tar.gz\", # path to your trained SageMaker model\n  role=role,                      # IAM role with permissions to create an endpoint\n  transformers_version=\"4.26\",              # Transformers version used\n  pytorch_version=\"1.13\",                # PyTorch version used\n  py_version='py39',                  # Python version used\n)\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n  initial_instance_count=1,\n  instance_type=\"ml.m5.xlarge\"\n)\n# example request: you always need to define \"inputs\"\ndata = {\n  \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n}\n# request\npredictor.predict(data)\n```\n\nAfter you run our request, you can delete the endpoint again with:\nCopied\n```\n# delete endpoint\npredictor.delete_endpoint()\n```\n\n### [](https://huggingface.co/docs/sagemaker/en/inference/#create-a-model-artifact-for-deployment) Create a model artifact for deployment\nFor later deployment, you can create a `model.tar.gz` file that contains all the required files, such as:\n  * `pytorch_model.bin`\n  * `tf_model.h5`\n  * `tokenizer.json`\n  * `tokenizer_config.json`\n\n\nFor example, your file should look like this:\nCopied\n```\nmodel.tar.gz/\n|- pytorch_model.bin\n|- vocab.txt\n|- tokenizer_config.json\n|- config.json\n|- special_tokens_map.json\n```\n\nCreate your own `model.tar.gz` from a model from the \ud83e\udd17 Hub:\n  1. Download a model:\n\n\nCopied\n```\ngit lfs install\ngit clone git@hf.co:{repository}\n```\n\n  1. Create a `tar` file:\n\n\nCopied\n```\ncd {repository}\ntar zcvf model.tar.gz *\n```\n\n  1. Upload `model.tar.gz` to S3:\n\n\nCopied\n```\naws s3 cp model.tar.gz <s3://{my-s3-path}>\n```\n\nNow you can provide the S3 URI to the `model_data` argument to deploy your model later.\n## [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-model-from-the--hub) Deploy a model from the \ud83e\udd17 Hub\nTo deploy a model directly from the \ud83e\udd17 Hub to SageMaker, define two environment variables when you create a `HuggingFaceModel`:\n  * `HF_MODEL_ID` defines the model ID which is automatically loaded from [huggingface.co/models](http://huggingface.co/models) when you create a SageMaker endpoint. Access 10,000+ models on he \ud83e\udd17 Hub through this environment variable.\n  * `HF_TASK` defines the task for the \ud83e\udd17 Transformers `pipeline`. A complete list of tasks can be found [here](https://huggingface.co/docs/transformers/main_classes/pipelines).\n\n\n> \u26a0\ufe0f ** Pipelines are not optimized for parallelism (multi-threading) and tend to consume a lot of RAM. For example, on a GPU-based instance, the pipeline operates on a single vCPU. When this vCPU becomes saturated with the inference requests preprocessing, it can create a bottleneck, preventing the GPU from being fully utilized for model inference. Learn more [here](https://huggingface.co/docs/transformers/en/pipeline_webserver#using-pipelines-for-a-webserver)\nCopied\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n# Hub model configuration <https://huggingface.co/models>\nhub = {\n 'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models\n 'HF_TASK':'question-answering'              # NLP task you want to use for predictions\n}\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n  env=hub,                        # configuration for loading model from Hub\n  role=role,                       # IAM role with permissions to create an endpoint\n  transformers_version=\"4.26\",               # Transformers version used\n  pytorch_version=\"1.13\",                 # PyTorch version used\n  py_version='py39',                   # Python version used\n)\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n  initial_instance_count=1,\n  instance_type=\"ml.m5.xlarge\"\n)\n# example request: you always need to define \"inputs\"\ndata = {\n\"inputs\": {\n\t\"question\": \"What is used for inference?\",\n\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n\t}\n}\n# request\npredictor.predict(data)\n```\n\nAfter you run our request, you can delete the endpoint again with:\nCopied\n```\n# delete endpoint\npredictor.delete_endpoint()\n```\n\n\ud83d\udcd3 Open the [deploy_transformer_model_from_hf_hub.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb) for an example of how to deploy a model from the \ud83e\udd17 Hub to SageMaker for inference.\n## [](https://huggingface.co/docs/sagemaker/en/inference/#run-batch-transform-with--transformers-and-sagemaker) Run batch transform with \ud83e\udd17 Transformers and SageMaker\nAfter training a model, you can use [SageMaker batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html) to perform inference with the model. Batch transform accepts your inference data as an S3 URI and then SageMaker will take care of downloading the data, running the prediction, and uploading the results to S3. For more details about batch transform, take a look [here](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html).\n\u26a0\ufe0f The Hugging Face Inference DLC currently only supports `.jsonl` for batch transform due to the complex structure of textual data.\n_Note: Make sure your`inputs` fit the `max_length` of the model during preprocessing._\nIf you trained a model using the Hugging Face Estimator, call the `transformer()` method to create a transform job for a model based on the training job (see [here](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform) for more details):\nCopied\n```\nbatch_job = huggingface_estimator.transformer(\n  instance_count=1,\n  instance_type='ml.p3.2xlarge',\n  strategy='SingleRecord')\n\nbatch_job.transform(\n  data='s3://s3-uri-to-batch-data',\n  content_type='application/json',  \n  split_type='Line')\n```\n\nIf you want to run your batch transform job later or with a model from the \ud83e\udd17 Hub, create a `HuggingFaceModel` instance and then call the `transformer()` method:\nCopied\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n# Hub model configuration <https://huggingface.co/models>\nhub = {\n\t'HF_MODEL_ID':'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n\t'HF_TASK':'text-classification'\n}\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n  env=hub,                        # configuration for loading model from Hub\n  role=role,                       # IAM role with permissions to create an endpoint\n  transformers_version=\"4.26\",               # Transformers version used\n  pytorch_version=\"1.13\",                 # PyTorch version used\n  py_version='py39',                   # Python version used\n)\n# create transformer to run a batch job\nbatch_job = huggingface_model.transformer(\n  instance_count=1,\n  instance_type='ml.p3.2xlarge',\n  strategy='SingleRecord'\n)\n# starts batch transform job and uses S3 data as input\nbatch_job.transform(\n  data='s3://sagemaker-s3-demo-test/samples/input.jsonl',\n  content_type='application/json',  \n  split_type='Line'\n)\n```\n\nThe `input.jsonl` looks like this:\nCopied\n```\n{\"inputs\":\"this movie is terrible\"}\n{\"inputs\":\"this movie is amazing\"}\n{\"inputs\":\"SageMaker is pretty cool\"}\n{\"inputs\":\"SageMaker is pretty cool\"}\n{\"inputs\":\"this movie is terrible\"}\n{\"inputs\":\"this movie is amazing\"}\n```\n\n\ud83d\udcd3 Open the [sagemaker-notebook.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb) for an example of how to run a batch transform job for inference.\n## [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-an-llm-to-sagemaker-using-tgi) Deploy an LLM to SageMaker using TGI\nIf you are interested in using a high-performance serving container for LLMs, you can use the Hugging Face TGI container. This utilizes the [Text Generation Inference](https://github.com/huggingface/text-generation-inference) library. A list of compatible models can be found [here](https://huggingface.co/docs/text-generation-inference/supported_models#supported-models).\nFirst, make sure that the latest version of SageMaker SDK is installed:\nCopied\n```\npip install sagemaker>=2.231.0\n```\n\nThen, we import the SageMaker Python SDK and instantiate a sagemaker_session to find the current region and execution role.\nCopied\n```\nimport sagemaker\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\nimport time\nsagemaker_session = sagemaker.Session()\nregion = sagemaker_session.boto_region_name\nrole = sagemaker.get_execution_role()\n```\n\nNext we retrieve the LLM image URI. We use the helper function get_huggingface_llm_image_uri() to generate the appropriate image URI for the Hugging Face Large Language Model (LLM) inference. The function takes a required parameter backend and several optional parameters. The backend specifies the type of backend to use for the model: \u201chuggingface\u201d refers to using Hugging Face TGI backend.\nCopied\n```\nimage_uri = get_huggingface_llm_image_uri(\n backend=\"huggingface\",\n region=region\n)\n```\n\nNow that we have the image uri, the next step is to configure the model object. We specify a unique name, the image_uri for the managed TGI container, and the execution role for the endpoint. Additionally, we specify a number of environment variables including the `HF_MODEL_ID` which corresponds to the model from the HuggingFace Hub that will be deployed, and the `HF_TASK` which configures the inference task to be performed by the model.\nYou should also define `SM_NUM_GPUS`, which specifies the tensor parallelism degree of the model. Tensor parallelism can be used to split the model across multiple GPUs, which is necessary when working with LLMs that are too big for a single GPU. To learn more about tensor parallelism with inference, see our previous blog post. Here, you should set `SM_NUM_GPUS` to the number of available GPUs on your selected instance type. For example, in this tutorial, we set `SM_NUM_GPUS` to 4 because our selected instance type ml.g4dn.12xlarge has 4 available GPUs.\nNote that you can optionally reduce the memory and computational footprint of the model by setting the `HF_MODEL_QUANTIZE` environment variable to `true`, but this lower weight precision could affect the quality of the output for some models.\nCopied\n```\nmodel_name = \"llama-3-1-8b-instruct\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\nhub = {\n  'HF_MODEL_ID':'meta-llama/Llama-3.1-8B-Instruct',\n  'SM_NUM_GPUS':'1',\n\t'HUGGING_FACE_HUB_TOKEN': '<REPLACE WITH YOUR TOKEN>',\n}\nassert hub['HUGGING_FACE_HUB_TOKEN'] != '<REPLACE WITH YOUR TOKEN>', \"You have to provide a token.\"\n\nmodel = HuggingFaceModel(\n  name=model_name,\n  env=hub,\n  role=role,\n  image_uri=image_uri\n)\n```\n\nNext, we invoke the deploy method to deploy the model.\nCopied\n```\npredictor = model.deploy(\n initial_instance_count=1,\n instance_type=\"ml.g5.2xlarge\",\n endpoint_name=model_name\n)\n```\n\nOnce the model is deployed, we can invoke it to generate text. We pass an input prompt and run the predict method to generate a text response from the LLM running in the TGI container.\nCopied\n```\ninput_data = {\n \"inputs\": \"The diamondback terrapin was the first reptile to\",\n \"parameters\": {\n  \"do_sample\": True,\n  \"max_new_tokens\": 100,\n  \"temperature\": 0.7,\n  \"watermark\": True\n }\n}\npredictor.predict(input_data)\n```\n\nWe receive the following auto-generated text response:\nCopied\n```\n[{'generated_text': 'The diamondback terrapin was the first reptile to make the list, followed by the American alligator, the American crocodile, and the American box turtle. The polecat, a ferret-like animal, and the skunk rounded out the list, both having gained their slots because they have proven to be particularly dangerous to humans.\\n\\nCalifornians also seemed to appreciate the new list, judging by the comments left after the election.\\n\\n\u201cThis is fantastic,\u201d one commenter declared.\\n\\n\u201cCalifornia is a very'}]\n```\n\nOnce we are done experimenting, we delete the endpoint and the model resources.\nCopied\n```\npredictor.delete_model()\npredictor.delete_endpoint()\n```\n\n## [](https://huggingface.co/docs/sagemaker/en/inference/#user-defined-code-and-modules) User defined code and modules\nThe Hugging Face Inference Toolkit allows the user to override the default methods of the `HuggingFaceHandlerService`. You will need to create a folder named `code/` with an `inference.py` file in it. See [here](https://huggingface.co/docs/sagemaker/en/inference/#create-a-model-artifact-for-deployment) for more details on how to archive your model artifacts. For example:\nCopied\n```\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n |- inference.py\n |- requirements.txt \n```\n\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:\n  * `model_fn(model_dir)` overrides the default method for loading a model. The return value `model` will be used in `predict` for predictions. `predict` receives argument the `model_dir`, the path to your unzipped `model.tar.gz`.\n  * `transform_fn(model, data, content_type, accept_type)` overrides the default transform function with your custom implementation. You will need to implement your own `preprocess`, `predict` and `postprocess` steps in the `transform_fn`. This method can\u2019t be combined with `input_fn`, `predict_fn` or `output_fn` mentioned below.\n  * `input_fn(input_data, content_type)` overrides the default method for preprocessing. The return value `data` will be used in `predict` for predictions. The inputs are:\n    * `input_data` is the raw body of your request.\n    * `content_type` is the content type from the request header.\n  * `predict_fn(processed_data, model)` overrides the default method for predictions. The return value `predictions` will be used in `postprocess`. The input is `processed_data`, the result from `preprocess`.\n  * `output_fn(prediction, accept)` overrides the default method for postprocessing. The return value `result` will be the response of your request (e.g.`JSON`). The inputs are:\n    * `predictions` is the result from `predict`.\n    * `accept` is the return accept type from the HTTP Request, e.g. `application/json`.\n\n\nHere is an example of a custom inference module with `model_fn`, `input_fn`, `predict_fn`, and `output_fn`:\nCopied\n```\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder\ndef model_fn(model_dir):\n  # implement custom code to load the model\n  loaded_model = ...\n  \n  return loaded_model \ndef input_fn(input_data, content_type):\n  # decode the input data (e.g. JSON string -> dict)\n  data = decoder_encoder.decode(input_data, content_type)\n  return data\ndef predict_fn(data, model):\n  # call your custom model with the data\n  outputs = model(data , ... )\n  return predictions\ndef output_fn(prediction, accept):\n  # convert the model output to the desired output format (e.g. dict -> JSON string)\n  response = decoder_encoder.encode(prediction, accept)\n  return response\n```\n\nCustomize your inference module with only `model_fn` and `transform_fn`:\nCopied\n```\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder\ndef model_fn(model_dir):\n  # implement custom code to load the model\n  loaded_model = ...\n  \n  return loaded_model \ndef transform_fn(model, input_data, content_type, accept):\n   # decode the input data (e.g. JSON string -> dict)\n  data = decoder_encoder.decode(input_data, content_type)\n  # call your custom model with the data\n  outputs = model(data , ... ) \n  # convert the model output to the desired output format (e.g. dict -> JSON string)\n  response = decoder_encoder.encode(output, accept)\n  return response\n```\n\n[< > Update on GitHub](https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md)\n[\u2190Run training on Amazon SageMaker](https://huggingface.co/docs/sagemaker/en/train) [Reference\u2192](https://huggingface.co/docs/sagemaker/en/reference)\n[Deploy models to Amazon SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#deploy-models-to-amazon-sagemaker) [Installation and setup](https://huggingface.co/docs/sagemaker/en/inference/#installation-and-setup) [Deploy a \ud83e\udd17 Transformers model trained in SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a--transformers-model-trained-in-sagemaker) [Deploy after training](https://huggingface.co/docs/sagemaker/en/inference/#deploy-after-training) [Deploy with model_data](https://huggingface.co/docs/sagemaker/en/inference/#deploy-with-modeldata) [Create a model artifact for deployment](https://huggingface.co/docs/sagemaker/en/inference/#create-a-model-artifact-for-deployment) [Deploy a model from the \ud83e\udd17 Hub](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-model-from-the--hub) [Run batch transform with \ud83e\udd17 Transformers and SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#run-batch-transform-with--transformers-and-sagemaker) [Deploy an LLM to SageMaker using TGI](https://huggingface.co/docs/sagemaker/en/inference/#deploy-an-llm-to-sagemaker-using-tgi) [User defined code and modules](https://huggingface.co/docs/sagemaker/en/inference/#user-defined-code-and-modules)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe Hugging Face documentation on deploying models to Amazon SageMaker outlines steps for zero-code deployment using the Inference Toolkit. Key topics include model deployment, batch transformations, and custom inference modules. Users can deploy models from Hugging Face Hub or trained models from S3, with support for LLMs via TGI. \n\n**Tags:** Generative AI, LLMs, Amazon SageMaker\n```"
  },
  {
    "instruction": "[Skip to main content](https://learn.microsoft.com/en-us/collections/n5p4a5z7keznp5/#main)\n## AI Skills Fest\nApril 8 \u2013 May 28, 2025\nDeepen your AI skills over 50 days of learning with deep dives, hackathons, and more. \n[ Register now ](https://aka.ms/AISkillsFest_LearnPromoBanner)\nDismiss alert\nThis browser is no longer supported.\nUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\n[Download Microsoft Edge](https://go.microsoft.com/fwlink/p/?LinkID=2092881 ) [More info about Internet Explorer and Microsoft Edge](https://learn.microsoft.com/en-us/lifecycle/faq/internet-explorer-microsoft-edge)\n[ Learn ](https://learn.microsoft.com/en-us/) [ ](https://www.microsoft.com)\nSuggestions will filter as you type \n[ Sign in ](https://learn.microsoft.com/en-us/collections/n5p4a5z7keznp5/)\n  * [ Profile ](https://learn.microsoft.com/en-us/users/me/activity/)\n  * [ Settings ](https://learn.microsoft.com/en-us/users/me/settings/)\n\n\n[ Sign out ](https://learn.microsoft.com/en-us/collections/n5p4a5z7keznp5/)\n[ ](https://www.microsoft.com) [ Learn ](https://learn.microsoft.com/en-us/)\n  * Discover\n    * [ Documentation ](https://learn.microsoft.com/en-us/docs/)\nIn-depth articles on Microsoft developer tools and technologies\n    * [ Training ](https://learn.microsoft.com/en-us/training/)\nPersonalized learning paths and courses\n    * [ Credentials ](https://learn.microsoft.com/en-us/credentials/)\nGlobally recognized, industry-endorsed credentials\n    * [ Q&A ](https://learn.microsoft.com/en-us/answers/)\nTechnical questions and answers moderated by Microsoft\n    * [ Code Samples ](https://learn.microsoft.com/en-us/samples/)\nCode sample library for Microsoft developer tools and technologies\n    * [ Assessments ](https://learn.microsoft.com/en-us/assessments/)\nInteractive, curated guidance and recommendations\n    * [ Shows ](https://learn.microsoft.com/en-us/shows/)\nThousands of hours of original programming from Microsoft experts\nMicrosoft AI Skills Fest\n[ Register Now! ](https://aka.ms/AISkillsFest_MSLearn)\nApril 8 - May 28, 2025\n  * Product documentation\n    * [ ASP.NET ](https://learn.microsoft.com/en-us/aspnet/)\n    * [ Azure ](https://learn.microsoft.com/en-us/azure/)\n    * [ Dynamics 365 ](https://learn.microsoft.com/en-us/dynamics365/)\n    * [ Microsoft 365 ](https://learn.microsoft.com/en-us/microsoft-365/)\n    * [ Microsoft Copilot ](https://learn.microsoft.com/en-us/copilot/)\n    * [ Microsoft Edge ](https://learn.microsoft.com/en-us/microsoft-edge/)\n    * [ Microsoft Entra ](https://learn.microsoft.com/en-us/entra/)\n    * [ Microsoft Graph ](https://learn.microsoft.com/en-us/graph/)\n    * [ Microsoft Intune ](https://learn.microsoft.com/en-us/mem/)\n    * [ Microsoft Purview ](https://learn.microsoft.com/en-us/purview/)\n    * [ Microsoft Teams ](https://learn.microsoft.com/en-us/microsoftteams/)\n    * [ .NET ](https://learn.microsoft.com/en-us/dotnet/)\n    * [ Power Apps ](https://learn.microsoft.com/en-us/power-apps/)\n    * [ Power BI ](https://learn.microsoft.com/en-us/power-bi/)\n    * [ Power Platform ](https://learn.microsoft.com/en-us/power-platform/)\n    * [ PowerShell ](https://learn.microsoft.com/en-us/powershell/)\n    * [ SQL ](https://learn.microsoft.com/en-us/sql/)\n    * [ Sysinternals ](https://learn.microsoft.com/en-us/sysinternals/)\n    * [ Visual Studio ](https://learn.microsoft.com/en-us/visualstudio/)\n    * [ Windows ](https://learn.microsoft.com/en-us/windows/)\n    * [ Windows Server ](https://learn.microsoft.com/en-us/windows-server/)\n[ View all products ](https://learn.microsoft.com/en-us/docs/)\nMicrosoft AI Skills Fest\n[ Register Now! ](https://aka.ms/AISkillsFest_MSLearn)\nApril 8 - May 28, 2025\n  * Development languages\n    * [ C++ ](https://learn.microsoft.com/en-us/cpp/)\n    * [ C# ](https://learn.microsoft.com/en-us/dotnet/csharp/)\n    * [ DAX ](https://learn.microsoft.com/en-us/dax/)\n    * [ Java ](https://learn.microsoft.com/en-us/java/)\n    * [ OData ](https://learn.microsoft.com/en-us/odata/)\n    * [ OpenAPI ](https://learn.microsoft.com/en-us/openapi/)\n    * [ Power Query M ](https://learn.microsoft.com/en-us/powerquery-m/)\n    * [ VBA ](https://learn.microsoft.com/en-us/office/vba/)\nMicrosoft AI Skills Fest\n[ Register Now! ](https://aka.ms/AISkillsFest_MSLearn)\nApril 8 - May 28, 2025\n  * Topics\n    * [ Learn for Organizations ](https://learn.microsoft.com/en-us/training/organizations/)\n    * [ Artificial intelligence ](https://learn.microsoft.com/en-us/ai/)\n    * [ Compliance ](https://learn.microsoft.com/en-us/compliance/)\n    * [ DevOps ](https://learn.microsoft.com/en-us/devops/)\n    * [ Platform engineering ](https://learn.microsoft.com/en-us/platform-engineering/)\n    * [ Security ](https://learn.microsoft.com/en-us/security/)\nMicrosoft AI Skills Fest\n[ Register Now! ](https://aka.ms/AISkillsFest_MSLearn)\nApril 8 - May 28, 2025\n\n\nSuggestions will filter as you type \n[ Sign in ](https://learn.microsoft.com/en-us/collections/n5p4a5z7keznp5/)\n  * [ Profile ](https://learn.microsoft.com/en-us/users/me/activity/)\n  * [ Settings ](https://learn.microsoft.com/en-us/users/me/settings/)\n\n\n[ Sign out ](https://learn.microsoft.com/en-us/collections/n5p4a5z7keznp5/)\nTable of contents Exit focus mode\n  1. [ Learn ](https://learn.microsoft.com/en-us/)\n\n\n  1. [Learn](https://learn.microsoft.com/en-us/)\n\n\nTable of contents\n[ English (United States)](https://learn.microsoft.com/en-us/locale?target=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fcollections%2Fn5p4a5z7keznp5%2F)\n[ Your Privacy Choices ](https://aka.ms/yourcaliforniaprivacychoices)\nTheme\n  * Light\n  * Dark\n  * High contrast\n\n\n  * [Previous Versions](https://learn.microsoft.com/en-us/previous-versions/)\n  * [Blog](https://techcommunity.microsoft.com/t5/microsoft-learn-blog/bg-p/MicrosoftLearnBlog)\n  * [Contribute](https://learn.microsoft.com/en-us/contribute/)\n  * [Privacy](https://go.microsoft.com/fwlink/?LinkId=521839)\n  * [Terms of Use](https://learn.microsoft.com/en-us/legal/termsofuse)\n  * [Trademarks](https://www.microsoft.com/legal/intellectualproperty/Trademarks/)\n  * \u00a9 Microsoft 2025\n\n\n[ English (United States)](https://learn.microsoft.com/en-us/locale?target=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fcollections%2Fn5p4a5z7keznp5%2F)\n[ Your Privacy Choices ](https://aka.ms/yourcaliforniaprivacychoices)\nTheme\n  * Light\n  * Dark\n  * High contrast\n\n\n  * [Previous Versions](https://learn.microsoft.com/en-us/previous-versions/)\n  * [Blog](https://techcommunity.microsoft.com/t5/microsoft-learn-blog/bg-p/MicrosoftLearnBlog)\n  * [Contribute](https://learn.microsoft.com/en-us/contribute/)\n  * [Privacy](https://go.microsoft.com/fwlink/?LinkId=521839)\n  * [Terms of Use](https://learn.microsoft.com/en-us/legal/termsofuse)\n  * [Trademarks](https://www.microsoft.com/legal/intellectualproperty/Trademarks/)\n  * \u00a9 Microsoft 2025\n\n\n",
    "answer": "```markdown\n## TL;DR Summary\n\n**Event:** Microsoft AI Skills Fest  \n**Dates:** April 8 \u2013 May 28, 2025  \n**Overview:** A 50-day learning event featuring deep dives, hackathons, and training to enhance AI skills.  \n**Focus Areas:** Artificial Intelligence, DevOps, Security, and more.  \n**Registration:** [Register now](https://aka.ms/AISkillsFest_LearnPromoBanner).\n```"
  },
  {
    "instruction": "[\ud83d\ude80 Open-source RAG evaluation and testing with Evidently. New release](https://www.evidentlyai.com/blog/open-source-rag-evaluation-tool)![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc2747cf_vector.svg)\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66180fbf4f40e9ed73ca2d39_evidently_ai_logo_fi.png)](https://www.evidentlyai.com/)\nProduct\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d2624dc76fe4c62697aff8_chats-teardrop-duotone%20\\(1\\).svg)LLM Testing PlatformEvaluate LLM quality and safety](https://www.evidentlyai.com/llm-testing)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d3354a454c59d5f7affee2_list-magnifying-glass-duotone%20\\(3\\).svg)RAG TestingImprove retrieval, cut hallucinations](https://www.evidentlyai.com/rag-testing)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d2624e2132e631148cfd1a_warning-duotone%20\\(2\\).svg)AI Risk AssessmentIdentify AI risks and get a plan](https://www.evidentlyai.com/llm-readiness)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d33514b0f63efed5f253e9_crosshair-simple-duotone%20\\(1\\).svg)Adversarial TestingTest AI for threats and edge cases](https://www.evidentlyai.com/llm-red-teaming)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6650a6bcbdd0a756edf0d829_chart-bar-duotone%20\\(1\\).svg)ML MonitoringTrack data drift and predictive quality](https://www.evidentlyai.com/ml-monitoring)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9a94702f76fd53c7983b6_tree-structure-duotone%20\\(1\\).svg)AI Agent TestingValidate multi-step workflows](https://www.evidentlyai.com/ai-agent-testing)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7ee755c4dc43cefc2_users-three-duotone%20\\(1\\).svg)Open-SourceOpen-source Evidently Python library](https://www.evidentlyai.com/evidently-oss)\n##### [See Evidently in action![Evidently AI Testing for LLM](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/663511956e05f300480f19fd_llm_2.png)](https://www.evidentlyai.com/get-demo)[Get demo now![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/get-demo)\n[Pricing](https://www.evidentlyai.com/pricing)[Docs](https://docs.evidentlyai.com/)\nResources\n[![book](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abba265e01ed75b39988c_book-duotone.svg)BlogInsights on building AI products](https://www.evidentlyai.com/blog)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/675a45076643cf7042075aac_star-duotone%20\\(1\\).svg)LLM benchmarks100+ LLM benchmarks and datasets](https://www.evidentlyai.com/llm-evaluation-benchmarks-datasets)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7fbdd19d6d3f1875c_code-duotone%20\\(1\\).svg)TutorialsAI observability and MLOps tutorials](https://www.evidentlyai.com/mlops-tutorials)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb798a210c5d2640337_lightbulb-duotone%20\\(1\\).svg)ML and LLM system design500 ML and LLM use cases](https://www.evidentlyai.com/ml-system-design)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb798a210c5d2640368_check-duotone%20\\(1\\).svg)GuidesIn-depth AI quality and MLOps guides ](https://www.evidentlyai.com/mlops-guides)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdbd5bfc045c59b3467f_gear-duotone%20\\(1\\).svg)ML and AI platforms45+ internal ML and AI platforms](https://www.evidentlyai.com/ml-platforms)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7ee755c4dc43cefc2_users-three-duotone%20\\(1\\).svg)CommunityGet support and chat about AI products](https://www.evidentlyai.com/community)\n##### [Course on LLM evaluations for AI product teams![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/673f571568470a0a8d49aa43_three-cheerful-robot-students-take-exam%20\\(4\\).jpg)](https://www.evidentlyai.com/llm-evaluations-course)[Sign up now![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/llm-evaluations-course)\n[Sign up](https://www.evidentlyai.com/register)[Get demo](https://www.evidentlyai.com/get-demo)\n[Sign up](https://www.evidentlyai.com/register)[Get demo](https://www.evidentlyai.com/get-demo)\n![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abe8f5bd7dda2d12a1e6d_list-bold.svg)\n# The testing stack for **AI products**\nYou can\u2019t trust what you don\u2019t test. Make sure your AI is safe, reliable and ready \u2014 on every update.\n[Get demo](https://www.evidentlyai.com/get-demo)\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635b8bd836172caa86e1ebb_chat-text_red.svg)\nLLM apps\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9072ddf885f4391af79a8_list-magnifying-glass-duotone.svg)\nRAG systems\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9a94702f76fd53c7983b6_tree-structure-duotone%20\\(1\\).svg)\nAI agents\n![Evidently AI Conversation topics](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6633c0b3d155078a768b9eb4_1_conversation_topics_l-min.png)\n![Evidently AI Test suites](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6633bff5fb244c8549a2bd64_2_model_tests_l-min.png)\n![Evidently AI Classification quality](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6633c01f2697c242a0bec5c4_3_classification_quality_l-min.png)\n![Evidently AI Inference](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6633c07a68d804affa7dd635_4_inferences_l-min.png)\n###### open source\n## **Powered by the leading****open-source tool**\nOur platform is built on top of Evidently, a trusted open-source AI evaluation tool. With 100+ metrics readily available, it is transparent and easy to extend.\n5500+\nGitHub stars\n25m+\nDownloads\n2500+\nCommunity members\n###### why ai testing matters\n## AI fails differently\nNon-deterministic AI systems break in ways traditional software doesn\u2019t.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9051b29f6c586d6ead360_shooting-star-duotone.svg)\nHallucinations\nLLMs confidently make things up.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9055d03b1301c0b1b8a52_corners-out-duotone.svg)\nEdge cases\nUnexpected inputs bring the quality down.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635b8bee5ecefcd77fcfedb_users_red.svg)\n**Data & PII leaks**\nSensitive data slips into responses.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635baa70a9e53a3c3f0e877_660ef16a9e0687d9cc2747a0_Icon-1.svg)\n**Risky outputs**\nFrom competitor mentions to unsafe content.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c905a978e94c735737ccd3_hash-straight-duotone.svg)\n**Jailbreaks**\nBad actors hijack your AI with clever prompts.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9066d569c3b50404284ec_warning-duotone.svg)\n**Cascading errors**\nOne wrong step and the whole chain collapses.\n###### why ai testing matters\n## AI fails differently\nNon-deterministic AI systems break in ways traditional software doesn\u2019t.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9051b29f6c586d6ead360_shooting-star-duotone.svg)\nHallucinations\nLLMs confidently make things up.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9055d03b1301c0b1b8a52_corners-out-duotone.svg)\nEdge cases\nUnexpected inputs bring the quality down.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635b8bee5ecefcd77fcfedb_users_red.svg)\n**Data & PII leaks**\nSensitive data slips into responses.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635baa70a9e53a3c3f0e877_660ef16a9e0687d9cc2747a0_Icon-1.svg)\n**Risky outputs**\nFrom competitor mentions to unsafe content.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c905a978e94c735737ccd3_hash-straight-duotone.svg)\n**Jailbreaks**\nBad actors hijack your AI with clever prompts.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9066d569c3b50404284ec_warning-duotone.svg)\n**Cascading errors**\nOne wrong step and the whole chain collapses.\n###### what we do\n## LLM evaluation platform\nFrom generating test cases to delivering proof your AI system is ready. \n[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc274741_Group%2012.svg)**Automated evaluation** Measure output accuracy, safety, and quality. Get a clear, shareable report showing exactly where AI breaks \u2014 down to each response.](https://www.evidentlyai.com/#w-tabs-0-data-w-pane-0)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635b934ae1ef0b5bb85613c_shield-check_white.svg)**Synthetic data** Create realistic, edge-case, and adversarial inputs tailored to your use case \u2014 from harmless prompts to hostile attacks.](https://www.evidentlyai.com/#w-tabs-0-data-w-pane-1)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635b93535b1476b4740c0b6_gauge_white.svg)**Continuous testing** AI testing isn\u2019t a one-time check. Track performance across every update with a live dashboard \u2014 so you catch drift, regressions, and emerging risks early. ](https://www.evidentlyai.com/#w-tabs-0-data-w-pane-2)\n![Evidently AI Continuous testing for LLM](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d3133407acd4a662f42dc7_main1_2-min.png)\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d31334ab522fbce3b9b8ef_main2-min.png)\n![Evidently AI Debugging LLM](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d31334210058605b1858a6_main3-min.png)\n[Explore all platform features![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc274747_Group%2023.svg)](https://www.evidentlyai.com/llm-testing)\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635b8bdb151554d558cd61b_list-checks_red.svg)\nAdherence to guidelines and format\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635b8bdae1ef0b5bb850bab_prohibit_red.svg)\nHallucinations and factuality\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635b8bd3208f81fa06ce74e_user_red.svg)\nPII detection\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635baa70edfd934c9a260b3_660ef16a9e0687d9cc274799_Icon-2.svg)\nRetrieval quality and context relevance \n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635baa70a9e53a3c3f0e877_660ef16a9e0687d9cc2747a0_Icon-1.svg)\nSentiment, toxicity, tone, trigger words \n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635baa7836172caa86f9f72_660ef16a9e0687d9cc2747ad_Icon.svg)\nCustom evals with any prompt, model, or rule\n###### LLM EVALS\n## Track what matters for your AI use case\nEasily design your own AI quality system. Use the library of 100+ in-built metrics, or add custom ones. Combine rules, classifiers, and LLM-based evaluations. \n[Learn more![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/llm-testing)\n###### use cases\n## What do you want to test first?\nBuilt for what you are building.\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9080e1decc69b48b39603_crosshair-simple-duotone.svg)\n**Adversarial testing**\nAttack your AI system \u2014 before others do. Probe for PII leaks, jailbreaks and harmful content.\u200d\n[Learn more![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/llm-red-teaming)\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c90824ba67b7a8095f85c9_list-magnifying-glass-duotone%20\\(1\\).svg)\n**RAG evaluation**\nPrevent hallucinations and test retrieval accuracy in RAG pipelines and chatbots.\u200d\n[Learn more![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/rag-testing)\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9a96758ebc3123cf18235_tree-structure-duotone.svg)\n**AI agents**\nGo beyond single responses \u2014 validate multi-step workflows, reasoning, and tool use.\u200d\n[Learn more![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/ai-agent-testing)\n![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664d495da051917fa714ac23_chart-bar-duotone.svg)\n**Predictive systems**\nStay on top of classifiers, summarizers, recommenders, and traditional ML models.\u200d\n[Learn more![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/ml-monitoring)\n###### testimonials\n## Trusted by AI teams worldwide\nEvidently is used in 1000s of companies, from startups to enterprise. \n![Dayle Fernandes](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6614093e3e6f9e9f13329f44_dayle%20fernandes.jpeg)\n**Dayle Fernandes**\nMLOps Engineer, DeepL\n\"We use Evidently daily to test data quality and monitor production data drift. It takes away a lot of headache of building monitoring suites, so we can focus on how to react to monitoring results. Evidently is a very well-built and polished tool. It is like a Swiss army knife we use more often than expected.\"\n![Iaroslav Polianskii](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66914c75db1dad61eeea44b4_Iaroslav%20Polianskii.jpeg)\n##### **Iaroslav Polianskii**\nSenior Data Scientist, Wise\n![Egor Kraev](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66914c75d4070b050449bfcd_egor_kraev.jpeg)\n##### **Egor Kraev**\nHead of AI, Wise\n\"At Wise, Evidently proved to be a great solution for monitoring data distribution in our production environment and linking model performance metrics directly to training data. Its wide range of functionality, user-friendly visualization, and detailed documentation make Evidently a flexible and effective tool for our work. These features allow us to maintain robust model performance and make informed decisions about our machine learning systems.\"\n![Demetris Papadopoulos](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66742ae93ed3081fa3c95db0_demetris.jpeg)\n##### **Demetris Papadopoulos**\nDirector of Engineering, Martech, Flo Health\n\"Evidently is a neat and easy to use product. My team built and owns the business' ML platform, and Evidently has been one of our choices for its composition. Our model performance monitoring module with Evidently at its core allows us to keep an eye on our productionized models and act early.\"\n![Moe Antar](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66140a05d5148746ac2bac7f_Moe%20Antar.png)\n##### **Moe Antar**\nSenior Data Engineer, PlushCare\n\"We use Evidently to continuously monitor our business-critical ML models at all stages of the ML lifecycle. It has become an invaluable tool, enabling us to flag model drift and data quality issues directly from our CI/CD and model monitoring DAGs. We can proactively address potential issues before they impact our end users.\"\n![Jonathan Bown](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66140ba20007e5bec99f8b36_jonathan_bown.jpeg)\n**Jonathan Bown**\nMLOps Engineer, Western Governors University\n\"The user experience of our MLOps platform has been greatly enhanced by integrating Evidently alongside MLflow. Evidently's preset tests and metrics expedited the provisioning of our infrastructure with the tools for monitoring models in production. Evidently enhanced the flexibility of our platform for data scientists to further customize tests, metrics, and reports to meet their unique requirements.\"\n![Maltzahn](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66140e7d56847bfa1b30d7fc_Maltzahn.png)\n**Niklas von Maltzahn**\nHead of Decision Science, JUMO\n\"Evidently is a first-of-its-kind monitoring tool that makes debugging machine learning models simple and interactive. It's really easy to get started!\"\n![Dayle Fernandes](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6614093e3e6f9e9f13329f44_dayle%20fernandes.jpeg)\n##### **Dayle Fernandes**\nMLOps Engineer, DeepL\n\"We use Evidently daily to test data quality and monitor production data drift. It takes away a lot of headache of building monitoring suites, so we can focus on how to react to monitoring results. Evidently is a very well-built and polished tool. It is like a Swiss army knife we use more often than expected.\"\n![Iaroslav Polianskii](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66914c75db1dad61eeea44b4_Iaroslav%20Polianskii.jpeg)\n##### **Iaroslav Polianskii**\nSenior Data Scientist, Wise\n![Egor Kraev](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66914c75d4070b050449bfcd_egor_kraev.jpeg)\n##### **Egor Kraev**\nHead of AI, Wise\n\"At Wise, Evidently proved to be a great solution for monitoring data distribution in our production environment and linking model performance metrics directly to training data. Its wide range of functionality, user-friendly visualization, and detailed documentation make Evidently a flexible and effective tool for our work. These features allow us to maintain robust model performance and make informed decisions about our machine learning systems.\"\n![Demetris Papadopoulos](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66742ae93ed3081fa3c95db0_demetris.jpeg)\n##### **Demetris Papadopoulos**\nDirector of Engineering, Martech, Flo Health\n\"Evidently is a neat and easy to use product. My team built and owns the business' ML platform, and Evidently has been one of our choices for its composition. Our model performance monitoring module with Evidently at its core allows us to keep an eye on our productionized models and act early.\"\n![Moe Antar](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66140a05d5148746ac2bac7f_Moe%20Antar.png)\n##### **Moe Antar**\nSenior Data Engineer, PlushCare\n\"We use Evidently to continuously monitor our business-critical ML models at all stages of the ML lifecycle. It has become an invaluable tool, enabling us to flag model drift and data quality issues directly from our CI/CD and model monitoring DAGs. We can proactively address potential issues before they impact our end users.\"\n![Jonathan Bown](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66140ba20007e5bec99f8b36_jonathan_bown.jpeg)\n##### **Jonathan Bown**\nMLOps Engineer, Western Governors University\n\"The user experience of our MLOps platform has been greatly enhanced by integrating Evidently alongside MLflow. Evidently's preset tests and metrics expedited the provisioning of our infrastructure with the tools for monitoring models in production. Evidently enhanced the flexibility of our platform for data scientists to further customize tests, metrics, and reports to meet their unique requirements.\"\n![Maltzahn](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66140e7d56847bfa1b30d7fc_Maltzahn.png)\n##### **Niklas von Maltzahn**\nHead of Decision Science, JUMO\n\"Evidently is a first-of-its-kind monitoring tool that makes debugging machine learning models simple and interactive. It's really easy to get started!\"\n![Evan Lutins](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6640d83d571c36fc1a78ec9a_evan_lutins.jpg)\n**Evan Lutins**\nMachine Learning Engineer, Realtor.com\n\"At Realtor.com, we implemented a production-level feature drift pipeline with Evidently. This allows us detect anomalies, missing values, newly introduced categorical values, or other oddities in upstream data sources that we do not want to be fed into our models. Evidently's intuitive interface and thorough documentation allowed us to iterate and roll out a drift pipeline rather quickly.\"\n![Valentin Min](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66140ae20007e5bec99ef577_valentine_plaid.png)\n**Ming-Ju Valentine Lin**\nML Infrastructure Engineer, Plaid\n\"We use Evidently for continuous model monitoring, comparing daily inference logs to corresponding days from the previous week and against initial training data. This practice prevents score drifts across minor versions and ensures our models remain fresh and relevant. Evidently\u2019s comprehensive suite of tests has proven invaluable, greatly improving our model reliability and operational efficiency.\"\n![Javier Lopez Pe\u00f1a](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66140b473eb1009a926bfb4c_javier%20lopez%20pena.jpeg)\n**Javier L\u00f3pez Pe\u00f1a**\nData Science Manager, Wayflyer\n\"Evidently is a fantastic tool! We find it incredibly useful to run the data quality reports during EDA and identify features that might be unstable or require further engineering. The Evidently reports are a substantial component of our Model Cards as well. We are now expanding to production monitoring.\"\n![Ben Wilson](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66140c0740eaec15840d5422_ben%20wilson_round.png)\n**Ben Wilson**\nPrincipal RSA, Databricks\n\"Check out Evidently: I haven't seen a more promising model drift detection framework released to open-source yet!\"\n![Evan Lutins](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6640d83d571c36fc1a78ec9a_evan_lutins.jpg)\n##### **Evan Lutins**\nMachine Learning Engineer, Realtor.com\n\"At Realtor.com, we implemented a production-level feature drift pipeline with Evidently. This allows us detect anomalies, missing values, newly introduced categorical values, or other oddities in upstream data sources that we do not want to be fed into our models. Evidently's intuitive interface and thorough documentation allowed us to iterate and roll out a drift pipeline rather quickly.\"\n![Valentin Min](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66140ae20007e5bec99ef577_valentine_plaid.png)\n##### **Ming-Ju Valentine Lin**\nML Infrastructure Engineer, Plaid\n\"We use Evidently for continuous model monitoring, comparing daily inference logs to corresponding days from the previous week and against initial training data. This practice prevents score drifts across minor versions and ensures our models remain fresh and relevant. Evidently\u2019s comprehensive suite of tests has proven invaluable, greatly improving our model reliability and operational efficiency.\"\n![Ben Wilson](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66140c0740eaec15840d5422_ben%20wilson_round.png)\n##### **Ben Wilson**\nPrincipal RSA, Databricks\n\"Check out Evidently: I haven't seen a more promising model drift detection framework released to open-source yet!\"\n![Javier Lopez Pe\u00f1a](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66140b473eb1009a926bfb4c_javier%20lopez%20pena.jpeg)\n##### **Javier L\u00f3pez Pe\u00f1a**\nData Science Manager, Wayflyer\n\"Evidently is a fantastic tool! We find it incredibly useful to run the data quality reports during EDA and identify features that might be unstable or require further engineering. The Evidently reports are a substantial component of our Model Cards as well. We are now expanding to production monitoring.\"\n[See all reviews![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/reviews)\n## Join 2500+ AI builders\nBe part of the Evidently community \u2014 join the conversation, share best practices, and help shape the future of AI quality.\n[Join our Discord community](https://discord.com/invite/PyAJuUD5mB)\n###### Scale\n## Ready for enterprise\nFor teams building AI at scale, we offer a custom risk assessment to map risks, define evaluation criteria, and design a production-ready testing process.\n[Learn more![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/llm-readiness)\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635b8bd1b603d515a4b1144_cloud-check_red.svg)\nPrivate cloud deployment in a region of choice\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635b8bee5ecefcd77fcfedb_users_red.svg)\nRole-based access control\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635baa70a9e53a3c3f0e877_660ef16a9e0687d9cc2747a0_Icon-1.svg)\nDedicated support and onboarding\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6635b8bd0edfd934c9a0cfbf_bank_red.svg)\nSupport for multiple organizations \n## Start testing your AI systems today\nBook a personalized 1:1 demo with our team or sign up for a free account.\n[Get demo](https://www.evidentlyai.com/get-demo)[Start free](https://www.evidentlyai.com/register)\n![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc274771_Group%2069.svg)\nNo credit card required\n[![Evidently AI logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664ac309d9d1086b0e8309f9_evidently%20logo_white.png)](https://www.evidentlyai.com/)\nEvaluate, test and monitor your AI-powered products.\nSubscribe to our monthly newsletter\nThank you! Your submission has been received!\nOops! Something went wrong while submitting the form.\n[LLM testing platform](https://www.evidentlyai.com/llm-testing)[AI risk assessment](https://www.evidentlyai.com/llm-readiness)[RAG testing](https://www.evidentlyai.com/rag-testing)[Adversarial testing](https://www.evidentlyai.com/llm-red-teaming)[AI Agent testing](https://www.evidentlyai.com/ai-agent-testing)[ML monitoring](https://www.evidentlyai.com/ml-monitoring)[Open-source](https://www.evidentlyai.com/evidently-oss)\n[Blog](https://www.evidentlyai.com/blog)[Tutorials](https://www.evidentlyai.com/mlops-tutorials)[Guides](https://www.evidentlyai.com/mlops-guides)[ML platforms](https://www.evidentlyai.com/ml-platforms)[ML use cases](https://www.evidentlyai.com/ml-system-design)[LLM evaluations course](https://www.evidentlyai.com/llm-evaluations-course)\n[Pricing](https://www.evidentlyai.com/pricing)[Docs](https://docs.evidentlyai.com/)[GitHub](https://github.com/evidentlyai/evidently)[Community](https://www.evidentlyai.com/community)\n[Privacy policy](https://www.evidentlyai.com/privacy)[Terms of service](https://www.evidentlyai.com/terms)\n\u00a9 2025, Evidently AI. All rights reserved\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc274742_Group%2027.svg)](https://www.linkedin.com/company/evidently-ai/)[![Twitter logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664f62d9e2ffd7e31ffae6c8_x-logo-duotone%20\\(1\\).svg)](http://twitter.com/EvidentlyAI)[![Discord logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664f6316d09cc4b5e975db27_discord-logo-duotone%20\\(2\\).svg)](https://discord.com/invite/PyAJuUD5mB)[![YouTube logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664f634afd92ff37de706ab9_youtube-logo-duotone.svg)](https://www.youtube.com/c/evidentlyai)\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66180fbf4f40e9ed73ca2d39_evidently_ai_logo_fi.png)](https://www.evidentlyai.com/)\nProduct\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d2624dc76fe4c62697aff8_chats-teardrop-duotone%20\\(1\\).svg)LLM Testing PlatformEvaluate LLM quality and safety](https://www.evidentlyai.com/llm-testing)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d3354a454c59d5f7affee2_list-magnifying-glass-duotone%20\\(3\\).svg)RAG TestingImprove retrieval, cut hallucinations](https://www.evidentlyai.com/rag-testing)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d2624e2132e631148cfd1a_warning-duotone%20\\(2\\).svg)AI Risk AssessmentIdentify AI risks and get a plan](https://www.evidentlyai.com/llm-readiness)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d33514b0f63efed5f253e9_crosshair-simple-duotone%20\\(1\\).svg)Adversarial TestingTest AI for threats and edge cases](https://www.evidentlyai.com/llm-red-teaming)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6650a6bcbdd0a756edf0d829_chart-bar-duotone%20\\(1\\).svg)ML MonitoringTrack data drift and predictive quality](https://www.evidentlyai.com/ml-monitoring)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9a94702f76fd53c7983b6_tree-structure-duotone%20\\(1\\).svg)AI Agent TestingValidate multi-step workflows](https://www.evidentlyai.com/ai-agent-testing)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7ee755c4dc43cefc2_users-three-duotone%20\\(1\\).svg)Open-SourceOpen-source Evidently Python library](https://www.evidentlyai.com/evidently-oss)\n##### [See Evidently in action![Evidently AI Testing for LLM](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/663511956e05f300480f19fd_llm_2.png)](https://www.evidentlyai.com/llm-evaluations-course)[Get demo now![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/llm-evaluations-course)\n[Pricing](https://www.evidentlyai.com/pricing)[Docs](https://docs.evidentlyai.com/)\nResources\n[![book](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abba265e01ed75b39988c_book-duotone.svg)BlogInsights on building AI products](https://www.evidentlyai.com/blog)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/675a45076643cf7042075aac_star-duotone%20\\(1\\).svg)LLM benchmarks100+ LLM benchmarks and datasets](https://www.evidentlyai.com/llm-evaluation-benchmarks-datasets)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7fbdd19d6d3f1875c_code-duotone%20\\(1\\).svg)TutorialsAI observability and MLOps tutorials](https://www.evidentlyai.com/mlops-tutorials)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb798a210c5d2640337_lightbulb-duotone%20\\(1\\).svg)ML and LLM system design500 ML and LLM use cases](https://www.evidentlyai.com/ml-system-design)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb798a210c5d2640368_check-duotone%20\\(1\\).svg)GuidesIn-depth AI quality and MLOps guides](https://www.evidentlyai.com/mlops-guides)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdbd5bfc045c59b3467f_gear-duotone%20\\(1\\).svg)ML and AI platforms45+ internal ML and AI platforms](https://www.evidentlyai.com/ml-platforms)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7ee755c4dc43cefc2_users-three-duotone%20\\(1\\).svg)CommunityGet support and chat about AI products](https://www.evidentlyai.com/community)\n##### [Course on LLM evaluations for AI product teams![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/673f571568470a0a8d49aa43_three-cheerful-robot-students-take-exam%20\\(4\\).jpg)](https://www.evidentlyai.com/llm-evaluations-course)[Sign up now![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/llm-evaluations-course)\n[Sign up](https://www.evidentlyai.com/register)[Get demo](https://www.evidentlyai.com/get-demo)\n[Sign up](https://www.evidentlyai.com/register)[Get demo](https://www.evidentlyai.com/get-demo)\n![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abe8f5bd7dda2d12a1e6d_list-bold.svg)\nBy clicking \u201cAccept\u201d, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts. View our [Privacy Policy](https://www.evidentlyai.com/privacy) for more information.\n[Deny](https://www.evidentlyai.com/)[Accept](https://www.evidentlyai.com/)\nPrivacy Preferences\nEssential cookies\nRequired\nMarketing cookies\nEssential\nPersonalization cookies\nEssential\nAnalytics cookies\nEssential\n[Reject all cookies](https://www.evidentlyai.com/)[Allow all cookies](https://www.evidentlyai.com/)[Save preferences](https://www.evidentlyai.com/)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nEvidently AI has released an open-source RAG evaluation tool for testing AI systems, focusing on LLM quality, safety, and risk assessment. The platform offers automated evaluations, synthetic data generation, and continuous testing to address AI failures like hallucinations and data leaks.\n```"
  },
  {
    "instruction": "Back to Top\n[Distilled AI](https://aman.ai/primers/ai/) [Back to aman.ai](https://aman.ai)\n# Primers \u2022 Generative Pre-trained Transformer (GPT)\n  * [Introduction](https://aman.ai/primers/ai/gpt/#introduction)\n  * [GPT-1: Improving Language Understanding by Generative Pre-Training](https://aman.ai/primers/ai/gpt/#gpt-1-improving-language-understanding-by-generative-pre-training)\n  * [GPT-2: Language Models are Unsupervised Multitask Learners](https://aman.ai/primers/ai/gpt/#gpt-2-language-models-are-unsupervised-multitask-learners)\n  * [GPT-3: Language Models are Few-Shot Learners](https://aman.ai/primers/ai/gpt/#gpt-3-language-models-are-few-shot-learners)\n  * [GPT-4](https://aman.ai/primers/ai/gpt/#gpt-4)\n  * [References](https://aman.ai/primers/ai/gpt/#references)\n\n\n## Introduction\n  * The [Generative Pre-trained Transformer (GPT)](https://s3-us-west-2.amazonaws.com/openai-assets/gpt/research-covers/language-unsupervised/language_understanding_paper.pdf) by OpenAI is a family of autoregressive language models.\n  * GPT utilizes the decoder architecture from the standard [Transformer](https://aman.ai/primers/ai/transformers) network (with a few engineering tweaks) as a independent unit. This is coupled with an unprecedented size of 2048 as the number of tokens as input and 175 billion parameters (requiring ~800 GB of storage).\n  * The training method is \u201cgenerative pretraining\u201d, meaning that it is trained to predict what the next token is. The model demonstrated strong few-shot learning on many text-based tasks.\n  * The end result is the ability to generate human-like text with swift response time and great accuracy. Owing to the GPT family of models having been exposed to a reasonably large dataset and number of parameters (175B), these language models require few or in some cases no examples to fine-tune the model (a process that is called \u201cprompt-based\u201d fine-tuning) to fit the downstream task. The quality of the text generated by GPT-3 is so high that it can be difficult to determine whether or not it was written by a human, which has both benefits and risks ([source](https://analyticsindiamag.com/open-ai-gpt-3-language-model/)).\n  * Before GPT, language models (LMs) were typically trained on a large amount of accurately labelled data, which was hard to come by. These LMs offered great performance on the particular supervised task that they were trained to do, but were unable to be domain-adapted to other tasks easily.\n  * Microsoft announced on September 22, 2020, that it had licensed \u201cexclusive\u201d use of GPT-3; others can still use the public API to receive output, but only Microsoft has access to GPT-3\u2019s underlying model.\n  * Let\u2019s look below at each GPT1, GPT2, and GPT3 (with emphasis on GPT-3 as its more widely used today) and how they were able to make a dent in Natural Language Processing tasks.\n\n\n![](https://aman.ai/primers/ai/assets/gpt/1.jpg)\n## GPT-1: [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/gpt/research-covers/language-unsupervised/language_understanding_paper.pdf)\n  * GPT-1 was released in 2018 by OpenAI. It contained 117 million parameters.\n  * Trained on an enormous BooksCorpus dataset, this generative language model was able to learn large range dependencies and acquire vast knowledge on a diverse corpus of contiguous text and long stretches.\n  * GPT-1 uses the 12-layer decoder from the original transformer architecture that contains self attention.\n  * GPT was able to use transfer learning and thus, was able to perform many NLP tasks with very little fine-tuning.\n  * The right half of the image below, from the original transformer paper, [\u201cAttention Is All You Need\u201d](https://arxiv.org/abs/1706.03762), represents the decoder model:\n\n\n![](https://aman.ai/primers/ai/assets/gpt/0.jpg)\n## GPT-2: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n  * GPT-2 was released in February 2019 by OpenAI and it used a larger dataset while also adding additional parameters to build a more robust language model.\n  * GPT-2 became 10 times larger than GPT-1 with 1.5 billion parameters and had 10 times the data compared to GPT-1.\n  * [Write With Transformer](https://transformer.huggingface.co/doc/gpt2-large) is a webapp created and hosted by Hugging Face showcasing the generative capabilities of several models. GPT-2 is one of them and is available in five different sizes: small, medium, large, XL and a distilled version of the small checkpoint: distilgpt-2.\n  * GPT-2 is an unsupervised deep learning transformer-based language model created by OpenAI to help in predicting the next word or words in a sentence.\n  * Language tasks such as reading, summarizing, and translation can be learned by GPT-2 from raw text without using domain specific training data.\n\n\n## GPT-3: [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)\n  * GPT-3 is a massive language prediction and generation model developed by OpenAI capable of generating long sequences of the original text. GPT-3 became the OpenAI\u2019s breakthrough AI language program.\n  * GPT-3 is able to generate paragraphs and texts to almost sound like a person has generated them instead.\n  * GPT-3 contains 175 billion parameters and is 100 times larger than GPT-2. Its trained on 500 billion word data set known as \u201cCommon Crawl\u201d.\n  * GPT-3 is also able to write code snippets, like SQL queries, and perform other intelligent tasks. However, it is expensive and inconvenient to perform inference owing to its 175B-parameter size.\n  * GPT-3 eliminates the finetuning step that was needed for its predecessors as well as for encoder models such as BERT.\n  * GPT-3 is capable of responding to any text by generating a new piece of text that is both creative and appropriate to its context.\n  * Here is a working use case of GPT-3 you can try out: [debuild.io](https://debuild.app) where GPT will give you the code to build the application you define.\n  * Just like its predecessors, GPT-3 is an API served by OpenAI, lets look at the image below ([source](https://www.youtube.com/watch?v=MQnJZuBGmSQ&t=655s)) to see a visual representation of how this black box works:\n\n\n![](https://aman.ai/primers/ai/assets/gpt/2.jpg)\n  * The output here is generated by what GPT has learned during the training phase. GPT-3\u2019s dataset was quite massive with 300 billion tokens or words that it was trained on.\n  * It was trained on only one specific task which was predicting the next word, and thus, is an unsupervised pre-trained model.\n  * In the image below ([source](https://www.youtube.com/watch?v=MQnJZuBGmSQ&t=655s)), we can see what the training process looks like for GPT-3. \n    * We feed words into our model.\n    * We expect an output.\n    * We then check if the output matches the expected output.\n    * If not, we calculate the error or loss and update the model and ask for a new prediction.\n    * Thus, the next time it comes across this example, it knows what the output should look like.\n    * This is the general loop used in supervised training, nothing out of the ordinary, lets look below.\n\n\n![](https://aman.ai/primers/ai/assets/gpt/3.jpg)\n  * Let\u2019s visualize another way of looking at the model in the figure below ([source](https://www.youtube.com/watch?v=MQnJZuBGmSQ&t=655s)).\n\n\n![](https://aman.ai/primers/ai/assets/gpt/4.jpg)\n  * Each word goes through its own track and the model has a context window of say 2048 tokens. Thus the input and output have to fit within this range. Note, there are ways to expand and adjust this number but we will use this number for now.\n  * Each token is only processed in its own parallel \u201ctrack\u201d. Looking at it even deeper. Each token/word is a vector on its own, so when we process a word, we process a vector.\n  * This vector will then go through many layers of Transformer Decoders (GPT-3 has 96 layers), which are all stacked one on top of another as shown in the figure below ([source](https://www.youtube.com/watch?v=MQnJZuBGmSQ&t=655s)).\n\n\n![](https://aman.ai/primers/ai/assets/gpt/5.jpg)\n  * The last token is the response or the next word prediction from the model.\n\n\n## GPT-4\n  * Please refer the [primer on GPT-4](https://aman.ai/primers/ai/gpt-4).\n\n\n## References\n  * [Jay Alammar: How GPT3 Works - Easily Explained with Animations](https://www.youtube.com/watch?v=MQnJZuBGmSQ&t=655s)\n  * [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/gpt/research-covers/language-unsupervised/language_understanding_paper.pdf)\n  * [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n  * [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)\n  * [Priya Shree: The Journey of Open AI GPT models](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2)\n  * [360digitmg: GPT-1, GPT-2 and GPT-3](https://360digitmg.com/types-of-gpt-in-artificial-intelligence)\n  * [dzone: GPT-2 (GPT2) vs. GPT-3 (GPT3): The OpenAI Showdown](https://dzone.com/articles/gpt-2-gpt2-vs-gpt-3-gpt3-the-openai-showdown)\n\n\n  * [ ](https://github.com/amanchadha) | [ ](https://citations.amanchadha.com/) |  [ ](https://twitter.com/i_amanchadha) |  | \n\n\n[www.amanchadha.com](https://www.amanchadha.com/)\n",
    "answer": "```markdown\n# TL;DR Summary of Generative Pre-trained Transformer (GPT)\n\nThe GPT family by OpenAI includes GPT-1, GPT-2, and GPT-3, utilizing autoregressive language models with increasing parameters (117M to 175B). GPT-3 excels in few-shot learning, generating human-like text, and performing diverse NLP tasks without extensive fine-tuning. Microsoft has exclusive access to GPT-3's underlying model.\n```"
  },
  {
    "instruction": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc4ad869824d2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Finfer-qwak%2Fci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Finfer-qwak%2Fci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)\n[Home](https://medium.com/?source=post_page--------------------------------------------)\nFollowing\nLibrary\n[Your lists](https://medium.com/me/lists?source=post_page--------------------------------------------)[Saved lists](https://medium.com/me/lists/saved?source=post_page--------------------------------------------)[Highlights](https://medium.com/me/list/highlights?source=post_page--------------------------------------------)[Reading history](https://medium.com/me/lists/reading-history?source=post_page--------------------------------------------)\n[Stories](https://medium.com/me/stories/drafts?source=post_page--------------------------------------------)[Stats](https://medium.com/me/stats?source=post_page--------------------------------------------)\n## [Infer](https://medium.com/infer-qwak?source=post_page---publication_nav-09337d33abc6-c4ad869824d2---------------------------------------)\n[Home](https://medium.com/infer-qwak?source=post_page---publication_nav-09337d33abc6-c4ad869824d2---------------------------------------)[About](https://medium.com/infer-qwak/about?source=post_page---publication_nav-09337d33abc6-c4ad869824d2---------------------------------------)\n\u00b7\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Finfer-qwak&operation=register&redirect=https%3A%2F%2Fmedium.com%2Finfer-qwak%2Fci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2&collection=Infer&collectionId=09337d33abc6&source=post_page---publication_nav-09337d33abc6-c4ad869824d2---------------------publication_nav------------------)\n[![Infer](https://miro.medium.com/v2/resize:fill:38:38/1*pJ3GMRYAI8s5kos6JkRflQ.png)](https://medium.com/infer-qwak?source=post_page---post_publication_sidebar-09337d33abc6-c4ad869824d2---------------------------------------)\nInfer is a publication by Qwak, an end-to-end ML platform that has everything you need for ML, all in one place. Streamline ML Development using a single, centralized platform for building ML projects at any scale with ease.\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Finfer-qwak&operation=register&redirect=https%3A%2F%2Fmedium.com%2Finfer-qwak%2Fci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2&collection=Infer&collectionId=09337d33abc6&source=post_page---post_publication_sidebar-09337d33abc6-c4ad869824d2---------------------post_publication_sidebar------------------)\n# CI/CD for Machine Learning in 2024: Best Practices to Build, Train, and Deploy\n## Explore best practices for CI/CD in Machine Learning in 2024. Learn to build, test, and deploy ML models efficiently with expert strategies.\n[![JFrog ML \\(formerly Qwak\\)](https://miro.medium.com/v2/resize:fill:44:44/1*iZ4xeTCp33XMhkPe0yAlOw.png)](https://medium.com/@mlengineering?source=post_page---byline--c4ad869824d2---------------------------------------)\n[![Infer](https://miro.medium.com/v2/resize:fill:24:24/1*pJ3GMRYAI8s5kos6JkRflQ.png)](https://medium.com/infer-qwak?source=post_page---byline--c4ad869824d2---------------------------------------)\n[JFrog ML (formerly Qwak)](https://medium.com/@mlengineering?source=post_page---byline--c4ad869824d2---------------------------------------)\n\u00b7\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ccc0b76c62e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Finfer-qwak%2Fci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2&user=JFrog+ML+%28formerly+Qwak%29&userId=5ccc0b76c62e&source=post_page-5ccc0b76c62e--byline--c4ad869824d2---------------------post_header------------------)\nPublished in\n[Infer](https://medium.com/infer-qwak?source=post_page---byline--c4ad869824d2---------------------------------------)\n\u00b7\n18 min read\n\u00b7\nDec 27, 2023\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Finfer-qwak%2Fc4ad869824d2&operation=register&redirect=https%3A%2F%2Fmedium.com%2Finfer-qwak%2Fci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2&user=JFrog+ML+%28formerly+Qwak%29&userId=5ccc0b76c62e&source=---header_actions--c4ad869824d2---------------------clap_footer------------------)\n--\n11\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc4ad869824d2&operation=register&redirect=https%3A%2F%2Fmedium.com%2Finfer-qwak%2Fci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2&source=---header_actions--c4ad869824d2---------------------bookmark_footer------------------)\nListen\nShare\n _By Grig Duta, Solutions Architect at_[ _J_ Frog ML](https://www.qwak.com/post/ci-cd-pipelines-for-machine-learning)\nBuilding and deploying code to production environments is a fundamental aspect of software development. This process is equally pivotal in the realm of production-grade Machine Learning, where models undergo regular retraining with new data and are deployed for serving predictions.\nIn this article, we delve into actionable strategies for designing a robust CI/CD pipeline for Machine Learning. Our goal is to achieve near-complete automation, streamlining the process of retraining and redeploying models in production.\nThis guide is tailored to assist Data Scientists and ML Engineers in applying DevOps principles to their Machine Learning projects and systems.\nThroughout this article, we\u2019ll generally refer to ML models as \u2018predictive,\u2019 acknowledging that while the landscape of machine learning now includes less about prediction and more about embedding, recognition and generative capabilities, the core principles of model deployment remain constant.\n# Understanding CI/CD Pipelines\nIn software development, a CI/CD (Continuous Integration / Continuous Deployment) pipeline is a sequential set of automated steps designed to facilitate the integration and delivery of incremental changes to a live application.\nTypically triggered by a new commit to the version control system, CI/CD pipelines guide changes through various stages until they safely reach the production environment.\nBetween the initial code commit and the deployment to production, the code undergoes processes such as:\n  * unit testing, linting, vulnerability checks\n  * packaging into binaries or container images\n  * deployment to a staging environment for end-to-end testing.\n\n\nDepending on the application type, these changes may also undergo manual testing (User Acceptance Testing or UAT) or be directly deployed to production as a [canary release](https://www.qwak.com/post/shadow-deployment-vs-canary-release-of-machine-learning-models).\nThe Integration pipeline focuses on the initial stages of software delivery, encompassing tasks like building, testing, and packaging the application. On the other hand, the Deployment pipeline ensures the smooth deployment of new software packages in both testing and production environments.\n[![](https://miro.medium.com/v2/resize:fit:680/1*p0nEXErePVgKxqyAMWAFrg.png)](https://www.qwak.com/platform/feature-store/?utm_source=medium&utm_medium=referral&utm_campaign=featurestore)\nThis shift from traditional Ops teams to fully automated workflows in software development stems from the necessity to deliver smaller, incremental changes more frequently than the traditional approach of larger updates. The traditional method, with multiple manual steps performed by a release team, increased the likelihood of human errors affecting live systems.\nCI/CD pipelines, and more recently, GitOps principles, have become standard procedures for DevOps teams. In the following sections, we will explore the distinctions between traditional CI/CD pipelines and those tailored for the Machine Learning lifecycle.\n![](https://miro.medium.com/v2/resize:fit:512/1*GEAkrx0M2mGd0eGSFSbFIQ.png)\n_The Traditional CI/CD Cycle \u2014 Source: mlops-guide.github.io_\n# Why Use CI/CD for Machine Learning\nI vividly recall a particular interview question: \u2018Why is it crucial to package a model into a container instead of just reloading the serialized model file in the existing deployment?\u2019 The answer to this question mirrors the essence of why automated pipelines play a pivotal role in the construction and deployment of machine learning models. The key lies in pursuing an \u2018immutable\u2019 process \u2014 one that seamlessly runs through the same steps from start to finish, devoid of any \u2018side effects\u2019 that could complicate the troubleshooting of model behavior changes, such as disparities in Python package versions between data scientists\u2019 laptops.\nContinuous Integration / Continuous Delivery (CI/CD), originating from and gaining prominence in Software Development, is centered around the idea of regularly delivering incremental changes to a live application via an automated process of rebuilding, retesting, and redeploying.\nCan this approach benefit Machine Learning models? Absolutely. Yet, the primary reason for automating these processes is the inherent susceptibility of Machine Learning models to [degradation](https://www.nature.com/articles/s41598-022-15245-z), necessitating regular (or at least occasional) retraining with new data and subsequent redeployment.\nIn contrast to traditional CI/CD pipelines for standard software applications, Machine Learning introduces two additional dimensions: **Model and Data**. While conventional software engineering practices revolve around code, ML involves extensive codebases alongside the management of substantial datasets and models to extract actionable insights.\nDesigning an ML system involves grappling with challenges like:\n  * Storing model artifacts and enabling teams to track experiments with their metadata for result comparability and reproducibility.\n  * Handling often large and rapidly changing datasets. In addition to monitoring model performance from an application standpoint, ML demands vigilance in tracking data changes and adjusting models accordingly.\n\n\nML systems demand consistent monitoring for performance and data drift. When model accuracy dips below a set baseline or data experiences concept drift, the entire system must undergo another cycle. This means replicating all steps, from data validation to model training and evaluation, testing, and deployment. This underscores why ML systems stand to gain significantly from automated pipelines, especially in the context of CI/CD.\nIn the upcoming sections, we\u2019ll delve into how CI/CD pipelines for Machine Learning deviate from traditional integration and delivery in software development.\n# Exploring The Machine Learning Lifecycle\nThe [Machine Learning (ML) lifecycle](https://www.qwak.com/post/the-machine-learning-lifecycle-in-2022) is a structured sequence of stages, critical for developing, deploying, and continuously improving ML models. This lifecycle is central to ensuring models remain effective and relevant in dynamic production environments.\n  1. **Model Development (Experimentation Phase)**\n\n\nMachine learning is fundamentally an exploratory field, diverging from traditional software development\u2019s linear paths. In this experimentation phase, data scientists and ML engineers engage in a creative process, prototyping and iterating over various models. They apply diverse techniques to explore data and algorithms, aiming to uncover the most effective solutions. While automation might not be central to this stage, the goal is clear: to identify and refine promising models for deployment, thereby enhancing the value delivered to end-users. This phase culminates with the selection of models ready to enter the ML CI/CD pipeline for further development and deployment.\n**2. Model Integration and Training (Build Phase)**\nOnce a model is conceptualized, it enters the build phase. Here, new or updated code is tested and constructed into container images, which are then used for both training the model and serving predictions. The result is a trained model, stored in a [Model Registry](https://www.qwak.com/products/mlops/?utm_source=medium&utm_medium=referral&utm_campaign=cicd), and its associated container images housed in a container registry. This phase ensures that the model is not only trained with the latest data but is also packaged for efficient deployment and testing, marking a critical integration point in the lifecycle.\n**3. Model Deployment**\nThe deployment phase sees the [new model deployed in a production](https://www.qwak.com/post/what-does-it-take-to-deploy-ml-models-in-production) environment, serving either all traffic or a portion, contingent on the chosen release strategy. Best practices recommend employing separate environments like staging and production or implementing shadow (canary) deployments to receive duplicated live traffic without impacting users. The outcome is a production service for the model.\n**4. Monitoring and Retraining (Maintenance)**\nThe final phase in the lifecycle is where the deployed model is closely monitored to assess its performance against predefined metrics and baselines. Monitoring helps identify when a model begins to drift or underperform due to changing data patterns or other factors. This phase is not the end but a trigger for a new cycle of improvement \u2014 initiating retraining, adjustments, or complete redevelopment as needed. It\u2019s here that the lifecycle truly embodies a continuous loop, with insights from monitoring feeding directly back into development and training, ensuring that models evolve and improve perpetually.\n![](https://miro.medium.com/v2/resize:fit:1000/1*_T6DkvaRFMwwdgjIz9ZSkg.png)\n# Continuous Integration (CI) in Machine Learning\nIn traditional software development, the activation of a Continuous Integration (CI) pipeline is an essential process triggered by the introduction of new code into a source control platform. This pipeline evaluates the code through various processes such as unit testing, linting, security vulnerability checks, and the assembly of a container image. Following this construction phase, the pipeline may proceed to local code execution or move towards deployment in a staging environment for comprehensive end-to-end testing. The culmination of this process is the publishing of the containerized code to a container registry.\nIn Machine Learning (ML) applications, while the CI process integrates many principles of traditional software CI, it also exhibits a distinct focus and scope. The primary goal in ML CI revolves around ensuring code quality and functionality, preparing the codebase for subsequent phases of the ML lifecycle.\nThe separation of model training and evaluation from the CI pipeline in ML is driven by several key reasons:\n  * **Complexity and Resource Requirements:** Model training often demands significant computational resources, including specialized hardware like GPUs. Integrating such resource-intensive tasks into the CI phase is impractical and could impede the efficiency of the code integration process.\n  * **Separation of Concerns:** By decoupling the model training phase from the integration process, there is greater flexibility in the development workflow. Training can be conducted independently with various parameters and evaluation methods. Simultaneously, CI can proceed unencumbered, enabling direct deployment of the model using pre-existing trained models without necessitating retraining during each integration cycle.\n  * **Iterative Nature of ML Development:** Model training in ML is inherently iterative, often involving experimentation with different parameters and methods. Including this iterative training within the CI workflow would significantly slow down the process, hindering the rapid iteration and integration that CI aims to achieve.\n\n\nAs a result of these considerations, the outcome of the CI phase in ML is a packaged model code, prepared and ready for deployment in either a prediction serving or a training environment. This separation ensures that the model is primed for training, evaluation, and eventual deployment, adhering to the unique requirements and workflows inherent in ML development.\n## Tools for Continuous Integration\nFor the purposes of this article, we\u2019ve chosen to demonstrate the CI pipeline using [GitHub Actions](https://github.com/features/actions). This choice is informed by the platform\u2019s widespread adoption and its rich library of community-curated \u2018Actions\u2019, which greatly simplify the creation of CI workflows.\n**Why GitHub Actions?**\n  * **Integration with GitHub** : As a GitHub-native solution, GitHub Actions offers the advantage of residing in the same ecosystem as your code. This integration eliminates the need for navigating between different platforms, streamlining the development process.\n  * **Workflow Automation** : GitHub Actions is a powerful tool for automating build, test, and deployment processes. It allows for the creation of workflows that automatically build and test each pull request, ensuring that changes are ready for deployment.\n\n\n**Alternatives: CircleCI and Jenkins**\nWhile GitHub Actions serves as our primary example, it\u2019s important to acknowledge alternatives like CircleCI and Jenkins. Both are capable of achieving similar outcomes:\n  * **CircleCI** : Known for its ease of use and quick setup, CircleCI is a strong contender, especially for teams looking for a straightforward, cloud-based CI solution.\n  * **Jenkins** : As an open-source platform, Jenkins offers a high degree of customization and control, making it a preferred choice for teams with complex, bespoke CI requirements.\n\n\n## CI Workflow for Machine Learning\nHere\u2019s how a Continuous Integration workflow looks like in practical terms:\n```\nname: Machine Learning CI Pipelineon:  workflow_dispatch: push:  branches: [ main ] pull_request:  branches: [ main ]jobs: build-and-test:  runs-on: ubuntu-latest  steps:  - uses: actions/checkout@v4   name: Check out source code  - name: Set up Python   uses: actions/setup-python@v5   with:    python-version: '3.X' # Adjust this to your project's Python version  - name: Install Python dependencies   run: |    python -m pip install --upgrade pip    pip install -r requirements.txt # Or use another way to install dependencies  - name: Run tests   run: |    # Add your testing commands here, e.g., pytest for Python tests    pytest tests/  - name: Set up Docker Buildx   uses: docker/setup-buildx-action@v3  - name: Build Docker Image   run: |    docker build -t my-ml-model:latest . # Replace with your preferred image name  - name: Log in to Docker Registry   uses: docker/login-action@v3   with:    registry: # Replace with your Docker registry URL    username: ${{ secrets.DOCKER_USERNAME }}    password: ${{ secrets.DOCKER_PASSWORD }}  - name: Push Docker Image to Registry   run: |    docker push my-ml-model:latest # Replace with your image name\n```\n\n**Triggering the Pipeline**\nJust like in traditional software development, Machine Learning (ML) code typically begins its journey in a version control system. A push or a pull request to the \u2018main\u2019 branch is the standard trigger that activates the Continuous Integration (CI) workflow. This event signals the start of the process, ensuring that any new code integrations are immediately tested and validated.\n**Setting up the Environment**\nThe CI pipeline\u2019s initial task is to check out the code and establish a pristine environment for testing. Given that most ML applications heavily rely on Python, our example workflow prioritizes setting up the correct Python version and installing the necessary dependencies. While we\u2019ve used requirements.txt for simplicity, alternative tools like Conda and Poetry are equally effective for managing dependencies, offering flexibility based on project needs.\n**Automated Testing**\nAutomated testing typically encompasses code lints and unit tests. These tests are crucial for verifying that the model code functions as intended and does not disrupt existing features. It\u2019s important to distinguish this step from model performance testing \u2014 which is conducted post-training and involves model validation and evaluation. This distinction aims for different objectives: ensuring code integrity versus assessing model effectiveness.\n**Model Code Packaging**\nPython, being the predominant language for ML models and applications, presents two primary packaging options:\n  1. **_PyPI Artifacts_** : Ideal for Python-centric applications, this method allows for easy distribution and usage of your code. Users can simply pip install your package in their environment. This approach is often favored by Data Science teams due to its ease of sharing and implementation.\n  2. **_Container Images_** : These offer enhanced reproducibility and portability by encapsulating not just the Python code, but all the required dependencies for training and deploying the model. This method is particularly advantageous for deploying models across various cloud platforms or edge devices, and is thus more aligned with production environments.\n\n\nIt\u2019s worth noting that these packaging methods are not mutually exclusive. For instance, you could publish your Python code to a PyPI repository, enabling Data Science teams to utilize it in their experiments, while also packaging it into a container image for production deployment.\n**Deployment Preparation** Finally, to facilitate the model\u2019s deployment for training or serving, this workflow pushes the container image to a container registry. This could be DockerHub, Amazon ECR, Google GCR, or another similar service. The choice of registry often depends on the deployment platform and organizational preferences.\n![](https://miro.medium.com/v2/resize:fit:700/1*a27s3oUWt5fzLdC6qoFMkA.png)\n# Continuous Training (CT) in Machine Learning\nContinuous Training (CT) is a pivotal component in the machine learning (ML) lifecycle, focusing on the ongoing process of training and retraining ML models. This process is essential in keeping models relevant and effective in the face of evolving data and changing environments.\n**What is Continuous Training?**\nContinuous Training refers to the systematic and automated retraining of ML models. Unlike a one-time training process, CT ensures that models adapt over time, learning from new data, and improving or maintaining their performance. It\u2019s particularly crucial for applications where data patterns change frequently, such as recommendation systems, fraud detection, and predictive maintenance.\n## Tools for Continuous Training\nContinuous Training (CT) in machine learning can be facilitated using a variety of tools, ranging from self-managed solutions like Kubernetes with open-source tracking tools to fully managed platforms. Here\u2019s a detailed look at some of these options:\n**Self-Managed Solutions**\nSelf-managed solutions offer flexibility and control, appealing to teams with specific infrastructure needs or those who prefer hands-on management of their ML workflows.\n  * **Kubernetes with**[**Kubeflow**](https://www.kubeflow.org/)**and**[**Training Operator**](https://www.kubeflow.org/docs/components/training/)**:** Kubernetes provides the backbone for scalable and efficient container orchestration. When combined with Kubeflow, it creates a powerful environment tailored for ML workflows, allowing teams to run training jobs seamlessly on a multi-node cluster. The Training Operator in Kubeflow further simplifies the management of training jobs, supporting various ML frameworks. This setup is ideal for those who seek granular control over their infrastructure and ML processes.\n\n\n**Dedicated MLOps Platforms** Adopting a dedicated ML training platform ensures that your training jobs are self-contained, automating several DevOps aspects and minimizing the need for direct intervention. These platforms offer unique advantages in simplifying and streamlining the training process:\n  * **Amazon SageMaker:** SageMaker offers an end-to-end [managed environment](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html) that simplifies building, training, and deploying ML models.It allows you to run custom training jobs using your own container images while leveraging AWS\u2019s cloud infrastructure for scalability.\n  * **Google Vertex AI:** As a unified platform, [Vertex AI](https://cloud.google.com/vertex-ai/docs/training/overview) integrates various Google Cloud ML services, making it easier to manage the ML lifecycle. Its strength lies in its seamless integration with other Google services and tools.\n  * **Qwak:** Specifically tailored for MLOps, [Qwak](https://www.qwak.com/platform/model-training) enables you to train and build your model code into a deployable artifact, starting from either your local machine or a GitHub repository. Qwak\u2019s native support for GitHub Actions through [custom actions](https://docs-saas.qwak.com/docs/build-and-deploy-actions) further simplifies the process, making it highly accessible for continuous integration and deployment workflows.\n\n\n# Pipeline Triggers for Model Retraining\n[ML pipeline](https://www.qwak.com/post/end-to-end-machine-learning-pipeline) triggers can be automated to facilitate the retraining of models with new data based on different scenarios:\n  1. **Automated Trigger Post-CI:** In many ML workflows, the training pipeline is automatically triggered once the CI pipeline successfully completes. This ensures that the model is trained on the most recent, validated code.\n  2. **Ad-hoc Manual Execution:** For some projects, especially those with complex models or expensive training processes, triggering training manually is preferred. This allows data scientists to decide when to train the model, often after significant code changes or dataset updates.\n  3. **Scheduled Retraining:** Training can be scheduled to occur at regular intervals (e.g., nightly, weekly) regardless of code changes. This is common in cases where models need to be updated frequently due to evolving data.\n  4. **Model Performance Degradation:** When the model monitoring triggers an alert for prediction performance falling beneath a certain baseline, a webhook can be created to initiate retraining.\n  5. **Significant Changes in Data Distributions:** Modern Feature Stores monitor data distributions for significant changes, indicating concept drift. You can set up alerts to retrain the model on fresh data when substantial shifts in feature distributions used for prediction are detected, suggesting the model has become outdated.\n\n\n## Continuous Training (CT) Workflow\nHere\u2019s how a Training Pipeline looks like with Qwak and Github Actions:\n```\nname: Machine Learning Training Workflowon: # Trigger when the previous workflow (CI) has finished workflow_run:  workflows: [\"Machine Learning CI Pipeline\"]  types:   - completed # Trigger daily at 12 AM schedule:  - cron: \"0 0 * * *\"  # Manual execution trigger workflow_dispatch:  # Webhook trigger from a monitoring tool monitoring_webhook:  types:   - webhookjobs: ml-training:  runs-on: ubuntu-latest  steps:  - name: Check out source code   uses: actions/checkout@v4  - name: Trigger Model Training   uses: qwak-ai/build-action@v1   with:    qwak-api-key: ${{ secrets.QWAK_API_KEY }}    model-id: ${{ vars.QWAK_MODEL_ID }}     # Qwak Model ID    instance: 'gpu.t4.xl'            # example with a GPU instance    tags: ${{ github.head_ref }}         # tagging with the Github branch    from-file: '.config/build_conf.yml'     # model config from YAML file    wait: false                 # shut down after triggering    # This step will execute ML training when triggered\n```\n\nIn our example, we simplify the process by focusing on a dedicated MLOps platform that facilitates custom [model training](https://www.qwak.com/products/mlops/?utm_source=medium&utm_medium=referral&utm_campaign=cicd).\nOur workflow begins with checking out the code into the working directory. The reason for this step is that our chosen training platform handles the building of the container image. Unlike traditional methods where the user needs to build and push the container image, this platform requires just the model code, streamlining the process.\nThe training platform is being configured with a YAML file on which parameters to use for the model training, and uses GPU cloud infrastructure to leverage parallelized computations.\nThe training job is triggered with a custom GitHub Action, detailed [here](https://github.com/marketplace/actions/build-qwak-model). It simply uploads your model code from the repository to a remote training platform, where the model is trained and automatically published to a model registry for later deployment.\nLet\u2019s take AWS SageMaker as an alternate example. Here, you would need to set up AWS credentials before triggering the training job, then pass the container image that was built in the integration phase. Further details are available in this [Medium article](https://medium.com/@haythemtellili/machine-learning-ci-cd-pipeline-with-github-actions-and-amazon-sagemaker-f99818b7506a).\nIt\u2019s advisable to trigger the training job and then conclude the GitHub Actions workflow. Given that model training can be a lengthy process, sometimes extending over hours or days, it\u2019s impractical and costly to have the GitHub runner wait for its completion.\nLet\u2019s visualize the outlined process with the following diagram:\n![](https://miro.medium.com/v2/resize:fit:700/1*NUsRvC8rxFKj6CJi40Qd8g.png)\n## Additional Components\n**Data Pipeline / Feature Store**\nWhile data is a crucial ingredient for a successful model, we position it at the end of the ML lifecycle because it relates more to data preparation than the model itself. Model Development covers the process of extracting, validating, and transforming data into ML features, forming the foundation for designing the Data Pipeline or [Feature Store](https://www.qwak.com/post/what-is-a-feature-store-in-ml).\n**Model Registry & Metadata Store**\nThe additional components outlined in the previous diagram are accessed via model code, as many tools for storing features and trained models, along with their metrics, provide an SDK or REST API for interaction. Examples include [DVC](https://dvc.org/), [Weights & Biases](https://wandb.ai/site), [MLflow](https://mlflow.org/), and others.\nMost tools serving as Model Registries not only store trained models but also capture model metadata such as parameters, metrics, and various artifacts. Although a model registry may differ from a model experimentation platform in typical scenarios, for this use case, we consider them to converge in functionalities.\nThe next section will delve into how to seamlessly deploy your newly trained model to a staging environment and subsequently to production.\n# Continuous Deployment (CD) in Machine Learning\nContinuous Deployment in ML is the process of automatically deploying ML models to production after they are trained and validated. This process ensures that the latest, most effective version of the model is always in use, thereby improving the overall efficiency and performance of the system.\n## Staging or Shadow Deployment\nIn the MLOps pipeline, models are initially deployed to a staging or [shadow environment](https://www.qwak.com/post/shadow-deployment-vs-canary-release-of-machine-learning-models). This environment is a close replica of the production setup, designed to mimic real-world conditions as closely as possible.\nIn a shadow deployment, the new model runs in parallel with the existing production model. However, it does not influence actual operational decisions or outputs. This setup allows for an observational assessment of the model under realistic conditions, without any risk to the current operations.\nThe key advantage here is the ability to validate the model in a safe, controlled environment that still provides valuable insights into how it would perform in production.\n![](https://miro.medium.com/v2/resize:fit:700/1*te4VWEtx6g5hNVSZakgmsg.png)\n**Monitoring and Evaluation**\nOnce deployed in this preliminary environment, the model\u2019s performance is rigorously monitored. This involves tracking key metrics that are critical for the model\u2019s success. These metrics could include accuracy, latency, throughput, and specific business KPIs.\nMonitoring in this phase is crucial to ensure that the model behaves as expected and meets the set standards before it impacts real users.\n**A/B Testing**\nA common approach in this stage is A/B testing, where decisions made by the new model are compared against those made by the current model. This comparison helps in evaluating whether the new model marks an improvement or shows regression.\nA/B testing provides a data-driven approach to decision-making, ensuring that changes in model performance are not just observed but quantified.\n**Automated Deployment to Production**\nUpon successful validation in the preliminary environment, the model is automatically deployed to production when the Pull Request is being merged.\n[Deployment strategies](https://docs.qwak.com/docs/deployment-stretegies), such as canary releases or blue-green deployments, can be used. These methods allow a gradual and controlled transition to the new model, minimizing risk.\nTo better visualize this process please see the following diagram:\n![](https://miro.medium.com/v2/resize:fit:700/1*Ye-4VuteXQOzJ6VQhnN0aA.png)\nIn this case, the workflow can be triggered by the successful closure of a pull request, indicating a successful merge to the main branch.\n```\nname: Deploy Model to Stagingon:  pull_request:  types: [closed] workflow_dispatch:        # Enables manual trigger jobs: deploy:  runs-on: ubuntu-latest  if: github.event.pull_request.merged == true  Steps:   # Deploy model to a shadow/canary endpoint    - name: Deploy Qwak Build    uses: qwak-ai/deploy-action@v1    with:     qwak-api-key: ${{ secrets.QWAK_API_KEY }}     deploy-type: 'realtime'        # Deploying a real time endpoint     model-id: ${{ vars.QWAK_MODEL_ID }}  # Qwak Model ID     build-id: ${{ github.head_ref }}    # Using the branch name as build ID     param-list: |                        variation-name=shadow,from-file=.config/deploy_conf.yaml\n```\n\nIn the provided example, the workflow deploys to the Qwak platform, specifically targeting a shadow endpoint. The deployment\u2019s configuration is sourced from a configuration file stored within the repository. This configuration encompasses various settings, including the number of serving workers per replica, the total number of replicas (which corresponds to Kubernetes pods), variation settings, batching parameters for parallel processing, timeout settings, and more.\nThe choice to use Qwak\u2019s platform for this process is driven by its simplicity and ease of use, especially when compared to the complexities that might be involved in explaining a deployment using ArgoCD on a Kubernetes cluster.\nThe workflow is designed to be triggered in two scenarios: either manually at the user\u2019s discretion or automatically when a pull request is merged. It leverages a [custom GitHub action](https://docs-saas.qwak.com/docs/advanced-github-actions#build-and-deploy-shadow-variations-on-pull-request) that interfaces with Qwak, streamlining the deployment process and ensuring efficient and reliable model deployment to the shadow environment.\n# Continuous Monitoring in the MLOps Lifecycle\nIn discussing the automation of the MLOps pipeline, it\u2019s crucial to emphasize the significance of closing the loop with Continuous Monitoring. This essential phase ensures that the entire cycle, from integration and deployment to continuous training, isn\u2019t a one-off process but a dynamic, ongoing journey\nContinuous Monitoring in the MLOps lifecycle, particularly following CI/CD/CT processes, is critical for maintaining and enhancing the performance of machine learning models in production. This phase ensures that models are not only deployed efficiently but also continue to operate effectively over time. Through continuous performance evaluation and drift detection, it enables timely responses to changes in data or model behavior. Importantly, it establishes a feedback loop, where insights from production directly inform and improve subsequent iterations of model development and training. This creates a dynamic cycle of continuous improvement, essential for the long-term success and relevance of ML models in real-world applications.\n![](https://miro.medium.com/v2/resize:fit:512/1*XYFRjiOfu99WV3DVuMEPaw.jpeg)\nSource: ml-ops.org\n# Conclusion\nIn conclusion, navigating the complexities of building and operating an integrated ML system in production represents the true challenge in machine learning. It extends beyond the initial model-building phase, considering the dynamic nature of datasets that organizations commonly encounter.\nThe journey involves designing effective CI/CD pipelines, accommodating not just code integration but also the intricacies of model training, deployment, and ongoing testing. Successfully orchestrating these workflows is crucial for maintaining model performance, addressing drift, and ensuring the reliability of predictions in real-world scenarios.\nFor a hands-on guide on implementing these workflows, explore the documentation in [JFrog ML\u2019s CI/CD for Machine Learning](https://docs.qwak.com/docs/build-and-deploy-actions) section. It offers practical insights into setting up integrated model training, building and deployment using Github Actions.\n_Originally published in the_[ _JFrog ML Blog_](https://www.qwak.com/post/ci-cd-pipelines-for-machine-learning) _._\n![](https://miro.medium.com/v2/da:true/resize:fit:0/5c50caa54067fd622d2f0fac18392213bf92f6e2fae89b691e62bceb40885e74)\n## Sign up to discover human stories that deepen your understanding of the world.\n## Free\nDistraction-free reading. No ads.\nOrganize your knowledge with lists and highlights.\nTell your story. Find your audience.\nSign up for free\n## Membership\nRead member-only stories\nSupport writers you read most\nEarn money for your writing\nListen to audio narrations\nRead offline with the Medium app\nTry for $5/month\n[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----c4ad869824d2---------------------------------------)\n[Ci Cd Pipeline](https://medium.com/tag/ci-cd-pipeline?source=post_page-----c4ad869824d2---------------------------------------)\n[Ml Model Deployment](https://medium.com/tag/ml-model-deployment?source=post_page-----c4ad869824d2---------------------------------------)\n[Mlops](https://medium.com/tag/mlops?source=post_page-----c4ad869824d2---------------------------------------)\n[Continuous Integration](https://medium.com/tag/continuous-integration?source=post_page-----c4ad869824d2---------------------------------------)\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Finfer-qwak%2Fc4ad869824d2&operation=register&redirect=https%3A%2F%2Fmedium.com%2Finfer-qwak%2Fci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2&user=JFrog+ML+%28formerly+Qwak%29&userId=5ccc0b76c62e&source=---footer_actions--c4ad869824d2---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Finfer-qwak%2Fc4ad869824d2&operation=register&redirect=https%3A%2F%2Fmedium.com%2Finfer-qwak%2Fci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2&user=JFrog+ML+%28formerly+Qwak%29&userId=5ccc0b76c62e&source=---footer_actions--c4ad869824d2---------------------clap_footer------------------)\n--\n11\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc4ad869824d2&operation=register&redirect=https%3A%2F%2Fmedium.com%2Finfer-qwak%2Fci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2&source=---footer_actions--c4ad869824d2---------------------bookmark_footer------------------)\n[![Infer](https://miro.medium.com/v2/resize:fill:48:48/1*pJ3GMRYAI8s5kos6JkRflQ.png)](https://medium.com/infer-qwak?source=post_page---post_publication_info--c4ad869824d2---------------------------------------)\n[![Infer](https://miro.medium.com/v2/resize:fill:64:64/1*pJ3GMRYAI8s5kos6JkRflQ.png)](https://medium.com/infer-qwak?source=post_page---post_publication_info--c4ad869824d2---------------------------------------)\nFollow\n## [Published in Infer](https://medium.com/infer-qwak?source=post_page---post_publication_info--c4ad869824d2---------------------------------------)\n[87 Followers](https://medium.com/infer-qwak/followers?source=post_page---post_publication_info--c4ad869824d2---------------------------------------)\n\u00b7[Last published Jun 25, 2024](https://medium.com/infer-qwak/breaking-down-the-cost-of-large-language-models-22704f821607?source=post_page---post_publication_info--c4ad869824d2---------------------------------------)\nInfer is a publication by Qwak, an end-to-end ML platform that has everything you need for ML, all in one place. Streamline ML Development using a single, centralized platform for building ML projects at any scale with ease.\nFollow\n[![JFrog ML \\(formerly Qwak\\)](https://miro.medium.com/v2/resize:fill:48:48/1*iZ4xeTCp33XMhkPe0yAlOw.png)](https://medium.com/@mlengineering?source=post_page---post_author_info--c4ad869824d2---------------------------------------)\n[![JFrog ML \\(formerly Qwak\\)](https://miro.medium.com/v2/resize:fill:64:64/1*iZ4xeTCp33XMhkPe0yAlOw.png)](https://medium.com/@mlengineering?source=post_page---post_author_info--c4ad869824d2---------------------------------------)\nFollow\n## [Written by JFrog ML (formerly Qwak)](https://medium.com/@mlengineering?source=post_page---post_author_info--c4ad869824d2---------------------------------------)\n[1.2K Followers](https://medium.com/@mlengineering/followers?source=post_page---post_author_info--c4ad869824d2---------------------------------------)\n\u00b7[84 Following](https://medium.com/@mlengineering/following?source=post_page---post_author_info--c4ad869824d2---------------------------------------)\nA fully managed MLOps platform that unifies ML engineering and data operations.\nFollow\n## Responses (11)\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--c4ad869824d2---------------------------------------)\n![](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)\nWrite a response\n[What are your thoughts?](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Finfer-qwak%2Fci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2&source=---post_responses--c4ad869824d2---------------------respond_sidebar------------------)\nCancel\nRespond\nAlso publish to my profile\nSee all responses\n[Help](https://help.medium.com/hc/en-us?source=post_page-----c4ad869824d2---------------------------------------)\n[Status](https://medium.statuspage.io/?source=post_page-----c4ad869824d2---------------------------------------)\n[About](https://medium.com/about?autoplay=1&source=post_page-----c4ad869824d2---------------------------------------)\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----c4ad869824d2---------------------------------------)\nPress\n[Blog](https://blog.medium.com/?source=post_page-----c4ad869824d2---------------------------------------)\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c4ad869824d2---------------------------------------)\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----c4ad869824d2---------------------------------------)\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c4ad869824d2---------------------------------------)\n[Text to speech](https://speechify.com/medium?source=post_page-----c4ad869824d2---------------------------------------)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe article discusses best practices for CI/CD in Machine Learning for 2024, emphasizing automation in model training, deployment, and monitoring. Key insights include the importance of continuous integration, the ML lifecycle, and tools like GitHub Actions and Qwak for efficient workflows. Continuous monitoring ensures models adapt to data changes, maintaining performance over time.\n```"
  },
  {
    "instruction": "[April 3: DBOS User Group Online Meeting!](https://www.dbos.dev/dbos-user-group-meeting)\n[![DBOS - Logo](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67241465129b580e4b924af4_Vectors-Wrapper.svg)](https://www.dbos.dev/)\nProducts\n[DBOS TransactOpen source durable execution library](https://www.dbos.dev/dbos-transact)[DBOS CloudDeploy with a click, scale to millions](https://www.dbos.dev/dbos-cloud)\nUse Cases\n[Durable Data PipelinesDurable, observable, scalable by default](https://www.dbos.dev/solutions/data-pipeline-platform)[Agentic AI WorkflowsCrash-proof AI automation](https://www.dbos.dev/solutions/agentic-ai-platform)[Cron JobsFree cron job hosting](https://www.dbos.dev/solutions/cron-hosting)[Cloud ModernizationCloud-enable or replatform legacy systems](https://www.dbos.dev/solutions/cloud-modernization)\n[Pricing](https://www.dbos.dev/dbos-pricing)[Docs](https://docs.dbos.dev/)\nResources\n[About DBOSSee our story, meet our team.](https://www.dbos.dev/about)[BlogThe latest on cloud & reliable backends.](https://www.dbos.dev/blog)[VideosDBOS concepts and best practices](https://www.dbos.dev/videos)\n[Start your project](https://console.dbos.dev/login-redirect)\n[Login](https://console.dbos.dev/login-redirect)\n[](https://github.com/dbos-inc/dbos-transact-py)[](https://discord.com/invite/jsmC6pXGgX)[Login](https://console.dbos.dev/launch)\n[Start your project](https://docs.dbos.dev/quickstart)\n# Reliable backends,built effortlessly\nReliable backends,built effortlessly\nDBOS provides an open-source library for building reliable and observable programs.Add a few annotations to your program to make it resilient to any failure.\n[Start your project](https://docs.dbos.dev/quickstart)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/6730407c420dae5eaf176275_Glow.svg)\n[Durable execution](https://www.dbos.dev/#w-tabs-0-data-w-pane-0)[Event-driven processing](https://www.dbos.dev/#w-tabs-0-data-w-pane-1)[Cron jobs](https://www.dbos.dev/#w-tabs-0-data-w-pane-2)[Data pipelines](https://www.dbos.dev/#w-tabs-0-data-w-pane-3)[Reliable AI agents](https://www.dbos.dev/#w-tabs-0-data-w-pane-4)[Webhooks & notifications](https://www.dbos.dev/#w-tabs-0-data-w-pane-5)\n## Durable workflow orchestration\nWrite your business logic in normal code, with branches, loops, subtasks, and retries. The DBOS Transact library makes it resilient to any failure.\n[See an example](https://docs.dbos.dev/python/tutorials/workflow-tutorial)\n@DBOS.workflow()def checkout_workflow(items): order = create_order() reserve_inventory(order, items) payment_status = process_payment(order, items) if payment_status == 'paid': fulfill_order(order) else: undo_reserve_inventory(order, items) cancel_order(order)\n## Event-driven processing\nConsume Kafka messages exactly-once, no need to worry about timeouts or offsets.\n[See an example](https://docs.dbos.dev/python/tutorials/kafka-integration)\n@DBOS.kafka_consumer(config,[\"alerts-topic\"])@DBOS.workflow()def process_kafka_alerts(msg: KafkaMessage): alerts = msg.value.decode() for alert in alerts: respond_to_alert(alert)\n## Cron jobs made easy\nSchedule your durable workflows to run exactly once per time interval.\n[See an example](https://docs.dbos.dev/python/examples/cron-starter)\n@DBOS.scheduled(\"0 * * * *\") # Run once an hour@DBOS.workflow()def run_hourly(scheduled_time: datetime, actual_time: datetime): results = search_hackernews(\"serverless\") for comment, url in results: post_to_slack(comment, url)\n## Resilient data pipelines\nBuild data pipelines that are reliable and observable by default.DBOS durable queues guarantee all your tasks complete.\n[See an example](https://docs.dbos.dev/python/examples/document-detective)\nqueue = Queue(\"indexing_queue\")@DBOS.workflow()def indexing_workflow(urls: List[HttpUrl]): handles: List[WorkflowHandle] = [] for url in urls: handle = queue.enqueue(index_document, url) handles.append(handle) indexed_pages = 0 for handle in handles: indexed_pages += handle.get_result() logger.info(f\"Indexed {len(urls)} documents totaling {indexed_pages} pages\")\n## Reliable AI agents\nEnhance your AI workflows with DBOS, build reliable AI agents with automatic retries and no limit on how long they can run for.\n[See an example](https://docs.dbos.dev/python/examples/reliable-ai-agent)\n@DBOS.step(retries_allowed=True, max_attempts=2)def process_refund(item, reason): return f\"Processed refund for item {item}, because {reason}\"@DBOS.step(retries_allowed=True, max_attempts=3)def apply_discount(amount): return f\"Applied discount of {amount}%\"refunds_agent = Agent( name=\"Refunds Agent\", instructions=\"Help the user with a refund. If the reason is that it was too expensive, offer the user a refund code.\", functions=[process_refund, apply_discount],)\n## Webhooks & notifications\nEffortlessly mix synchronous webhook code with asynchronous event processing. Reliably wait weeks or months for events, then use idempotency and durable execution to process them exactly once.\n[See an example](https://docs.dbos.dev/python/examples/rag-slackbot)\n@slackapp.message()def handle_message(request: BoltRequest) -> None: event_id = request.body[\"event_id\"] with SetWorkflowID(event_id):DBOS.start_workflow(message_workflow, request.body[\"event\"])\n\"I love the design of DBOS. If you're a gamer, it's like having a \u201csave point\u201d in your programs. If a function fails, a new function can start, picking up at the last checkpoint.\"\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e21d345ceb6064084225f8_Paul-Copplestone.jpeg)\nPaul Copplestone\nCEO & Co-Founder, Supabase.com\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/678ac6f8c8e18a35ad94d208_supabase-logo-wordmark-dark.png)\n\"With DBOS, developers can build applications in days that now take months with conventional approaches.\"\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/673ab74082767acb2d4c191c_Matei-Zaharia.png)\nMatei Zaharia\nCo-Founder, Databricks\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/673ab683b32cf03625b9fa9b_databricks-logo.png)\n\u201cWhat took us 2 months to build using a labyrinth of AWS resources took just 2 days with DBOS.\u201d\n![Thomas McNally Portrait](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/6724211a9ec6f0803fe3cdd8_TMcN.webp)\nThomas McNally\nVP Technology, TMG.io\n![tmg Logo](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/6729b592490a06a163cae7c3_TMG.png)\n\"DBOS isn't kidding when they say they make serverless simple. Their claims about speed and ease are right on.\"\n![Vince Fulco - Portrait](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/675b7ab0c18c12e6de867bb0_vince-fulco.jpeg)\nVince Fulco\nCo-Founder, Bighire.io\n![Bighire.io Logo](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/6729b592b8ea54af0f836896_Bighire.io.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a448eddd3bf98456f5530b_playfix-Logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a4478dc32b9c82f6d90472_Pin-Out-Logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a44180c41e3f2060b782a6_Strobe-Alert-logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a440a450b5c5639417bb2a_Digital-Underground-Technology-logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a434bf698fe4c1cea87770_Ravenvolt-logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a430264b81088f346238d4_Pearstop-logo-white.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/673d27061af0558b20254b3e_zeep-chat.svg)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67242fbc49dd3e04041ba3db_Bighire.io.svg)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67242fd05d5d91b38b2f3a9a_TMG.svg)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a448eddd3bf98456f5530b_playfix-Logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a4478dc32b9c82f6d90472_Pin-Out-Logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a44180c41e3f2060b782a6_Strobe-Alert-logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a440a450b5c5639417bb2a_Digital-Underground-Technology-logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a434bf698fe4c1cea87770_Ravenvolt-logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a430264b81088f346238d4_Pearstop-logo-white.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/673d27061af0558b20254b3e_zeep-chat.svg)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67242fbc49dd3e04041ba3db_Bighire.io.svg)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67242fd05d5d91b38b2f3a9a_TMG.svg)\n## Make code durable and observable in minutes\nDurable & observable in minutes, not months.\n### Open Source DBOS Transact\nLightweight open source durable execution library.\nSimply add a few annotations to your program to make it resilient to any failure. \nAbstract away the complexity of dealing with crashes, network outages, flaky endpoints, and infrastructure nuances.\nBuilt-in observability\u2013emit OpenTelemetry traces and monitor the execution of your workflows in real time.\n[DBOS Transact Quickstart](https://docs.dbos.dev/quickstart)\n## Reliability at any scale, anywhere\nTooling and hosting to make your DBOS Transact production deployments a success, on any platform.\n[![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e0ea70f549dd14d7092573_DBOS-Conductor-Workflows.webp)](https://www.dbos.dev/dbos-transact)\n### DBOS Pro \u2014 Run DBOS in production, effortlessly\nTooling to operate DBOS Transact applications anywhere.\n  * Manage application deployment, versioning, scaling \n  * Automatically detect and seamlessly recover interrupted workflows\n  * View and manage your workflows from anywhere\n\n[Explore DBOS Pro](https://www.dbos.dev/dbos-transact)\n[![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e450bf176b1e7bec8a7ea5_DBOS-Cloud-Serverless-Compute-Price-Performance.png)](https://www.dbos.dev/dbos-cloud)\n### DBOS Cloud \u2014 Durable app hosting\nA seriously fast serverless platform for DBOS Transact applications. \n  * 25x better price-performance than AWS Lambda + Step Functions\n  * Automatic app restart / resume with exactly-once processing\n  * Deploy with a click, scale to millions. \n\n[Explore DBOS Cloud](https://www.dbos.dev/dbos-cloud)\n## The fastest path to production-ready\nThe fastest path to production-ready\n### Rooted in cutting-edge research >\nBased on years of MIT and Stanford research, brought to you by the creators of Postgres, Apache Spark, and Chaos Engineering.\n### Works with your infrastructure >\nDBOS is open source and standards-based. It fits your dev and production environments like a glove.\n### Your data is safe >\nThe safety and privacy of your code and data is a top priority for DBOS.\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e1bb3fb99125a28f3abdbd_DBOS-Creators-of-PostgreSQL-Apache-Spark-MIT-Stanford.png)![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e0db56b795eb7cff2fa7c9_DBOS-Standards-Based.png)![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e0d43fffdc94caccbabbab_DBOS-SOC-GDPR-HIPAA.png)\n[Start your project](https://docs.dbos.dev/quickstart)\n## What you can build with DBOS\nMake backends more durable, cost-efficient, and easier to build.\n### [Document PipelineBuild a reliable and scalable document ingestion pipeline for a RAG-based chat agent.Learn more](https://docs.dbos.dev/python/examples/document-detective)\n### [Fault-Tolerant CheckoutUse durable workflows to build an online storefront that's resilient to any failure.Learn more](https://docs.dbos.dev/python/examples/widget-store)\n### [Stock TrackerTrack stock prices and receive alerts when they cross a threshold.Learn more](https://docs.dbos.dev/python/examples/stock-tracker)\n### [AI-Powered SlackbotUse DBOS and LlamaIndex to build an AI-powered Slackbot that answers questions.Learn more](https://docs.dbos.dev/python/examples/rag-slackbot)\n### [LLM-Powered ChatbotBuild a chatbot with DBOS, and deploy it 50x cheaper than AWS.Learn more](https://docs.dbos.dev/python/examples/chatbot)\n## Start for Free\nUse the open source DBOS Transact library, free forever.Try DBOS Pro fee for 30 days.\n[DBOS Pro Free Trial](https://console.dbos.dev/launch)\n[Browse pricing plans](https://www.dbos.dev/dbos-pricing)\n[![Gartner Cool Vendor 2024 - Badge](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/6724211a9ec6f0803fe3cdea_Gartner-Cool-Vendor-Badge-DBOS.jpg)2024 Gartner\u00ae Cool Vendor\u2122Enabling Efficient Cloud Operations](https://www.dbos.dev/blog/dbos-2024-gartner-cool-vendor)\n![DBOS Transact Workflow Example](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/673a7fab98fc4326f533ad6c_dbos-transact-workflow-hero.svg)![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e0e8b22b675cf0d1563159_DBOS-Conductor-Workflow-Management%20\\(2\\).jpg)\n## FAQs\n### Will DBOS add support for other languages?\n+\nDBOS applications are developed with the open source DBOS Transact framework, available for TypeScript and Python. We will implement the DBOS Transact framework in other languages based on popular demand.Please join the [DBOS Community on Discord](https://discord.gg/jsmC6pXGgX) and let us know what language(s) you'd like us to support!\n### What happens if my monthly usage exceeds my subscription settings?\n+\nIf your usage exceeds the your current DBOS subscription limits, we will not interrupt your service. What we will do is notify you via email that you've exceeded your usage limits. In response to the email, you can either reduce your usage or increase your subscription usage limits.\n### Can I run DBOS applications on-premises?\n+\nYes, you can run applications built with the open source DBOS Transact library on any Linux, MacOS, or Windows platform. Manage them with DBOS Pro.\n\u200d\n## Follow the future of cloud architecture\n[![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6789880d179631f2c2752989_DBOS-Transact-2-TypeScript.jpg)Announcing V2.0 of DBOS Transact for TypeScriptDBOS ArchitectureJanuary 16, 2025](https://www.dbos.dev/blog/dbos-transact-v2-typescript)\n[![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67327c6f1e5a6763385d906f_66cab52f9f89ac4f354dd5eb_How-to-add-observability-to-your-application.jpeg)How to Make Your Apps Observable by DefaultHow ToAugust 25, 2024](https://www.dbos.dev/blog/how-to-make-your-apps-observable-by-default)\n[![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67e5be8ba3eabdb025ac8b28_Introducing-DBOS-Conductor.jpg)Durable Execution Anywhere - Introducing DBOS ConductorDBOS ArchitectureMarch 27, 2025](https://www.dbos.dev/blog/introducing-dbos-conductor)\n#### Improve your knowledge of durable application architecture.\n  * Forward-thinking, monthly updates.\n  * Perfect for devs building on cloud architecture.\n  * Advice on creating high-performing applications.\n\n\nYou've successfully subscribed. Stay tuned!\nOops! Please enter a valid email and try again.\nOnly valuable insights, no spam. Unsubscribe in a single click.\n### Build your reliable backend. Effortlessly.\n[Contact sales](https://www.dbos.dev/contact)\n[Start your project](https://docs.dbos.dev/quickstart)\n![DBOS - Logo](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67241465129b580e4b924af4_Vectors-Wrapper.svg)\nDBOS radically simplifies cloud application devops and deployment.\n[](https://www.linkedin.com/company/dbos-inc/)[](https://github.com/dbos-inc)[](https://discord.com/invite/jsmC6pXGgX)[](https://twitter.com/DBOS_Inc)[](https://www.dbos.dev/contact)\nProducts\n[DBOS Cloud](https://www.dbos.dev/dbos-cloud)[DBOS Transact](https://www.dbos.dev/dbos-transact)[Pricing Plans](https://www.dbos.dev/dbos-pricing)[Contact Us](https://www.dbos.dev/contact)\nSolutions\n[Cron Job Platform](https://www.dbos.dev/solutions/cron-hosting)[Durable AI Workflows](https://www.dbos.dev/solutions/agentic-ai-platform)[Durable Data Pipelines](https://www.dbos.dev/solutions/data-pipeline-platform)[Cloud Modernization](https://www.dbos.dev/solutions/cloud-modernization)\nDevelopers\n[Docs](https://docs.dbos.dev/)[Quickstart Guide](https://docs.dbos.dev/quickstart)[Examples](https://docs.dbos.dev/examples)[Tutorials](https://docs.dbos.dev/cloud-tutorials/application-management)\nCompany\n[About Us](https://www.dbos.dev/about)[Privacy Policy](https://www.dbos.dev/privacy)[Terms of Service](https://www.dbos.dev/terms-of-service)[Cookies](https://www.dbos.dev/)\nCopyright \u00a9 DBOS, Inc. 2025\n",
    "answer": "```markdown\n# TL;DR Summary\n\nDBOS offers an open-source library, DBOS Transact, for building reliable, observable applications with minimal effort. It supports durable workflows, event-driven processing, and cron jobs, enhancing AI workflows and data pipelines. DBOS Cloud provides efficient serverless hosting, outperforming AWS. \n\nKey insights:\n- Simplifies backend development.\n- Built on research from MIT and Stanford.\n- Free trial available for DBOS Pro.\n```"
  },
  {
    "instruction": "[![nexocode](https://nexocode.com/img/logo-nexo.svg)](https://nexocode.com/)\nmenu\n  * [Services](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/)\n    * [AI Design Sprint](https://nexocode.com/ai-design-sprint/)\n    * [AI Consulting Services](https://nexocode.com/ai-consulting-services/)\n    * [Cloud Development Services](https://nexocode.com/cloud-development-services/)\n    * [Data Engineering Services](https://nexocode.com/data-engineering-services/)\n    * [Data Strategy Bootcamp](https://nexocode.com/data-strategy-bootcamp/)\n    * [Generative AI Development Services](https://nexocode.com/generative-ai-development-company/)\n  * [Solutions](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/)\n    * [Computer Vision](https://nexocode.com/computer-vision-development-services/)\n    * [Natural Language Processing](https://nexocode.com/natural-language-processing-development-services/)\n    * [Predictive Analytics](https://nexocode.com/predictive-analytics-development-services/)\n    * [Recommendation Systems](https://nexocode.com/recommendation-systems-development-services/)\n    * [Dynamic Pricing Services](https://nexocode.com/dynamic-pricing-engines-development-services/)\n  * [Case Studies](https://nexocode.com/case-studies/)\n  * [About Us](https://nexocode.com/about-us/)\n  * [Careers](https://nexocode.com/careers/)\n  * [Resources](https://nexocode.com/resources/)\n  * [Blog](https://nexocode.com/blog/)\n  * [Contact Us](https://nexocode.com/contact/)\n\n\n![Lambda vs. Kappa Architecture. A Guide to Choosing the Right Data Processing Architecture for Your Needs](https://nexocode.com/images/thumbnail_lambda-vs-kappa-architecture.webp)\n[Cloud Computing](https://nexocode.com/blog/categories/cloud-computing) [Data Intensive Applications](https://nexocode.com/blog/categories/data-intensive-applications) [Application Architecture](https://nexocode.com/blog/categories/application-architecture)\n# Lambda vs. Kappa Architecture. A Guide to Choosing the Right Data Processing Architecture for Your Needs\n[Dorota Owczarek](https://nexocode.com/blog/authors/dorota-owczarek) - December 30, 2022 - updated on April 24, 2023 \nInside this article: \n  * [Data Processing Architectures](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/#data-processing-architectures)\n  * [Lambda Architecture](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/#lambda-architecture)\n  * [Kappa Architecture](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/#kappa-architecture)\n  * [Comparison of Lambda and Kappa Architectures](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/#comparison-of-lambda-and-kappa-architectures)\n  * [The Importance of Choosing the Right Data Processing Architecture for a Business](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/#the-importance-of-choosing-the-right-data-processing-architecture-for-a-business)\n\n\nIn today\u2019s digital age, data is a crucial asset for businesses of all sizes. Unsurprisingly, choosing the right data processing architecture is a top priority for many organizations. Two popular options for data processing architectures are Lambda and Kappa.\nIn this blog post, we\u2019ll dive deeply into the key features and characteristics of these architectures and provide a comparison to help you decide which one is the best fit for your business needs. Whether you\u2019re a data scientist, engineer, or business owner, this guide will provide valuable insights into the pros and cons of each architecture and help you make an informed decision on which one to choose.\n> **TL;DR** \u2022 **Data processing architectures** like Lambda and Kappa help businesses analyze and extract valuable insights from their data in real-time. \u2022 **Lambda architecture** uses separate batch and stream processing systems, making it scalable and fault-tolerant but complex to set up and maintain (as it duplicates processing logic). \u2022 **Kappa architecture** simplifies the pipeline with a single stream processing system as it treats all data as streams, providing flexibility and ease of maintenance, but requires experience in stream processing and distributed systems. **\u2022 Lambda architecture** is well-suited when companies have mixed requirements for stream and batch processing, e.g., for real-time analytics and multiple batch processing tasks or data lakes, while Kappa architecture is ideal for continuous data pipelines, real-time data processing, and IoT systems. **\u2022 Businesses should consider their specific data processing** needs and choose an architecture that aligns with their goals and requirements. \u2022 If you\u2019re considering implementing a modern data processing architecture, **nexocode\u2019s data engineers can help you make the right choice** and ensure a seamless transition. [Contact us today to get started!](https://nexocode.com/contact/ \"Contact Us\")\n## Data Processing Architectures\nData processing architectures are systems designed to efficiently handle the ingestion, processing, and storage of large amounts of data. These architectures play a crucial role in modern businesses. They allow organizations to analyze and extract valuable insights from their data, which can be used to improve decision-making, optimize operations, and drive growth.\n## Harness the full potential of AI for your business\n[Sign up for newsletter](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/#nsl-section)\nThere are several different types of data processing architectures, each with its own set of characteristics and capabilities. Some famous examples include Lambda and Kappa architectures, which are designed to handle different types of data processing workloads like batch processing or real-time data processing and have their own unique strengths and weaknesses. It\u2019s essential for businesses to carefully consider their specific data processing needs and choose an architecture that aligns with their goals and requirements.\n## Lambda Architecture\n### Key Features and Characteristics of Lambda Architecture\nLambda architecture is a data processing architecture that aims to provide a scalable, fault-tolerant, and flexible system for processing large amounts of data. It was developed by Nathan Marz in 2011 as a solution to the challenges of processing data in real time at scale.\nThe critical feature of Lambda architecture is that it uses two separate data processing systems to handle different types of data processing workloads. The first system is a batch processing system, which processes data in large batches and stores the results in a centralized data store, such as a data warehouse or a distributed file system. The second system is a [stream processing](https://nexocode.com/blog/posts/stream-processing-use-cases/) system, which processes data in real-time as it arrives and stores the results in a distributed data store.\n![Real-time stream processing and batch processing in Lambda Architecture](https://nexocode.com/cdn-cgi/image/width=600,quality=75/images/real-time-and-batch-processing-with-flink.webp)\nReal-time stream processing and batch processing in Lambda Architecture\nIn Lambda architecture, the four main layers work together to process and store large volumes of data. Here\u2019s a brief overview of how each layer functions:\n#### Data Ingestion Layer\nThis layer collects and stores raw data from various sources, such as log files, sensors, message queues, and APIs. The data is typically ingested in real time and fed to the batch layer and speed layer simultaneously.\n#### Batch Layer (Batch processing)\nThe batch processing layer is responsible for processing historical data in large batches and storing the results in a centralized data store, such as a data warehouse or a distributed file system. This layer typically uses a batch processing framework, such as [Hadoop](https://nexocode.com/blog/posts/what-is-apache-hadoop/) or [Spark](https://nexocode.com/blog/posts/what-is-apache-spark/), to process the data. The batch layer is designed to handle large volumes of data and provide a complete view of all data.\n#### Speed Layer (Real-Time Data Processing)\nThe speed layer is responsible for processing real-time data as it arrives and storing the results in a distributed data store, such as a message queue or a NoSQL database. This layer typically uses a stream processing framework, such as Apache Flink or Apache Storm, to process data streams. The stream processing layer is designed to handle high-volume data streams and provide up-to-date views of the data.\n![Becoming AI Driven](https://nexocode.com/images/nexo_becoming-ai-driven.webp) [Check this series](https://nexocode.com/blog/series/becoming-ai-driven)\n#### Serving Layer\nThe serving layer is a component of Lambda architecture that is responsible for serving query results to users in real time. It is typically implemented as a layer on top of the batch and stream processing layers. It is accessed through a query layer, which allows users to query the data using a query language, such as SQL or [Apache Hive\u2019s ](https://nexocode.com/blog/posts/what-is-apache-hive/ \"Apache Hive\")HiveQL.\nThe serving layer is designed to provide fast and reliable access to query results, regardless of whether the data is being accessed from the batch or stream processing layers. It typically uses a distributed data store, such as a NoSQL database or a distributed cache, to store the query results and serve them to users in real time.\nThe serving layer is an essential component of Lambda architecture, as it allows users to access the data in a seamless and consistent manner, regardless of the underlying data processing architecture. It also plays a crucial role in supporting real-time applications, such as dashboards and analytics, which require fast access to up-to-date data.\n[ ![Would you like to understand the difference between batch and stream?](https://nexocode.com/images/nexo_blog_thumbnail_batch_vs_stream_processing.webp) ](https://nexocode.com/blog/posts/batch-processing-vs-stream-processing-comparison/)\n### [Would you like to understand the difference between batch and stream?](https://nexocode.com/blog/posts/batch-processing-vs-stream-processing-comparison/)\n[ Head over to our recent comparison on batch and stream processing \u00bb ](https://nexocode.com/blog/posts/batch-processing-vs-stream-processing-comparison/)\n### Pros and Cons of Using Lambda Architecture\n**Here are some advantages of Lambda architecture:**\n  * **Scalability** : Lambda architecture is designed to handle large volumes of data and scale horizontally to meet the needs of the business.\n  * **Fault-tolerance:** Lambda architecture is designed to be fault-tolerant, with multiple layers and systems working together to ensure that data is processed and stored reliably.\n  * **Flexibility:** Lambda architecture is flexible and can handle a wide range of data processing workloads, from historical batch processing to streaming architecture.\n\n\nWhile Lambda architecture offers a lot of advantages, it also has some significant drawbacks that businesses need to consider before deciding whether it is the right fit for their needs. **Here are some disadvantages of using the Lambda architecture system:**\n  * **Complexity:** Lambda architecture is a complex system that uses multiple layers and systems to process and store data. It can be challenging to set up and maintain, especially for businesses that are not familiar with distributed systems and data processing frameworks. Although its layers are designed for different pipelines, the underlining logic has duplicated parts causing unnecessary coding overhead for programmers.\n  * **Errors and data discrepancies:** With doubled implementations of different workflows (although following the same logic, implementation matters), you may run into a problem of different results from batch and stream processing engines. Hard to find, hard to debug.\n  * **Architecture lock-in:** It may be super hard to reorganize or migrate existing data stored in the Lambda architecture.\n\n\n### Use Cases for Lambda Architecture\nLambda architecture is a data processing architecture that is well-suited for a wide range of data processing workloads. It is particularly useful for handling large volumes of data and providing low-latency query results, making it well-suited for real-time analytics applications, such as dashboards and reporting. Lambda architecture is also useful for batch processing tasks, such as data cleansing, transformation, and aggregation, and stream processing tasks, such as event processing, machine learning models, anomaly detection, and fraud detection. In addition, Lambda architecture is often used to build data lakes, which are centralized repositories that store structured and unstructured data at rest, and is well-suited for handling the high-volume data streams generated by IoT devices.\n## Kappa Architecture\n### Key Features and Characteristics of Kappa Architecture\n[Kappa architecture](https://nexocode.com/blog/posts/kappa-architecture/ \"Kappa Architecture\") is a data processing architecture that is designed to provide a scalable, fault-tolerant, and flexible system for processing large amounts of data in real time. It was developed as an alternative to Lambda architecture, which, as mentioned above, uses two separate data processing systems to handle different types of data processing workloads.\nIn contrast to Lambda, Kappa architecture uses a single data processing system to handle both batch processing and stream processing workloads, as it treats everything as streams. This allows it to provide a more streamlined and simplified data processing pipeline while still providing fast and reliable access to query results.\n![Real-time stream processing in Kappa Architecture](https://nexocode.com/cdn-cgi/image/width=600,quality=75/images/modern_big_data_kappa-architecture.webp)\nReal-time stream processing in Kappa Architecture\n#### Speed Layer (Stream Layer)\nIn Kappa architecture, there is only one main layer: the stream processing layer. This layer is responsible for collecting, processing, and storing live streaming data. You can think of it as an evolution of the Lambda approach with the batch processing system removed. It is typically implemented using a stream processing engine, such as [Apache Flink](https://nexocode.com/blog/posts/what-is-apache-flink/), Apache Storm, Apache Kinesis, [Apache Kafka](https://nexocode.com/blog/posts/apache-kafka-architecture-for-big-data-processing/), (or many other [stream processing frameworks](https://nexocode.com/blog/posts/stream-processing-frameworks-compared-top-tools-for-processing-data-streams/)) and is designed to handle high-volume data streams and provide fast and reliable access to query results.\nThe stream processing layer in Kappa architecture is divided into two main components: the ingestion component and the processing component.\n  * **Ingestion component:** This component is responsible for collecting incoming data and storing raw data from various sources, such as log files, sensors, and APIs. The data is typically ingested in real-time and stored in a distributed data store, such as a message queue or a NoSQL database.\n  * **Processing component:** This component is responsible for processing the data as it arrives and storing the results in a distributed data store. It is typically implemented using a stream processing engine, such as Apache Flink or Apache Storm, and is designed to handle high-volume data streams and provide fast and reliable access to query results. In Kappa architecture, there is no separate serving layer. Instead, the stream processing layer is responsible for serving query results to users in real time.\n\n\nThe stream processing platforms in Kappa architecture are designed to be fault-tolerant and scalable, with each component providing a specific function in the real time data processing pipeline.\n![Continuous stream processing - stream processing tools run operations on streaming data to enable real time analytics](https://nexocode.com/cdn-cgi/image/width=600,quality=75/images/stream-processing-app.webp)\nContinuous stream processing - stream processing tools run operations on streaming data to enable real time analytics\n[ ![Stay Ahead in the Big Data Game with Kappa Architecture!](https://nexocode.com/images/thumbnail_Kappa_Architecture.webp) ](https://nexocode.com/blog/posts/kappa-architecture/)\n### [Stay Ahead in the Big Data Game with Kappa Architecture!](https://nexocode.com/blog/posts/kappa-architecture/)\n[ Uncover how modern businesses are leveraging the Kappa Architecture for efficient stream processing. Don't get left behind - Dive in and understand Kappa Architecture now! ](https://nexocode.com/blog/posts/kappa-architecture/)\n### Pros and Cons of Using Kappa Architecture\n**Here are some advantages of using of Kappa architecture:**\n  * **Simplicity and streamlined pipeline:** Kappa architecture uses a single data processing system to handle both batch processing and stream processing workloads, which makes it simpler to set up and maintain compared to Lambda architecture. This can make it easier to manage and optimize the data processing pipeline by reducing the coding overhead.\n  * **Enables high-throughput big data processing of historical data:** Although it may feel that it is not designed for these set of problems, Kappa architecture can support these use cases with grace, enabling reprocessing directly from our stream processing job.\n  * **Ease of migrations and reorganizations:** As there is only stream processing pipeline, you can perform migrations and reorganizations with new data streams created from the canonical data store.\n  * **Tiered storage:** Tiered storage is a method of storing data in different storage tiers, based on the access patterns and performance requirements of the data. The idea behind tiered storage is to optimize storage costs and performance by storing different types of data on the most appropriate storage tier. In Kappa architecture, tiered storage is not a core concept. However, it is possible to use tiered storage in conjunction with Kappa architecture, as a way to optimize storage costs and performance. For example, businesses may choose to store historical data in a lower-cost fault tolerant distributed storage tier, such as object storage, while storing real-time data in a more performant storage tier, such as a distributed cache or a NoSQL database. **Tiered storage Kappa architecture makes it a cost-efficient and elastic data processing technique without the need for a traditional data lake.**\n\n\n**When it comes to disadvantages of Kappa architecture, we can mention the following aspects:**\n  * **Complexity:** While Kappa architecture is simpler than Lambda, it can still be complex to set up and maintain, especially for businesses that are not familiar with stream processing frameworks (review [common challenges in stream processing](https://nexocode.com/blog/posts/data-stream-processing-challenges/)).\n  * **Costly infrastructure with scalability issues (when not set properly):** Storing big data in an event streaming platform can be costly. To make it more cost-efficient you may want to use data lake approach from your cloud provider (like [AWS](https://nexocode.com/blog/posts/top-benefits-of-aws-cloud-computing/) S3 or GCP Google Cloud Storage). Another common approach for big data architecture is building a \u201cstreaming data lake\u201d with Apache Kafka as a streaming layer and object storage to enable long-term data storage.\n![Streaming architecture based on Kafka for Kappa approach - Kafka message flow through components](https://nexocode.com/cdn-cgi/image/width=600,quality=75/images/kafka-streams.webp)\nStreaming architecture based on Kafka for Kappa approach - Kafka message flow through components\n\n\n### Use Cases for Kappa Architecture\nKappa architecture is a data processing architecture that is designed to provide a flexible, fault-tolerant, and scalable architecture for processing large amounts of data in real-time. It is well-suited for a wide range of data processing workloads, including continuous data pipelines, real time data processing, machine learning models and real-time data analytics, IoT systems, and many other use cases with a single technology stack.\n![Bounded and unbounded streams offer different use cases for stream processing. ](https://nexocode.com/cdn-cgi/image/width=600,quality=75/images/bounded-stream.webp)\nBounded and unbounded streams offer different use cases for stream processing. \n[ ![Curious how to structure modern big data architecture?](https://nexocode.com/images/hadoop-spark-kafka-modern-big-data-architecture-thumbnail.webp) ](https://nexocode.com/blog/posts/hadoop-spark-kafka-modern-big-data-architecture/)\n### [Curious how to structure modern big data architecture?](https://nexocode.com/blog/posts/hadoop-spark-kafka-modern-big-data-architecture/)\n[ Head over to our insightful comparison on Hadoop, Spark, and Kafka for big data processing \u00bb ](https://nexocode.com/blog/posts/hadoop-spark-kafka-modern-big-data-architecture/)\n## Comparison of Lambda and Kappa Architectures\nBoth architectures are designed to provide scalable, fault-tolerant, and low latency systems for processing data, but they differ in terms of their underlying design and approach to data processing.\n### Data Processing Systems\nLambda architecture uses two separate data processing systems to handle different types of data processing workloads: a batch processing system and a stream processing system. In Kappa architecture, on the other hand, a single stream processing engine acts (stream layer) to handle complete data processing. In Lambda, programmers need to learn and maintain two processing frameworks and support any daily code changes in a doubled way. This separation (when not implemented in the same way) may cause different results in stream vs. batch processing, which may cause further business issues. **Kappa Architecture uses the same code for processing data in real time, eliminating the need for additional effort to maintain separate codebases for batch and stream processing. This makes it a more efficient and error-proof solution.**\n[ ![Which stream processing framework should you choose?](https://nexocode.com/images/nexo_blog_thumbnail_stream_processing_frameworks.webp) ](https://nexocode.com/blog/posts/stream-processing-frameworks-compared-top-tools-for-processing-data-streams/)\n### [Which stream processing framework should you choose?](https://nexocode.com/blog/posts/stream-processing-frameworks-compared-top-tools-for-processing-data-streams/)\n[ Compare top cloud and open-source stream processing frameworks and learn when to use specific stream processors \u00bb ](https://nexocode.com/blog/posts/stream-processing-frameworks-compared-top-tools-for-processing-data-streams/)\n### Data Storage\nLambda architecture has a separate long-term data storage layer, which is used to store historical data and perform complex aggregations. **Kappa architecture does not have a separate long-term data storage layer, and all data is processed and stored by the stream processing system.**\n### Complexity\nLambda architecture is generally more complex to set up and maintain compared to Kappa architecture, as it requires two separate data processing systems and ongoing maintenance to ensure that the batch and stream processing systems are working correctly and efficiently. **Kappa architecture is generally simpler, as it uses a single data processing system to handle all data processing workloads. On the other hand, Kappa, requires a mindset switch to think about all data as streams an it requires lots of experience in stream processing and distributed systems.**\n> If you\u2019re looking for more insights on building data-intensive applications head over to a classic position from Martin Kleppman, [Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems](https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/), or check our take on this book with key insights highlighted by our colleague, Piotr Kubowicz in his article - **[Future According to Designing Data-Intensive Applications](https://nexocode.com/blog/posts/future-designing-data-intensive-applications/)**.\n[ ![Are you ready for AI implementation?](https://nexocode.com/images/the-art-of-building-an-ai-driven-company-ebook-thumbnail.webp) ](https://nexocode.com/resources/ai-for-business-leaders-the-art-of-building-an-ai-driven-company-ebook/)\n### [Are you ready for AI implementation?](https://nexocode.com/resources/ai-for-business-leaders-the-art-of-building-an-ai-driven-company-ebook/)\n[ Download our free ebook and learn how to kickstart your next AI project \u00bb ](https://nexocode.com/resources/ai-for-business-leaders-the-art-of-building-an-ai-driven-company-ebook/)\n## The Importance of Choosing the Right Data Processing Architecture for a Business\nThe choice of data processing architecture is a critical one for businesses, as it impacts the scalability, performance, and flexibility of the data processing pipeline. It is important for businesses to choose a big data architecture that meets their specific needs and to carefully consider the pros and cons of each option before making a decision. Generally, if you\u2019re building a system that needs real-time data access, start with Kappa. And as you learn, you\u2019ll be able to master streams in a way that supports all your workflows.\n[ ![Want to choose the right data processing architecture for your business?](https://nexocode.com/images/Nexo_www_zdj_03.webp) ](https://calendly.com/dorota-owczarek/30min?month=2024-09)\n### [Want to choose the right data processing architecture for your business?](https://calendly.com/dorota-owczarek/30min?month=2024-09)\n[ schedule a free consultation with our expert! ](https://calendly.com/dorota-owczarek/30min?month=2024-09)\nIf you are a business owner or data engineer who wants to develop data systems at scale, nexocode\u2019s data engineering experts can help. Our team has extensive experience in building and optimizing modern data processing pipelines and can help you deploy big data architecture that will benefit your business. If you want to know more about streaming data architecture read our article [here](https://nexocode.com/blog/posts/streaming-data-architecture/ \"https://nexocode.com/blog/posts/streaming-data-architecture/\"). Contact us today to learn more about our services and how we can help you develop a data processing architecture that meets your needs and requirements.\n#### About the author\n![Dorota Owczarek](https://nexocode.com/images/authors/dorota-owczarek.webp)\n[Dorota Owczarek](https://nexocode.com/blog/authors/dorota-owczarek)\nAI Product Lead & Design Thinking Facilitator\n[![Linkedin profile](https://nexocode.com/images/blog/linkedin.svg)](https://www.linkedin.com/in/dorotaowczarek/) [![Twitter](https://nexocode.com/images/blog/twitter.svg)](https://twitter.com/_dorotaowczarek)\nWith over ten years of professional experience in designing and developing software, Dorota is quick to recognize the best ways to serve users and stakeholders by shaping strategies and ensuring their execution by working closely with engineering and design teams. She acts as a Product Leader, covering the ongoing AI agile development processes and operationalizing AI throughout the business.\nWould you like to discuss AI opportunities in your business?\nLet us know and Dorota will arrange a call with our experts.\n![Dorota Owczarek](https://nexocode.com/images/authors/dorota-owczarek.webp)\n**Dorota Owczarek** AI Product Lead \n**Thanks for the message!**\nWe'll do our best to get back to youas soon as possible.\nLet's Talk! \n### This article is a part of\n![Becoming AI Driven](https://nexocode.com/images/nexo_becoming-ai-driven.webp)\n102 articles\n[Becoming AI Driven](https://nexocode.com/blog/series/becoming-ai-driven)\nArtificial Intelligence solutions are becoming the next competitive edge for many companies within various industries. How do you know if your company should invest time into emerging tech? How to discover and benefit from AI opportunities? How to run AI projects?\n**Follow our article series to learn how to get on a path towards AI adoption. Join us as we explore the benefits and challenges that come with AI implementation and guide business leaders in creating AI-based companies.**\n[check it out](https://nexocode.com/blog/series/becoming-ai-driven)\nBecoming AI Driven \n#### Insights on practical AI applications just one click away\nSign up for our newsletter and don't miss out on the latest insights, trends and innovations from this sector.\nI agree to the information on [data processing](https://nexocode.com/nsl_information_on_data_processing.pdf), [privacy policy](https://nexocode.com/privacy_policy.pdf) and newsletter rules described [here](https://nexocode.com/nsl_rules.pdf).  I agree to receive marketing communication from nexocode. \nSign up for newsletter\n#### Done! \n#### Thanks for joining the newsletter\nCheck your inbox for the confirmation email & enjoy the read!\n![](https://nexocode.com/img/nsl-greetings.webp)\n## Transform your business with AI\n### Subscribe to our newsletter\nI agree to the information on [data processing](https://nexocode.com/nsl_information_on_data_processing.pdf), [privacy policy](https://nexocode.com/privacy_policy.pdf) and [newsletter rules](https://nexocode.com/nsl_rules.pdf). I agree to receive marketing communication from nexocode.\n![Google Cloud Partner](https://nexocode.com/images/google-cloud-partner.svg)\n### Follow us\n[![Facebook](https://nexocode.com/images/footer/facebook.svg)](https://www.facebook.com/nexocode/) [![LinkedIn](https://nexocode.com/images/footer/linkedin.svg)](https://pl.linkedin.com/company/nexocode) [![Twitter](https://nexocode.com/images/footer/twitter.svg)](https://twitter.com/nexocode_com) [![Medium](https://nexocode.com/images/footer/medium.svg)](https://medium.com/nexocode)\n### SERVICES\n  * [AI Design Sprint](https://nexocode.com/ai-design-sprint/)\n  * [AI Consulting Services](https://nexocode.com/ai-consulting-services/)\n  * [Cloud Development Services](https://nexocode.com/cloud-development-services/)\n  * [Data Engineering Services](https://nexocode.com/data-engineering-services/)\n  * [Generative AI Development Services](https://nexocode.com/generative-ai-development-company/)\n\n\n### SOLUTIONS\n  * [Computer Vision](https://nexocode.com/computer-vision-development-services/)\n  * [Natural Language Processing](https://nexocode.com/natural-language-processing-development-services/)\n  * [Predictive Analytics](https://nexocode.com/predictive-analytics-development-services/)\n  * [Recommendation Systems](https://nexocode.com/recommendation-systems-development-services/)\n  * [Dynamic Pricing Services](https://nexocode.com/dynamic-pricing-engines-development-services/)\n  * [Data Strategy Bootcamp](https://nexocode.com/data-strategy-bootcamp/)\n\n\n### [RESOURCES](https://nexocode.com/resources/)\n  * [AI Design Sprint](https://nexocode.com/resources/ai-design-sprint-workshops-ebook/)\n  * [The Art of Buliding an AI-Driven Company](https://nexocode.com/resources/ai-for-business-leaders-the-art-of-building-an-ai-driven-company-ebook/)\n  * [Mastering Data Strategy](https://nexocode.com/resources/mastering-data-strategy-ebook/)\n  * [Data Mesh in Practice](https://nexocode.com/resources/data-mesh-in-practice-ebook/)\n\n\n### INDUSTRIES\n  * [AI in Logistics](https://nexocode.com/ai-in-logistics/)\n  * [AI in Pharma](https://nexocode.com/ai-in-pharma/)\n\n\n### COMPANY\n  * [About us](https://nexocode.com/about-us/)\n  * [Blog](https://nexocode.com/blog/)\n  * [Careers](https://nexocode.com/careers/)\n  * [Case Studies](https://nexocode.com/case-studies/)\n  * [Contact](https://nexocode.com/contact/)\n\n\n### nexocode\n  * ul. Wadowicka 7\n  * 30-347 Krak\u00f3w, Poland\n\n\nCopyright 2025 \u00a9 Nexocode | [Privacy policy](https://nexocode.com/privacy_policy.pdf) | [RSS](https://nexocode.com/index.xml)\nThis site uses cookies for analytical purposes.\nAccept [Privacy Policy](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/#read-more)\nIn the interests of your safety and to implement the principle of lawful, reliable and transparent processing of your personal data when using our services, we developed this document called the Privacy Policy. This document regulates the processing and protection of Users\u2019 personal data in connection with their use of the Website and has been prepared by Nexocode. \nTo ensure the protection of Users' personal data, Nexocode applies appropriate organizational and technical solutions to prevent privacy breaches. Nexocode implements measures to ensure security at the level which ensures compliance with applicable Polish and European laws such as: \n  1. Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (published in the Official Journal of the European Union L 119, p 1); Act of 10 May 2018 on personal data protection (published in the Journal of Laws of 2018, item 1000);\n  2. Act of 18 July 2002 on providing services by electronic means;\n  3. Telecommunications Law of 16 July 2004.\n\n\nThe Website is secured by the SSL protocol, which provides secure data transmission on the Internet. \n#### 1. Definitions\n  1. User \u2013 a person that uses the Website, i.e. a natural person with full legal capacity, a legal person, or an organizational unit which is not a legal person to which specific provisions grant legal capacity.\n  2. Nexocode \u2013 NEXOCODE sp. z o.o. with its registered office in Krak\u00f3w, ul. Wadowicka 7, 30-347 Krak\u00f3w, entered into the Register of Entrepreneurs of the National Court Register kept by the District Court for Krak\u00f3w-\u015ar\u00f3dmie\u015bcie in Krak\u00f3w, 11th Commercial Department of the National Court Register, under the KRS number: 0000686992, NIP: 6762533324.\n  3. Website \u2013 website run by Nexocode, at the URL: nexocode.com whose content is available to authorized persons.\n  4. Cookies \u2013 small files saved by the server on the User's computer, which the server can read when when the website is accessed from the computer.\n  5. SSL protocol \u2013 a special standard for transmitting data on the Internet which unlike ordinary methods of data transmission encrypts data transmission.\n  6. System log \u2013 the information that the User's computer transmits to the server which may contain various data (e.g. the user\u2019s IP number), allowing to determine the approximate location where the connection came from.\n  7. IP address \u2013 individual number which is usually assigned to every computer connected to the Internet. The IP number can be permanently associated with the computer (static) or assigned to a given connection (dynamic).\n  8. GDPR \u2013 Regulation 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of individuals regarding the processing of personal data and onthe free transmission of such data, repealing Directive 95/46 / EC (General Data Protection Regulation).\n  9. Personal data \u2013 information about an identified or identifiable natural person (\"data subject\"). An identifiable natural person is a person who can be directly or indirectly identified, in particular on the basis of identifiers such as name, identification number, location data, online identifiers or one or more specific factors determining the physical, physiological, genetic, mental, economic, cultural or social identity of a natural person.\n  10. Processing \u2013 any operations performed on personal data, such as collecting, recording, storing, developing, modifying, sharing, and deleting, especially when performed in IT systems.\n\n\n#### 2. Cookies\nThe Website is secured by the SSL protocol, which provides secure data transmission on the Internet. The Website, in accordance with art. 173 of the Telecommunications Act of 16 July 2004 of the Republic of Poland, uses Cookies, i.e. data, in particular text files, stored on the User's end device.Cookies are used to: \n  1. improve user experience and facilitate navigation on the site;\n  2. help to identify returning Users who access the website using the device on which Cookies were saved;\n  3. creating statistics which help to understand how the Users use websites, which allows to improve their structure and content;\n  4. adjusting the content of the Website pages to specific User\u2019s preferences and optimizing the websites website experience to the each User's individual needs.\n\n\nCookies usually contain the name of the website from which they originate, their storage time on the end device and a unique number. On our Website, we use the following types of Cookies: \n  * \"Session\" \u2013 cookie files stored on the User's end device until the Uses logs out, leaves the website or turns off the web browser;\n  * \"Persistent\" \u2013 cookie files stored on the User's end device for the time specified in the Cookie file parameters or until they are deleted by the User;\n  * \"Performance\" \u2013 cookies used specifically for gathering data on how visitors use a website to measure the performance of a website;\n  * \"Strictly necessary\" \u2013 essential for browsing the website and using its features, such as accessing secure areas of the site;\n  * \"Functional\" \u2013 cookies enabling remembering the settings selected by the User and personalizing the User interface;\n  * \"First-party\" \u2013 cookies stored by the Website;\n  * \"Third-party\" \u2013 cookies derived from a website other than the Website;\n  * \"Facebook cookies\" \u2013 You should read Facebook cookies policy: [www.facebook.com](https://www.facebook.com/policy/cookies)\n  * \"Other Google cookies\" \u2013 Refer to Google cookie policy: [google.com](https://www.google.com/policies/technologies/types/)\n\n\n#### 3. How System Logs work on the Website\nUser's activity on the Website, including the User\u2019s Personal Data, is recorded in System Logs. The information collected in the Logs is processed primarily for purposes related to the provision of services, i.e. for the purposes of: \n  * analytics \u2013 to improve the quality of services provided by us as part of the Website and adapt its functionalities to the needs of the Users. The legal basis for processing in this case is the legitimate interest of Nexocode consisting in analyzing Users' activities and their preferences;\n  * fraud detection, identification and countering threats to stability and correct operation of the Website.\n\n\n#### 4. Cookie mechanism on the Website\nOur site uses basic cookies that facilitate the use of its resources. Cookies contain useful information and are stored on the User's computer \u2013 our server can read them when connecting to this computer again. Most web browsers allow cookies to be stored on the User's end device by default. Each User can change their Cookie settings in the web browser settings menu: Google ChromeOpen the menu (click the three-dot icon in the upper right corner), Settings > Advanced. In the \"Privacy and security\" section, click the Content Settings button. In the \"Cookies and site date\" section you can change the following Cookie settings: \n  * Deleting cookies,\n  * Blocking cookies by default,\n  * Default permission for cookies,\n  * Saving Cookies and website data by default and clearing them when the browser is closed,\n  * Specifying exceptions for Cookies for specific websites or domains\n\n\n**Internet Explorer 6.0 and 7.0** From the browser menu (upper right corner): Tools > Internet Options > Privacy, click the Sites button. Use the slider to set the desired level, confirm the change with the OK button. \n**Mozilla Firefox** browser menu: Tools > Options > Privacy and security. Activate the \u201cCustom\u201d field. From there, you can check a relevant field to decide whether or not to accept cookies. \n**Opera** Open the browser\u2019s settings menu: Go to the Advanced section > Site Settings > Cookies and site data. From there, adjust the setting: Allow sites to save and read cookie data \n**Safari** In the Safari drop-down menu, select Preferences and click the Security icon.From there, select the desired security level in the \"Accept cookies\" area. \nDisabling Cookies in your browser does not deprive you of access to the resources of the Website. Web browsers, by default, allow storing Cookies on the User's end device. Website Users can freely adjust cookie settings. The web browser allows you to delete cookies. It is also possible to automatically block cookies. Detailed information on this subject is provided in the help or documentation of the specific web browser used by the User. The User can decide not to receive Cookies by changing browser settings. However, disabling Cookies necessary for authentication, security or remembering User preferences may impact user experience, or even make the Website unusable. \n#### 5. Additional information\nExternal links may be placed on the Website enabling Users to directly reach other website. Also, while using the Website, cookies may also be placed on the User\u2019s device from other entities, in particular from third parties such as Google, in order to enable the use the functionalities of the Website integrated with these third parties. Each of such providers sets out the rules for the use of cookies in their privacy policy, so for security reasons we recommend that you read the privacy policy document before using these pages. We reserve the right to change this privacy policy at any time by publishing an updated version on our Website. After making the change, the privacy policy will be published on the page with a new date. For more information on the conditions of providing services, in particular the rules of using the Website, contracting, as well as the conditions of accessing content and using the Website, please refer to the the Website\u2019s Terms and Conditions. \nNexocode Team \n[![Close](https://nexocode.com/img/cross.svg)](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/)\n![](https://nexocode.com/images/the-art-of-building-ai-driven-company-cover.webp)\n## Want to unlock the full potential of Artificial Intelligence technology?\nDownload our ebook and learn how to drive AI adoption in your business.\n[GET EBOOK NOW](https://nexocode.com/resources/ai-for-business-leaders-the-art-of-building-an-ai-driven-company-ebook/)\n",
    "answer": "```markdown\n# TL;DR Summary\n\n**Lambda vs. Kappa Architecture**: Lambda uses separate batch and stream systems, offering scalability but complexity. Kappa simplifies with a single stream system, ideal for real-time processing. Choose based on specific data needs. Nexocode can assist in implementation.\n```"
  },
  {
    "instruction": "A Metrics-First Approach to LLM Evaluation - Galileo AI\n[![](https://cdn.sanity.io/images/tf66morw/production/b6ba804260aa8c85927b129e1fba84a92ea3ad87-3400x744.svg?w=3400&h=744&auto=format)](https://www.rungalileo.io/)\n  * Products\n    * [Agents](https://www.rungalileo.io/agentic-evaluations)\n    * [Evaluate](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-evaluate)\n    * [Observe](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-observe)\n    * [Protect](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-protect)\n  * [Docs](https://docs.galileo.ai/galileo)\n  * [Pricing](https://www.rungalileo.io/pricing)\n  * [Blog](https://www.rungalileo.io/blog)\n  * Resources\n    * [Agent Leaderboard](https://www.rungalileo.io/agent-leaderboard)\n    * [Mastering Agents eBook](https://www.rungalileo.io/ebook-mastering-agents)\n    * [Hallucination Index](https://www.rungalileo.io/hallucinationindex)\n    * [Mastering RAG eBook](https://www.rungalileo.io/mastering-rag)\n    * [Chain of Thought podcast](https://pod.link/1776879655)\n    * [Case Studies](https://www.rungalileo.io/case-studies)\n    * [Research](https://www.rungalileo.io/research)\n    * [GenAI Productionize 2.0](https://www.rungalileo.io/genai-productionize-2-0)\n  * Company\n    * [Team](https://www.rungalileo.io/team)\n    * [Careers](https://ats.rippling.com/galileo/jobs)\n[Login](https://app.galileo.ai/sign-in)[Contact Sales](https://www.rungalileo.io/get-started)[Sign up](https://app.galileo.ai/sign-up)\n\n\n[![](https://cdn.sanity.io/images/tf66morw/production/b6ba804260aa8c85927b129e1fba84a92ea3ad87-3400x744.svg?w=3400&h=744&auto=format)](https://www.rungalileo.io/)\nProducts\n    * [Agents](https://www.rungalileo.io/agentic-evaluations)\n    * [Evaluate](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-evaluate)\n    * [Observe](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-observe)\n    * [Protect](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-protect)\n[Docs](https://docs.galileo.ai/galileo)[Pricing](https://www.rungalileo.io/pricing)[Blog](https://www.rungalileo.io/blog)\nResources\n    * [Agent Leaderboard](https://www.rungalileo.io/agent-leaderboard)\n    * [Mastering Agents eBook](https://www.rungalileo.io/ebook-mastering-agents)\n    * [Hallucination Index](https://www.rungalileo.io/hallucinationindex)\n    * [Mastering RAG eBook](https://www.rungalileo.io/mastering-rag)\n    * [Chain of Thought podcast](https://pod.link/1776879655)\n    * [Case Studies](https://www.rungalileo.io/case-studies)\n    * [Research](https://www.rungalileo.io/research)\n    * [GenAI Productionize 2.0](https://www.rungalileo.io/genai-productionize-2-0)\nCompany\n    * [Team](https://www.rungalileo.io/team)\n    * [Careers](https://ats.rippling.com/galileo/jobs)\n\n\n[Login](https://app.galileo.ai/sign-in)[Contact Sales](https://www.rungalileo.io/get-started)[Sign up](https://app.galileo.ai/sign-up)\n# A Metrics-First Approach to LLM Evaluation\n![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='56'%20width='56'%20xmlns='http://www.w3.org/2000/svg'%20version='1.1'%3E%3C/svg%3E)\n![Pratik Bhavsar](https://cdn.sanity.io/images/tf66morw/production/06a39f484014944a7a05678b71ab2cf008adf1dd-394x394.jpg?w=56&h=56&auto=format)\nPratik BhavsarGalileo Labs\n![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='655'%20width='1280'%20xmlns='http://www.w3.org/2000/svg'%20version='1.1'%3E%3C/svg%3E)\n![Learn about different types of LLM evaluation metrics](https://cdn.sanity.io/images/tf66morw/production/adc8a29efafd3ac996e3153c443ef6c93457c95c-1228x628.png?w=1228&h=628&auto=format)\n10 min readSeptember 19 2023\nTable of contents\nShow\n  1. [ Need for new LLM evaluation metrics](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#need-for-new-llm-evaluation-metrics)\n  2. [Types of LLM Evaluation Metrics](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#types-of-llm-evaluation-metrics)\n    1. [Top Level Metrics for LLM Evaluation](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#top-level-metrics-for-llm-evaluation)\n    2. [Metrics for Evaluating RAG Effectiveness](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#metrics-for-evaluating-rag-effectiveness)\n    3. [Metrics for Evaluating Safety ](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#metrics-for-evaluating-safety)\n    4. [Custom Evaluation Metrics](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#custom-evaluation-metrics)\n    5. [Final thoughts](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#final-thoughts)\n\n\nThere has been tremendous progress in the world of Large Language Models (LLMs). We have seen a series of blockbuster models like GPT3, GPT3.5, GPT4, Falcon, MPT and Llama pushing the state of the art. The industry has started adopting them for various applications but there's a big problem. It's hard to figure out how well these models are performing. Companies are struggling to compare different LLMs for generative applications. The tendency of LLMs to hallucinate requires us to measure them carefully. In this blog, we discuss helpful metrics for evaluating LLMs and understanding their suitability for your use cases. \n##  Need for new LLM evaluation metrics\nIt is easy to start building with Large Language Models (LLMs) such as OpenAI's ChatGPT, but they can be very difficult to evaluate. The main concerns around evaluations are:\n  1. Human evaluation is costly and prone to errors: LLMs are used for varied generative tasks which [cannot be judged](https://arxiv.org/abs/2008.12009) by these metrics. In desperation, companies often rely on human vibe check. But human annotations are costly and full of biases which make evaluation slow and unreliable.\n  2. Poor correlations with human judgment: Traditional metrics like BLEU / ROUGE have shown [poor correlation](https://arxiv.org/abs/2303.16634) with how humans evaluate model output. Without reliable metrics it becomes hard to know if the LLM app is release worthy.\n  3. Absence of reliable benchmarks: Researchers have come up with benchmarks like [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), but they do not evaluate the generative capability since the tasks contain multiple choice questions. The datasets used are also limited and might not have domain coverage for the target use case.\n\n\nGiven these challenges, companies should prioritize investments in the development of evaluation metrics. These metrics will enable them to make data-driven decisions without depending solely on human judgment. Let's explore some key metrics that can assist companies in designing an evaluation system to enhance their generative applications.\n## Types of LLM Evaluation Metrics\nOver time, many metrics have been proposed to measure the quality of LLM outputs. How to best evaluate LLMs is still an active research work, but we have found some which are more useful than others. We categorize them below to make them easier to understand.\n![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='769.9999999999999'%20width='1654'%20xmlns='http://www.w3.org/2000/svg'%20version='1.1'%3E%3C/svg%3E)\n![LLM Evaluation Metrics](https://cdn.sanity.io/images/tf66morw/production/53f3548af41c40a2fba24d4e94f337d304850656-1654x770.png?w=1654&h=770&auto=format)\nLLM Evaluation Metrics\n### Top Level Metrics for LLM Evaluation\nThese are well known metrics which can be used for any application. They work for any input/output of the LLMs.\n#### 1. Context Adherence\nContext adherence measures whether the model\u2019s response is supported by the context given to it. This checks if the model is stating facts that are out of context\u2014hallucinations. A score of 1 means it\u2019s grounded, and there is a lower chance of hallucination.\nMetric signal: Higher context adherence is correlated with higher output quality.\nExamples:\nQuery: \"What is the population of Paris, France?\"\nRetrieved docs:\nDoc1: Census 2023 reported population of Paris to be 2.2 million.\nDoc2: Census 2022 reported population of Paris to be 2.1 million.\nDoc3: The population of Paris is more than 2 million.\nHigh context adherence:\nResponse: \"The population of Paris, France, according to the most recent census report, is approximately 2.2 million people.\"\nIn this example, the model's response is directly supported by the information present in the retrieved documents. It provides a specific population figure based on a reliable source, demonstrating groundedness.\nLow context adherence:\nResponse: \"Paris, France has a population of 10 million people.\"\nIn this example, the model's response is not grounded in the provided documents, and it seems to have fabricated a population figure that is not supported by the context.\n#### 2. Correctness\nAs the name suggests, correctness measures the factual correctness of the output. It was developed by Galileo, which leverages GPT-3.5 with chain of thought(CoT) prompting and self-consistency. It surfaces errors of precision and not recall. It is very useful for [detecting hallucinations](https://www.rungalileo.io/blog/5-techniques-for-detecting-llm-hallucinations) in different scenarios, like summarisation or open-domain QA.\nMetric signal: Higher factuality is correlated with higher output quality.\nPrompt: When did Aliens invade earth?\nHigh factuality:\nResponse: Aliens have never invaded earth.\nLow factuality:\nResponse: Aliens invaded earth on July 4th 2020.\n#### 3. LLM Uncertainty\nA recent [study](https://aclanthology.org/2023.eacl-main.75/) has shown that log probability can help us find low quality generations. Uncertainty leverages the same philosophy as prompt perplexity but on the generated text. It is calculated by leveraging log probability given by the LLM for each generated token. For models like GPT-3.5 and GPT4 which do not give logprob, we use other models as proxy.\nMetric signal: Lower LLM uncertainty is correlated with higher output quality.\nLow uncertainty:\nPrompt: \u201cWhere did the inventors of GPT3 architecture work?\u201d\nResponse: \u201cOpenAI\u201d\nThe response here is correct and contains low uncertainty.\nHigh uncertainty:\nPrompt: \u201cWhere did the inventors of GPT5 architecture work?\u201d\nResponse: \u201cDeepmind\u201d\nThe response here is incorrect and contains high uncertainty.\n#### 4. Prompt Perplexity\nPrompt perplexity is simply the perplexity of the prompt given as input to the LLM. A recent [study](https://arxiv.org/abs/2212.04037) showed that the lower the perplexity of the prompt, the better suited the prompt is for the given task. High perplexity indicates lower understanding of the text which means the model has not understood the prompt. If the model is unable to understand its input (the prompt) it is more likely to generate poor outputs.\nMetric signal: Lower prompt perplexity is correlated with higher output quality.\nLow perplexity:\nTranslate the following English sentence into French: 'The quick brown fox jumps over the lazy dog.'\nIn this case, the prompt is clear and directly instructs the model on what task to perform. The model is likely to have a low perplexity because it can confidently understand and execute the translation task.\nHigh perplexity:\n\"Can you, like, if you don't mind, convert to French for me? The quick brown fox jumps over the lazy dog.\nIn this example, the instruction is not very clear that it\u2019s a translation task and does not highlight what is to be translated.\n### Metrics for Evaluating RAG Effectiveness\n![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='1255'%20width='2398'%20xmlns='http://www.w3.org/2000/svg'%20version='1.1'%3E%3C/svg%3E)\n![Galileo RAG Analytics](https://cdn.sanity.io/images/tf66morw/production/3f94840ff011ee42b76c784cc7b9cbbcf97fc80e-2398x1255.png?w=2398&h=1255&auto=format)\nGalileo RAG Analytics\n[RAG](https://www.rungalileo.io/blog/mastering-rag-how-to-architect-an-enterprise-rag-system) refers to retrieval augmented generation where we add domain specific knowledge(DSK) in the prompt with the help of a search engine. This is required to make LLMs work with DSK which can be totally missing during its training on open web data. Let\u2019s discuss metrics which help with improving the RAG system.\n#### 1. Context Relevance\nContext relevance measures how relevant the context fetched was to the user query. Low score could be a sign of a bad doc chunking/retrieval strategy or of missing info. To address this, one would add more documents to context DB or change the retrieval indexing/retrieval strategy. This can help debug reasons for bad generations.\nMetric signal: Higher context relevance is correlated with higher output quality.\nQuery: Please provide information about the impact of climate change on polar bears.\nHigh context relevance:\nRetrieved Documents:\nDoc 1 title: \"The Effects of Climate Change on Polar Bear Populations\"\nDoc 2 title: \"Arctic Ecosystems and Climate Change\"\nDoc 3 title: \"Polar Bear Conservation Efforts in a Warming World\"\nIn this case, the context relevance is good because all three retrieved documents are directly related to the query about the impact of climate change on polar bears. T\nLow context relevance:\nRetrieved Documents:\nDoc 1 title: \"Polar bears are fascinating creatures living in the Arctic.\"\nDoc 2 title: \"Climate change is a global issue affecting various ecosystems.\"\nDoc 3 title: \"The significance of bears in cultural folklore.\"\nIn this example, there is an overlap of words like \"polar bears\" and \"climate change\" between the query and documents. However, the context relevance is low because none of the retrieved documents directly address the impact of climate change on polar bears.\n#### 2. Completeness\nCompleteness measures how thoroughly your model's response covers the relevant information available in the provided context. Completeness and Context Adherence work in tandem to ensure that the model's response is both consistent with the context and comprehensive in addressing all relevant information.\nContext Adherence vs. Completeness: Context Adherence evaluates whether the model's response is consistent with the information in the context, while Completeness assesses whether all relevant information in the context is adequately addressed in the response. In essence, Context Adherence represents precision, whereas Completeness represents recall.\nMetric signal: Higher context similarity is correlated with higher output quality.\nExamples:\nUser Query: \"Who was Galileo Galilei?\"\nContext: \"Galileo Galilei was an Italian astronomer. Galileo was the first to see the Milky Way Galaxy in 1610 as individual stars through the telescope.\"\nLow completeness\nGenerated Answer: \"Galileo Galilei was Italian.\"\nIn this example, the model's response achieves perfect Context Adherence because the context supports everything it states. However, the response lacks completeness because it fails to mention that Galileo Galilei was an astronomer, which is relevant information provided in the context.\nHigh completeness\nGenerated Answer: \"Galileo Galilei was Italian astronomer.\"\nIn this example, the model's response achieves perfect completeness.\n#### 3. Chunk Attribution\nChunk Attribution measures whether each retrieved chunk affected the model's response. It is a binary metric, classifying each chunk as either Attributed or Not Attributed.\nChunk Attribution offers valuable insights for optimizing RAG pipelines in various ways:\n1. Tuning the number of retrieved chunks: If the system generates satisfactory responses but many chunks are Not Attributed, reducing the number of retrieved chunks per example may enhance efficiency without compromising response quality. This optimization can lead to cost savings and reduced latency.\n2. \"Debugging\" anomalous model behavior in individual examples: When analyzing specific model responses that are unsatisfactory or unusual, Chunk Attribution helps identify the chunks that influenced the response. This facilitates a quicker understanding of the underlying factors contributing to the issue, streamlining the debugging process for individual examples.\nMetric signal: Higher chunk attribution is correlated with higher output quality.\nExample:\nQuery: \"Tell me about the discoveries of Galileo Galilei.\"\nRetrieved Chunks:\nChunk 1: \"Galileo Galilei was an Italian physicist, mathematician, and astronomer.\"\nChunk 2: \"Galileo's telescopic observations supported the Copernican heliocentric model of the solar system.\"\nChunk 3: \"He discovered the four largest moons of Jupiter, now known as the Galilean moons.\"\nHigh chunk attribution:\nGenerated Answer: \"Galileo Galilei, an Italian physicist, mathematician, and astronomer, made significant discoveries that reshaped our understanding of the universe. His telescopic observations provided evidence for the Copernican heliocentric model and led to the discovery of the four largest moons of Jupiter, now known as the Galilean moons.\"\nIn this case, all retrieved chunks are attributed since each contributes essential information to the model's response. Therefore, each chunk affected the generated answer, indicating high Chunk Attribution.\nLow chunk attribution:\nQuery: \"What was Galileo Galilei's nationality?\"\nRetrieved Chunks:\nChunk 1: \"Galileo Galilei was an Italian physicist, mathematician, and astronomer.\"\nChunk 2: \"Galileo's telescopic observations supported the Copernican heliocentric model of the solar system.\"\nChunk 3: \"He discovered the four largest moons of Jupiter, now known as the Galilean moons.\"\nGenerated Answer: \"Galileo Galilei was born in Pisa in 1564.\"\nIn this example, none of the retrieved chunks directly addresses Galileo Galilei's nationality. Although Chunk 1 mentions that he was Italian, the model's response does not incorporate this information. Therefore, none of the retrieved chunks significantly influenced the model's response regarding Galileo Galilei's nationality, indicating that they are Not Attributed.\n#### 4. Chunk Utilization\nChunk Utilization measures the fraction of text in each retrieved chunk that is used for model's response. It ranges from 0 to 1, where a value of 1 indicates that the entire chunk affected the response, while a lower value, such as 0.5, suggests that some text within the chunk did not contribute to the response. Low chunk utilization scores indicate that the chunks retrieved may be longer than necessary.\nMetric signal: Higher chunk utilization is correlated with higher output quality.\nHigh chunk utilization\nUser Query: \"What are the main ingredients used in making pizza?\"\nRetrieved Chunk:\nChunk 1: \"Pizza, a popular Italian dish, typically consists of a flattened bread base topped with tomato sauce, cheese, and various toppings. The base is traditionally made from wheat flour, water, yeast, salt, and olive oil. Tomato sauce, often seasoned with herbs and spices, serves as the primary topping, while mozzarella cheese is commonly used for its melting properties. Additional toppings may include meats such as pepperoni, vegetables like mushrooms and bell peppers, and other ingredients based on regional preferences.\"\nGenerated Answer: \"Pizza typically consists of a flattened bread base topped with tomato sauce, cheese, and various toppings. The base is traditionally made from wheat flour, water, yeast, salt, and olive oil. Tomato sauce, often seasoned with herbs and spices, serves as the primary topping, while mozzarella cheese is commonly used for its melting properties. Additional toppings may include meats such as pepperoni, vegetables like mushrooms and bell peppers, and other ingredients based on regional preferences.\"\nIn this example, the retrieved chunk provides comprehensive information about the main ingredients used in making pizza, including details about the bread base, tomato sauce, cheese, and various toppings. The model's response utilizes the entire chunk effectively, as all the information contributes directly to answering the user query about pizza ingredients. Therefore, the chunk utilization is high.\nLow chunk utilization\nUser Query: \"What is the population of Tokyo, Japan?\"\nRetrieved Chunk:\nChunk 1: \"Tokyo, officially Tokyo Metropolis, is the capital and most populous prefecture of Japan. It is the political, economic, and cultural center of the country. Tokyo is one of the 47 prefectures of Japan, consisting of 23 central city wards and multiple cities, towns, and villages west of the city center. The population of the Tokyo Metropolis is approximately 14 million people, making it one of the most populous urban areas in the world.\"\nGenerated Answer: \"The population of the Tokyo Metropolis is approximately 14 million people.\"\nIn this example, the model's response only utilizes a small portion of the chunk, focusing solely on the population figure. Despite the chunk being attributed to the response, the utilization is low because the majority of the text in the chunk is not directly relevant to answering the user query.\n### Metrics for Evaluating Safety \n![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='1254'%20width='2398'%20xmlns='http://www.w3.org/2000/svg'%20version='1.1'%3E%3C/svg%3E)\n![Galileo Protect](https://cdn.sanity.io/images/tf66morw/production/8d4314e57d008e25a7f477011d8b085b48b7923a-2398x1254.png?w=2398&h=1254&auto=format)\nGalileo Protect\nSafety metrics are essential tools used to evaluate and maintain the safety and integrity of AI models and systems. These metrics encompass various aspects of safety, including the identification and mitigation of potential risks such as the presence of sensitive information, toxic content, inappropriate tones, sexism, and malicious prompt injections. By systematically assessing these factors, [safety metrics](https://www.rungalileo.io/blog/introducing-protect-realtime-hallucination-firewall) play a crucial role in safeguarding users and promoting responsible AI deployment.\n#### PII\nThe PII (Private Identifiable Information) metric identifies instances of PII within a model's responses, specifically flagging sensitive information such as addresses, credit card details, dates, emails, names, network information, personal identification (ID), passwords, phone numbers, social security numbers (SSN), and usernames.\nThis metric automatically detects and flags responses containing PII, enabling the implementation of guardrails or other preventative measures to protect user privacy and prevent potential security breaches.\n#### Toxicity\nThe Toxicity metric flags whether a response contains hateful or toxic content. It provides a binary classification indicating whether a response is toxic or not.\nBy identifying responses with toxic comments, this metric facilitates the implementation of measures such as fine-tuning models or implementing guardrails to flag and mitigate toxic responses, ensuring a safer and more respectful user experience.\n#### Tone\nThe Tone metric classifies the emotional tone of a response into nine different categories, including neutral, joy, love, fear, surprise, sadness, anger, annoyance, and confusion.\nBy categorizing the emotional tone of responses, this metric enables the alignment of responses with user preferences. It allows for optimization by discouraging undesirable tones and promoting preferred emotional responses, thereby enhancing user satisfaction.\n#### Sexism\nThe Sexism metric flags whether a response contains sexist content. It provides a binary classification indicating whether a response is sexist or not.\nThis metric helps identify responses containing sexist comments, allowing for preventive measures such as fine-tuning models or implementing guardrails to flag and mitigate sexist responses, fostering a more inclusive and respectful environment.\n#### Prompt Injection\nThe Prompt Injection metric identifies instances of prompt injection within a model's input, including attacks such as simple instruction attacks, few-shot attacks, impersonation, obfuscation, and context switching.\nBy automatically detecting and classifying user queries with prompt injection attacks, this metric enables the implementation of guardrails or other preventative measures to mitigate potential risks and ensure the model operates within intended parameters, enhancing safety and reliability.\n### Custom Evaluation Metrics\nWe acknowledge that the metrics listed above may not cover all scenarios comprehensively. For instance, LLMs can produce outputs with multiple predictions, necessitating custom metrics for evaluating various aspects of these outputs. Certain applications demand LLM-based chatbots to maintain a specific tone, as deviations can prompt user dissatisfaction. Additionally, developers may seek to identify specific words associated with errors within the outputs. These specialized requirements fall under the umbrella of custom metrics, addressing the diverse range of use cases beyond standard evaluation metrics.\n### Final thoughts\nOver the span of three years, Galileo has been deeply immersed in the study of language model metrics. With Galileo Evaluate, harnessing these metrics for swift experimentation is effortless. You gain immediate access to a concise overview of all runs, facilitating the identification of your top-performing setup. Moreover, the option to craft custom metrics tailored to your precise requirements is at your fingertips for unique applications.\n![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='2070'%20width='2974'%20xmlns='http://www.w3.org/2000/svg'%20version='1.1'%3E%3C/svg%3E)\n![Galileo GenAI Studio](https://cdn.sanity.io/images/tf66morw/production/82130a417b2027cc00983da2ccb86884bbd1a904-2974x2070.png?w=2974&h=2070&auto=format)\nGalileo GenAI Studio\nGalileo [GenAI Studio](https://docs.rungalileo.io/galileo#llm-studio) is the leading platform for rapid evaluation, experimentation and observability for teams building LLM powered applications. It is powered by a [suite of metrics](https://docs.rungalileo.io/galileo/llm-studio/prompt-inspector/choosing-your-guardrail-metrics) to identify and mitigate hallucinations. [Get started](https://www.rungalileo.io/get-started) today!\nExplore our research-backed evaluation metric for hallucination \u2013 read our paper on [Chainpoll](https://arxiv.org/abs/2310.18344).\nTable of contents\nHide\n  1. [ Need for new LLM evaluation metrics](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#need-for-new-llm-evaluation-metrics)\n  2. [Types of LLM Evaluation Metrics](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#types-of-llm-evaluation-metrics)\n    1. [Top Level Metrics for LLM Evaluation](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#top-level-metrics-for-llm-evaluation)\n    2. [Metrics for Evaluating RAG Effectiveness](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#metrics-for-evaluating-rag-effectiveness)\n    3. [Metrics for Evaluating Safety ](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#metrics-for-evaluating-safety)\n    4. [Custom Evaluation Metrics](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#custom-evaluation-metrics)\n    5. [Final thoughts](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#final-thoughts)\n\n\n### Subscribe to Newsletter\n[![](https://cdn.sanity.io/images/tf66morw/production/b6ba804260aa8c85927b129e1fba84a92ea3ad87-3400x744.svg?w=3400&h=744&auto=format)](https://www.rungalileo.io/)\nSubscribe to our newsletter\nresearch\n  * [Evaluation Efficiency](https://www.rungalileo.io/research#evaluation-efficiency)\n  * [Hallucination Detection](https://www.rungalileo.io/research#hallucination-detection)\n  * [AI System Diagnostics](https://www.rungalileo.io/research#al-system-diagnostics)\n  * [High Quality Fine-Tuning](https://www.rungalileo.io/research#high-quality-fine-tuning)\n  * [Powerful Metrics](https://www.rungalileo.io/research#powerful-metrics)\n\n\nmodules\n  * [Evaluate](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-evaluate?_gl=1*1rhbtfm*_gcl_au*MTU0MzAwOTU5NS4xNzM2ODY0NDAx)\n  * [Observe](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-observe?_gl=1*1rhbtfm*_gcl_au*MTU0MzAwOTU5NS4xNzM2ODY0NDAx)\n  * [Protect](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-protect)\n  * [Guardrail Metric](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-guardrail-metrics)\n\n\nresources\n  * [Docs](https://docs.rungalileo.io/galileo)\n  * [Blog](https://www.rungalileo.io/blog)\n  * [Hallucination Index](https://www.rungalileo.io/hallucinationindex)\n  * [Mastering RAG eBook](https://www.rungalileo.io/mastering-rag)\n  * [Podcast](https://pod.link/1776879655)\n  * [Request a Demo](https://www.rungalileo.io/get-started)\n\n\ncompany\n  * [Careers](https://www.rungalileo.io/team)\n  * [Privacy Policy](https://drive.google.com/file/d/1kIVXS3F6YAg7KUpZwCpB0BB5vikt2cy8/view)\n  * [Term of Service](https://docs.google.com/document/d/e/2PACX-1vRANTV4gmxpLFggXZRxGofzj65o0bRs8Bp8he2_3psEEPg113D0HD0krqydg-rk-g/pub)\n  * [Case Studies](https://www.rungalileo.io/case-studies)\n\n\n  * [![](https://cdn.sanity.io/images/tf66morw/production/9d89afe946dee14fe310a7aff00a3171d3321340-24x24.svg)![](https://cdn.sanity.io/images/tf66morw/production/2dd33ae72f1ecc3ca644c05b6d8ffcea8def210f-24x24.svg)](https://www.linkedin.com/company/galileo-ai)\n  * [![](https://cdn.sanity.io/images/tf66morw/production/61322ff99c5872369cad5c94091866e4662579dd-24x24.svg)![](https://cdn.sanity.io/images/tf66morw/production/fde049779533431f0cecc3ba12f45cc5c6222bd0-24x24.svg)](https://x.com/rungalileo)\n  * [![](https://cdn.sanity.io/images/tf66morw/production/e74b8448396b837a533c09d917591e50d343d760-24x24.svg)![](https://cdn.sanity.io/images/tf66morw/production/4d7bdaa3d3f37ec29118e7fee4008655d319b838-24x24.svg)](https://www.youtube.com/@rungalileo)\n  * [![](https://cdn.sanity.io/images/tf66morw/production/51fd03602d9c78f66d6cad3095e025d8bc82f0b6-24x24.svg)![](https://cdn.sanity.io/images/tf66morw/production/2ac9c6c1ebacb7536f421200a5f0fdfa51dfe452-24x24.svg)](https://pod.link/1776879655)\n\n\nContact us at info@galileo.ai\n\u00a9 2025 Galileo. All rights reserved.\n![AICPA SOC Logo](https://www.rungalileo.io/footer-logo-right-corner.svg)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nGalileo AI emphasizes the need for new metrics to evaluate Large Language Models (LLMs) due to challenges like high costs of human evaluation and poor correlation with human judgment. Key metrics include Context Adherence, Correctness, and Safety metrics. Custom metrics are also essential for specific applications. The document advocates for a metrics-first approach to enhance LLM evaluation and application effectiveness.\n```"
  },
  {
    "instruction": "[We raised a $75m series C to build the future of inference](https://www.baseten.co/blog/announcing-baseten-75m-series-c/)\n[](https://www.baseten.co/)\n  * [Model library](https://www.baseten.co/library/)\n  * [Solutions](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n**Use Case**\n    * [Transcription](https://www.baseten.co/solutions/transcription/)\n    * [Large language models](https://www.baseten.co/solutions/llms/)\n    * [Image generation](https://www.baseten.co/solutions/image-generation/)\n    * [Text-to-speech](https://www.baseten.co/solutions/text-to-speech/)\n    * [Compound AI](https://www.baseten.co/solutions/compound-ai/)\n    * [Embeddings](https://www.baseten.co/solutions/embeddings/)\n**Platform**\n    * [Model performance](https://www.baseten.co/platform/model-performance/)\n    * [Cloud-native infrastructure](https://www.baseten.co/platform/cloud-native-infrastructure/)\n    * [Embedded engineering](https://www.baseten.co/platform/embedded-engineering/)\n    * [Model management](https://www.baseten.co/platform/model-management/)\n**Deployment**\n    * [Baseten Cloud](https://www.baseten.co/deployments/baseten-cloud/)\n    * [Baseten Self-hosted](https://www.baseten.co/deployments/baseten-self-hosted/)\n    * [Baseten Hybrid](https://www.baseten.co/deployments/baseten-hybrid/)\n  * [Documentation](https://docs.baseten.co/)\n  * [Customers](https://www.baseten.co/customers/)\n  * [Pricing](https://www.baseten.co/pricing/)\n  * [Resources](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n    * [Blog](https://www.baseten.co/blog/)\n    * [Careers](https://www.baseten.co/careers/)\n    * [Changelog](https://www.baseten.co/changelog/)\n    * [Events](https://www.baseten.co/resources/event/)\n    * [Guides](https://www.baseten.co/resources/guide/)\n    * [Webinars](https://www.baseten.co/resources/webinar/)\n\n\n  * [Sign in](https://app.baseten.co/login/)\n  * [Sign up](https://app.baseten.co/signup/)\n\n\n[](https://www.baseten.co/)\n  * [Model library](https://www.baseten.co/library/)\n  * [Solutions](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n**Use Case**\n    * [Transcription](https://www.baseten.co/solutions/transcription/)\n    * [Large language models](https://www.baseten.co/solutions/llms/)\n    * [Image generation](https://www.baseten.co/solutions/image-generation/)\n    * [Text-to-speech](https://www.baseten.co/solutions/text-to-speech/)\n    * [Compound AI](https://www.baseten.co/solutions/compound-ai/)\n    * [Embeddings](https://www.baseten.co/solutions/embeddings/)\n**Platform**\n    * [Model performance](https://www.baseten.co/platform/model-performance/)\n    * [Cloud-native infrastructure](https://www.baseten.co/platform/cloud-native-infrastructure/)\n    * [Embedded engineering](https://www.baseten.co/platform/embedded-engineering/)\n    * [Model management](https://www.baseten.co/platform/model-management/)\n**Deployment**\n    * [Baseten Cloud](https://www.baseten.co/deployments/baseten-cloud/)\n    * [Baseten Self-hosted](https://www.baseten.co/deployments/baseten-self-hosted/)\n    * [Baseten Hybrid](https://www.baseten.co/deployments/baseten-hybrid/)\n  * [Documentation](https://docs.baseten.co/)\n  * [Customers](https://www.baseten.co/customers/)\n  * [Pricing](https://www.baseten.co/pricing/)\n  * [Resources](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n    * [Blog](https://www.baseten.co/blog/)\n    * [Careers](https://www.baseten.co/careers/)\n    * [Changelog](https://www.baseten.co/changelog/)\n    * [Events](https://www.baseten.co/resources/event/)\n    * [Guides](https://www.baseten.co/resources/guide/)\n    * [Webinars](https://www.baseten.co/resources/webinar/)\n\n\n  * [Sign in](https://app.baseten.co/login/)\n  * [Sign up](https://app.baseten.co/signup/)\n\n\n[Baseten](https://www.baseten.co/) / [Blog](https://www.baseten.co/blog/) / [Glossary](https://www.baseten.co/blog/category/glossary/)\n# Continuous vs dynamic batching for AI inference\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n[Matt Howard](https://www.baseten.co/author/matt-howard/)\n[Philip Kiely](https://www.baseten.co/author/philip-kiely/)\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n### Share\n  * [](https://twitter.com/intent/tweet?text=Continuous vs dynamic batching for AI inference&url=https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/&via=basetenco)\n  * [](https://www.linkedin.com/shareArticle?mini=true&url=https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/&title=Continuous vs dynamic batching for AI inference)\n  * [](https://www.facebook.com/sharer/sharer.php?u=https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n\n\n### TL;DR\nWhen using AI models in production, batching makes good use of GPU resources by processing multiple requests to a model simultaneously. But different methods for batching are appropriate in different cases. To maximize throughput of AI inference, use continuous batching for most LLM deployments and dynamic batching for most other models.\nBatch inference is essential for serving LLMs and other generative models in production. If you only run one request at a time through a GPU, most of its capacity is sitting idle. Running multiple inputs through the model simultaneously uses more of the GPU\u2019s resources to massively increase the throughput of your model deployment. However, it\u2019s important to choose the right strategy for batch inference to make sure your model still performs well on other important metrics like latency.\nThere are four ways inference requests can be batched on a GPU:\n  1. No batching: each request is processed one at a time.\n  2. Static batching: requests are placed in batches that are run when full.\n  3. Dynamic batching: requests are placed in batches as they\u2019re received and batches run once full or once enough time has elapsed since the first request.\n  4. Continuous batching: requests are processed token-by-token, with new requests getting processed as older requests finish and free up space on the GPU.\n\n\nBatching method depends on model architecture and modality. In production, you\u2019ll generally want continuous batching for LLMs and dynamic batching for most other generative models.\nIn this article, we\u2019ll examine the different methods for batching inference requests to AI models and the suitable uses for each approach. We\u2019ll limit our discussion to batch inference on the GPU itself (there are other [opportunities to add concurrency](https://docs.baseten.co/performance/concurrency) in an end-to-end system) with a goal of maximizing the utilization of GPU resources.\n##  Naive implementation for basic testing\nThe most naive implementation of any model server has no batching. Every request is processed individually in the order that it\u2019s received.\nIf you spin up a quick service with, say, FastAPI and PyTorch, you won\u2019t get batching out of the box. That\u2019s fine for basic development and testing, but wastes valuable GPU resources in production.\nWithout batching, most of the GPU capacity on the model serving instance is idle. Imagine a road full of cars where every car has only its driver, no passengers. Most cars can fit at least four people, so a lot of capacity is going unused. This is exactly what\u2019s happening on your GPU.\n##  Static batching for scheduled load\nStatic batching is the simplest method for batching requests. But it can increase latency substantially, limiting its use cases.\nWhen running inference on an LLM, or many other ML models, your bottleneck is the memory bandwidth used to load model weights. Model weights are much bigger than the activations (which are the \"state\" of a request mid-processing), so when loading a single layer of weights into the GPU's cache, you want to share that cost across processing many independent sets of activations. This gets you much better throughput than loading one layer of weights and computing it on one activation at a time.\nIf running each request individually is like everyone driving their own car, batching is like a bus. If the bus operates on static batching, the driver waits for the bus to fill entirely, then drives off to the destination. This ensures that the bus is full every time it goes through its route. Similarly, static batching for model inference waits until a set number of requests has been received, then runs a single batch to process the requests simultaneously.\nStatic batching is most appropriate when latency isn\u2019t an issue, like processing a huge corpus of documents on a daily cadence. Static batching pushes the complexity of orchestrating requests elsewhere in the system. Using static batching requires a well-managed queue of requests to feed the model and a method for accepting model output in large chunks.\n##  Dynamic batching for AI inference in production\nStatic batching works well for daily jobs or behind-the-scenes processing. But for latency-sensitive production deployments, like generating images in response to user input, static batching won\u2019t cut it.\nReturning to our bus analogy, imagine being the first person getting on the bus on a slow day. If you have to wait for the entire bus to fill up before you leave, you\u2019ll be waiting a long time. But what if the driver started a timer when the first passenger got on the bus, and left when the bus was full or the timer ran out, whichever happens first. This way, you\u2019re guaranteed to only wait a few minutes maximum.\nDynamic batching works the same way. You set up dynamic batching with:\n  1. A preset maximum batch size, which you hope to reach before kicking off each run.\n  2. A window to wait after receiving the first request before running a partial batch.\n\n\nIn the diagram below, we can see how dynamic batching results in shorter wait times when there is less traffic.\n\u2715\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\nDynamic batching runs batches once full or once a maximum time has elapsed, improving latency versus static batching while maintaining throughput in high-traffic periods.\nLet\u2019s say you set up your model server with a batch size of 16 requests and a window of 100 milliseconds. When the server receives its first request, it will:\n  1. Receive 15 more requests in under 100 milliseconds and immediately run a full batch, or\n  2. Receive fewer than 15 requests and run a partial batch once 100 milliseconds passes.\n\n\nDynamic batching is great for live traffic on models like [Stable Diffusion XL](https://www.baseten.co/library/stable-diffusion-xl/), where each inference request takes about the same amount of time. The right settings for your specific deployment depend on traffic patterns and latency requirements, but dynamic batching gives you flexibility across a wide range of options.\n##  Continuous batching for LLM inference in production\nWhile dynamic batching is great for modalities like image generation where each output takes about the same amount of time to create, we can do even better for LLMs with continuous batching.\nLLMs create a sequence of tokens as output. These output sequences will vary in length \u2013 the model could be answering a simple question or performing a detailed analysis with step-by-step reasoning. If you use a dynamic batching approach, each batch of requests is going to need to wait for the longest output to finish before the next batch can begin. This leaves GPU resources idle.\nContinuous batching works at the token level rather than at the request level. The bottleneck in LLM inference is loading model weights. So for continuous batching, the model server loads each layer of the model sequentially and applies it to the next token of each request. In continuous batching, the same model weights could be used to generate the fifth token of one response and the eighty-fifth token of another.\nIn the bus example, continuous batching is similar to how bus routes work in the real world. As the driver goes through the route, passengers ride the bus for different amounts of time. When one passenger reaches their destination, that opens up a seat for a new passenger.\n\u2715\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\nContinuous batching improves GPU utilization over dynamic batching by eliminating the idle time waiting for the longest response of each batch to finish.\nOne complication with continuous batching is that it takes a lot longer to generate the first token of a response than each subsequent token. This relies on a process called prefill, which is actually compute bound. But over the course of an entire request, next token prediction is the most expensive part, so it\u2019s the part we focus on optimizing with continuous batching.\nLike with dynamic batching, you need to configure continuous batching based on your anticipated traffic patterns. You need to specify:\n  * Maximum batch size: how many requests the model can process at once.\n  * Anticipated sequence shapes: how many tokens the input and output sequences are expected to contain.\n\n\nContinuous batching is implemented at the inference server layer. Model servers like TGI and VLLM offer continuous batching, while TensorRT-LLM uses \u201cin-flight batching\u201d to essentially the same effect.\nThanks to continuous batching, you can massively increase the throughput of your LLM deployments while still hitting ambitious latency targets. Read more about the tradeoffs between batch size and latency in our [guide to LLM benchmarking](https://www.baseten.co/blog/understanding-performance-benchmarks-for-llm-inference/), or see a concrete example in our [benchmarking results for Mistral 7B](https://www.baseten.co/blog/benchmarking-fast-mistral-7b-inference/).\nTable of Contents\n  * [Naive implementation for basic testing](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/#112924-naive-implementation-for-basic-testing)\n  * [Static batching for scheduled load](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/#112928-static-batching-for-scheduled-load)\n  * [Dynamic batching for AI inference in production](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/#3500572-dynamic-batching-for-ai-inference-in-production)\n  * [Continuous batching for LLM inference in production](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/#3500603-continuous-batching-for-llm-inference-in-production)\n\n\n## Subscribe to our newsletter\nStay up to date on model performance, GPUs, and more.\n\u200c\n### Related Glossary posts\n[View all **Glossary**](https://www.baseten.co/blog/category/glossary/)\n## [A quick introduction to speculative decoding](https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/)\nSpeculative decoding improves LLM inference latency by using a smaller model to generate draft tokens that the larger target model can accept during inference.\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n[Pankaj Gupta](https://www.baseten.co/author/pankaj-gupta/)\n2 others\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n## [Building high-performance compound AI applications with MongoDB Atlas and Baseten](https://www.baseten.co/blog/building-high-performance-compound-ai-applications-with-mongodb-atlas-and-baseten/)\nUsing MongoDB Atlas and Baseten\u2019s Chains framework for compound AI, you can build high-performance compound AI systems.\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n[Philip Kiely](https://www.baseten.co/author/philip-kiely/)\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n## [Compound AI systems explained](https://www.baseten.co/blog/compound-ai-systems-explained/)\nCompound AI systems combine multiple models and processing steps, and are forming the next generation of AI products.\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n[Rachel Rapp](https://www.baseten.co/author/rachel-rapp/)\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\nPopular models\n  * [DeepSeek-R1](https://www.baseten.co/library/deepseek-r1/)\n  * [Llama 3.3 70B Instruct](https://www.baseten.co/library/llama-3-3-70b-instruct/)\n  * [MARS6](https://www.baseten.co/library/mars6/)\n  * [Qwen 2.5 32B Coder Instruct](https://www.baseten.co/library/qwen-2-5-32b-coder-instruct/)\n  * [flux-schnell](https://www.baseten.co/library/flux-schnell/)\n\n\nProduct\n  * [Pricing](https://www.baseten.co/pricing/)\n  * [Customer stories](https://www.baseten.co/customers/)\n  * [Terms of service](https://www.baseten.co/terms-and-conditions/)\n  * [Privacy policy](https://www.baseten.co/privacy-policy/)\n  * [Security and trust](https://www.baseten.co/trust/)\n\n\nDevelopers\n  * [Changelog](https://www.baseten.co/changelog/)\n  * [Status](https://status.baseten.co/)\n  * [Docs](https://docs.baseten.co/)\n\n\nCompany\n  * [About](https://www.baseten.co/about/)\n  * [Blog](https://www.baseten.co/blog/)\n  * [Careers](https://www.baseten.co/careers/) We\u2019re hiring\n\n\n[](https://www.baseten.co/)\n[](https://github.com/basetenlabs)[](http://twitter.com/basetenco)[](https://www.linkedin.com/company/baseten)[](https://www.youtube.com/channel/UCOCLmqf7Jy3LcsO0SMBGP_Q)\n![Hipaa Compliant](https://www.baseten.co/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhipaa.2eb9b526.png&w=256&q=75)![SOC 2 Type II Certified](https://www.baseten.co/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsoc2-type2.0821ffd3.png&w=256&q=75)\n",
    "answer": "```markdown\n# TL;DR\n\nBaseten raised $75M in Series C funding to enhance AI inference. For optimal GPU utilization, use **continuous batching** for LLMs and **dynamic batching** for other models. This approach maximizes throughput while managing latency effectively.\n```"
  },
  {
    "instruction": "This website stores cookies on your computer. These cookies are used to collect information about how you interact with our website and allow us to remember you. We use this information in order to improve and customize your browsing experience and for analytics and metrics about our visitors both on this website and other media. To find out more about the cookies we use, see our Privacy Policy\nIf you decline, your information won\u2019t be tracked when you visit this website. A single cookie will be used in your browser to remember your preference not to be tracked.\nCookies settings\nAcceptDecline\n[ ![Arize AI Logo](https://arize.com/wp-content/themes/arize-2022/images/logo2.svg) ](https://arize.com)\n  * [Products](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [![](https://arize.com/wp-content/uploads/2024/07/Group-11691.svg)PRODUCT](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Arize AX - Generative](https://arize.com/generative-ai/)\n    * [Arize AX - ML & CV](https://arize.com/ml-cv-observability)\n    * [Arize PX - Phoenix OSS](https://arize.com/phoenix-oss/)\n[](https://arize.com/resource/arize-platform-demo-2/)\n[![](https://arize.com/wp-content/uploads/2024/07/Platform-demo-thumbnail-2024.jpg)](https://arize.com/resource/arize-platform-demo-2/)\n#### Arize demo\nSee how it works\n  * [Pricing](https://arize.com/pricing)\n  * [Docs](https://docs.arize.com/arize)\n  * [Learn](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [![](https://arize.com/wp-content/uploads/2024/07/Layer_1-1.svg)LEARN](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Blog](https://arize.com/blog/)\n    * [Agents Hub](https://arize.com/ai-agents/)\n    * [LLM Evals Hub](https://arize.com/llm-evaluation)\n    * [AI Fundamentals](https://arize.com/blog-course/introduction-fundamentals/)\n    * [Paper Readings](https://arize.com/ai-research-papers/)\n    * [Video Tutorials](https://www.youtube.com/@arizeai/featured)\n    * [Courses](https://courses.arize.com/courses/)\n    * [Community & Events](https://arize.com/community/)\n[](https://arize.com/observe-2025/)\n[![](https://arize.com/wp-content/uploads/2025/02/Observe-Reg-open-nav-promo-2.jpg)](https://arize.com/observe-2025/)\n#### Arize:Observe is Back!\nGet Special Pricing Until April 30\n  * [Company](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [![](https://arize.com/wp-content/uploads/2024/07/Layer_1-2.svg)COMPANY](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [About](https://arize.com/about-us)\n    * [Careers](https://arize.com/careers/)\n    * [Partners](https://arize.com/partners/)\n    * [Press](https://arize.com/press/)\n    * [Security](https://arize.com/trust-center/)\n[](https://arize.com/customers/)\n[![](https://arize.com/wp-content/uploads/2024/07/Group-11696.png)](https://arize.com/customers/)\n#### Customers\nSee who is achieving better outcomes in production.\n  * [Star](https://github.com/Arize-ai/phoenix)\n  * [Get a Demo](https://arize.com/request-a-demo/)\n  * [Sign in](https://app.arize.com/auth/login?redirect_uri=/)\n  * [Sign Up](https://app.arize.com/auth/join)\n\n\n![](https://arize.com/wp-content/themes/arize-2022/images/icon-menu.svg)\n\u00d7\n###### ARIZE MACHINE LEARNING COURSE\n####  ![](https://arize.com/wp-content/themes/arize-2022/images/icon-courses.svg) The Fundamentals \n  * [ML Observability Course Overview](https://arize.com/blog-course/)\n  * [ML Observability Basics](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [What Is Observability?](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [ML Observability: The Essentials](https://arize.com/blog-course/machine-learning-observability-essentials/)\n    * [Tracing In Machine Learning](https://arize.com/blog-course/performance-tracing-machine-learning/)\n  * [Model Evaluation Metrics](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Top Model Evaluation Metrics](https://arize.com/blog-course/model-evaluation-metrics/)\n    * [Binary Cross Entropy](https://arize.com/blog-course/binary-cross-entropy-log-loss/)\n    * [Precision](https://arize.com/blog-course/precision-ml/)\n    * [Recall](https://arize.com/blog-course/precision-vs-recall/)\n    * [F1 Score](https://arize.com/blog-course/f1-score/)\n    * [Calibration Curve](https://arize.com/blog-course/what-is-calibration-reliability-curve/)\n    * [PR AUC](https://arize.com/blog/what-is-pr-auc/)\n    * [AUC ROC](https://arize.com/blog/what-is-auc/)\n    * [Mean Absolute Error](https://arize.com/blog-course/mean-absolute-error-in-machine-learning-what-you-need-to-know/)\n    * [Mean Absolute Percentage Error (MAPE)](https://arize.com/blog-course/mean-absolute-percentage-error-mape-what-you-need-to-know/)\n    * [R Squared](https://arize.com/blog-course/r-squared-understanding-the-coefficient-of-determination/)\n    * [RMSE](https://arize.com/blog-course/root-mean-square-error-rmse-what-you-need-to-know/)\n    * [Normalized Discounted Cumulative Gain (NDCG)](https://arize.com/blog-course/ndcg/)\n    * [Other Rank Aware Evaluation Metrics](https://arize.com/blog-course/monitoring-collaborative-filtering-recsys/)\n    * [Bleu Score and LLM Metrics](https://arize.com/blog-course/generative-ai-metrics-bleu-score/)\n  * [Drift Metrics](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Drift Metrics: a Quickstart Guide](https://arize.com/blog-course/drift/)\n    * [Data Binning](https://arize.com/blog-course/data-binning-production/)\n    * [Population Stability Index (PSI)](https://arize.com/blog-course/population-stability-index-psi/)\n    * [KL Divergence](https://arize.com/blog-course/kl-divergence/)\n    * [Jensen Shannon Divergence](https://arize.com/blog-course/jensen-shannon-divergence/)\n    * [Kolmogorov Smirnov Test](https://arize.com/blog-course/kolmogorov-smirnov-test/)\n  * [Model Fairness](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Algorithmic Bias](https://arize.com/blog-course/algorithmic-bias-examples-tools/)\n    * [Building Transparent AI Software Systems](https://arize.com/blog-course/transparent-ethical-ai-software-systems/)\n  * [Data Quality](https://arize.com/blog-course/data-quality-management-for-mlops/)\n  * [Service-Level Monitoring](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Service Monitoring Metrics](https://arize.com/blog-course/service-monitoring-metrics/)\n    * [ML Service-Level Performance Monitoring Essentials](https://arize.com/blog-course/ml-service-level-performance-monitoring/)\n  * [Explainability](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Explainability](https://arize.com/blog-course/explainability-xai-primer/)\n    * [Explainability Techniques](https://arize.com/blog-course/explainability-techniques-shap/)\n  * [Monitoring Image and Language Models and Embeddings](https://arize.com/blog-course/embeddings-meaning-examples-and-how-to-compute/)\n    * [KNN Algorithm](https://arize.com/blog-course/knn-algorithm-k-nearest-neighbor/)\n    * [Tokenization](https://arize.com/blog-course/tokenization/)\n    * [Embedding Versioning](https://arize.com/blog-course/embedding-versioning/)\n    * [Dimensionality Reduction](https://arize.com/blog-course/reduction-of-dimensionality-top-techniques/)\n    * [What Is LLMOps?](https://arize.com/blog-course/llmops-operationalizing-llms-at-scale/)\n    * [Monitoring Embedding/Vector Drift](https://arize.com/blog-course/embedding-drift-euclidean-distance/)\n    * [BERT](https://arize.com/blog-course/unleashing-bert-transformer-model-nlp/)\n    * [Fine-Tuning LLM Applications with LangChain](https://arize.com/blog-course/langchain-llm-agent-monitoring/)\n    * [Evals from OpenAI](https://arize.com/blog-course/evals-openai-simplifying-llm-evaluation/)\n    * [Guide To LLMs Evaluating LLMs](https://arize.com/blog-course/assessing-large-language-models/)\n\n\n  * [Certifications](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [LLM Certification](https://courses.arize.com/courses/)\n    * [Agents, Tools, and Chains](https://courses.arize.com/p/agents-tools-and-chains)\n    * [LLM Evaluations](https://courses.arize.com/p/llm-evaluations)\n    * [Traces and Spans](https://courses.arize.com/p/llm-observability-traces-spans)\n    * [ML Observability Fundamentals](https://arize.com/ml-observability-fundamentals/)\n    * [Advanced Model Performance Metrics](https://courses.arize.com/p/ml-observability-advanced-metrics-course)\n  * [New to Arize?](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Arize onboarding](https://docs.arize.com/arize/quickstart)\n\n\n#  What Is Observability? \n![](https://arize.com/wp-content/uploads/2020/12/bg-speaker-98x98.png)\n[ Aparna Dhinakaran, ](https://arize.com/author/aparna-dhinakaran/) Co-founder & Chief Product Officer  | Published February 02, 2023\nIn 1969, humans first stepped on the moon thanks to a lot of clever engineering and 150,000 lines of code. Among other things, this code enabled engineers at mission control to have the full view of the mission and make near-real-time decisions. The amount of code was so small that engineers were able to thoroughly debug the software, and its performance was nearly flawless. Today\u2019s search engines, on the other hand, execute billions of lines of code, making it much more likely that engineers will make mistakes or extraordinary circumstances will crop up somewhere in the system. On top of that, both data and machine learning models (code that computers essentially write for themselves) come with their own challenges. In short, it is nearly impossible to guarantee that a modern software system will always perform as you expect. That\u2019s why observability is crucial on all production systems.\nLuckily, we can automate observability of complex software systems so you can have timely and actionable information about your software, just like NASA engineers had about the Apollo missions.\n## What Is Observability?\nObservability is the practice of quickly finding out what the problem is and getting to the root cause. Most modern software systems include infrastructure, data, and machine learning models. All three need observability, but each has different requirements, workflows, and personas. Let\u2019s take a closer look!\n![](https://arize.com/wp-content/uploads/2021/05/observability-iceberg-2.png)\n## What are the Types of Observability?\nObservability is of course not unique to technical personas like DevOps, data engineers, and MLOps engineers. It is the same discipline used in tracking business metrics and industry-specific requirements, but in this post we will focus on what observability means to people in these three more technical roles.\nThe three types of engineers use observability to solve different problems with distinct workflows. At the highest level, infrastructure observability is concerned with software problems that often revolve around latency, failures, and resource utilization. Data observability, on the other hand, cares more about changes in data that your application is using, either at rest in the database or in any step that feeds into your data storage system. Machine learning observability is a newer discipline; it focuses on the performance of your models, measuring how well they predict values either on aggregate in certain slices of the data or on the level of individual predictions with techniques like SHAP analysis.\n![](https://arize.com/wp-content/uploads/2022/06/observability-personas-examples.jpg)\n## Infrastructure Observability\n### Persona\nSoftware/DevOps engineers are responsible for development and operations of software infrastructure. Their code is the mortar that holds all the bricks of modern software systems together.\n### Problem\nMany modern applications rely on several systems coming together, and a small problem in one system can show up as a significant slowdown in the overall application.\nA new version of the code can have unexpected bugs. Configuration changes, passwords, and rare occurrences like leap seconds can all cause troubles at this layer. Additionally, DevOps engineers are often responsible for software systems they did not design. Virtual machines or Kubernetes clusters running their code can fail.\n### Workflow\nSeveral workflows can be employed at this layer, depending on where the problem manifests. Let\u2019s look at one of them.\nSay you are a DevOps engineer for a large ecommerce site\u2014let\u2019s call it Nile. Suddenly, pages start taking just a little bit longer to load. It is well established that a 1% slowdown on such an application can cause a 15% drop in customer satisfaction, which translates to huge loss of revenue.\nNo sweat\u2014you have integrated your application with a market-leading observability solution. You get notified about the problem right away and can start troubleshooting.\nFirst, since you logged your page load times as well as individual query times for each query on the page, you can identify which pages load slower than others. Then you can look at individual query times and sort them by which are taking the longest.\nDeeper down in your system, you can identify sub-queries that are slowing you down. By analyzing the timing around these sub-traces, DevOps engineers like you are easily able to see patterns that point them to the root cause of the problem. In this case, a recent code change introduced more latency in history lookup for customers with more than 300 purchases in the last year, and thanks to tail latency amplification, all your customers paid the price. You back out the change until your team can figure out how to deploy it without incurring such costs.\nProblem solved.\n![](https://arize.com/wp-content/uploads/2022/06/system-infra-observability-4.jpg)\n## Data Observability\n### Persona\nData Engineers and Analytics Engineers are responsible for building and operating data pipelines. They ensure the timeliness, completeness, and other quality characteristics of the data that your application receives.\n### Problem\nWhen the original source of the data makes a change, it may alter the schema, contents, or arrival times of the data into the data lake or warehouse. For example, say your data vendor switched from US zip codes (number) to global postal codes (strings). Suddenly a step in your pipeline malfunctions because it relies on a zip code being a number or being exactly 5 digits long, and you get partial or null postal code for all your Canadian customers.\nOther problems can include stale data (from a pipeline not running on time), volume (accidentally dropped or duplicated records), and distribution (a negative value for a person\u2019s age or a new categorical value). These can be caused by changes in the data\u2019s original source, issues with the data infrastructure, or changes to the code in various transformation steps the data goes through as it moves down the pipeline.\nIdentifying these issues across all data pipelines, 24/7 is crucial to preventing data outages from reaching the data used by models for training and serving, and quickly scales beyond manual approaches like spot checks and even semi-automated approaches like data pipeline tests.\n### Workflow\nNow let\u2019s say you are a data engineer for a fast-growing analytics company. You got your first huge customer, and they have just launched a crucial marketing campaign that depends on your company\u2019s customer segmentation model. The recommendations your model serves over the next 24 hours will make a huge difference to their business and yours. And that model depends on fresh audience data to serve up.\nYou get an alert from your data observability tool: a table that is supposed to update every 60 minutes has not been updated in 75. Clicking the link in the alert takes you to the SLA for one of your third party data providers. You see that the data is stale, and that a similar issue happened a few months ago. Reading the incident notes your teammate left when they fixed this last time, you see which provider this data comes from, and their phone number. You pick up the phone and give them a call. Guess what? They didn\u2019t even know they had an issue until you called. Their SSL certificate expired and the cron job they are using to renew it got stuck. They rerun the script manually, and your data observability tool confirms that everything is back to normal. Time to detect: 15 minutes. Time to fix: 10 minutes. Customer impact: negligible. No one even knows you just saved your company\u2019s biggest deal to date.\n![](https://arize.com/wp-content/uploads/2022/06/data-observability-5.png)\n## Machine Learning Observability\n### Persona\nMachine learning engineers are responsible for developing, productionizing, and operating machine learning models.\n### Problem\nMore and more software systems rely on machine learning models. More and more economic value relies on machine learning models. A poorly performing model can lead to poor decisions or worse, loss of reputation and revenue for many customers.\nUnlike traditional software, however, machine learning models are not static. Performance usually degrades over time. In fact, [one survey](https://arize.com/resource/survey-machine-learning-observability-results/) has shown that on average models degrade within 10 days of being in production. It is vital to have the tools to detect problems, establish root cause, and improve models as quickly as possible.\n### Workflow\nFor this example, let\u2019s say you are a machine learning engineer maintaining a [fraud model](https://arize.com/blog/best-practices-in-ml-observability-for-monitoring-mitigating-and-preventing-fraud/) for a major bank. Your model labels transactions as fraudulent (or not). One day your [ML observability](https://arize.com/ml-observability/) system sends you an alert. You log in and can quickly arrive at the very slice of inputs that seem to generate unexpected output distribution. Among the inputs, you can identify a new scammer. Perhaps this new scammer exhibits behaviors the model has not seen before, but you can retrain it with this new data included and catch them and anyone else who tries this scheme in the future.\nCollecting useful information is once again key. In this case, you are looking for model inputs and outputs over time. Once you have the data you need, having some idea of baseline performance is very useful.\nWhen the model performance goes outside the baseline bounds, you can zoom in on the timeframe where the issues are present. Next you would sort your data by slices and evaluate the problematic ones.\n![](https://arize.com/wp-content/uploads/2022/06/ml-observe-example-workflow.jpg)\n## Conclusion\nAs software becomes more integral in the world at large and in your company in particular, the need for observability grows. The level of observability required for a complex software system today necessitates automated tools. Which tools you need depends on what problems you are solving. Typically software and DevOps engineers will mainly rely on system and infrastructure observability tools. Data engineers have specific challenges that require solutions tailored for data observability. Machine learning engineers require a whole different set of tools to troubleshoot degrading model performance. And as your team and company scale, you will probably find yourself needing all three.\n**_This post is written in partnership with Kyle Kirwan, Co-founder and CEO at data observability platform_[ _Bigeye_](https://www.bigeye.com/)**\n##### On this page\n  * [ What Is Observability? ](https://arize.com/blog-course/what-is-observability-data-machine-learning#what-is-observability)\n  * [ Types of Observability ](https://arize.com/blog-course/what-is-observability-data-machine-learning#types-of-observability)\n  * [ Infrastructure Observability ](https://arize.com/blog-course/what-is-observability-data-machine-learning#infrastructure-observability)\n  * [ Data Observability ](https://arize.com/blog-course/what-is-observability-data-machine-learning#data-observability)\n  * [ Machine Learning Observability ](https://arize.com/blog-course/what-is-observability-data-machine-learning#machine-learning-observability)\n\n\n[ ![Arize icon](https://arize.com/wp-content/uploads/2023/02/Arize-icon@2x.png) Try Arize  ](https://app.arize.com/auth/join)\n[ ![Slack icon](https://arize.com/wp-content/uploads/2023/02/Slack-icon@2x.png) Ask any AI question  ](https://arize-ai.slack.com/)\n## Sign up for our monthly newsletter, The Evaluator.\n[Subscribe](https://arize.com/blog-course/what-is-observability-data-machine-learning/#blog-subscribe-modal)\n[ ![arize logo](https://arize.com/wp-content/themes/arize-2022/images/Arize-logo.svg) ](https://arize.com)\n  * [Platform](https://arize.com)\n  * [Docs](https://docs.arize.com/arize)\n  * [Pricing](https://arize.com/pricing)\n  * [Phoenix](https://phoenix.arize.com/)\n\n\nLearn\n  * [Course](https://arize.com/elearning/)\n  * [Community & Events](https://arize.com/community/)\n  * [Blog](https://arize.com/blog/)\n\n\nTopics\n  * [LLM Evaluation](https://arize.com/llm-evaluation)\n  * [AI Agents](https://arize.com/ai-agents/)\n  * [LLM Tracing](https://docs.arize.com/phoenix/tracing/llm-traces)\n  * [RAG Evaluation](https://arize.com/blog-course/rag-evaluation/)\n\n\nCompany\n  * [About](https://arize.com/about-company/)\n  * [Customers](https://arize.com/customers/)\n  * [Careers](https://arize.com/careers/)\n  * [Press](https://arize.com/press/)\n  * [Security](https://arize.com/trust-center/)\n\n\n  * [Try now](https://app.arize.com/auth/join)\n  * [Book a Demo](https://arize.com/request-a-demo/)\n  * [Sign In](https://app.arize.com/auth/login)\n  * [Contact](https://arize.com/contact/)\n  * [Privacy Policy](https://arize.com/privacy-policy/)\n  * [Twitter](https://twitter.com/arizeai?lang=en)\n  * [Linkedin](https://www.linkedin.com/company/arizeai/)\n\n\nCopyright \u00a9 2025 Arize AI, Inc\nJoin the Arize ML Observability Community \nSubscribe to the Arize blog \nSubscribe to get the latest news, expertise, and product updates from Arize. Your inbox is sacred, so we\u2019ll only curate and send the best stuff. \nUTM Campaign\nUTM Content\nUTM Medium\nUTM Source\nUTM Term\n*We\u2019re committed to your privacy. Arize uses the information you provide to contact you about relevant content, products, and services. You may unsubscribe from these communications at any time. For more information, check out our [privacy policy](https://arize.com/privacy-policy/).\nContact us \nLike what you see? Let\u2019s chat. Fill out this form and we will be in contact with you soon! \n",
    "answer": "```markdown\n# TL;DR Summary\n\n**Observability** is crucial for modern software systems, encompassing **infrastructure**, **data**, and **machine learning** observability. Each type addresses specific challenges faced by engineers, ensuring timely detection and resolution of issues to maintain performance and reliability.\n```"
  },
  {
    "instruction": "![An image of the Aquarium logo and the Notion logo with a plus sign between them.](https://www.tidepool.so/blog/emerging-ux-patterns-for-generative-ai-apps-copilots/images/hires_notion_announcement.png)\n# Aquarium is joining Notion!\nAquarium\u2019s mission has always been to accelerate the process of building and deploying production AI systems. Over the last five years, we\u2019ve seen a massive leap in the capabilities of AI models, and we\u2019ve been lucky to help a variety of AI teams across computer vision and natural language processing to improve the quality of their AI systems more quickly and easily than ever before.\nWhen we first met the Notion team, we realized that there was a massive opportunity to use generative AI to bring value to more than 100 million users. We saw a good fit between the two teams: Aquarium has built a lot of AI retrieval technology over the years, while Notion is working towards their vision of bringing AI to their vast user base spanning college students to the enterprise. It ended up being such a good fit that we decided to join the Notion AI team!\nAs part of bringing our teams together, we have made the difficult decision to wind down our products. We recognize this impacts our existing customers, and we are doing our best to ensure the transition is as smooth as possible.\nThanks to everyone who supported Aquarium and helped get us to where we are today - our team members, customers, investors, and everyone who we\u2019ve met on the startup journey. We\u2019re excited to be joining Notion to help make AI a ubiquitous and useful part of everyday life!\n- The Aquarium Team\n",
    "answer": "```markdown\n# TL;DR Summary\n\nAquarium is joining Notion to enhance generative AI capabilities for over 100 million users. They will wind down their products to integrate with Notion's AI team, aiming to make AI more accessible and useful. Thanks to supporters for their journey.\n\n**Tags:** Generative AI, LLMs\n```"
  },
  {
    "instruction": "[ Skip to content ](https://mlops-coding-course.fmind.dev/#mlops-coding-course)\n[ ![logo](https://mlops-coding-course.fmind.dev/assets/logo.png) ](https://mlops-coding-course.fmind.dev/index.html \"MLOps Coding Course\")\nMLOps Coding Course \nMLOps Coding Course \nInitializing search \n[ MLOPS-Courses/mlops-coding-course  ](https://github.com/MLOPS-Courses/mlops-coding-course \"Go to repository\")\n  * [ MLOps Coding Course ](https://mlops-coding-course.fmind.dev/index.html)\n  * [ 0. Overview ](https://mlops-coding-course.fmind.dev/0.%20Overview/index.html)\n  * [ 1. Initializing ](https://mlops-coding-course.fmind.dev/1.%20Initializing/index.html)\n  * [ 2. Prototyping ](https://mlops-coding-course.fmind.dev/2.%20Prototyping/index.html)\n  * [ 3. Productionizing ](https://mlops-coding-course.fmind.dev/3.%20Productionizing/index.html)\n  * [ 4. Validating ](https://mlops-coding-course.fmind.dev/4.%20Validating/index.html)\n  * [ 5. Refining ](https://mlops-coding-course.fmind.dev/5.%20Refining/index.html)\n  * [ 6. Sharing ](https://mlops-coding-course.fmind.dev/6.%20Sharing/index.html)\n  * [ 7. Observability ](https://mlops-coding-course.fmind.dev/7.%20Observability/index.html)\n\n\n[ ![logo](https://mlops-coding-course.fmind.dev/assets/logo.png) ](https://mlops-coding-course.fmind.dev/index.html \"MLOps Coding Course\") MLOps Coding Course \n[ MLOPS-Courses/mlops-coding-course  ](https://github.com/MLOPS-Courses/mlops-coding-course \"Go to repository\")\n  * MLOps Coding Course  [ MLOps Coding Course  ](https://mlops-coding-course.fmind.dev/index.html) Table of contents \n    * [ Chapter 0: Overview  ](https://mlops-coding-course.fmind.dev/#chapter-0-overview)\n    * [ Chapter 1: Initializing  ](https://mlops-coding-course.fmind.dev/#chapter-1-initializing)\n    * [ Chapter 2: Prototyping  ](https://mlops-coding-course.fmind.dev/#chapter-2-prototyping)\n    * [ Chapter 3: Productionizing  ](https://mlops-coding-course.fmind.dev/#chapter-3-productionizing)\n    * [ Chapter 4: Validating  ](https://mlops-coding-course.fmind.dev/#chapter-4-validating)\n    * [ Chapter 5: Refining  ](https://mlops-coding-course.fmind.dev/#chapter-5-refining)\n    * [ Chapter 6: Sharing  ](https://mlops-coding-course.fmind.dev/#chapter-6-sharing)\n    * [ Chapter 7: Observability  ](https://mlops-coding-course.fmind.dev/#chapter-7-observability)\n    * [ Let's journey together!  ](https://mlops-coding-course.fmind.dev/#lets-journey-together)\n  * [ 0. Overview  ](https://mlops-coding-course.fmind.dev/0.%20Overview/index.html)\n0. Overview \n    * [ 0.0. Course  ](https://mlops-coding-course.fmind.dev/0.%20Overview/0.0.%20Course.html)\n    * [ 0.1. Projects  ](https://mlops-coding-course.fmind.dev/0.%20Overview/0.1.%20Projects.html)\n    * [ 0.2. Datasets  ](https://mlops-coding-course.fmind.dev/0.%20Overview/0.2.%20Datasets.html)\n    * [ 0.3. Platforms  ](https://mlops-coding-course.fmind.dev/0.%20Overview/0.3.%20Platforms.html)\n    * [ 0.4. Mentoring  ](https://mlops-coding-course.fmind.dev/0.%20Overview/0.4.%20Mentoring.html)\n    * [ 0.5. Assistants  ](https://mlops-coding-course.fmind.dev/0.%20Overview/0.5.%20Assistants.html)\n    * [ 0.6. Resources  ](https://mlops-coding-course.fmind.dev/0.%20Overview/0.6.%20Resources.html)\n  * [ 1. Initializing  ](https://mlops-coding-course.fmind.dev/1.%20Initializing/index.html)\n1. Initializing \n    * [ 1.0. System  ](https://mlops-coding-course.fmind.dev/1.%20Initializing/1.0.%20System.html)\n    * [ 1.1. Python  ](https://mlops-coding-course.fmind.dev/1.%20Initializing/1.1.%20Python.html)\n    * [ 1.2. uv  ](https://mlops-coding-course.fmind.dev/1.%20Initializing/1.2.%20uv.html)\n    * [ 1.3. uv (project)  ](https://mlops-coding-course.fmind.dev/1.%20Initializing/1.3.%20uv%20%28project%29.html)\n    * [ 1.4. Git  ](https://mlops-coding-course.fmind.dev/1.%20Initializing/1.4.%20git.html)\n    * [ 1.5. GitHub  ](https://mlops-coding-course.fmind.dev/1.%20Initializing/1.5.%20GitHub.html)\n    * [ 1.6. VS Code  ](https://mlops-coding-course.fmind.dev/1.%20Initializing/1.6.%20VS%20Code.html)\n    * Archives  Archives \n      * [ 1.2. pyenv  ](https://mlops-coding-course.fmind.dev/1.%20Initializing/archives/1.2.%20pyenv.html)\n      * [ 1.3 Poetry  ](https://mlops-coding-course.fmind.dev/1.%20Initializing/archives/1.3.%20Poetry.html)\n  * [ 2. Prototyping  ](https://mlops-coding-course.fmind.dev/2.%20Prototyping/index.html)\n2. Prototyping \n    * [ 2.0. Notebooks  ](https://mlops-coding-course.fmind.dev/2.%20Prototyping/2.0.%20Notebooks.html)\n    * [ 2.1. Imports  ](https://mlops-coding-course.fmind.dev/2.%20Prototyping/2.1.%20Imports.html)\n    * [ 2.2. Configs  ](https://mlops-coding-course.fmind.dev/2.%20Prototyping/2.2.%20Configs.html)\n    * [ 2.3. Datasets  ](https://mlops-coding-course.fmind.dev/2.%20Prototyping/2.3.%20Datasets.html)\n    * [ 2.4. Analysis  ](https://mlops-coding-course.fmind.dev/2.%20Prototyping/2.4.%20Analysis.html)\n    * [ 2.5. Modeling  ](https://mlops-coding-course.fmind.dev/2.%20Prototyping/2.5.%20Modeling.html)\n    * [ 2.6. Evaluations  ](https://mlops-coding-course.fmind.dev/2.%20Prototyping/2.6.%20Evaluations.html)\n  * [ 3. Productionizing  ](https://mlops-coding-course.fmind.dev/3.%20Productionizing/index.html)\n3. Productionizing \n    * [ 3.0. Package  ](https://mlops-coding-course.fmind.dev/3.%20Productionizing/3.0.%20Package.html)\n    * [ 3.1. Modules  ](https://mlops-coding-course.fmind.dev/3.%20Productionizing/3.1.%20Modules.html)\n    * [ 3.2. Paradigms  ](https://mlops-coding-course.fmind.dev/3.%20Productionizing/3.2.%20Paradigms.html)\n    * [ 3.3. Entrypoints  ](https://mlops-coding-course.fmind.dev/3.%20Productionizing/3.3.%20Entrypoints.html)\n    * [ 3.4. Configurations  ](https://mlops-coding-course.fmind.dev/3.%20Productionizing/3.4.%20Configurations.html)\n    * [ 3.5. Documentations  ](https://mlops-coding-course.fmind.dev/3.%20Productionizing/3.5.%20Documentations.html)\n    * [ 3.6. VS Code Workspace  ](https://mlops-coding-course.fmind.dev/3.%20Productionizing/3.6.%20VS%20Code%20Workspace.html)\n  * [ 4. Validating  ](https://mlops-coding-course.fmind.dev/4.%20Validating/index.html)\n4. Validating \n    * [ 4.0. Typing  ](https://mlops-coding-course.fmind.dev/4.%20Validating/4.0.%20Typing.html)\n    * [ 4.1. Linting  ](https://mlops-coding-course.fmind.dev/4.%20Validating/4.1.%20Linting.html)\n    * [ 4.2. Testing  ](https://mlops-coding-course.fmind.dev/4.%20Validating/4.2.%20Testing.html)\n    * [ 4.3. Logging  ](https://mlops-coding-course.fmind.dev/4.%20Validating/4.3.%20Logging.html)\n    * [ 4.4. Security  ](https://mlops-coding-course.fmind.dev/4.%20Validating/4.4.%20Security.html)\n    * [ 4.5. Formatting  ](https://mlops-coding-course.fmind.dev/4.%20Validating/4.5.%20Formatting.html)\n    * [ 4.6. Debugging  ](https://mlops-coding-course.fmind.dev/4.%20Validating/4.6.%20Debugging.html)\n  * [ 5. Refining  ](https://mlops-coding-course.fmind.dev/5.%20Refining/index.html)\n5. Refining \n    * [ 5.0. Design Patterns  ](https://mlops-coding-course.fmind.dev/5.%20Refining/5.0.%20Design%20Patterns.html)\n    * [ 5.1. Task Automation  ](https://mlops-coding-course.fmind.dev/5.%20Refining/5.1.%20Task%20Automation.html)\n    * [ 5.2. Pre-Commit Hooks  ](https://mlops-coding-course.fmind.dev/5.%20Refining/5.2.%20Pre-Commit%20Hooks.html)\n    * [ 5.3. CI/CD Workflows  ](https://mlops-coding-course.fmind.dev/5.%20Refining/5.3.%20CI-CD%20Workflows.html)\n    * [ 5.4. Software Containers  ](https://mlops-coding-course.fmind.dev/5.%20Refining/5.4.%20Software%20Containers.html)\n    * [ 5.5. AI/ML Experiments  ](https://mlops-coding-course.fmind.dev/5.%20Refining/5.5.%20AI-ML%20Experiments.html)\n    * [ 5.6. Model Registries  ](https://mlops-coding-course.fmind.dev/5.%20Refining/5.6.%20Model%20Registries.html)\n    * Archives  Archives \n      * [ 5.1. Task Automation  ](https://mlops-coding-course.fmind.dev/5.%20Refining/archives/5.1.%20PyInvoke.html)\n  * [ 6. Sharing  ](https://mlops-coding-course.fmind.dev/6.%20Sharing/index.html)\n6. Sharing \n    * [ 6.0. Repository  ](https://mlops-coding-course.fmind.dev/6.%20Sharing/6.0.%20Repository.html)\n    * [ 6.1. License  ](https://mlops-coding-course.fmind.dev/6.%20Sharing/6.1.%20License.html)\n    * [ 6.2. Readme  ](https://mlops-coding-course.fmind.dev/6.%20Sharing/6.2.%20Readme.html)\n    * [ 6.3. Releases  ](https://mlops-coding-course.fmind.dev/6.%20Sharing/6.3.%20Releases.html)\n    * [ 6.4. Templates  ](https://mlops-coding-course.fmind.dev/6.%20Sharing/6.4.%20Templates.html)\n    * [ 6.5. Workstations  ](https://mlops-coding-course.fmind.dev/6.%20Sharing/6.5.%20Workstations.html)\n    * [ 6.6. Contributions  ](https://mlops-coding-course.fmind.dev/6.%20Sharing/6.6.%20Contributions.html)\n  * [ 7. Observability  ](https://mlops-coding-course.fmind.dev/7.%20Observability/index.html)\n7. Observability \n    * [ 7.0. Reproducibility  ](https://mlops-coding-course.fmind.dev/7.%20Observability/0.%20Reproducibility.html)\n    * [ 7.1. Monitoring  ](https://mlops-coding-course.fmind.dev/7.%20Observability/1.%20Monitoring.html)\n    * [ 7.2. Alerting  ](https://mlops-coding-course.fmind.dev/7.%20Observability/2.%20Alerting.html)\n    * [ 7.3. Lineage  ](https://mlops-coding-course.fmind.dev/7.%20Observability/3.%20Lineage.html)\n    * [ 7.4. Costs and KPIs  ](https://mlops-coding-course.fmind.dev/7.%20Observability/4.%20Costs-KPIs.html)\n    * [ 7.5. Explainability  ](https://mlops-coding-course.fmind.dev/7.%20Observability/5.%20Explainability.html)\n    * [ 7.6. Infrastructure  ](https://mlops-coding-course.fmind.dev/7.%20Observability/6.%20Infrastructure.html)\n\n\nTable of contents \n  * [ Chapter 0: Overview  ](https://mlops-coding-course.fmind.dev/#chapter-0-overview)\n  * [ Chapter 1: Initializing  ](https://mlops-coding-course.fmind.dev/#chapter-1-initializing)\n  * [ Chapter 2: Prototyping  ](https://mlops-coding-course.fmind.dev/#chapter-2-prototyping)\n  * [ Chapter 3: Productionizing  ](https://mlops-coding-course.fmind.dev/#chapter-3-productionizing)\n  * [ Chapter 4: Validating  ](https://mlops-coding-course.fmind.dev/#chapter-4-validating)\n  * [ Chapter 5: Refining  ](https://mlops-coding-course.fmind.dev/#chapter-5-refining)\n  * [ Chapter 6: Sharing  ](https://mlops-coding-course.fmind.dev/#chapter-6-sharing)\n  * [ Chapter 7: Observability  ](https://mlops-coding-course.fmind.dev/#chapter-7-observability)\n  * [ Let's journey together!  ](https://mlops-coding-course.fmind.dev/#lets-journey-together)\n\n\n# MLOps Coding Course\n![MLOps Coding Course Banner](https://mlops-coding-course.fmind.dev/assets/banner.png) _Learn how to create, develop, and maintain a state-of-the-art MLOps code base._\nWelcome to our comprehensive [MLOps Coding Course](https://mlops-coding-course.fmind.dev/), designed to integrate robust software development practices with cutting-edge data science techniques. This course is tailored for both beginners who are just starting their journey and experienced professionals looking to enhance their skills in managing and executing AI and machine learning projects using Python. Here, you will learn through a hands-on approach that emphasizes real-world applications and efficient project execution.\n  * **Donation Link** : <https://donate.stripe.com/4gw8xT9oVbCc98s7ss>\n  * **GitHub Repository** : <https://github.com/MLOps-Courses/mlops-coding-course>\n  * **MLOps Coding Assistant** : <https://mlops-coding-assistant.fmind.dev/>\n\n\n## [Chapter 0: Overview](https://mlops-coding-course.fmind.dev/0. Overview/)\nIn the introductory chapter, we introduce the foundational concepts of MLOps and outline the structure and goals of the course. This chapter sets the stage for what you will learn and provides an overview of the key components and skills you will develop as you progress through the course.\n## [Chapter 1: Initializing](https://mlops-coding-course.fmind.dev/1. Initializing/)\nThe first chapter focuses on the initialization phase, which is crucial for setting up a robust development environment. Here, we guide you through configuring tools and environments essential for Python-based MLOps projects. This foundational setup ensures a smooth workflow and minimizes delays related to technical setups.\n## [Chapter 2: Prototyping](https://mlops-coding-course.fmind.dev/2. Prototyping/)\nThis chapter dives into the prototyping phase, a critical stage where various approaches are tested to identify the most effective solutions, usually with notebooks. We cover essential tools and practices that improve the efficiency of this process, helping you understand the problem and experiment with different models before finalizing the project architecture.\n## [Chapter 3: Productionizing](https://mlops-coding-course.fmind.dev/3. Productionizing/)\nLearn to refine your Python codebase for better maintainability, scalability, and efficiency in Chapter 3. This includes transitioning from notebooks to structured Python packages, understanding different programming paradigms, and optimizing your development environment. These practices are crucial for enhancing code quality and collaboration.\n## [Chapter 4: Validating](https://mlops-coding-course.fmind.dev/4. Validating/)\nChapter 4 emphasizes the importance of code validation in the MLOps landscape. We explore key practices such as typing and debugging that ensure the robustness of ML pipelines, facilitate collaboration, and streamline deployment processes, all of which are essential for creating scalable and efficient systems.\n## [Chapter 5: Refining](https://mlops-coding-course.fmind.dev/5. Refining/)\nIn this chapter, we delve into refining MLOps projects to enhance their efficiency, reliability, and scalability. We discuss methodologies that streamline the development pipeline from code formulation to deployment, focusing on maintaining high code quality and automating repetitive tasks.\n## [Chapter 6: Sharing](https://mlops-coding-course.fmind.dev/6. Sharing/)\nThe chapter focuses on sharing and distributing MLOps projects. We explore tools and practices that enhance collaboration, promote reuse, and facilitate the scaling of machine learning solutions. You will learn how to effectively organize, document, and disseminate your projects to make them more accessible and beneficial to others.\n## [Chapter 7: Observability](https://mlops-coding-course.fmind.dev/7. Observability/)\nThis chapter dives into the essential aspects of observability in MLOps, equipping you with the knowledge and strategies to gain comprehensive insights into the performance, behavior, and health of your deployed models and infrastructure. You'll learn how to ensure reproducibility, implement monitoring and alerting systems, track data and model lineage, manage costs and KPIs, understand model explainability, and monitor infrastructure performance.\n## Let's journey together!\nWe are excited to have you join us on this journey to mastering MLOps. By the end of this course, you will be well-equipped to manage and execute ML projects with a high degree of professionalism and skill. Let\u2019s get started on transforming your data science capabilities with effective MLOps practices!\n[ Next  0. Overview  ](https://mlops-coding-course.fmind.dev/0.%20Overview/index.html)\nCopyright \u00a9 MLOps Courses \nMade with [ Material for MkDocs ](https://squidfunk.github.io/mkdocs-material/)\n[ ](https://github.com/MLOPS-Courses/mlops-coding-course \"github.com\") [ ](https://twitter.com/fmind_dev \"twitter.com\") [ ](https://www.linkedin.com/in/fmind-dev/ \"www.linkedin.com\")\n",
    "answer": "```markdown\n# TL;DR Summary of MLOps Coding Course\n\nThe MLOps Coding Course provides a hands-on approach to mastering MLOps, covering initialization, prototyping, productionizing, validating, refining, sharing, and observability of ML projects. Ideal for beginners and professionals, it emphasizes real-world applications and efficient project execution.\n```"
  },
  {
    "instruction": "[Skip to content](https://towardsdatascience.com/how-to-build-a-graph-rag-app-b323fc33ba06/#wp--skip-link--target)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nThe world\u2019s leading publication for data science, AI, and ML professionals.\nSign in\nSign out\n[Contributor Portal](https://contributor.insightmediagroup.io/)\n  * [Latest](https://towardsdatascience.com/latest/)\n  * [Editor\u2019s Picks](https://towardsdatascience.com/tag/editors-pick/)\n  * [Deep Dives](https://towardsdatascience.com/tag/deep-dives/)\n  * [Contribute](https://towardsdatascience.com/questions-96667b06af5/)\n  * [Newsletter](https://newsletter.towardsdatascience.com/subscription-to-the-newsletter)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\n\n\nToggle Mobile Navigation\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [X](https://x.com/TDataScience)\n\n\nToggle Search\nSearch\n# How to Build a Graph RAG App\nUsing knowledge graphs and AI to retrieve, filter, and summarize medical journal articles \n[Steve Hedden](https://towardsdatascience.com/author/steve-hedden/)\nDec 30, 2024\n30 min read\nShare \n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_fsYI8Riyq2lAsH8wTCuKhw.webp)Image by Author\n_The accompanying code for the app and notebook are_[ _here._](https://github.com/SteveHedden/kg_llm/tree/main/graphRAGapp)\nKnowledge graphs (KGs) and Large Language Models (LLMs) are a match made in heaven. My [previous](https://medium.com/towards-data-science/how-to-implement-knowledge-graphs-and-large-language-models-llms-together-at-the-enterprise-level-cf2835475c47) [posts](https://medium.com/towards-data-science/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759) discuss the complementarities of these two technologies in more detail but the short version is, \u201csome of the main weaknesses of LLMs, that they are black-box models and struggle with factual knowledge, are some of KGs\u2019 greatest strengths. KGs are, essentially, collections of facts, and they are fully interpretable.\u201d\nThis article is all about building a simple Graph RAG app. What is RAG? RAG, or Retrieval-Augmented Generation, is about **retrieving** relevant information to **augment** a prompt that is sent to an LLM, which **generates** a response. Graph RAG is RAG that uses a knowledge graph as part of the retrieval portion. If you\u2019ve never heard of Graph RAG, or want a refresher, I\u2019d watch [this video](https://www.youtube.com/watch?v=knDDGYHnnSI).\nThe basic idea is that, rather than sending your prompt directly to an LLM, which was not trained on your data, you can supplement your prompt with the relevant information needed for the LLM to answer your prompt accurately. The example I use often is copying a job description and my resume into ChatGPT to write a cover letter. The LLM is able to provide a much more relevant response to my prompt, \u2018write me a cover letter,\u2019 if I give it my resume and the description of the job I am applying for. Since knowledge graphs are built to store knowledge, they are a perfect way to store internal data and supplement LLM prompts with additional context, improving the accuracy and contextual understanding of the responses.\nThis technology has many, many, applications such [customer service bots](https://arxiv.org/pdf/2404.17723), [drug](https://academic.oup.com/bioinformatics/article/40/6/btae353/7687047) [discovery](https://blog.biostrand.ai/integrating-knowledge-graphs-and-large-language-models-for-next-generation-drug-discovery), [automated regulatory report generation in life sciences](https://www.weave.bio/), [talent acquisition and management for HR](https://beamery.com/resources/news/beamery-announces-talentgpt-the-world-s-first-generative-ai-for-hr), [legal research and writing](https://legal.thomsonreuters.com/blog/retrieval-augmented-generation-in-legal-tech/), and [wealth advisor assistants](https://www.cnbc.com/amp/2023/03/14/morgan-stanley-testing-openai-powered-chatbot-for-its-financial-advisors.html). Because of the wide applicability and the potential to improve the performance of LLM tools, Graph RAG (that\u2019s the term I\u2019ll use here) has been blowing up in popularity. Here is a graph showing interest over time based on Google searches.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_vuRkEjW9AcQPld5PsqWjkg.png)Source: <https://trends.google.com/>\nGraph RAG has experienced a surge in search interest, even surpassing terms like knowledge graphs and retrieval-augmented generation. Note that Google Trends measures  _relative_ search interest, not absolute number of searches. The spike in July 2024 for searches of Graph RAG coincides with the week Microsoft [announced](https://www.microsoft.com/en-us/research/blog/graphrag-new-tool-for-complex-data-discovery-now-on-github/) that their GraphRAG application would be available on [GitHub](https://github.com/microsoft/graphrag).\nThe excitement around Graph RAG is broader than just Microsoft, however. Samsung acquired RDFox, a knowledge graph company, in July of 2024. The [article announcing that acquisition](https://news.samsung.com/global/samsung-electronics-announces-acquisition-of-oxford-semantic-technologies-uk-based-knowledge-graph-startup) did not mention Graph RAG explicitly, but in [this article in Forbes](https://www.forbes.com/sites/zakdoffman/2024/11/09/samsung-confirms-new-upgrade-choice-millions-of-galaxy-owners-must-now-decide/) published in November 2024, a Samsung spokesperson stated, \u201cWe plan to develop knowledge graph technology, one of the main technologies of personalized AI, and organically connect with generated AI to support user-specific services.\u201d\nIn October 2024, Ontotext, a leading graph database company, and Semantic Web company, the maker of PoolParty, a knowledge graph curation platform, merged to form [Graphwise](https://graphwise.ai/). According to [the press release](https://www.prnewswire.com/news-releases/semantic-web-company-and-ontotext-merge-to-create-knowledge-graph-and-ai-powerhouse-graphwise-302283427.html?utm_source=chatgpt.com), the merger aims to \u201cdemocratize the evolution of Graph RAG as a category.\u201d\nWhile some of the buzz around Graph RAG may come from the broader excitement surrounding chatbots and generative AI, it reflects a genuine evolution in how knowledge graphs are being applied to solve complex, real-world problems. One example is that LinkedIn [applied Graph RAG](https://arxiv.org/pdf/2404.17723) to improve their customer service technical support. Because the tool was able to retrieve the relevant data (like previously solved similar tickets or questions) to feed the LLM, the responses were more accurate and the mean resolution time dropped from 40 hours to 15 hours.\nThis post will go through the construction of a pretty simple, but I think illustrative, example of how Graph RAG can work in practice. The end result is an app that a non-technical user can interact with. Like my last post, I will use a dataset consisting of medical journal articles from PubMed. The idea is that this is an app that someone in the medical field could use to do literature review. The same principles can be applied to many use cases however, which is why Graph RAG is so exciting.\nThe structure of the app, along with this post is as follows:\nStep zero is preparing the data. I will explain the details below but the overall goal is to vectorize the raw data and, separately, turn it into an RDF graph. As long as we keep URIs tied to the articles before we vectorize, we can navigate across a graph of articles and a vector space of articles. Then, we can:\n  1. **Search Articles:** use the power of the vector database to do an initial search of relevant articles given a search term. I will use vector similarity to retrieve articles with the most similar vectors to that of the search term.\n  2. **Refine Terms:** explore the [Medical Subject Headings (MeSH) biomedical vocabulary](https://id.nlm.nih.gov/mesh/) to select terms to use to filter the articles from step 1. This controlled vocabulary contains medical terms, alternative names, narrower concepts, and many other properties and relationships.\n  3. **Filter & Summarize: **use the MeSH terms to filter the articles to avoid \u2018context poisoning\u2019. Then send the remaining articles to an LLM along with an additional prompt like, \u201csummarize in bullets.\u201d\n\n\nSome notes on this app and tutorial before we get started:\n  * This set-up uses knowledge graphs exclusively for metadata. This is only possible because each article in my dataset has already been tagged with terms that are part of a rich controlled vocabulary. I am using the graph for structure and semantics and the vector database for similarity-based retrieval, ensuring each technology is used for what it does best. Vector similarity can tell us \u201cesophageal cancer\u201d is semantically similar to \u201cmouth cancer\u201d, but knowledge graphs can tell us the details of the relationship between \u201cesophageal cancer\u201d and \u201cmouth cancer.\u201d\n  * The data I used for this app is a collection of medical journal articles from PubMed (more on the data below). I chose this dataset because it is structured (tabular) but also contains text in the form of abstracts for each article, and because it is already tagged with topical terms that are aligned with a well-established controlled vocabulary (MeSH). Because these are medical articles, I have called this app \u2018Graph RAG for Medicine.\u2019 But this same structure can be applied to any domain and is not specific to the medical field.\n  * What I hope this tutorial and app demonstrate is that you can improve the results of your RAG application in terms of accuracy and explainability by incorporating a knowledge graph into the retrieval step. I will show how KGs can improve the accuracy of RAG applications in two ways: by giving the user a way of filtering the context to ensure the LLM is only being fed the most relevant information; and by using domain specific controlled vocabularies with dense relationships that are maintained and curated by domain experts to do the filtering.\n  * What this tutorial and app don\u2019t directly showcase are two other significant ways KGs can enhance RAG applications: governance, access control, and regulatory compliance; and efficiency and scalability. For governance, KGs can do more than filter content for relevancy to improve accuracy \u2014 they can enforce data governance policies. For instance, if a user lacks permission to access certain content, that content can be excluded from their RAG pipeline. On the efficiency and scalability side, KGs can help ensure RAG applications don\u2019t die on the shelf. While it\u2019s easy to create an impressive one-off RAG app (that\u2019s literally the purpose of this tutorial), many companies struggle with a proliferation of disconnected POCs that lack a cohesive framework, structure, or platform. That means many of those apps are not going to survive long. A metadata layer powered by KGs can break down data silos, providing the foundation needed to build, scale, and maintain RAG applications effectively. Using a rich controlled vocabulary like MeSH for the metadata tags on these articles is a way of ensuring this Graph RAG app can be integrated with other systems and reducing the risk that it becomes a silo.\n\n\n# Step 0: Prepare the data\n_The code to prepare the data is in_[ _this_](https://github.com/SteveHedden/kg_llm/blob/main/graphRAGapp/VectorVsKG_updated.ipynb) _notebook._\nAs mentioned, I\u2019ve again decided to use [this](https://www.kaggle.com/datasets/owaiskhan9654/pubmed-multilabel-text-classification) dataset of 50,000 research articles from the PubMed repository (License [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)). This dataset contains the title of the articles, their abstracts, as well as a field for metadata tags. These tags are from the Medical Subject Headings (MeSH) controlled vocabulary thesaurus. The PubMed articles are really just metadata on the articles \u2014 there are abstracts for each article but we don\u2019t have the full text. The data is already in tabular format and tagged with MeSH terms.\nWe can vectorize this tabular dataset directly. We could turn it into a graph (RDF) before we vectorize, but I didn\u2019t do that for this app and I don\u2019t know that it would help the final results for this kind of data. The most important thing about vectorizing the raw data is that we add [Unique Resource Identifiers](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier) (URIs) to each article first. A URI is a unique ID for navigating RDF data and it is necessary for us to go back and forth between vectors and entities in our graph. Additionally, we will create a separate collection in our vector database for the MeSH terms. This will allow the user to search for relevant terms without having prior knowledge of this controlled vocabulary. Below is a diagram of what we are doing to prepare our data.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_RiYNNyxGIEGClobHedTnuQ.png)Image by Author\nWe have two collections in our vector database to query: articles and terms. We also have the data represented as a graph in RDF format. Since MeSH has an API, I am just going to query the API directly to get alternative names and narrower concepts for terms.\n## Vectorize data in Weaviate\nFirst import the required packages and set up the Weaviate client:\n```\nimport weaviate<br>from weaviate.util import generate_uuid5<br>from weaviate.classes.init import Auth<br>import os<br>import json<br>import pandas as pd<br><br>client = weaviate.connect_to_weaviate_cloud(<br>  cluster_url=\"XXX\", # Replace with your Weaviate Cloud URL<br>  auth_credentials=Auth.api_key(\"XXX\"), # Replace with your Weaviate Cloud key<br>  headers={'X-OpenAI-Api-key': \"XXX\"} # Replace with your OpenAI API key<br>)\n```\n\nRead in the PubMed journal articles. I am using [Databricks](https://www.databricks.com/) to run this notebook so you may need to change this, depending on where you run it. The goal here is just to get the data into a pandas DataFrame.\n```\ndf = spark.sql(\"SELECT * FROM workspace.default.pub_med_multi_label_text_classification_dataset_processed\").toPandas()\n```\n\nIf you\u2019re running this locally, just do:\n```\ndf = pd.read_csv(\"PubMed Multi Label Text Classification Dataset Processed.csv\")\n```\n\nThen clean the data up a bit:\n```\nimport numpy as np<br># Replace infinity values with NaN and then fill NaN values<br>df.replace([np.inf, -np.inf], np.nan, inplace=True)<br>df.fillna('', inplace=True)<br><br># Convert columns to string type<br>df['Title'] = df['Title'].astype(str)<br>df['abstractText'] = df['abstractText'].astype(str)<br>df['meshMajor'] = df['meshMajor'].astype(str)\n```\n\nNow we need to create a URI for each article and add that in as a new column. This is important because the URI is the way we can connect the vector representation of an article with the knowledge graph representation of the article.\n```\nimport urllib.parse<br>from rdflib import Graph, RDF, RDFS, Namespace, URIRef, Literal<br><br><br># Function to create a valid URI<br>def create_valid_uri(base_uri, text):<br>  if pd.isna(text):<br>    return None<br>  # Encode text to be used in URI<br>  sanitized_text = urllib.parse.quote(text.strip().replace(' ', '_').replace('\"', '').replace('<', '').replace('>', '').replace(\"'\", \"_\"))<br>  return URIRef(f\"{base_uri}/{sanitized_text}\")<br><br><br># Function to create a valid URI for Articles<br>def create_article_uri(title, base_namespace=\"http://example.org/article/\"):<br>  \"\"\"<br>  Creates a URI for an article by replacing non-word characters with underscores and URL-encoding.<br><br>  Args:<br>    title (str): The title of the article.<br>    base_namespace (str): The base namespace for the article URI.<br><br>  Returns:<br>    URIRef: The formatted article URI.<br>  \"\"\"<br>  if pd.isna(title):<br>    return None<br>  # Replace non-word characters with underscores<br>  sanitized_title = re.sub(r'\\W+', '_', title.strip())<br>  # Condense multiple underscores into a single underscore<br>  sanitized_title = re.sub(r'_+', '_', sanitized_title)<br>  # URL-encode the term<br>  encoded_title = quote(sanitized_title)<br>  # Concatenate with base_namespace without adding underscores<br>  uri = f\"{base_namespace}{encoded_title}\"<br>  return URIRef(uri)<br><br># Add a new column to the DataFrame for the article URIs<br>df['Article_URI'] = df['Title'].apply(lambda title: create_valid_uri(\"http://example.org/article\", title))\n```\n\nWe also want to create a DataFrame of all of the MeSH terms that are used to tag the articles. This will be helpful later when we want to search for similar MeSH terms.\n```\n# Function to clean and parse MeSH terms<br>def parse_mesh_terms(mesh_list):<br>  if pd.isna(mesh_list):<br>    return []<br>  return [<br>    term.strip().replace(' ', '_')<br>    for term in mesh_list.strip(\"[]'\").split(',')<br>  ]<br><br># Function to create a valid URI for MeSH terms<br>def create_valid_uri(base_uri, text):<br>  if pd.isna(text):<br>    return None<br>  sanitized_text = urllib.parse.quote(<br>    text.strip()<br>    .replace(' ', '_')<br>    .replace('\"', '')<br>    .replace('<', '')<br>    .replace('>', '')<br>    .replace(\"'\", \"_\")<br>  )<br>  return f\"{base_uri}/{sanitized_text}\"<br><br># Extract and process all MeSH terms<br>all_mesh_terms = []<br>for mesh_list in df[\"meshMajor\"]:<br>  all_mesh_terms.extend(parse_mesh_terms(mesh_list))<br><br># Deduplicate terms<br>unique_mesh_terms = list(set(all_mesh_terms))<br><br># Create a DataFrame of MeSH terms and their URIs<br>mesh_df = pd.DataFrame({<br>  \"meshTerm\": unique_mesh_terms,<br>  \"URI\": [create_valid_uri(\"http://example.org/mesh\", term) for term in unique_mesh_terms]<br>})<br><br># Display the DataFrame<br>print(mesh_df)\n```\n\nVectorize the articles DataFrame:\n```\nfrom weaviate.classes.config import Configure<br><br><br>#define the collection<br>articles = client.collections.create(<br>  name = \"Article\",<br>  vectorizer_config=Configure.Vectorizer.text2vec_openai(), # If set to \"none\" you must always provide vectors yourself. Could be any other \"text2vec-*\" also.<br>  generative_config=Configure.Generative.openai(), # Ensure the `generative-openai` module is used for generative queries<br>)<br><br>#add ojects<br>articles = client.collections.get(\"Article\")<br><br>with articles.batch.dynamic() as batch:<br>  for index, row in df.iterrows():<br>    batch.add_object({<br>      \"title\": row[\"Title\"],<br>      \"abstractText\": row[\"abstractText\"],<br>      \"Article_URI\": row[\"Article_URI\"],<br>      \"meshMajor\": row[\"meshMajor\"],<br>    })\n```\n\nNow vectorize the MeSH terms:\n```\n#define the collection<br>terms = client.collections.create(<br>  name = \"term\",<br>  vectorizer_config=Configure.Vectorizer.text2vec_openai(), # If set to \"none\" you must always provide vectors yourself. Could be any other \"text2vec-*\" also.<br>  generative_config=Configure.Generative.openai(), # Ensure the `generative-openai` module is used for generative queries<br>)<br><br>#add ojects<br>terms = client.collections.get(\"term\")<br><br>with terms.batch.dynamic() as batch:<br>  for index, row in mesh_df.iterrows():<br>    batch.add_object({<br>      \"meshTerm\": row[\"meshTerm\"],<br>      \"URI\": row[\"URI\"],<br>    })\n```\n\nYou can, at this point, run semantic search, similarity search, and RAG directly against the vectorized dataset. I won\u2019t go through all of that here but you can look at the code in my [accompanying notebook](https://github.com/SteveHedden/kg_llm/tree/main/graphRAGapp) to do that.\n## Turn data into a knowledge graph\nI am just using the same code we used in the [last post](https://medium.com/towards-data-science/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759) to do this. We are basically turning every row in the data into an \u201cArticle\u201d entity in our KG. Then we are giving each of these articles properties for title, abstract, and MeSH terms. We are also turning every MeSH term into an entity as well. This code also adds random dates to each article for a property called date published and a random number between 1 and 10 to a property called access. We won\u2019t use those properties in this demo. Below is a visual representation of the graph we are creating from the data.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_R1NIm9fLYdKFTsXzZmH33w.png)\nHere is how to iterate through the DataFrame and turn it into RDF data:\n```\nfrom rdflib import Graph, RDF, RDFS, Namespace, URIRef, Literal<br>from rdflib.namespace import SKOS, XSD<br>import pandas as pd<br>import urllib.parse<br>import random<br>from datetime import datetime, timedelta<br>import re<br>from urllib.parse import quote<br><br># --- Initialization ---<br>g = Graph()<br><br># Define namespaces<br>schema = Namespace('http://schema.org/')<br>ex = Namespace('http://example.org/')<br>prefixes = {<br>  'schema': schema,<br>  'ex': ex,<br>  'skos': SKOS,<br>  'xsd': XSD<br>}<br>for p, ns in prefixes.items():<br>  g.bind(p, ns)<br><br># Define classes and properties<br>Article = URIRef(ex.Article)<br>MeSHTerm = URIRef(ex.MeSHTerm)<br>g.add((Article, RDF.type, RDFS.Class))<br>g.add((MeSHTerm, RDF.type, RDFS.Class))<br><br>title = URIRef(schema.name)<br>abstract = URIRef(schema.description)<br>date_published = URIRef(schema.datePublished)<br>access = URIRef(ex.access)<br><br>g.add((title, RDF.type, RDF.Property))<br>g.add((abstract, RDF.type, RDF.Property))<br>g.add((date_published, RDF.type, RDF.Property))<br>g.add((access, RDF.type, RDF.Property))<br><br># Function to clean and parse MeSH terms<br>def parse_mesh_terms(mesh_list):<br>  if pd.isna(mesh_list):<br>    return []<br>  return [term.strip() for term in mesh_list.strip(\"[]'\").split(',')]<br><br># Enhanced convert_to_uri function<br>def convert_to_uri(term, base_namespace=\"http://example.org/mesh/\"):<br>  \"\"\"<br>  Converts a MeSH term into a standardized URI by replacing spaces and special characters with underscores,<br>  ensuring it starts and ends with a single underscore, and URL-encoding the term.<br><br>  Args:<br>    term (str): The MeSH term to convert.<br>    base_namespace (str): The base namespace for the URI.<br><br>  Returns:<br>    URIRef: The formatted URI.<br>  \"\"\"<br>  if pd.isna(term):<br>    return None # Handle NaN or None terms gracefully<br>  <br>  # Step 1: Strip existing leading and trailing non-word characters (including underscores)<br>  stripped_term = re.sub(r'^\\W+|\\W+$', '', term)<br>  <br>  # Step 2: Replace non-word characters with underscores (one or more)<br>  formatted_term = re.sub(r'\\W+', '_', stripped_term)<br>  <br>  # Step 3: Replace multiple consecutive underscores with a single underscore<br>  formatted_term = re.sub(r'_+', '_', formatted_term)<br>  <br>  # Step 4: URL-encode the term to handle any remaining special characters<br>  encoded_term = quote(formatted_term)<br>  <br>  # Step 5: Add single leading and trailing underscores<br>  term_with_underscores = f\"_{encoded_term}_\"<br>  <br>  # Step 6: Concatenate with base_namespace without adding an extra underscore<br>  uri = f\"{base_namespace}{term_with_underscores}\"<br><br>  return URIRef(uri)<br><br># Function to generate a random date within the last 5 years<br>def generate_random_date():<br>  start_date = datetime.now() - timedelta(days=5*365)<br>  random_days = random.randint(0, 5*365)<br>  return start_date + timedelta(days=random_days)<br><br># Function to generate a random access value between 1 and 10<br>def generate_random_access():<br>  return random.randint(1, 10)<br><br># Function to create a valid URI for Articles<br>def create_article_uri(title, base_namespace=\"http://example.org/article\"):<br>  \"\"\"<br>  Creates a URI for an article by replacing non-word characters with underscores and URL-encoding.<br><br>  Args:<br>    title (str): The title of the article.<br>    base_namespace (str): The base namespace for the article URI.<br><br>  Returns:<br>    URIRef: The formatted article URI.<br>  \"\"\"<br>  if pd.isna(title):<br>    return None<br>  # Encode text to be used in URI<br>  sanitized_text = urllib.parse.quote(title.strip().replace(' ', '_').replace('\"', '').replace('<', '').replace('>', '').replace(\"'\", \"_\"))<br>  return URIRef(f\"{base_namespace}/{sanitized_text}\")<br><br># Loop through each row in the DataFrame and create RDF triples<br>for index, row in df.iterrows():<br>  article_uri = create_article_uri(row['Title'])<br>  if article_uri is None:<br>    continue<br>  <br>  # Add Article instance<br>  g.add((article_uri, RDF.type, Article))<br>  g.add((article_uri, title, Literal(row['Title'], datatype=XSD.string)))<br>  g.add((article_uri, abstract, Literal(row['abstractText'], datatype=XSD.string)))<br>  <br>  # Add random datePublished and access<br>  random_date = generate_random_date()<br>  random_access = generate_random_access()<br>  g.add((article_uri, date_published, Literal(random_date.date(), datatype=XSD.date)))<br>  g.add((article_uri, access, Literal(random_access, datatype=XSD.integer)))<br>  <br>  # Add MeSH Terms<br>  mesh_terms = parse_mesh_terms(row['meshMajor'])<br>  for term in mesh_terms:<br>    term_uri = convert_to_uri(term, base_namespace=\"http://example.org/mesh/\")<br>    if term_uri is None:<br>      continue<br>    <br>    # Add MeSH Term instance<br>    g.add((term_uri, RDF.type, MeSHTerm))<br>    g.add((term_uri, RDFS.label, Literal(term.replace('_', ' '), datatype=XSD.string)))<br>    <br>    # Link Article to MeSH Term<br>    g.add((article_uri, schema.about, term_uri))<br><br># Path to save the file<br>file_path = \"/Workspace/PubMedGraph.ttl\"<br><br># Save the file<br>g.serialize(destination=file_path, format='turtle')<br><br>print(f\"File saved at {file_path}\")\n```\n\nOK, so now we have a vectorized version of the data, and a graph (RDF) version of the data. Each vector has a URI associated with it, which corresponds to an entity in the KG, so we can go back and forth between the data formats.\n# Build an app\nI decided to use [Streamlit](https://streamlit.io/) to build the interface for this graph RAG app. Similar to the last blog post, I have kept the user flow the same.\n  1. **Search Articles:** First, the user searches for articles using a search term. This relies exclusively on the vector database. The user\u2019s search term(s) is sent to the vector database and the ten articles nearest the term in vector space are returned.\n  2. **Refine Terms:** Second, the user decides the MeSH terms to use to filter the returned results. Since we also vectorized the MeSH terms, we can have the user enter a natural language prompt to get the most relevant MeSH terms. Then, we allow the user to expand these terms to see their alternative names and narrower concepts. The user can select as many terms as they want for their filter criteria.\n  3. **Filter & Summarize: **Third, the user applies the selected terms as filters to the original ten journal articles. We can do this since the PubMed articles are tagged with MeSH terms. Finally, we let the user enter an additional prompt to send to the LLM along with the filtered journal articles. This is the generative step of the RAG app.\n\n\nLet\u2019s go through these steps one at a time. You can see the full app and code on my GitHub, but here is the structure:\n```\n-- app.py (a python file that drives the app and calls other functions as needed)<br>-- query_functions (a folder containing python files with queries)<br> -- rdf_queries.py (python file with RDF queries)<br> -- weaviate_queries.py (python file containing weaviate queries)<br>-- PubMedGraph.ttl (the pubmed data in RDF format, stored as a ttl file)\n```\n\n## Search Articles\nFirst, want to do is implement Weaviate\u2019s [vector similarity search](https://weaviate.io/developers/weaviate/search/similarity). Since our articles are vectorized, we can send a search term to the vector database and get similar articles back.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_SkbzqnwLMiDoxavXhSoxyg.png)Image by Author\nThe main function that searches for relevant journal articles in the vector database is in app.py:\n```\n# --- TAB 1: Search Articles ---<br>with tab_search:<br>  st.header(\"Search Articles (Vector Query)\")<br>  query_text = st.text_input(\"Enter your vector search term (e.g., Mouth Neoplasms):\", key=\"vector_search\")<br><br>  if st.button(\"Search Articles\", key=\"search_articles_btn\"):<br>    try:<br>      client = initialize_weaviate_client()<br>      article_results = query_weaviate_articles(client, query_text)<br><br>      # Extract URIs here<br>      article_uris = [<br>        result[\"properties\"].get(\"article_URI\")<br>        for result in article_results<br>        if result[\"properties\"].get(\"article_URI\")<br>      ]<br><br>      # Store article_uris in the session state<br>      st.session_state.article_uris = article_uris<br><br>      st.session_state.article_results = [<br>        {<br>          \"Title\": result[\"properties\"].get(\"title\", \"N/A\"),<br>          \"Abstract\": (result[\"properties\"].get(\"abstractText\", \"N/A\")[:100] + \"...\"),<br>          \"Distance\": result[\"distance\"],<br>          \"MeSH Terms\": \", \".join(<br>            ast.literal_eval(result[\"properties\"].get(\"meshMajor\", \"[]\"))<br>            if result[\"properties\"].get(\"meshMajor\") else []<br>          ),<br><br>        }<br>        for result in article_results<br>      ]<br>      client.close()<br>    except Exception as e:<br>      st.error(f\"Error during article search: {e}\")<br><br>  if st.session_state.article_results:<br>    st.write(\"**Search Results for Articles:**\")<br>    st.table(st.session_state.article_results)<br>  else:<br>    st.write(\"No articles found yet.\")\n```\n\nThis function uses the queries stored in weaviate_queries to establish the Weaviate client (initialize_weaviate_client) and search for articles (query_weaviate_articles). Then we display the returned articles in a table, along with their abstracts, distance (how close they are to the search term), and the MeSH terms that they are tagged with.\nThe function to query Weaviate in weaviate_queries.py looks like this:\n```\n# Function to query Weaviate for Articles<br>def query_weaviate_articles(client, query_text, limit=10):<br>  # Perform vector search on Article collection<br>  response = client.collections.get(\"Article\").query.near_text(<br>    query=query_text,<br>    limit=limit,<br>    return_metadata=MetadataQuery(distance=True)<br>  )<br><br>  # Parse response<br>  results = []<br>  for obj in response.objects:<br>    results.append({<br>      \"uuid\": obj.uuid,<br>      \"properties\": obj.properties,<br>      \"distance\": obj.metadata.distance,<br>    })<br>  return results\n```\n\nAs you can see, I put a limit of ten results here just to make it simpler, but you can change that. This is just using vector similarity search in Weaviate to return relevant results.\nThe end result in the app looks like this:\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_SghQV-iph3ZQftZiNC6fNg.png)Image by Author\nAs a demo, I will search the term \u201ctreatments for mouth cancer\u201d. As you can see, 10 articles are returned, mostly relevant. This demonstrates both the strengths and weaknesses of vector based retrieval.\nThe strength is that we can build a semantic search functionality on our data with minimal effort. As you can see above, all we did was set up the client and send the data to a vector database. Once our data has been vectorized, we can do semantic searches, similarity searches, and even RAG. I have put some of that in the notebook accompanying this post, but there\u2019s a lot more in Weaviate\u2019s [official docs](https://weaviate.io/developers/weaviate).\nThe weakness of vector based retrieval, as I mentioned above are that they are black-box and struggle with factual knowledge. In our example, it looks like most of the articles are about some kind of treatment or therapy for some kind of cancer. Some of the articles are about mouth cancer specifically, some are about a sub-type of mouth cancer like gingival cancer (cancer of the gums), and palatal cancer (cancer of the palate). But there are also articles about nasopharyngeal cancer (cancer of the upper throat), mandibular cancer (cancer of the jaw), and esophageal cancer (cancer of the esophagus). None of these (upper throat, jaw, or esophagus) are considered mouth cancer. It is understandable why an article about a specific cancer radiation therapy for nasopharyngeal neoplasms would be considered similar to the prompt \u201ctreatments for mouth cancer\u201d but it may not be relevant if you are only looking for treatments for mouth cancer. If we were to plug these ten articles directly into our prompt to the LLM and ask it to \u201csummarize the different treatment options,\u201d we would be getting incorrect information.\nThe purpose of RAG is to give an LLM a very specific set of additional information to better answer your question \u2014 if that information is incorrect or irrelevant, it can lead to misleading responses from the LLM. This is often referred to as \u201ccontext poisoning\u201d. What is especially dangerous about context poisoning is that the response isn\u2019t necessarily factually inaccurate (the LLM may accurately summarize the treatment options we feed it), and it isn\u2019t necessarily based on an inaccurate piece of data (presumably the journal articles themselves are accurate), it\u2019s just using the wrong data to answer your question. In this example, the user could be reading about how to treat the wrong kind of cancer, which seems very bad.\n## Refine Terms\nKGs can help improve the accuracy of responses and reduce the likelihood of context poisoning by refining the results from the vector database. The next step is for selecting what MeSH terms we want to use to filter the articles. First, we do another vector similarity search against the vector database but on the Terms collection. This is because the user may not be familiar with the MeSH controlled vocabulary. In our example above, I searched for, \u201ctherapies for mouth cancer\u201d, but \u201cmouth cancer\u201d is not a term in MeSH \u2014 they use \u201cMouth Neoplasms\u201d. We want the user to be able to start exploring the MeSH terms without having a prior understanding of them \u2014 this is good practice regardless of the metadata used to tag the content.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_38L-k83P82FOmNwCnKhPdQ.png)Image by Author\nThe function to get relevant MeSH terms is nearly identical to the previous Weaviate query. Just replace Article with term:\n```\n# Function to query Weaviate for MeSH Terms<br>def query_weaviate_terms(client, query_text, limit=10):<br>  # Perform vector search on MeshTerm collection<br>  response = client.collections.get(\"term\").query.near_text(<br>    query=query_text,<br>    limit=limit,<br>    return_metadata=MetadataQuery(distance=True)<br>  )<br><br>  # Parse response<br>  results = []<br>  for obj in response.objects:<br>    results.append({<br>      \"uuid\": obj.uuid,<br>      \"properties\": obj.properties,<br>      \"distance\": obj.metadata.distance,<br>    })<br>  return results\n```\n\nHere is what it looks like in the app:\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_OI1hVbBIMyHTEZy7Oy4FGQ.png)Image by Author\nAs you can see, I searched for \u201cmouth cancer\u201d and the most similar terms were returned. Mouth cancer was not returned, as that is not a term in MeSH, but Mouth Neoplasms is on the list.\nThe next step is to allow the user to expand the returned terms to see alternative names and narrower concepts. This requires querying the [MeSH API](https://id.nlm.nih.gov/mesh/). This was the trickiest part of this app for a number of reasons. The biggest problem is that Streamlit requires that everything has a unique ID but MeSH terms can repeat \u2014 if one of the returned concepts is a child of another, then when you expand the parent you will have a duplicate of the child. I think I took care of most of the big issues and the app should work, but there are probably bugs to find at this stage.\nThe functions we rely on are found in rdf_queries.py. We need one to get the alternative names for a term:\n```\n# Fetch alternative names and triples for a MeSH term<br>def get_concept_triples_for_term(term):<br>  term = sanitize_term(term) # Sanitize input term<br>  sparql = SPARQLWrapper(\"https://id.nlm.nih.gov/mesh/sparql\")<br>  query = f\"\"\"<br>  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#><br>  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#><br>  PREFIX meshv: <http://id.nlm.nih.gov/mesh/vocab#><br>  PREFIX mesh: <http://id.nlm.nih.gov/mesh/><br><br>  SELECT ?subject ?p ?pLabel ?o ?oLabel<br>  FROM <http://id.nlm.nih.gov/mesh><br>  WHERE {{<br>    ?subject rdfs:label \"{term}\"@en .<br>    ?subject ?p ?o .<br>    FILTER(CONTAINS(STR(?p), \"concept\"))<br>    OPTIONAL {{ ?p rdfs:label ?pLabel . }}<br>    OPTIONAL {{ ?o rdfs:label ?oLabel . }}<br>  }}<br>  \"\"\"<br>  try:<br>    sparql.setQuery(query)<br>    sparql.setReturnFormat(JSON)<br>    results = sparql.query().convert()<br><br>    triples = set()<br>    for result in results[\"results\"][\"bindings\"]:<br>      obj_label = result.get(\"oLabel\", {}).get(\"value\", \"No label\")<br>      triples.add(sanitize_term(obj_label)) # Sanitize term before adding<br><br>    # Add the sanitized term itself to ensure it's included<br>    triples.add(sanitize_term(term))<br>    return list(triples)<br><br>  except Exception as e:<br>    print(f\"Error fetching concept triples for term '{term}': {e}\")<br>    return []\n```\n\nWe also need functions to get the narrower (child) concepts for a given term. I have two functions that achieve this \u2014 one that gets the immediate children of a term and one recursive function that returns all children of a given depth.\n```\n# Fetch narrower concepts for a MeSH term<br>def get_narrower_concepts_for_term(term):<br>  term = sanitize_term(term) # Sanitize input term<br>  sparql = SPARQLWrapper(\"https://id.nlm.nih.gov/mesh/sparql\")<br>  query = f\"\"\"<br>  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#><br>  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#><br>  PREFIX meshv: <http://id.nlm.nih.gov/mesh/vocab#><br>  PREFIX mesh: <http://id.nlm.nih.gov/mesh/><br><br>  SELECT ?narrowerConcept ?narrowerConceptLabel<br>  WHERE {{<br>    ?broaderConcept rdfs:label \"{term}\"@en .<br>    ?narrowerConcept meshv:broaderDescriptor ?broaderConcept .<br>    ?narrowerConcept rdfs:label ?narrowerConceptLabel .<br>  }}<br>  \"\"\"<br>  try:<br>    sparql.setQuery(query)<br>    sparql.setReturnFormat(JSON)<br>    results = sparql.query().convert()<br><br>    concepts = set()<br>    for result in results[\"results\"][\"bindings\"]:<br>      subject_label = result.get(\"narrowerConceptLabel\", {}).get(\"value\", \"No label\")<br>      concepts.add(sanitize_term(subject_label)) # Sanitize term before adding<br><br>    return list(concepts)<br><br>  except Exception as e:<br>    print(f\"Error fetching narrower concepts for term '{term}': {e}\")<br>    return []<br><br># Recursive function to fetch narrower concepts to a given depth<br>def get_all_narrower_concepts(term, depth=2, current_depth=1):<br>  term = sanitize_term(term) # Sanitize input term<br>  all_concepts = {}<br>  try:<br>    narrower_concepts = get_narrower_concepts_for_term(term)<br>    all_concepts[sanitize_term(term)] = narrower_concepts<br><br>    if current_depth < depth:<br>      for concept in narrower_concepts:<br>        child_concepts = get_all_narrower_concepts(concept, depth, current_depth + 1)<br>        all_concepts.update(child_concepts)<br><br>  except Exception as e:<br>    print(f\"Error fetching all narrower concepts for term '{term}': {e}\")<br><br>  return all_concepts\n```\n\nThe other important part of step 2 is to allow the user to select terms to add to a list of \u201cSelected Terms\u201d. These will appear in the sidebar on the left of the screen. There are a lot of things that can improve this step like:\n  * There is no way to clear all but you can clear the cache or refresh the browser if needed.\n  * There is no way to \u2018select all narrower concepts\u2019 which would be helpful.\n  * There is no option to add rules for filtering. Right now, we are just assuming that the article must contain term A OR term B OR term C etc. The rankings at the end are based on the number of terms the articles are tagged with.\n\n\nHere is what it looks like in the app:\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_PHOQxQnPdDCiISTrbIeCHA.png)Image by Author\nI can expand Mouth Neoplasms to see the alternative names, in this case, \u201cCancer of Mouth\u201d, along with all of the narrower concepts. As you can see, most of the narrower concepts have their own children, which you can expand as well. For the purposes of this demo, I am going to select all children of Mouth Neoplasms.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_1Nodj6EQ51R8GPuliWqONQ.png)Image by Author\nThis step is important not just because it allows the user to filter the search results, but also because it is a way for the user to explore the MeSH graph itself and learn from it. For example, this would be the place for the user to learn that nasopharyngeal neoplasms are not a subset of mouth neoplasms.\n## Filter & Summarize\nNow that you\u2019ve got your articles and your filter terms, you can apply the filter and summarize the results. This is where we bring the original 10 articles returned in step one together with the refined list of MeSH terms. We allow the user to add additional context to the prompt before sending it to the LLM.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_fIT2zmJGryYgpUT6iZr01A.png)Image by Author\nThe way we do this filtering is that we need to get the URIs for the 10 articles from the original search. Then we can query our knowledge graph for which of those articles have been tagged with the associated MeSH terms. Additionally, we save the abstracts of these articles for use in the next step. This would be the place where we could filter based on access control or other user-controlled parameters like author, filetype, date published, etc. I didn\u2019t include any of that in this app but I did add in properties for access control and date published in case we want to add that in this UI later.\nHere is what the code looks like in app.py:\n```\n    if st.button(\"Filter Articles\"):<br>      try:<br>        # Check if we have URIs from tab 1<br>        if \"article_uris\" in st.session_state and st.session_state.article_uris:<br>          article_uris = st.session_state.article_uris<br><br>          # Convert list of URIs into a string for the VALUES clause or FILTER<br>          article_uris_string = \", \".join([f\"<{str(uri)}>\" for uri in article_uris])<br><br>          SPARQL_QUERY = \"\"\"<br>          PREFIX schema: <http://schema.org/><br>          PREFIX ex: <http://example.org/><br><br>          SELECT ?article ?title ?abstract ?datePublished ?access ?meshTerm<br>          WHERE {{<br>           ?article a ex:Article ;<br>                schema:name ?title ;<br>                schema:description ?abstract ;<br>                schema:datePublished ?datePublished ;<br>                ex:access ?access ;<br>                schema:about ?meshTerm .<br><br>           ?meshTerm a ex:MeSHTerm .<br><br>           FILTER (?article IN ({article_uris}))<br>          }}<br>          \"\"\"<br>          # Insert the article URIs into the query<br>          query = SPARQL_QUERY.format(article_uris=article_uris_string)<br>        else:<br>          st.write(\"No articles selected from Tab 1.\")<br>          st.stop()<br><br>        # Query the RDF and save results in session state<br>        top_articles = query_rdf(LOCAL_FILE_PATH, query, final_terms)<br>        st.session_state.filtered_articles = top_articles<br><br>        if top_articles:<br><br>          # Combine abstracts from top articles and save in session state<br>          def combine_abstracts(ranked_articles):<br>            combined_text = \" \".join(<br>              [f\"Title: {data['title']} Abstract: {data['abstract']}\" for article_uri, data in<br>               ranked_articles]<br>            )<br>            return combined_text<br><br><br>          st.session_state.combined_text = combine_abstracts(top_articles)<br><br>        else:<br>          st.write(\"No articles found for the selected terms.\")<br>      except Exception as e:<br>        st.error(f\"Error filtering articles: {e}\")\n```\n\nThis uses the function query_rdf in the rdf_queries.py file. That function looks like this:\n```\n# Function to query RDF using SPARQL<br>def query_rdf(local_file_path, query, mesh_terms, base_namespace=\"http://example.org/mesh/\"):<br>  if not mesh_terms:<br>    raise ValueError(\"The list of MeSH terms is empty or invalid.\")<br><br>  print(\"SPARQL Query:\", query)<br><br>  # Create and parse the RDF graph<br>  g = Graph()<br>  g.parse(local_file_path, format=\"ttl\")<br><br>  article_data = {}<br><br>  for term in mesh_terms:<br>    # Convert the term to a valid URI<br>    mesh_term_uri = convert_to_uri(term, base_namespace)<br>    #print(\"Term:\", term, \"URI:\", mesh_term_uri)<br><br>    # Perform SPARQL query with initBindings<br>    results = g.query(query, initBindings={'meshTerm': mesh_term_uri})<br><br>    for row in results:<br>      article_uri = row['article']<br>      if article_uri not in article_data:<br>        article_data[article_uri] = {<br>          'title': row['title'],<br>          'abstract': row['abstract'],<br>          'datePublished': row['datePublished'],<br>          'access': row['access'],<br>          'meshTerms': set()<br>        }<br>      article_data[article_uri]['meshTerms'].add(str(row['meshTerm']))<br>    #print(\"DEBUG article_data:\", article_data)<br><br>  # Rank articles by the number of matching MeSH terms<br>  ranked_articles = sorted(<br>    article_data.items(),<br>    key=lambda item: len(item[1]['meshTerms']),<br>    reverse=True<br>  )<br>  return ranked_articles[:10]\n```\n\nAs you can see, this function also converts the MeSH terms to URIs so we can filter using the graph. Be careful in the way you convert terms to URIs and ensure it aligns with the other functions.\nHere is what it looks like in the app:\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_lAAPdTjfFOzD1DM3QUofAA.png)Image by Author\nAs you can see, the two MeSH terms we selected from the previous step are here. If I click \u201cFilter Articles,\u201d it will filter the original 10 articles using our filter criteria in step 2. The articles will be returned with their full abstracts, along with their tagged MeSH terms (see image below).\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_wUsJ9sIQdMcro6ximwPxFg.png)Image by Author\nThere are 5 articles returned. Two are tagged with \u201cmouth neoplasms,\u201d one with \u201cgingival neoplasms,\u201d and two with \u201cpalatal neoplasms\u201d.\nNow that we have a refined list of articles we want to use to generate a response, we can move to the final step. We want to send these articles to an LLM to generate a response but we can also add in additional context to the prompt. I have a default prompt that says, \u201cSummarize the key information here in bullet points. Make it understandable to someone without a medical degree.\u201d For this demo, I am going to adjust the prompt to reflect our original search term:\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_zu6wCN1c8f4AqI3Gzfq5tA.png)\nThe results are as follows:\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_3efzwyaNjWsWFizCwhCb0g.png)\nThe results look better to me, mostly because I know that the articles we are summarizing are, presumably, about treatments for mouth cancer. The dataset doesn\u2019t contain the actual journal articles, just the abstracts. So these results are just summaries of summaries. There may be some value to this, but if we were building a real app and not just a demo, this is the step where we could incorporate the full text of the articles. Alternatively, this is where the user/researcher would go read these articles themselves, rather than relying exclusively on the LLM for the summaries.\n# Conclusion\nThis tutorial demonstrates how combining vector databases and knowledge graphs can significantly enhance RAG applications. By leveraging vector similarity for initial searches and structured knowledge graph metadata for filtering and organization, we can build a system that delivers accurate, explainable, and domain-specific results. The integration of MeSH, a well-established controlled vocabulary, highlights the power of domain expertise in curating metadata, which ensures that the retrieval step aligns with the unique needs of the application while maintaining interoperability with other systems. This approach is not limited to medicine \u2014 its principles can be applied across domains wherever structured data and textual information coexist.\nThis tutorial underscores the importance of leveraging each technology for what it does best. Vector databases excel at similarity-based retrieval, while knowledge graphs shine in providing context, structure, and semantics. Additionally, scaling RAG applications demands a metadata layer to break down data silos and enforce governance policies. Thoughtful design, rooted in domain-specific metadata and robust governance, is the path to building RAG systems that are not only accurate but also scalable.\nWritten By\nSteve Hedden\n[See all from Steve Hedden](https://towardsdatascience.com/author/steve-hedden/)\nTopics:\nShare this article:\n  * [ Share on Facebook  ](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-graph-rag-app-b323fc33ba06%2F&title=How%20to%20Build%20a%20Graph%20RAG%20App)\n  * [ Share on LinkedIn  ](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-graph-rag-app-b323fc33ba06%2F&title=How%20to%20Build%20a%20Graph%20RAG%20App)\n  * [ Share on X  ](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-graph-rag-app-b323fc33ba06%2F&text=How%20to%20Build%20a%20Graph%20RAG%20App)\n\n\n## Related Articles\n  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/Hybrid-Architecture_01-snapshot.png)\n## [The Art of Hybrid Architectures](https://towardsdatascience.com/the-art-of-hybrid-architectures/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nCombining CNNs and Transformers to Elevate Fine-Grained Visual Classification \n[Eric Chung](https://towardsdatascience.com/author/eric-chung/)\nMarch 28, 2025\n32 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/ai-generated-city-banner-integration.png)\n## [A Little More Conversation, A Little Less Action \u2014 A Case Against Premature Data Integration](https://towardsdatascience.com/a-little-more-conversation-a-little-less-action-a-case-against-premature-data-integration/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nRunning a large data integration project before embarking on the ML part is easily a\u2026 \n[Daniel Bakkelund](https://towardsdatascience.com/author/danielbakkelund/)\nMarch 28, 2025\n14 min read\n  * ![3D Reconstruction cover image](https://towardsdatascience.com/wp-content/uploads/2025/03/cover-3d-reconstruction.jpg)\n## [Master the 3D Reconstruction Process: A Step-by-Step Guide](https://towardsdatascience.com/master-the-3d-reconstruction-process-step-by-step-guide/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nLearn the complete 3D reconstruction pipeline from feature extraction to dense matching. Master photogrammetry with\u2026 \n[Florent Poux, Ph.D.](https://towardsdatascience.com/author/florentpoux/)\nMarch 28, 2025\n17 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/nik-n1ccr-zVG68-unsplash-scaled-1.jpg)\n## [AI Agents from Zero to Hero \u2014 Part 3](https://towardsdatascience.com/ai-agents-from-zero-to-hero-part-3/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nBuild from scratch using only Ollama (no GPU, no APIKEY) \n[Mauro Di Pietro](https://towardsdatascience.com/author/maurodp/)\nMarch 28, 2025\n14 min read\n  * ## [From Physics to Probability: Hamiltonian Mechanics for Generative Modeling and MCMC](https://towardsdatascience.com/from-physics-to-probability-hamiltonian-mechanics-for-generative-modeling-and-mcmc/)\n[ Math ](https://towardsdatascience.com/category/math/)\nHamiltonian mechanics is a way to describe how physical systems, like planets or pendulums, move\u2026 \n[Soran Ghaderi](https://towardsdatascience.com/author/soran-ghaderi/)\nMarch 28, 2025\n17 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/Screenshot-2025-03-26-at-10.54.07\u202fPM.png)\n## [How to Format Your TDS Draft: A Quick(ish) Guide](https://towardsdatascience.com/how-to-format-your-tds-draft-a-quickish-guide/)\n[ Writing ](https://towardsdatascience.com/category/writing/)\nEverything you need to know about creating a draft on our Contributor Portal \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nMarch 28, 2025\n12 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/image1-1024x576-1.png)\n## [Data Science: From School to Work, Part III](https://towardsdatascience.com/data-science-from-school-to-work-part-iii/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nGood practices for Python error handling and logging \n[Vincent Margot](https://towardsdatascience.com/author/vmargot/)\nMarch 27, 2025\n12 min read\n  * ![A colorful, hand-drawn, cartoon-style illustration set against a vibrant blue background. In the center stands a cute, vintage-style robot with a white body and red accents \\(shoulders, hands, waist, and feet\\). Above the robot's head are three speech bubbles. The left bubble is orange and contains the text \"EN\" in bold black letters, representing English. The center bubble is white with the Chinese character \"\u4e2d\" \\(for Chinese\\), and the right bubble is yellow with the Japanese character \"\u65e5\" \\(for Japanese\\). The robot appears to be choosing or processing languages, symbolizing multilingual communication or translation.](https://towardsdatascience.com/wp-content/uploads/2025/03/article-cover-robot-ratio.png)\n## [Japanese-Chinese Translation with GenAI: What Works and What Doesn\u2019t](https://towardsdatascience.com/japanese-chinese-translation-with-genai-what-works-and-what-doesnt/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nEvaluating GenAI in Japanese-Chinese translation: current limits and opportunities \n[Qian (Alex) Wan](https://towardsdatascience.com/author/qianalex-wan/)\nMarch 27, 2025\n20 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/featured-image.png)\n## [Talk to Videos](https://towardsdatascience.com/talk-to-videos/)\n[ Large Language Models ](https://towardsdatascience.com/category/artificial-intelligence/large-language-models/)\nDeveloping an interactive AI application for video-based learning in education and business \n[Umair Ali Khan](https://towardsdatascience.com/author/umairali-khan/)\nMarch 27, 2025\n28 min read\n\n\n  * [YouTube](https://www.youtube.com/c/TowardsDataScience)\n  * [X](https://x.com/TDataScience)\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [Threads](https://www.threads.net/@towardsdatascience)\n  * [Bluesky](https://bsky.app/profile/towardsdatascience.com)\n\n\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nYour home for data science and Al. The world\u2019s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\n\u00a9  Insight Media Group, LLC 2025 \n  * [About](https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/)\n  * [Privacy Policy](https://towardsdatascience.com/privacy-policy/)\n  * [Terms of Use](https://towardsdatascience.com/website-terms-of-use/)\n\n\n[Towards Data Science is now independent!](https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/)\nCookies Settings\n## Sign up to our newsletter\nEmail address*\nFirst name*\nLast name*\nJob title*\nJob level*\nPlease SelectC-LevelVP/DirectorManager/SupervisorMid Level or Senior Non-Managerial StaffEntry Level/Junior StaffFreelancer/ContractorStudent/InternOther\nCompany name*\n  * I consent to receive newsletters and other communications from Towards Data Science publications.*\n\n\n![Company Logo](https://cdn.cookielaw.org/logos/static/ot_company_logo.png)\n## Privacy Preference Center\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer. [More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nAllow All\n###  Manage Consent Preferences\n#### Functional Cookies\nFunctional Cookies Active\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\n#### Strictly Necessary Cookies\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n#### Performance Cookies\nPerformance Cookies Active\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n#### Targeting Cookies\nTargeting Cookies Active\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nBack Button\n### Cookie List\nSearch Icon\nFilter Icon\nClear\ncheckbox label label\nApply Cancel\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nReject All Confirm My Choices\n[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-consent/)\nSome areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis article outlines building a Graph RAG app using knowledge graphs and LLMs to enhance information retrieval in medical literature. It emphasizes the synergy between vector databases and knowledge graphs for accurate, explainable results, showcasing applications in various fields like customer service and drug discovery.\n```"
  },
  {
    "instruction": "[Skip to content](https://towardsdatascience.com/learning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4/#wp--skip-link--target)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nThe world\u2019s leading publication for data science, AI, and ML professionals.\nSign in\nSign out\n[Contributor Portal](https://contributor.insightmediagroup.io/)\n  * [Latest](https://towardsdatascience.com/latest/)\n  * [Editor\u2019s Picks](https://towardsdatascience.com/tag/editors-pick/)\n  * [Deep Dives](https://towardsdatascience.com/tag/deep-dives/)\n  * [Contribute](https://towardsdatascience.com/questions-96667b06af5/)\n  * [Newsletter](https://newsletter.towardsdatascience.com/subscription-to-the-newsletter)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\n\n\nToggle Mobile Navigation\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [X](https://x.com/TDataScience)\n\n\nToggle Search\nSearch\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\n# Learning to Rank: A Complete Guide to Ranking using Machine Learning\nSorting items by relevance is crucial for information retrieval and recommender systems. \n[Francesco Casalegno](https://towardsdatascience.com/author/francesco-casalegno/)\nFeb 28, 2022\n10 min read\nShare \n![Photo by Nick Fewings on Unsplash](https://towardsdatascience.com/wp-content/uploads/2022/02/1B2oK2SifPRPRE4nFSA5FMA-scaled.jpeg)Photo by [Nick Fewings](https://unsplash.com/@jannerboy62?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/library?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n## Ranking: What and Why?\nIn this post, by \"**ranking** \" we mean **sorting documents by relevance** to find contents of interest **with respect to a query**. This is a fundamental problem of **[Information Retrieval](https://en.wikipedia.org/wiki/Information_retrieval)** , but this task also arises in many other applications:\n  1. **[Search Engines](https://en.wikipedia.org/wiki/Search_engine)** \u2013 Given a user profile (location, age, sex, \u2026) a textual query, sort web pages results by relevance.\n  2. **[Recommender Systems](https://en.wikipedia.org/wiki/Recommender_system)** \u2013 Given a user profile and purchase history, sort the other items to find new potentially interesting products for the user.\n  3. **[Travel Agencies](https://en.wikipedia.org/wiki/Travel_agency)** \u2013 Given a user profile and filters (check-in/check-out dates, number and age of travelers, \u2026), sort available rooms by relevance.\n\n![Ranking applications: 1\\) search engines; 2\\) recommender systems; 3\\) travel agencies. \\(Image by author\\)](https://towardsdatascience.com/wp-content/uploads/2022/02/1v4MqXSXWyjCcjpdW9tD3Og.png)Ranking applications: 1) search engines; 2) recommender systems; 3) travel agencies. (Image by author)\nRanking models typically work by predicting a **relevance score _s = f_(_x_)** for each input **_x_ = (_q, d_)** where _**q**_ **is a** **query** and _**d**_ **is a document**. Once we have the relevance of each document, we can sort (i.e. rank) the documents according to those scores.\n![Ranking models rely on a scoring function. \\(Image by author\\)](https://towardsdatascience.com/wp-content/uploads/2022/02/1gX8d-0eerlTHEAv716QC8Q.png)Ranking models rely on a scoring function. (Image by author)\nThe scoring model can be implemented using various approaches.\n  * **[Vector Space Models](https://en.wikipedia.org/wiki/Vector_space_model)** \u2013 Compute a vector embedding (e.g. using [Tf-Idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) or [BERT](https://arxiv.org/abs/1908.10084)) for each query and document, and then compute the relevance score **_f_(_x_) _= f_(_q, d_)** as the cosine similarity between the vectors embeddings of _**q**_ and _**d**_.\n  * **[Learning to Rank](https://en.wikipedia.org/wiki/Learning_to_rank)** \u2013 The scoring model is a Machine Learning model that learns to predict a score _**s**_ given an input **_x_ = (_q, d_)** during a training phase where some sort of ranking loss is minimized.\n\n\nIn this article we focus on the latter approach, and we show how to implement **Machine Learning models for Learning to Rank**.\n## Ranking Evaluation Metrics\nBefore analyzing various ML models for Learning to Rank, we need to define which metrics are used to evaluate ranking models. These metrics are computed on the **predicted documents ranking** , i.e. the **_k-_th top retrieved document** is the _k_ -th document with highest predicted score _**s**_.\n### Mean Average Precision (MAP)\n![MAP - Mean Average Precision. \\(Image by author\\)](https://towardsdatascience.com/wp-content/uploads/2022/02/1I_Ab19M4GKj2PrtJaZ_Nuw.png)MAP \u2013 Mean Average Precision. (Image by author)\n[Mean Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_\\(information_retrieval\\)#Mean_average_precision) is used for tasks with **binary relevance** , i.e. when the true score _y_ of a document _d_ can be only **0 (_non relevant_) or 1 (_relevant_)**.\nFor a given query _q_ and corresponding documents _D_ = **_{_d\u2081, \u2026, _d_\u2099}, we check how many of the top** k retrieved documents are relevant **(y=1) or no _t_ (y=0)., in order to compu**te precisi** o _n_ P\u2096 a**nd reca** l _l_ R\u2096. F _or k_ = ** 1\u2026n we get differe _nt_ P\u2096 __ _an_d R\u2096 values that define **the precision-recall** curve: the area under this curve is **the Average Precision** (AP).\nFinally, by computing the average of AP values for a set of _m_ queries, we obtain the **Mean Average Precision (MAP)**.\n### Discounted Cumulative Gain (DCG)\n![DCG - Discounted Cumulative Gain. \\(Image by author\\)](https://towardsdatascience.com/wp-content/uploads/2022/02/1a8eRGOleBZYqehb-SNLCZw.png)DCG \u2013 Discounted Cumulative Gain. (Image by author)\n[Discounted Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) is used for tasks with **graded relevance** , i.e. when the true score _y_ of a document _d_ is a discrete value in a scale measuring the relevance w.r.t. a query _q_. A typical scale is **0 (_bad_), 1 (_fair_), 2 (_good_), 3 (_excellent_), 4 (_perfect_)**.\nFor a given query _q_ and corresponding documents _D_ = **_{_d\u2081, \u2026, _d_\u2099}, we consider the the** k-th top retrieved document. Th**e gai** n _G\u2096_ = _2^y_\u2096 \u2013 1 measures how useful is this document (we want documents with high relevance!), while th**e discoun** t _D\u2096_ = 1/lo _g_(k+1) penalizes documents that are retrieved with a lower rank (we want relevant documents in the top ranks!).\nThe sum of the **discounted gain** terms G_\u2096_D _\u2096_ for _k =_ 1\u2026 _n_ is the **Discounted Cumulative Gain (DCG)**. To make sure that this score is bound between 0 and 1, we can divide the measured DCG by the ideal score IDCG obtained if we ranked documents by the true value _y\u2096_. This gives us the **Normalized Discounted Cumulative Gain (NDCG)** , where NDCG = DCG/IDCG.\nFinally, as for MAP, we usually compute the average of DCG or NDCG values for a set of _m_ queries to obtain a mean value.\n## Machine Learning Models for Learning to Rank\nTo build a Machine Learning model for ranking, we need to define **inputs** , **outputs** and **loss function**.\n  * **Input** \u2013 For a query _**q**_ we have _**n**_ documents **_D_ = _{_**d\u2081, \u2026, _**d** _\u2099**}** to be ranked by relevance. The elements **_x _\u1d62 =_(_q, _d_\u1d62)** are the inputs to our model.\n  * **Output** \u2013 For a query-document input _x\u1d62_ = (_q_ , _d\u1d62_), we assume there exists a true **relevance score _y\u1d62_**. Our model outputs a **predicted score** **_s\u1d62 = f_(_x\u1d62_)**_._\n\n\nAll Learning to Rank models use a base Machine Learning model (e.g. [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree_learning) or [Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network)) to compute _s_ = _f_(_x_). The choice of the **loss function** is the distinctive element for Learning to Rank models. In general, we have **3 approaches** , depending on how the loss is computed.\n  1. **Pointwise Methods** \u2013 The total loss is computed as the sum of loss terms defined on **each document _d\u1d62_** (hence _**pointwise**_) as the distance between the predicted score _**s\u1d62**_ and the ground truth _**y\u1d62**_ , for _i=_1\u2026 _n_. By doing this, we transform our task into a **regression problem,** where we train a model to predict _y._\n  2. **Pairwise Methods** \u2013 The total loss is computed as the sum of loss terms defined on **each pair of documents _d\u1d62, d\u2c7c** _(hence_**pairwise** _) , for _i, j=_1\u2026 _n_. The objective on which the model is trained is to predict whether _**y\u1d62 > y\u2c7c**_ or not, i.e. which of two documents is more relevant. By doing this, we transform our task into a **binary classification problem**.\n  3. **Listwise Methods** \u2013 The loss is directly computed on the whole list of documents (hence _**listwise**_) with corresponding predicted ranks. In this way, ranking metrics can be more directly incorporated into the loss.\n\n![Machine Learning approaches to Learning to Rank: pointwise, pairwise, listwise. \\(Image by author\\)](https://towardsdatascience.com/wp-content/uploads/2022/02/1s3CQuNRWcQNkQKd8Met-MA.png)Machine Learning approaches to Learning to Rank: pointwise, pairwise, listwise. (Image by author)\n### Pointwise Methods\nThe pointwise approach is the simplest to implement, and it was the first one to be proposed for Learning to Rank tasks. The loss directly measures the distance between ground true score _**y\u1d62**_ and predicted _**s\u1d62**_ so we treat this task by effectively solving a regression problem. As an example, **[Subset Ranking](https://link.springer.com/chapter/10.1007/11776420_44)** uses a [Mean Square Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) loss.\n![MSE loss for pointwise methods as in Subset Ranking. \\(Image by author\\)](https://towardsdatascience.com/wp-content/uploads/2022/02/1h8zgsEZCktLerxfkY51bQA.png)MSE loss for pointwise methods as in Subset Ranking. (Image by author)\n### Pairwise Methods\nThe main issue with pointwise models is that true relevance scores are needed to train the model. But in many scenarios training data is available only with **partial information,** e.g. we only know which document in a list of documents was chosen by a user (and therefore is _more relevant_), but we don\u2019t know exactly _how relevant_ is any of these documents!\nFor this reason, pairwise methods don\u2019t work with absolute relevance. Instead, they work with **relative preference** : given two documents, we want to predict if the first is more relevant than the second. This way we solve a **binary classification task** where we only need the ground truth _y\u1d62\u2c7c_ (_=_1 if _y\u1d62 > y\u2c7c_, 0 otherwise) and we map from the model outputs to probabilities using a [logistic function](https://en.wikipedia.org/wiki/Logistic_function): _s\u1d62\u2c7c_ = \u03c3(_s\u1d62 \u2013 s\u2c7c_). This approach was first used by **[RankNet](https://icml.cc/Conferences/2015/wp-content/uploads/2015/06/icml_ranking.pdf)** , which used a [Binary Cross Entropy (BCE)](https://en.wikipedia.org/wiki/Cross_entropy) loss.\n![BCE loss for pairwise methods as in RankNet. \\(Image by author\\)](https://towardsdatascience.com/wp-content/uploads/2022/02/1OnqFlRq7aYN8szI52jS0MA.png)BCE loss for pairwise methods as in RankNet. (Image by author)\nRankNet is an improvement over pointwise methods, but all documents are still given the same importance during training, while we would want to give **more importance to documents in higher ranks** (as the DCG metric does with the discount terms).\nUnfortunately, rank information is available only after sorting, and sorting is non differentiable. However, to run [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) optimization we don\u2019t need a loss function, we only need its gradient! **[LambdaRank](https://www.microsoft.com/en-us/research/publication/learning-to-rank-with-non-smooth-cost-functions/)** defines the gradients of an implicit loss function so that documents with high rank have much bigger gradients:\n![Gradients of an implicit loss function as in LambdaRank. \\(Image by author\\)](https://towardsdatascience.com/wp-content/uploads/2022/02/1BFTD0koJ-AbNNQd2TOd92w.png)Gradients of an implicit loss function as in LambdaRank. (Image by author)\nHaving gradients is also enough to build a [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) model. This is the idea that **[LambdaMART](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf)** used, yielding even better results than with LambdaRank.\n### Listwise Methods\nPointwise and pairwise approaches transform the ranking problem into a surrogate regression or classification task. Listwise methods, in contrast, solve the problem more **directly by maximizing the evaluation metric**.\nIntuitively, this approach **should give the best results** , as information about ranking is fully exploited and the NDCG is directly optimized. But the obvious problem with setting **Loss = 1 \u2013 NDCG** is that the rank information needed to compute the discounts D\u2096 is only available after **** sortin** g**documents based on predicted scores, and** sortin**g** is non-differentiabl**e. How can we solve this?\nA **first approach** is to use an iterative method where **ranking metrics are used to re-weight** instances at each iteration. This is the approach used by **[LambdaRank](https://www.microsoft.com/en-us/research/publication/learning-to-rank-with-non-smooth-cost-functions/)** and **[LambdaMART](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf)** , which are indeed between the pairwise and the listwise approach.\nA **second approach** is to approximate the objective to make it differentiable, which is the idea behind **[SoftRank](https://www.researchgate.net/publication/221520227_SoftRank_optimizing_non-smooth_rank_metrics)**. Instead of predicting a deterministic score _s_ = _f_(_x_), we predict a **smoothened probabilistic score** _s~_ \ud835\udca9(f_(_x_)_, \u03c3 _\u00b2_). The r**anks k _** a _re non-continuous functions of p**redicted scores s_ ,_**but thanks to the smoothening we can compute p** robability distributions for the ranks**of each document. Finally, we optimize S** oftNDCG,** the expected NDCG over this rank distribution, which is a smooth function.\n![Uncertainty in scores produce a smooth loss in SoftRank. \\(Image by author\\)](https://towardsdatascience.com/wp-content/uploads/2022/02/1MqY4bnrcmOVhHKBg5LsBCA.png)Uncertainty in scores produce a smooth loss in SoftRank. (Image by author)\nA **third approach** is consider that each ranked list corresponds to a permutation, and define a **loss over space of permutations**. In **[ListNet](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf)** , given a list of scores _s_ we define the probability of any permutation using the **[Plackett-Luce model](https://en.wikipedia.org/wiki/Discrete_choice#J._Exploded_logit)**. **** Then, our loss is easily computed as the** Binary Cross-Entropy distance**between true and predicted** probability distributions over the space of permutations**.\n![Probability of various permutation using Plackett-Luce model in ListNet. \\(Image by author\\)](https://towardsdatascience.com/wp-content/uploads/2022/02/1ulNdD-rW6OF0x46sPg5jqw.png)Probability of various permutation using Plackett-Luce model in ListNet. (Image by author)\nFinally, the **[LambdaLoss](https://research.google/pubs/pub47258.pdf)** paper introduced a new perspective on this problem, and created a **generalized framework** to define new listwise loss functions and achieve **state-of-the-art accuracy**. The main idea is to frame the problem in a rigorous and general way, as a **[mixture model](https://en.wikipedia.org/wiki/Mixture_model)** where the ranked list _\u03c0_ is treated as a hidden variable. Then, the loss is defined as the negative log likelihood of such model.\n![LambdaLoss loss function. \\(Image by author\\)](https://towardsdatascience.com/wp-content/uploads/2022/02/1zj5QvE_HkcKrkOrr96Mq4w.png)LambdaLoss loss function. (Image by author)\nThe authors of the LambdaLoss framework proved two essential results.\n  1. All other listwise methods (RankNet, LambdaRank, SoftRank, ListNet, \u2026) are **special configurations** of this general framework. Indeed, their losses are obtained by accurately choosing the **likelihood** **_p_(_y | s, \u03c0_)** and the **ranked list distribution _p_(_\u03c0 | s_)**.\n  2. This framework allows us to define metric-driven loss functions directly connected to the ranking metrics that we want to optimize. This allows to **significantly improve the state-of-the-art on Learningt to Rank tasks.**\n\n\n## Conclusions\nRanking problem are found everywhere, from [Information Retrieval](https://towardsdatascience.com/tag/information-retrieval/ \"Information Retrieval\") to recommender systems and travel booking. Evaluation metrics like MAP and NDCG take into account both rank and relevance of retrieved documents, and therefore are difficult to optimize directly.\nLearning to Rank methods use Machine Learning models to predicting the relevance score of a document, and are divided into 3 classes: pointwise, pairwise, listwise. On most ranking problems, listwise methods like LambdaRank and the generalized framework LambdaLoss achieve state-of-the-art.\n## References\n  * [Wikipedia page on \"Learning to Rank\"](https://en.wikipedia.org/wiki/Learning_to_rank)\n  * [Li, Hang. \"A short introduction to learning to rank.\"](http://times.cs.uiuc.edu/course/598f14/l2r.pdf) 2011\n  * [L. Tie-Yan. \"Learning to Rank for Information Retrieval\", 2009](https://web.archive.org/web/20170808044438/http://wwwconference.org/www2009/pdf/T7A-LEARNING%20TO%20RANK%20TUTORIAL.pdf)\n  * [L. Tie-Yan \"Learning to Rank\", ](http://didawiki.di.unipi.it/lib/exe/fetch.php/magistraleinformatica/ir/ir13/1_-_learning_to_rank.pdf)2009\n  * [X. Wang, \"The LambdaLoss Framework for Ranking Metric Optimization\", 2018](https://research.google/pubs/pub47258/)\n  * [Z. Cao, \"Learning to rank: from pairwise approach to listwise approach\", 2007](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf)\n  * [M Taylor, \"SoftRank: optimizing non-smooth rank metrics\", 2008](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/SoftRankWsdm08Submitted.pdf)\n\n\nWritten By\nFrancesco Casalegno\n[See all from Francesco Casalegno](https://towardsdatascience.com/author/francesco-casalegno/)\nTopics:\n[Data Science](https://towardsdatascience.com/tag/data-science/), [Editors Pick](https://towardsdatascience.com/tag/editors-pick/), [Information Retrieval](https://towardsdatascience.com/tag/information-retrieval/), [Learning To Rank](https://towardsdatascience.com/tag/learning-to-rank/), [Machine Learning](https://towardsdatascience.com/tag/machine-learning/)\nShare this article:\n  * [ Share on Facebook  ](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4%2F&title=Learning%20to%20Rank%3A%20A%20Complete%20Guide%20to%20Ranking%20using%20Machine%20Learning)\n  * [ Share on LinkedIn  ](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4%2F&title=Learning%20to%20Rank%3A%20A%20Complete%20Guide%20to%20Ranking%20using%20Machine%20Learning)\n  * [ Share on X  ](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4%2F&text=Learning%20to%20Rank%3A%20A%20Complete%20Guide%20to%20Ranking%20using%20Machine%20Learning)\n\n\n## Related Articles\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/0c09RmbCCpfjAbSMq.png)\n## [Implementing Convolutional Neural Networks in TensorFlow](https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nStep-by-step code guide to building a Convolutional Neural Network \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nAugust 20, 2024\n6 min read\n  * ![Photo by Krista Mangulsone on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/0GyVVTbgotH-DhGPH-scaled.jpg)\n## [How to Forecast Hierarchical Time Series](https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA beginner\u2019s guide to forecast reconciliation \n[Dr. Robert K\u00fcbler](https://towardsdatascience.com/author/dr-robert-kuebler/)\nAugust 20, 2024\n13 min read\n  * ![Photo by davisuko on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/1bAABgtZtAIG5YW1oEjW3pA-scaled.jpeg)\n## [Hands-on Time Series Anomaly Detection using Autoencoders, with Python](https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nHere\u2019s how to use Autoencoders to detect signals with anomalies in a few lines of\u2026 \n[Piero Paialunga](https://towardsdatascience.com/author/piero-paialunga/)\nAugust 21, 2024\n12 min read\n  * ![Image from Canva.](https://towardsdatascience.com/wp-content/uploads/2024/08/1UAA9jQVdqMXnwzYiz8Q53Q.png)\n## [3 AI Use Cases (That Are Not a Chatbot)](https://towardsdatascience.com/3-ai-use-cases-that-are-not-a-chatbot-f4f328a2707a/)\n[ Machine Learning ](https://towardsdatascience.com/category/artificial-intelligence/machine-learning/)\nFeature engineering, structuring unstructured data, and lead scoring \n[Shaw Talebi](https://towardsdatascience.com/author/shawhin/)\nAugust 21, 2024\n7 min read\n  * ## [Solving a Constrained Project Scheduling Problem with Quantum Annealing](https://towardsdatascience.com/solving-a-constrained-project-scheduling-problem-with-quantum-annealing-d0640e657a3b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nSolving the resource constrained project scheduling problem (RCPSP) with D-Wave\u2019s hybrid constrained quadratic model (CQM) \n[Luis Fernando P\u00c9REZ ARMAS, Ph.D.](https://towardsdatascience.com/author/luisfernandopa1212/)\nAugust 20, 2024\n29 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2023/02/1VEUgT5T4absnTqBMOEuNig.png)\n## [Back To Basics, Part Uno: Linear Regression and Cost Function](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nAn illustrated guide on essential machine learning concepts \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nFebruary 3, 2023\n6 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/1kM8tfYcdaoccB1HX71YDig.png)\n## [Must-Know in Statistics: The Bivariate Normal Projection Explained](https://towardsdatascience.com/must-know-in-statistics-the-bivariate-normal-projection-explained-ace7b2f70b5b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nDerivation and practical examples of this powerful concept \n[Luigi Battistoni](https://towardsdatascience.com/author/lu-battistoni/)\nAugust 14, 2024\n7 min read\n  * ![Photo by Jess Bailey on Unsplash](https://towardsdatascience.com/wp-content/uploads/2022/09/11tHmNYFaWWtWG5I7bNeN6g-scaled.jpeg)\n## [How to Make the Most of Your Experience as a TDS Author](https://towardsdatascience.com/how-to-make-the-most-of-your-experience-as-a-tds-author-b1e056be63f1/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nA quick guide to our resources and FAQ \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nSeptember 13, 2022\n4 min read\n  * ![Photo by Alex Geerts on Unsplash](https://towardsdatascience.com/wp-content/uploads/2020/11/0BF38u2sw4WQdaMLS-scaled.jpg)\n## [Our Columns](https://towardsdatascience.com/our-columns-53501f74c86d/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nColumns on TDS are carefully curated collections of posts on a particular idea or category\u2026 \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nNovember 14, 2020\n4 min read\n\n\n  * [YouTube](https://www.youtube.com/c/TowardsDataScience)\n  * [X](https://x.com/TDataScience)\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [Threads](https://www.threads.net/@towardsdatascience)\n  * [Bluesky](https://bsky.app/profile/towardsdatascience.com)\n\n\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nYour home for data science and Al. The world\u2019s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\n\u00a9  Insight Media Group, LLC 2025 \n  * [About](https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/)\n  * [Privacy Policy](https://towardsdatascience.com/privacy-policy/)\n  * [Terms of Use](https://towardsdatascience.com/website-terms-of-use/)\n\n\n[Towards Data Science is now independent!](https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/)\nCookies Settings\n## Sign up to our newsletter\nEmail address*\nFirst name*\nLast name*\nJob title*\nJob level*\nPlease SelectC-LevelVP/DirectorManager/SupervisorMid Level or Senior Non-Managerial StaffEntry Level/Junior StaffFreelancer/ContractorStudent/InternOther\nCompany name*\n  * I consent to receive newsletters and other communications from Towards Data Science publications.*\n\n\nSome areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe article discusses **Learning to Rank** using machine learning, crucial for applications like search engines and recommender systems. It highlights three main approaches: **pointwise**, **pairwise**, and **listwise**, with listwise methods like **LambdaRank** achieving state-of-the-art results. Evaluation metrics include **Mean Average Precision (MAP)** and **Normalized Discounted Cumulative Gain (NDCG)**, emphasizing the importance of ranking in information retrieval.\n```"
  },
  {
    "instruction": "[Skip to main content](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#__docusaurus_skipToContent_fallback)\n[![LlamaIndex](https://docs.cloud.llamaindex.ai/img/llamacloud-black.svg)![LlamaIndex](https://docs.cloud.llamaindex.ai/img/llamacloud-white.svg)](https://docs.cloud.llamaindex.ai/)\n[Docs](https://docs.cloud.llamaindex.ai/)[API Reference](https://docs.cloud.llamaindex.ai/API/llama-platform)\n  * [Welcome to LlamaCloud \ud83e\udd99](https://docs.cloud.llamaindex.ai/)\n  * [Indexing & Search \ud83d\udd0e](https://docs.cloud.llamaindex.ai/llamacloud/getting_started)\n  * [Parsing \ud83d\udcc4](https://docs.cloud.llamaindex.ai/llamaparse/getting_started)\n    * [Getting Started](https://docs.cloud.llamaindex.ai/llamaparse/getting_started)\n      * [Using the UI](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/web_ui)\n      * [Api](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/get_an_api_key)\n      * [Using in Python](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/python)\n      * [Using in TypeScript](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/typescript)\n      * [Using the REST API](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/api)\n    * [Input](https://docs.cloud.llamaindex.ai/llamaparse/input)\n    * [Examples](https://docs.cloud.llamaindex.ai/llamaparse/examples)\n    * [Parsing Modes](https://docs.cloud.llamaindex.ai/llamaparse/parsing/parsing_modes)\n    * [Schemas](https://docs.cloud.llamaindex.ai/llamaparse/schemas/invoice)\n    * [Features](https://docs.cloud.llamaindex.ai/llamaparse/features/parsing_options)\n    * [Pricing and usage data](https://docs.cloud.llamaindex.ai/llamaparse/usage_data)\n    * [Latency](https://docs.cloud.llamaindex.ai/llamaparse/latency)\n    * [Privacy](https://docs.cloud.llamaindex.ai/llamaparse/privacy)\n    * [FAQ](https://docs.cloud.llamaindex.ai/llamaparse/faq)\n    * [Limitations](https://docs.cloud.llamaindex.ai/llamaparse/limits)\n  * [Document Extraction (beta) \ud83e\udde9](https://docs.cloud.llamaindex.ai/llamaextract/getting_started)\n  * [Report Generation (beta) \ud83d\udccb](https://docs.cloud.llamaindex.ai/llamareport/learn)\n  * [Organizations](https://docs.cloud.llamaindex.ai/organizations)\n  * [Regions](https://docs.cloud.llamaindex.ai/regions)\n  * [Managing your subscription](https://docs.cloud.llamaindex.ai/upgrading)\n\n\n  * [](https://docs.cloud.llamaindex.ai/)\n  * Parsing \ud83d\udcc4\n  * Getting Started\n\n\nOn this page\n# Getting Started\n## Overview[\u200b](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#overview \"Direct link to Overview\")\nWelcome to LlamaParse, the world's best document parsing service, from [LlamaIndex](https://llamaindex.ai). LlamaParse allows you to securely parse complex documents such as PDFs, PowerPoints, Word documents and spreadsheets into structured data using state-of-the-art AI.\nLlamaParse is available as a standalone REST API, a Python package, a TypeScript SDK, and a web UI.\nYou can [sign up](https://cloud.llamaindex.ai/login) to try it out or read the [onboarding documentation](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/web_ui).\n## Original Document\n![uploading](https://docs.cloud.llamaindex.ai/assets/images/parse_original-23ccfa27d97317b118d5d4dd4098fc36.png)\n## Parsing Results\n![uploading](https://docs.cloud.llamaindex.ai/assets/images/parse_result-d4b4f0fe7bdaa6097a972ce0c5b3cbe0.png)\n### Why LlamaParse?[\u200b](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#why-llamaparse \"Direct link to Why LlamaParse?\")\nAt LlamaIndex we have a mission to connect your data to LLMs. A key factor in the effectiveness of presenting your data to LLMs is that it be easily understood by the model.\nOur experiments show that high-quality parsing makes a significant difference to the outcomes of your generative AI applications. So we compiled all of our expertise in document parsing into LlamaParse, to make it easy for you to get your data into the best possible shape for your LLMs.\n## Quick starts[\u200b](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#quick-starts \"Direct link to Quick starts\")\n### Using the web UI[\u200b](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#using-the-web-ui \"Direct link to Using the web UI\")\nThe fastest way to try out LlamaParse is to [use the web UI](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/web_ui). Just drag and drop any PDF, PowerPoint, Word document, or spreadsheet into LlamaCloud and see your results in real time. This is great for getting a sense of what LlamaParse can do before you integrate it into your code.\n### Get an API key[\u200b](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#get-an-api-key \"Direct link to Get an API key\")\nOnce you're ready to start coding, [get an API key](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/get_an_api_key) to use LlamaParse in Python, TypeScript or as a standalone REST API.\n### Use our libraries[\u200b](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#use-our-libraries \"Direct link to Use our libraries\")\nWe have libraries available for Python and TypeScript. Check out the [Python quick start](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/python) or the [TypeScript quick start](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/typescript) to get started.\n### Use the REST API[\u200b](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#use-the-rest-api \"Direct link to Use the REST API\")\nIf you're using a different language, you can use the [LlamaParse REST API](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/api) to parse your documents.\n[PreviousEnterprise Rollout](https://docs.cloud.llamaindex.ai/llamacloud/cookbooks/enterprise_onboarding)[NextUsing the UI](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/web_ui)\n  * [Overview](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#overview)\n    * [Why LlamaParse?](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#why-llamaparse)\n  * [Quick starts](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#quick-starts)\n    * [Using the web UI](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#using-the-web-ui)\n    * [Get an API key](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#get-an-api-key)\n    * [Use our libraries](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#use-our-libraries)\n    * [Use the REST API](https://docs.cloud.llamaindex.ai/llamaparse/getting_started/#use-the-rest-api)\n\n\nLlamaIndex\n  * [Blog](https://llamaindex.ai/blog)\n  * [Careers](https://llamaindex.ai/careers)\n  * [Contact](https://llamaindex.ai/contact)\n\n\nOpen Source\n  * [Python package](https://pypi.org/project/llama-index/)\n  * [Python docs](https://docs.llamaindex.ai/)\n  * [TypeScript package](https://www.npmjs.com/package/llamaindex)\n  * [TypeScript docs](https://ts.llamaindex.ai/)\n  * [LlamaHub](https://llamahub.ai/)\n  * [GitHub](https://github.com/run-llama)\n\n\nCommunity\n  * [Newsletter](https://www.llamaindex.ai/community#newsletter)\n  * [Discord](https://discord.com/invite/eN6D2HQ4aX)\n  * [Twitter/X](https://twitter.com/llama_index)\n  * [LinkedIn](https://www.linkedin.com/company/91154103/)\n  * [YouTube](https://www.youtube.com/@LlamaIndex)\n\n\nStarter projects\n  * [create-llama](https://www.npmjs.com/package/create-llama)\n  * [SEC Insights](https://secinsights.ai/)\n  * [Chat LlamaIndex](https://chat.llamaindex.ai/)\n  * [LlamaBot](https://github.com/run-llama/llamabot)\n  * [RAG CLI](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html)\n\n\nCopyright \u00a9 2025 LlamaIndex, Inc.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nLlamaParse by LlamaIndex is a document parsing service that converts complex documents (PDFs, Word, etc.) into structured data using AI. It offers a web UI, REST API, and libraries for Python and TypeScript. High-quality parsing enhances data presentation for LLMs, improving generative AI outcomes.\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/langchain-ai/langchain/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=langchain-ai%2Flangchain) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/langchain-ai/langchain/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/langchain-ai/langchain/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/langchain-ai/langchain/) to refresh your session. Dismiss alert\n{{ message }}\n[ langchain-ai ](https://github.com/langchain-ai) / **[langchain](https://github.com/langchain-ai/langchain) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Flangchain-ai%2Flangchain) You must be signed in to change notification settings\n  * [ Fork 16.9k ](https://github.com/login?return_to=%2Flangchain-ai%2Flangchain)\n  * [ Star  105k ](https://github.com/login?return_to=%2Flangchain-ai%2Flangchain)\n\n\n\ud83e\udd9c\ud83d\udd17 Build context-aware reasoning applications \n[python.langchain.com](https://python.langchain.com \"https://python.langchain.com\")\n### License\n[ MIT license ](https://github.com/langchain-ai/langchain/blob/master/LICENSE)\n[ 105k stars ](https://github.com/langchain-ai/langchain/stargazers) [ 16.9k forks ](https://github.com/langchain-ai/langchain/forks) [ Branches ](https://github.com/langchain-ai/langchain/branches) [ Tags ](https://github.com/langchain-ai/langchain/tags) [ Activity ](https://github.com/langchain-ai/langchain/activity)\n[ Star  ](https://github.com/login?return_to=%2Flangchain-ai%2Flangchain)\n[ Notifications ](https://github.com/login?return_to=%2Flangchain-ai%2Flangchain) You must be signed in to change notification settings\n  * [ Code ](https://github.com/langchain-ai/langchain)\n  * [ Issues 324 ](https://github.com/langchain-ai/langchain/issues)\n  * [ Pull requests 54 ](https://github.com/langchain-ai/langchain/pulls)\n  * [ Discussions ](https://github.com/langchain-ai/langchain/discussions)\n  * [ Actions ](https://github.com/langchain-ai/langchain/actions)\n  * [ Projects 2 ](https://github.com/langchain-ai/langchain/projects)\n  * [ Security ](https://github.com/langchain-ai/langchain/security)\n  * [ Insights ](https://github.com/langchain-ai/langchain/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/langchain-ai/langchain)\n  * [ Issues  ](https://github.com/langchain-ai/langchain/issues)\n  * [ Pull requests  ](https://github.com/langchain-ai/langchain/pulls)\n  * [ Discussions  ](https://github.com/langchain-ai/langchain/discussions)\n  * [ Actions  ](https://github.com/langchain-ai/langchain/actions)\n  * [ Projects  ](https://github.com/langchain-ai/langchain/projects)\n  * [ Security  ](https://github.com/langchain-ai/langchain/security)\n  * [ Insights  ](https://github.com/langchain-ai/langchain/pulse)\n\n\n# langchain-ai/langchain\nmaster\n[Branches](https://github.com/langchain-ai/langchain/branches)[Tags](https://github.com/langchain-ai/langchain/tags)\n[](https://github.com/langchain-ai/langchain/branches)[](https://github.com/langchain-ai/langchain/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[13,023 Commits](https://github.com/langchain-ai/langchain/commits/master/)[](https://github.com/langchain-ai/langchain/commits/master/)  \n[.devcontainer](https://github.com/langchain-ai/langchain/tree/master/.devcontainer \".devcontainer\")| [.devcontainer](https://github.com/langchain-ai/langchain/tree/master/.devcontainer \".devcontainer\")  \n[.github](https://github.com/langchain-ai/langchain/tree/master/.github \".github\")| [.github](https://github.com/langchain-ai/langchain/tree/master/.github \".github\")  \n[cookbook](https://github.com/langchain-ai/langchain/tree/master/cookbook \"cookbook\")| [cookbook](https://github.com/langchain-ai/langchain/tree/master/cookbook \"cookbook\")  \n[docs](https://github.com/langchain-ai/langchain/tree/master/docs \"docs\")| [docs](https://github.com/langchain-ai/langchain/tree/master/docs \"docs\")  \n[libs](https://github.com/langchain-ai/langchain/tree/master/libs \"libs\")| [libs](https://github.com/langchain-ai/langchain/tree/master/libs \"libs\")  \n[scripts](https://github.com/langchain-ai/langchain/tree/master/scripts \"scripts\")| [scripts](https://github.com/langchain-ai/langchain/tree/master/scripts \"scripts\")  \n[.gitattributes](https://github.com/langchain-ai/langchain/blob/master/.gitattributes \".gitattributes\")| [.gitattributes](https://github.com/langchain-ai/langchain/blob/master/.gitattributes \".gitattributes\")  \n[.gitignore](https://github.com/langchain-ai/langchain/blob/master/.gitignore \".gitignore\")| [.gitignore](https://github.com/langchain-ai/langchain/blob/master/.gitignore \".gitignore\")  \n[.pre-commit-config.yaml](https://github.com/langchain-ai/langchain/blob/master/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/langchain-ai/langchain/blob/master/.pre-commit-config.yaml \".pre-commit-config.yaml\")  \n[.readthedocs.yaml](https://github.com/langchain-ai/langchain/blob/master/.readthedocs.yaml \".readthedocs.yaml\")| [.readthedocs.yaml](https://github.com/langchain-ai/langchain/blob/master/.readthedocs.yaml \".readthedocs.yaml\")  \n[CITATION.cff](https://github.com/langchain-ai/langchain/blob/master/CITATION.cff \"CITATION.cff\")| [CITATION.cff](https://github.com/langchain-ai/langchain/blob/master/CITATION.cff \"CITATION.cff\")  \n[LICENSE](https://github.com/langchain-ai/langchain/blob/master/LICENSE \"LICENSE\")| [LICENSE](https://github.com/langchain-ai/langchain/blob/master/LICENSE \"LICENSE\")  \n[MIGRATE.md](https://github.com/langchain-ai/langchain/blob/master/MIGRATE.md \"MIGRATE.md\")| [MIGRATE.md](https://github.com/langchain-ai/langchain/blob/master/MIGRATE.md \"MIGRATE.md\")  \n[Makefile](https://github.com/langchain-ai/langchain/blob/master/Makefile \"Makefile\")| [Makefile](https://github.com/langchain-ai/langchain/blob/master/Makefile \"Makefile\")  \n[README.md](https://github.com/langchain-ai/langchain/blob/master/README.md \"README.md\")| [README.md](https://github.com/langchain-ai/langchain/blob/master/README.md \"README.md\")  \n[SECURITY.md](https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"SECURITY.md\")| [SECURITY.md](https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"SECURITY.md\")  \n[poetry.toml](https://github.com/langchain-ai/langchain/blob/master/poetry.toml \"poetry.toml\")| [poetry.toml](https://github.com/langchain-ai/langchain/blob/master/poetry.toml \"poetry.toml\")  \n[pyproject.toml](https://github.com/langchain-ai/langchain/blob/master/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/langchain-ai/langchain/blob/master/pyproject.toml \"pyproject.toml\")  \n[uv.lock](https://github.com/langchain-ai/langchain/blob/master/uv.lock \"uv.lock\")| [uv.lock](https://github.com/langchain-ai/langchain/blob/master/uv.lock \"uv.lock\")  \n[yarn.lock](https://github.com/langchain-ai/langchain/blob/master/yarn.lock \"yarn.lock\")| [yarn.lock](https://github.com/langchain-ai/langchain/blob/master/yarn.lock \"yarn.lock\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/langchain-ai/langchain/)\n  * [Code of conduct](https://github.com/langchain-ai/langchain/)\n  * [MIT license](https://github.com/langchain-ai/langchain/)\n  * [Security](https://github.com/langchain-ai/langchain/)\n\n\n![LangChain Logo](https://github.com/langchain-ai/langchain/raw/master/docs/static/img/logo-dark.svg)\n[![Release Notes](https://camo.githubusercontent.com/41ac979ad4e5a6f01a6fd1525321f58afbd22c1d895c491ad35fd02b36ae5f43/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6c616e67636861696e2d61692f6c616e67636861696e3f7374796c653d666c61742d737175617265)](https://github.com/langchain-ai/langchain/releases) [![CI](https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml/badge.svg)](https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml) [![PyPI - License](https://camo.githubusercontent.com/c11a4ab9212c5029f425ffcc8b7e96503b684fc9ddaf782bf094217a4fd64d74/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f6c616e67636861696e2d636f72653f7374796c653d666c61742d737175617265)](https://opensource.org/licenses/MIT) [![PyPI - Downloads](https://camo.githubusercontent.com/4d8dab9ad311d70f41203de5d42d818c679d5d59bcba0200f262f934ae4dbaf6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6c616e67636861696e2d636f72653f7374796c653d666c61742d737175617265)](https://pypistats.org/packages/langchain-core) [![GitHub star chart](https://camo.githubusercontent.com/1dfba37328291056d23ea687f51e983d739a39526f75947331650c57fe755b1d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c616e67636861696e2d61692f6c616e67636861696e3f7374796c653d666c61742d737175617265)](https://star-history.com/#langchain-ai/langchain) [![Open Issues](https://camo.githubusercontent.com/f4b106d240df0c680c5b98f5c5f7071b2bd6a66ee37e9c6b691e24fabd358a0b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f6c616e67636861696e2d61692f6c616e67636861696e3f7374796c653d666c61742d737175617265)](https://github.com/langchain-ai/langchain/issues) [![Open in Dev Containers](https://camo.githubusercontent.com/45e9addd26ffda5e9fd54b1390f4419b1b1dbe4cdbbef267ebac1661705d2cb9/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d446576253230436f6e7461696e657273266d6573736167653d4f70656e26636f6c6f723d626c7565266c6f676f3d76697375616c73747564696f636f6465267374796c653d666c61742d737175617265)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain) [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/langchain-ai/langchain) [![Twitter](https://camo.githubusercontent.com/4dcbdca1cef6d6a3176ae1c41ff677586ca10c970626d4602caa031edb9f1dff/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f747769747465722e636f6d2f6c616e67636861696e61692e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f772532302534304c616e67436861696e4149)](https://twitter.com/langchainai)\nNote\nLooking for the JS/TS library? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\nLangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development \u2014 all while future-proofing decisions as the underlying technology evolves.\n```\npip install -U langchain\n```\n\nTo learn more about LangChain, check out [the docs](https://python.langchain.com/docs/introduction/). If you\u2019re looking for more advanced customization or agent orchestration, check out [LangGraph](https://langchain-ai.github.io/langgraph/), our framework for building controllable agent workflows.\n## Why use LangChain?\n[](https://github.com/langchain-ai/langchain/#why-use-langchain)\nLangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.\nUse LangChain for:\n  * **Real-time data augmentation**. Easily connect LLMs to diverse data sources and external / internal systems, drawing from LangChain\u2019s vast library of integrations with model providers, tools, vector stores, retrievers, and more.\n  * **Model interoperability**. Swap models in and out as your engineering team experiments to find the best choice for your application\u2019s needs. As the industry frontier evolves, adapt quickly \u2014 LangChain\u2019s abstractions keep you moving without losing momentum.\n\n\n## LangChain\u2019s ecosystem\n[](https://github.com/langchain-ai/langchain/#langchains-ecosystem)\nWhile the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.\nTo improve your LLM application development, pair LangChain with:\n  * [LangSmith](http://www.langchain.com/langsmith) - Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\n  * [LangGraph](https://langchain-ai.github.io/langgraph/) - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows \u2014 and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.\n  * [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/#langgraph-platform) - Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in [LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/).\n\n\n## Additional resources\n[](https://github.com/langchain-ai/langchain/#additional-resources)\n  * [Tutorials](https://python.langchain.com/docs/tutorials/): Simple walkthroughs with guided examples on getting started with LangChain.\n  * [How-to Guides](https://python.langchain.com/docs/how_to/): Quick, actionable code snippets for topics such as tool calling, RAG use cases, and more.\n  * [Conceptual Guides](https://python.langchain.com/docs/concepts/): Explanations of key concepts behind the LangChain framework.\n  * [API Reference](https://python.langchain.com/api_reference/): Detailed reference on navigating base packages and integrations for LangChain.\n\n\n## About\n\ud83e\udd9c\ud83d\udd17 Build context-aware reasoning applications \n[python.langchain.com](https://python.langchain.com \"https://python.langchain.com\")\n### Resources\n[ Readme ](https://github.com/langchain-ai/langchain/#readme-ov-file)\n### License\n[ MIT license ](https://github.com/langchain-ai/langchain/#MIT-1-ov-file)\n### Code of conduct\n[ Code of conduct ](https://github.com/langchain-ai/langchain/#coc-ov-file)\n### Security policy\n[ Security policy ](https://github.com/langchain-ai/langchain/#security-ov-file)\n[ Activity](https://github.com/langchain-ai/langchain/activity)\n[ Custom properties](https://github.com/langchain-ai/langchain/custom-properties)\n### Stars\n[ **105k** stars](https://github.com/langchain-ai/langchain/stargazers)\n### Watchers\n[ **740** watching](https://github.com/langchain-ai/langchain/watchers)\n### Forks\n[ **16.9k** forks](https://github.com/langchain-ai/langchain/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain&report=langchain-ai+%28user%29)\n##  [Releases 848](https://github.com/langchain-ai/langchain/releases)\n[ langchain-core==0.3.49 Latest  Mar 26, 2025 ](https://github.com/langchain-ai/langchain/releases/tag/langchain-core%3D%3D0.3.49)\n[+ 847 releases](https://github.com/langchain-ai/langchain/releases)\n##  [Packages 0](https://github.com/orgs/langchain-ai/packages?repo_name=langchain)\nNo packages published \n##  [Used by 210k](https://github.com/langchain-ai/langchain/network/dependents)\n[\n  * ![@om-blip](https://avatars.githubusercontent.com/u/80588362?s=64&v=4)\n  * ![@lablup](https://avatars.githubusercontent.com/u/11258248?s=64&v=4)\n  * ![@OmkarBhikle](https://avatars.githubusercontent.com/u/103316347?s=64&v=4)\n  * ![@Hasnulsanna](https://avatars.githubusercontent.com/u/116332124?s=64&v=4)\n  * ![@alisa-collab](https://avatars.githubusercontent.com/u/205439463?s=64&v=4)\n  * ![@Ssalgia](https://avatars.githubusercontent.com/u/37534160?s=64&v=4)\n  * ![@sskyBlessed](https://avatars.githubusercontent.com/u/50995025?s=64&v=4)\n  * ![@anubratabhowmick](https://avatars.githubusercontent.com/u/7466987?s=64&v=4)\n\n+ 209,860  ](https://github.com/langchain-ai/langchain/network/dependents)\n##  [Contributors 3,540](https://github.com/langchain-ai/langchain/graphs/contributors)\n  * [ ![@baskaryan](https://avatars.githubusercontent.com/u/22008038?s=64&v=4) ](https://github.com/baskaryan)\n  * [ ![@hwchase17](https://avatars.githubusercontent.com/u/11986836?s=64&v=4) ](https://github.com/hwchase17)\n  * [ ![@ccurme](https://avatars.githubusercontent.com/u/26529506?s=64&v=4) ](https://github.com/ccurme)\n  * [ ![@eyurtsev](https://avatars.githubusercontent.com/u/3205522?s=64&v=4) ](https://github.com/eyurtsev)\n  * [ ![@nfcampos](https://avatars.githubusercontent.com/u/56902?s=64&v=4) ](https://github.com/nfcampos)\n  * [ ![@leo-gan](https://avatars.githubusercontent.com/u/2256422?s=64&v=4) ](https://github.com/leo-gan)\n  * [ ![@hinthornw](https://avatars.githubusercontent.com/u/13333726?s=64&v=4) ](https://github.com/hinthornw)\n  * [ ![@dev2049](https://avatars.githubusercontent.com/u/130488702?s=64&v=4) ](https://github.com/dev2049)\n  * [ ![@cbornet](https://avatars.githubusercontent.com/u/11633333?s=64&v=4) ](https://github.com/cbornet)\n  * [ ![@rlancemartin](https://avatars.githubusercontent.com/u/122662504?s=64&v=4) ](https://github.com/rlancemartin)\n  * [ ![@vowelparrot](https://avatars.githubusercontent.com/u/130414180?s=64&v=4) ](https://github.com/vowelparrot)\n  * [ ![@jacoblee93](https://avatars.githubusercontent.com/u/6952323?s=64&v=4) ](https://github.com/jacoblee93)\n  * [ ![@tomasonjo](https://avatars.githubusercontent.com/u/19948365?s=64&v=4) ](https://github.com/tomasonjo)\n  * [ ![@obi1kenobi](https://avatars.githubusercontent.com/u/2348618?s=64&v=4) ](https://github.com/obi1kenobi)\n\n\n[+ 3,526 contributors](https://github.com/langchain-ai/langchain/graphs/contributors)\n## Languages\n  * [ Jupyter Notebook 61.6% ](https://github.com/langchain-ai/langchain/search?l=jupyter-notebook)\n  * [ Python 38.2% ](https://github.com/langchain-ai/langchain/search?l=python)\n  * [ Makefile 0.1% ](https://github.com/langchain-ai/langchain/search?l=makefile)\n  * [ MDX 0.1% ](https://github.com/langchain-ai/langchain/search?l=mdx)\n  * [ Shell 0.0% ](https://github.com/langchain-ai/langchain/search?l=shell)\n  * [ XSLT 0.0% ](https://github.com/langchain-ai/langchain/search?l=xslt)\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary\n\n**LangChain** is a framework for building LLM-powered applications, enabling developers to create context-aware reasoning applications. It supports real-time data augmentation and model interoperability, integrating with tools like **LangSmith** and **LangGraph** for enhanced functionality. \n\nFor more details, visit [LangChain Documentation](https://python.langchain.com/docs/introduction/).\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/kelvins/awesome-mlops/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fkelvins%2Fawesome-mlops%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fkelvins%2Fawesome-mlops%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=kelvins%2Fawesome-mlops) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/kelvins/awesome-mlops/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/kelvins/awesome-mlops/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/kelvins/awesome-mlops/) to refresh your session. Dismiss alert\n{{ message }}\n[ kelvins ](https://github.com/kelvins) / **[awesome-mlops](https://github.com/kelvins/awesome-mlops) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Fkelvins%2Fawesome-mlops) You must be signed in to change notification settings\n  * [ Fork 604 ](https://github.com/login?return_to=%2Fkelvins%2Fawesome-mlops)\n  * [ Star  4.4k ](https://github.com/login?return_to=%2Fkelvins%2Fawesome-mlops)\n\n\n\ud83d\ude0e A curated list of awesome MLOps tools \n[ 4.4k stars ](https://github.com/kelvins/awesome-mlops/stargazers) [ 604 forks ](https://github.com/kelvins/awesome-mlops/forks) [ Branches ](https://github.com/kelvins/awesome-mlops/branches) [ Tags ](https://github.com/kelvins/awesome-mlops/tags) [ Activity ](https://github.com/kelvins/awesome-mlops/activity)\n[ Star  ](https://github.com/login?return_to=%2Fkelvins%2Fawesome-mlops)\n[ Notifications ](https://github.com/login?return_to=%2Fkelvins%2Fawesome-mlops) You must be signed in to change notification settings\n  * [ Code ](https://github.com/kelvins/awesome-mlops)\n  * [ Issues 4 ](https://github.com/kelvins/awesome-mlops/issues)\n  * [ Pull requests 2 ](https://github.com/kelvins/awesome-mlops/pulls)\n  * [ Actions ](https://github.com/kelvins/awesome-mlops/actions)\n  * [ Security ](https://github.com/kelvins/awesome-mlops/security)\n  * [ Insights ](https://github.com/kelvins/awesome-mlops/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/kelvins/awesome-mlops)\n  * [ Issues  ](https://github.com/kelvins/awesome-mlops/issues)\n  * [ Pull requests  ](https://github.com/kelvins/awesome-mlops/pulls)\n  * [ Actions  ](https://github.com/kelvins/awesome-mlops/actions)\n  * [ Security  ](https://github.com/kelvins/awesome-mlops/security)\n  * [ Insights  ](https://github.com/kelvins/awesome-mlops/pulse)\n\n\n# kelvins/awesome-mlops\nmain\n[Branches](https://github.com/kelvins/awesome-mlops/branches)[Tags](https://github.com/kelvins/awesome-mlops/tags)\n[](https://github.com/kelvins/awesome-mlops/branches)[](https://github.com/kelvins/awesome-mlops/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[430 Commits](https://github.com/kelvins/awesome-mlops/commits/main/)[](https://github.com/kelvins/awesome-mlops/commits/main/)  \n[.github](https://github.com/kelvins/awesome-mlops/tree/main/.github \".github\")| [.github](https://github.com/kelvins/awesome-mlops/tree/main/.github \".github\")  \n[CODE_OF_CONDUCT.md](https://github.com/kelvins/awesome-mlops/blob/main/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| [CODE_OF_CONDUCT.md](https://github.com/kelvins/awesome-mlops/blob/main/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")  \n[CONTRIBUTING.md](https://github.com/kelvins/awesome-mlops/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/kelvins/awesome-mlops/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")  \n[README.md](https://github.com/kelvins/awesome-mlops/blob/main/README.md \"README.md\")| [README.md](https://github.com/kelvins/awesome-mlops/blob/main/README.md \"README.md\")  \n[check_order.py](https://github.com/kelvins/awesome-mlops/blob/main/check_order.py \"check_order.py\")| [check_order.py](https://github.com/kelvins/awesome-mlops/blob/main/check_order.py \"check_order.py\")  \n[mlc_config.json](https://github.com/kelvins/awesome-mlops/blob/main/mlc_config.json \"mlc_config.json\")| [mlc_config.json](https://github.com/kelvins/awesome-mlops/blob/main/mlc_config.json \"mlc_config.json\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/kelvins/awesome-mlops/)\n  * [Code of conduct](https://github.com/kelvins/awesome-mlops/)\n\n\n# Awesome MLOps [![Awesome](https://camo.githubusercontent.com/8693bde04030b1670d5097703441005eba34240c32d1df1eb82a5f0d6716518e/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667)](https://github.com/sindresorhus/awesome)\n[](https://github.com/kelvins/awesome-mlops/#awesome-mlops-)\nA curated list of awesome MLOps tools.\nInspired by [awesome-python](https://github.com/vinta/awesome-python).\n  * [Awesome MLOps](https://github.com/kelvins/awesome-mlops/#awesome-mlops)\n    * [AutoML](https://github.com/kelvins/awesome-mlops/#automl)\n    * [CI/CD for Machine Learning](https://github.com/kelvins/awesome-mlops/#cicd-for-machine-learning)\n    * [Cron Job Monitoring](https://github.com/kelvins/awesome-mlops/#cron-job-monitoring)\n    * [Data Catalog](https://github.com/kelvins/awesome-mlops/#data-catalog)\n    * [Data Enrichment](https://github.com/kelvins/awesome-mlops/#data-enrichment)\n    * [Data Exploration](https://github.com/kelvins/awesome-mlops/#data-exploration)\n    * [Data Management](https://github.com/kelvins/awesome-mlops/#data-management)\n    * [Data Processing](https://github.com/kelvins/awesome-mlops/#data-processing)\n    * [Data Validation](https://github.com/kelvins/awesome-mlops/#data-validation)\n    * [Data Visualization](https://github.com/kelvins/awesome-mlops/#data-visualization)\n    * [Drift Detection](https://github.com/kelvins/awesome-mlops/#drift-detection)\n    * [Feature Engineering](https://github.com/kelvins/awesome-mlops/#feature-engineering)\n    * [Feature Store](https://github.com/kelvins/awesome-mlops/#feature-store)\n    * [Hyperparameter Tuning](https://github.com/kelvins/awesome-mlops/#hyperparameter-tuning)\n    * [Knowledge Sharing](https://github.com/kelvins/awesome-mlops/#knowledge-sharing)\n    * [Machine Learning Platform](https://github.com/kelvins/awesome-mlops/#machine-learning-platform)\n    * [Model Fairness and Privacy](https://github.com/kelvins/awesome-mlops/#model-fairness-and-privacy)\n    * [Model Interpretability](https://github.com/kelvins/awesome-mlops/#model-interpretability)\n    * [Model Lifecycle](https://github.com/kelvins/awesome-mlops/#model-lifecycle)\n    * [Model Serving](https://github.com/kelvins/awesome-mlops/#model-serving)\n    * [Model Testing & Validation](https://github.com/kelvins/awesome-mlops/#model-testing--validation)\n    * [Optimization Tools](https://github.com/kelvins/awesome-mlops/#optimization-tools)\n    * [Simplification Tools](https://github.com/kelvins/awesome-mlops/#simplification-tools)\n    * [Visual Analysis and Debugging](https://github.com/kelvins/awesome-mlops/#visual-analysis-and-debugging)\n    * [Workflow Tools](https://github.com/kelvins/awesome-mlops/#workflow-tools)\n  * [Resources](https://github.com/kelvins/awesome-mlops/#resources)\n    * [Articles](https://github.com/kelvins/awesome-mlops/#articles)\n    * [Books](https://github.com/kelvins/awesome-mlops/#books)\n    * [Events](https://github.com/kelvins/awesome-mlops/#events)\n    * [Other Lists](https://github.com/kelvins/awesome-mlops/#other-lists)\n    * [Podcasts](https://github.com/kelvins/awesome-mlops/#podcasts)\n    * [Slack](https://github.com/kelvins/awesome-mlops/#slack)\n    * [Websites](https://github.com/kelvins/awesome-mlops/#websites)\n  * [Contributing](https://github.com/kelvins/awesome-mlops/#contributing)\n\n\n## AutoML\n[](https://github.com/kelvins/awesome-mlops/#automl)\n_Tools for performing AutoML._\n  * [AutoGluon](https://github.com/awslabs/autogluon) - Automated machine learning for image, text, tabular, time-series, and multi-modal data.\n  * [AutoKeras](https://github.com/keras-team/autokeras) - AutoKeras goal is to make machine learning accessible for everyone.\n  * [AutoPyTorch](https://github.com/automl/Auto-PyTorch) - Automatic architecture search and hyperparameter optimization for PyTorch.\n  * [AutoSKLearn](https://github.com/automl/auto-sklearn) - Automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator.\n  * [EvalML](https://github.com/alteryx/evalml) - A library that builds, optimizes, and evaluates ML pipelines using domain-specific functions.\n  * [FLAML](https://github.com/microsoft/FLAML) - Finds accurate ML models automatically, efficiently and economically.\n  * [H2O AutoML](https://h2o.ai/platform/h2o-automl) - Automates ML workflow, which includes automatic training and tuning of models.\n  * [MindsDB](https://github.com/mindsdb/mindsdb) - AI layer for databases that allows you to effortlessly develop, train and deploy ML models.\n  * [MLBox](https://github.com/AxeldeRomblay/MLBox) - MLBox is a powerful Automated Machine Learning python library.\n  * [Model Search](https://github.com/google/model_search) - Framework that implements AutoML algorithms for model architecture search at scale.\n  * [NNI](https://github.com/microsoft/nni) - An open source AutoML toolkit for automate machine learning lifecycle.\n\n\n## CI/CD for Machine Learning\n[](https://github.com/kelvins/awesome-mlops/#cicd-for-machine-learning)\n_Tools for performing CI/CD for Machine Learning._\n  * [ClearML](https://github.com/allegroai/clearml) - Auto-Magical CI/CD to streamline your ML workflow.\n  * [CML](https://github.com/iterative/cml) - Open-source library for implementing CI/CD in machine learning projects.\n  * [KitOps](https://github.com/jozu-ai/kitops) \u2013 Open source MLOps project that eases model handoffs between data scientist and DevOps.\n\n\n## Cron Job Monitoring\n[](https://github.com/kelvins/awesome-mlops/#cron-job-monitoring)\n_Tools for monitoring cron jobs (recurring jobs)._\n  * [Cronitor](https://cronitor.io/cron-job-monitoring) - Monitor any cron job or scheduled task.\n  * [HealthchecksIO](https://healthchecks.io/) - Simple and effective cron job monitoring.\n\n\n## Data Catalog\n[](https://github.com/kelvins/awesome-mlops/#data-catalog)\n_Tools for data cataloging._\n  * [Amundsen](https://www.amundsen.io/) - Data discovery and metadata engine for improving the productivity when interacting with data.\n  * [Apache Atlas](https://atlas.apache.org) - Provides open metadata management and governance capabilities to build a data catalog.\n  * [CKAN](https://github.com/ckan/ckan) - Open-source DMS (data management system) for powering data hubs and data portals.\n  * [DataHub](https://github.com/linkedin/datahub) - LinkedIn's generalized metadata search & discovery tool.\n  * [Magda](https://github.com/magda-io/magda) - A federated, open-source data catalog for all your big data and small data.\n  * [Metacat](https://github.com/Netflix/metacat) - Unified metadata exploration API service for Hive, RDS, Teradata, Redshift, S3 and Cassandra.\n  * [OpenMetadata](https://open-metadata.org/) - A Single place to discover, collaborate and get your data right.\n\n\n## Data Enrichment\n[](https://github.com/kelvins/awesome-mlops/#data-enrichment)\n_Tools and libraries for data enrichment._\n  * [Snorkel](https://github.com/snorkel-team/snorkel) - A system for quickly generating training data with weak supervision.\n  * [Upgini](https://github.com/upgini/upgini) - Enriches training datasets with features from public and community shared data sources.\n\n\n## Data Exploration\n[](https://github.com/kelvins/awesome-mlops/#data-exploration)\n_Tools for performing data exploration._\n  * [Apache Zeppelin](https://zeppelin.apache.org/) - Enables data-driven, interactive data analytics and collaborative documents.\n  * [BambooLib](https://github.com/tkrabel/bamboolib) - An intuitive GUI for Pandas DataFrames.\n  * [DataPrep](https://github.com/sfu-db/dataprep) - Collect, clean and visualize your data in Python.\n  * [Google Colab](https://colab.research.google.com) - Hosted Jupyter notebook service that requires no setup to use.\n  * [Jupyter Notebook](https://jupyter.org/) - Web-based notebook environment for interactive computing.\n  * [JupyterLab](https://jupyterlab.readthedocs.io) - The next-generation user interface for Project Jupyter.\n  * [Jupytext](https://github.com/mwouts/jupytext) - Jupyter Notebooks as Markdown Documents, Julia, Python or R scripts.\n  * [Pandas Profiling](https://github.com/ydataai/pandas-profiling) - Create HTML profiling reports from pandas DataFrame objects.\n  * [Polynote](https://polynote.org/) - The polyglot notebook with first-class Scala support.\n\n\n## Data Management\n[](https://github.com/kelvins/awesome-mlops/#data-management)\n_Tools for performing data management._\n  * [Arrikto](https://www.arrikto.com/) - Dead simple, ultra fast storage for the hybrid Kubernetes world.\n  * [BlazingSQL](https://github.com/BlazingDB/blazingsql) - A lightweight, GPU accelerated, SQL engine for Python. Built on RAPIDS cuDF.\n  * [Delta Lake](https://github.com/delta-io/delta) - Storage layer that brings scalable, ACID transactions to Apache Spark and other engines.\n  * [Dolt](https://github.com/dolthub/dolt) - SQL database that you can fork, clone, branch, merge, push and pull just like a git repository.\n  * [Dud](https://github.com/kevin-hanselman/dud) - A lightweight CLI tool for versioning data alongside source code and building data pipelines.\n  * [DVC](https://dvc.org/) - Management and versioning of datasets and machine learning models.\n  * [Git LFS](https://git-lfs.github.com) - An open source Git extension for versioning large files.\n  * [Hub](https://github.com/activeloopai/Hub) - A dataset format for creating, storing, and collaborating on AI datasets of any size.\n  * [Intake](https://github.com/intake/intake) - A lightweight set of tools for loading and sharing data in data science projects.\n  * [lakeFS](https://github.com/treeverse/lakeFS) - Repeatable, atomic and versioned data lake on top of object storage.\n  * [Marquez](https://github.com/MarquezProject/marquez) - Collect, aggregate, and visualize a data ecosystem's metadata.\n  * [Milvus](https://github.com/milvus-io/milvus/) - An open source embedding vector similarity search engine powered by Faiss, NMSLIB and Annoy.\n  * [Pinecone](https://www.pinecone.io) - Managed and distributed vector similarity search used with a lightweight SDK.\n  * [Qdrant](https://github.com/qdrant/qdrant) - An open source vector similarity search engine with extended filtering support.\n  * [Quilt](https://github.com/quiltdata/quilt) - A self-organizing data hub with S3 support.\n\n\n## Data Processing\n[](https://github.com/kelvins/awesome-mlops/#data-processing)\n_Tools related to data processing and data pipelines._\n  * [Airflow](https://airflow.apache.org/) - Platform to programmatically author, schedule, and monitor workflows.\n  * [Azkaban](https://github.com/azkaban/azkaban) - Batch workflow job scheduler created at LinkedIn to run Hadoop jobs.\n  * [Dagster](https://github.com/dagster-io/dagster) - A data orchestrator for machine learning, analytics, and ETL.\n  * [Hadoop](https://hadoop.apache.org/) - Framework that allows for the distributed processing of large data sets across clusters.\n  * [OpenRefine](https://github.com/OpenRefine/OpenRefine) - Power tool for working with messy data and improving it.\n  * [Spark](https://spark.apache.org/) - Unified analytics engine for large-scale data processing.\n\n\n## Data Validation\n[](https://github.com/kelvins/awesome-mlops/#data-validation)\n_Tools related to data validation._\n  * [Cerberus](https://github.com/pyeve/cerberus) - Lightweight, extensible data validation library for Python.\n  * [Cleanlab](https://github.com/cleanlab/cleanlab) - Python library for data-centric AI and machine learning with messy, real-world data and labels.\n  * [Great Expectations](https://greatexpectations.io) - A Python data validation framework that allows to test your data against datasets.\n  * [JSON Schema](https://json-schema.org/) - A vocabulary that allows you to annotate and validate JSON documents.\n  * [TFDV](https://github.com/tensorflow/data-validation) - An library for exploring and validating machine learning data.\n\n\n## Data Visualization\n[](https://github.com/kelvins/awesome-mlops/#data-visualization)\n_Tools for data visualization, reports and dashboards._\n  * [Count](https://count.co) - SQL/drag-and-drop querying and visualisation tool based on notebooks.\n  * [Dash](https://github.com/plotly/dash) - Analytical Web Apps for Python, R, Julia, and Jupyter.\n  * [Data Studio](https://datastudio.google.com) - Reporting solution for power users who want to go beyond the data and dashboards of GA.\n  * [Facets](https://github.com/PAIR-code/facets) - Visualizations for understanding and analyzing machine learning datasets.\n  * [Grafana](https://grafana.com/grafana/) - Multi-platform open source analytics and interactive visualization web application.\n  * [Lux](https://github.com/lux-org/lux) - Fast and easy data exploration by automating the visualization and data analysis process.\n  * [Metabase](https://www.metabase.com/) - The simplest, fastest way to get business intelligence and analytics to everyone.\n  * [Redash](https://redash.io/) - Connect to any data source, easily visualize, dashboard and share your data.\n  * [SolidUI](https://github.com/CloudOrc/SolidUI) - AI-generated visualization prototyping and editing platform, support 2D and 3D models.\n  * [Superset](https://superset.incubator.apache.org/) - Modern, enterprise-ready business intelligence web application.\n  * [Tableau](https://www.tableau.com) - Powerful and fastest growing data visualization tool used in the business intelligence industry.\n\n\n## Drift Detection\n[](https://github.com/kelvins/awesome-mlops/#drift-detection)\n_Tools and libraries related to drift detection._\n  * [Alibi Detect](https://github.com/SeldonIO/alibi-detect) - An open source Python library focused on outlier, adversarial and drift detection.\n  * [Frouros](https://github.com/IFCA/frouros) - An open source Python library for drift detection in machine learning systems.\n  * [TorchDrift](https://github.com/torchdrift/torchdrift/) - A data and concept drift library for PyTorch.\n\n\n## Feature Engineering\n[](https://github.com/kelvins/awesome-mlops/#feature-engineering)\n_Tools and libraries related to feature engineering._\n  * [Feature Engine](https://github.com/feature-engine/feature_engine) - Feature engineering package with SKlearn like functionality.\n  * [Featuretools](https://github.com/alteryx/featuretools) - Python library for automated feature engineering.\n  * [TSFresh](https://github.com/blue-yonder/tsfresh) - Python library for automatic extraction of relevant features from time series.\n\n\n## Feature Store\n[](https://github.com/kelvins/awesome-mlops/#feature-store)\n_Feature store tools for data serving._\n  * [Butterfree](https://github.com/quintoandar/butterfree) - A tool for building feature stores. Transform your raw data into beautiful features.\n  * [ByteHub](https://github.com/bytehub-ai/bytehub) - An easy-to-use feature store. Optimized for time-series data.\n  * [Feast](https://feast.dev/) - End-to-end open source feature store for machine learning.\n  * [Feathr](https://github.com/linkedin/feathr) - An enterprise-grade, high performance feature store.\n  * [Featureform](https://github.com/featureform/featureform) - A Virtual Feature Store. Turn your existing data infrastructure into a feature store.\n  * [Tecton](https://www.tecton.ai/) - A fully-managed feature platform built to orchestrate the complete lifecycle of features.\n\n\n## Hyperparameter Tuning\n[](https://github.com/kelvins/awesome-mlops/#hyperparameter-tuning)\n_Tools and libraries to perform hyperparameter tuning._\n  * [Advisor](https://github.com/tobegit3hub/advisor) - Open-source implementation of Google Vizier for hyper parameters tuning.\n  * [Hyperas](https://github.com/maxpumperla/hyperas) - A very simple wrapper for convenient hyperparameter optimization.\n  * [Hyperopt](https://github.com/hyperopt/hyperopt) - Distributed Asynchronous Hyperparameter Optimization in Python.\n  * [Katib](https://github.com/kubeflow/katib) - Kubernetes-based system for hyperparameter tuning and neural architecture search.\n  * [KerasTuner](https://github.com/keras-team/keras-tuner) - Easy-to-use, scalable hyperparameter optimization framework.\n  * [Optuna](https://optuna.org/) - Open source hyperparameter optimization framework to automate hyperparameter search.\n  * [Scikit Optimize](https://github.com/scikit-optimize/scikit-optimize) - Simple and efficient library to minimize expensive and noisy black-box functions.\n  * [Talos](https://github.com/autonomio/talos) - Hyperparameter Optimization for TensorFlow, Keras and PyTorch.\n  * [Tune](https://docs.ray.io/en/latest/tune.html) - Python library for experiment execution and hyperparameter tuning at any scale.\n\n\n## Knowledge Sharing\n[](https://github.com/kelvins/awesome-mlops/#knowledge-sharing)\n_Tools for sharing knowledge to the entire team/company._\n  * [Knowledge Repo](https://github.com/airbnb/knowledge-repo) - Knowledge sharing platform for data scientists and other technical professions.\n  * [Kyso](https://kyso.io/) - One place for data insights so your entire team can learn from your data.\n\n\n## Machine Learning Platform\n[](https://github.com/kelvins/awesome-mlops/#machine-learning-platform)\n_Complete machine learning platform solutions._\n  * [aiWARE](https://www.veritone.com/aiware/aiware-os/) - aiWARE helps MLOps teams evaluate, deploy, integrate, scale & monitor ML models.\n  * [Algorithmia](https://algorithmia.com/) - Securely govern your machine learning operations with a healthy ML lifecycle.\n  * [Allegro AI](https://allegro.ai/) - Transform ML/DL research into products. Faster.\n  * [Bodywork](https://bodywork.readthedocs.io/en/latest/) - Deploys machine learning projects developed in Python, to Kubernetes.\n  * [CNVRG](https://cnvrg.io/) - An end-to-end machine learning platform to build and deploy AI models at scale.\n  * [DAGsHub](https://dagshub.com/) - A platform built on open source tools for data, model and pipeline management.\n  * [Dataiku](https://www.dataiku.com/) - Platform democratizing access to data and enabling enterprises to build their own path to AI.\n  * [DataRobot](https://www.datarobot.com/) - AI platform that democratizes data science and automates the end-to-end ML at scale.\n  * [Domino](https://www.dominodatalab.com/) - One place for your data science tools, apps, results, models, and knowledge.\n  * [Edge Impulse](https://edgeimpulse.com/) - Platform for creating, optimizing, and deploying AI/ML algorithms for edge devices.\n  * [envd](https://github.com/tensorchord/envd) - Machine learning development environment for data science and AI/ML engineering teams.\n  * [FedML](https://fedml.ai/) - Simplifies the workflow of federated learning anywhere at any scale.\n  * [Gradient](https://gradient.paperspace.com/) - Multicloud CI/CD and MLOps platform for machine learning teams.\n  * [H2O](https://www.h2o.ai/) - Open source leader in AI with a mission to democratize AI for everyone.\n  * [Hopsworks](https://www.hopsworks.ai/) - Open-source platform for developing and operating machine learning models at scale.\n  * [Iguazio](https://www.iguazio.com/) - Data science platform that automates MLOps with end-to-end machine learning pipelines.\n  * [Katonic](https://katonic.ai/) - Automate your cycle of intelligence with Katonic MLOps Platform.\n  * [Knime](https://www.knime.com/) - Create and productionize data science using one easy and intuitive environment.\n  * [Kubeflow](https://www.kubeflow.org/) - Making deployments of ML workflows on Kubernetes simple, portable and scalable.\n  * [LynxKite](https://lynxkite.com/) - A complete graph data science platform for very large graphs and other datasets.\n  * [ML Workspace](https://github.com/ml-tooling/ml-workspace) - All-in-one web-based IDE specialized for machine learning and data science.\n  * [MLReef](https://github.com/MLReef/mlreef) - Open source MLOps platform that helps you collaborate, reproduce and share your ML work.\n  * [Modzy](https://www.modzy.com/) - Deploy, connect, run, and monitor machine learning (ML) models in the enterprise and at the edge.\n  * [Neu.ro](https://neu.ro) - MLOps platform that integrates open-source and proprietary tools into client-oriented systems.\n  * [Omnimizer](https://www.omniml.ai) - Simplifies and accelerates MLOps by bridging the gap between ML models and edge hardware.\n  * [Pachyderm](https://www.pachyderm.com/) - Combines data lineage with end-to-end pipelines on Kubernetes, engineered for the enterprise.\n  * [Polyaxon](https://www.github.com/polyaxon/polyaxon/) - A platform for reproducible and scalable machine learning and deep learning on kubernetes.\n  * [Sagemaker](https://aws.amazon.com/sagemaker/) - Fully managed service that provides the ability to build, train, and deploy ML models quickly.\n  * [SAS Viya](https://www.sas.com/en_us/software/viya.html) - Cloud native AI, analytic and data management platform that supports the analytics life cycle.\n  * [Sematic](https://sematic.dev) - An open-source end-to-end pipelining tool to go from laptop prototype to cloud in no time.\n  * [SigOpt](https://sigopt.com/) - A platform that makes it easy to track runs, visualize training, and scale hyperparameter tuning.\n  * [TrueFoundry](https://www.truefoundry.com) - A Cloud-native MLOps Platform over Kubernetes to simplify training and serving of ML Models.\n  * [Valohai](https://valohai.com/) - Takes you from POC to production while managing the whole model lifecycle.\n\n\n## Model Fairness and Privacy\n[](https://github.com/kelvins/awesome-mlops/#model-fairness-and-privacy)\n_Tools for performing model fairness and privacy in production._\n  * [AIF360](https://github.com/Trusted-AI/AIF360) - A comprehensive set of fairness metrics for datasets and machine learning models.\n  * [Fairlearn](https://github.com/fairlearn/fairlearn) - A Python package to assess and improve fairness of machine learning models.\n  * [Opacus](https://github.com/pytorch/opacus) - A library that enables training PyTorch models with differential privacy.\n  * [TensorFlow Privacy](https://github.com/tensorflow/privacy) - Library for training machine learning models with privacy for training data.\n\n\n## Model Interpretability\n[](https://github.com/kelvins/awesome-mlops/#model-interpretability)\n_Tools for performing model interpretability/explainability._\n  * [Alibi](https://github.com/SeldonIO/alibi) - Open-source Python library enabling ML model inspection and interpretation.\n  * [Captum](https://github.com/pytorch/captum) - Model interpretability and understanding library for PyTorch.\n  * [ELI5](https://github.com/eli5-org/eli5) - Python package which helps to debug machine learning classifiers and explain their predictions.\n  * [InterpretML](https://github.com/interpretml/interpret) - A toolkit to help understand models and enable responsible machine learning.\n  * [LIME](https://github.com/marcotcr/lime) - Explaining the predictions of any machine learning classifier.\n  * [Lucid](https://github.com/tensorflow/lucid) - Collection of infrastructure and tools for research in neural network interpretability.\n  * [SAGE](https://github.com/iancovert/sage) - For calculating global feature importance using Shapley values.\n  * [SHAP](https://github.com/slundberg/shap) - A game theoretic approach to explain the output of any machine learning model.\n\n\n## Model Lifecycle\n[](https://github.com/kelvins/awesome-mlops/#model-lifecycle)\n_Tools for managing model lifecycle (tracking experiments, parameters and metrics)._\n  * [Aeromancy](https://github.com/quant-aq/aeromancy) - A framework for performing reproducible AI and ML for Weights and Biases.\n  * [Aim](https://github.com/aimhubio/aim) - A super-easy way to record, search and compare 1000s of ML training runs.\n  * [Cascade](https://github.com/Oxid15/cascade) - Library of ML-Engineering tools for rapid prototyping and experiment management.\n  * [Comet](https://github.com/comet-ml/comet-examples) - Track your datasets, code changes, experimentation history, and models.\n  * [Guild AI](https://guild.ai/) - Open source experiment tracking, pipeline automation, and hyperparameter tuning.\n  * [Keepsake](https://github.com/replicate/keepsake) - Version control for machine learning with support to Amazon S3 and Google Cloud Storage.\n  * [Losswise](https://losswise.com) - Makes it easy to track the progress of a machine learning project.\n  * [MLflow](https://mlflow.org/) - Open source platform for the machine learning lifecycle.\n  * [ModelDB](https://github.com/VertaAI/modeldb/) - Open source ML model versioning, metadata, and experiment management.\n  * [Neptune AI](https://neptune.ai/) - The most lightweight experiment management tool that fits any workflow.\n  * [Sacred](https://github.com/IDSIA/sacred) - A tool to help you configure, organize, log and reproduce experiments.\n  * [Weights and Biases](https://github.com/wandb/client) - A tool for visualizing and tracking your machine learning experiments.\n\n\n## Model Serving\n[](https://github.com/kelvins/awesome-mlops/#model-serving)\n_Tools for serving models in production._\n  * [Banana](https://banana.dev) - Host your ML inference code on serverless GPUs and integrate it into your app with one line of code.\n  * [Beam](https://beam.cloud) - Develop on serverless GPUs, deploy highly performant APIs, and rapidly prototype ML models.\n  * [BentoML](https://github.com/bentoml/BentoML) - Open-source platform for high-performance ML model serving.\n  * [BudgetML](https://github.com/ebhy/budgetml) - Deploy a ML inference service on a budget in less than 10 lines of code.\n  * [Cog](https://github.com/replicate/cog) - Open-source tool that lets you package ML models in a standard, production-ready container.\n  * [Cortex](https://www.cortex.dev/) - Machine learning model serving infrastructure.\n  * [Geniusrise](https://docs.geniusrise.ai) - Host inference APIs, bulk inference and fine tune text, vision, audio and multi-modal models.\n  * [Gradio](https://github.com/gradio-app/gradio) - Create customizable UI components around your models.\n  * [GraphPipe](https://oracle.github.io/graphpipe) - Machine learning model deployment made simple.\n  * [Hydrosphere](https://github.com/Hydrospheredata/hydro-serving) - Platform for deploying your Machine Learning to production.\n  * [KFServing](https://github.com/kubeflow/kfserving) - Kubernetes custom resource definition for serving ML models on arbitrary frameworks.\n  * [LocalAI](https://github.com/mudler/LocalAI) - Drop-in replacement REST API that\u2019s compatible with OpenAI API specifications for inferencing.\n  * [Merlin](https://github.com/gojek/merlin) - A platform for deploying and serving machine learning models.\n  * [MLEM](https://github.com/iterative/mlem) - Version and deploy your ML models following GitOps principles.\n  * [Opyrator](https://github.com/ml-tooling/opyrator) - Turns your ML code into microservices with web API, interactive GUI, and more.\n  * [PredictionIO](https://github.com/apache/predictionio) - Event collection, deployment of algorithms, evaluation, querying predictive results via APIs.\n  * [Quix](https://quix.io) - Serverless platform for processing data streams in real-time with machine learning models.\n  * [Rune](https://github.com/hotg-ai/rune) - Provides containers to encapsulate and deploy EdgeML pipelines and applications.\n  * [Seldon](https://www.seldon.io/) - Take your ML projects from POC to production with maximum efficiency and minimal risk.\n  * [Streamlit](https://github.com/streamlit/streamlit) - Lets you create apps for your ML projects with deceptively simple Python scripts.\n  * [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) - Flexible, high-performance serving system for ML models, designed for production.\n  * [TorchServe](https://github.com/pytorch/serve) - A flexible and easy to use tool for serving PyTorch models.\n  * [Triton Inference Server](https://github.com/triton-inference-server/server) - Provides an optimized cloud and edge inferencing solution.\n  * [Vespa](https://github.com/vespa-engine/vespa) - Store, search, organize and make machine-learned inferences over big data at serving time.\n  * [Wallaroo.AI](https://wallaroo.ai/) - A platform for deploying, serving, and optimizing ML models in both cloud and edge environments.\n\n\n## Model Testing & Validation\n[](https://github.com/kelvins/awesome-mlops/#model-testing--validation)\n_Tools for testing and validating models._\n  * [Deepchecks](https://github.com/deepchecks/deepchecks) - Open-source package for validating ML models & data, with various checks and suites.\n  * [Starwhale](https://github.com/star-whale/starwhale) - An MLOps/LLMOps platform for model building, evaluation, and fine-tuning.\n  * [Trubrics](https://github.com/trubrics/trubrics-sdk) - Validate machine learning with data science and domain expert feedback.\n\n\n## Optimization Tools\n[](https://github.com/kelvins/awesome-mlops/#optimization-tools)\n_Optimization tools related to model scalability in production._\n  * [Accelerate](https://github.com/huggingface/accelerate) - A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision.\n  * [Dask](https://dask.org/) - Provides advanced parallelism for analytics, enabling performance at scale for the tools you love.\n  * [DeepSpeed](https://github.com/microsoft/DeepSpeed) - Deep learning optimization library that makes distributed training easy, efficient, and effective.\n  * [Fiber](https://uber.github.io/fiber/) - Python distributed computing library for modern computer clusters.\n  * [Horovod](https://github.com/horovod/horovod) - Distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.\n  * [Mahout](https://mahout.apache.org/) - Distributed linear algebra framework and mathematically expressive Scala DSL.\n  * [MLlib](https://spark.apache.org/mllib/) - Apache Spark's scalable machine learning library.\n  * [Modin](https://github.com/modin-project/modin) - Speed up your Pandas workflows by changing a single line of code.\n  * [Nebullvm](https://github.com/nebuly-ai/nebullvm) - Easy-to-use library to boost AI inference.\n  * [Nos](https://github.com/nebuly-ai/nos) - Open-source module for running AI workloads on Kubernetes in an optimized way.\n  * [Petastorm](https://github.com/uber/petastorm) - Enables single machine or distributed training and evaluation of deep learning models.\n  * [Rapids](https://rapids.ai/index.html) - Gives the ability to execute end-to-end data science and analytics pipelines entirely on GPUs.\n  * [Ray](https://github.com/ray-project/ray) - Fast and simple framework for building and running distributed applications.\n  * [Singa](http://singa.apache.org/en/index.html) - Apache top level project, focusing on distributed training of DL and ML models.\n  * [Tpot](https://github.com/EpistasisLab/tpot) - Automated ML tool that optimizes machine learning pipelines using genetic programming.\n\n\n## Simplification Tools\n[](https://github.com/kelvins/awesome-mlops/#simplification-tools)\n_Tools related to machine learning simplification and standardization._\n  * [Chassis](https://chassisml.io) - Turns models into ML-friendly containers that run just about anywhere.\n  * [Hermione](https://github.com/a3data/hermione) - Help Data Scientists on setting up more organized codes, in a quicker and simpler way.\n  * [Hydra](https://github.com/facebookresearch/hydra) - A framework for elegantly configuring complex applications.\n  * [Koalas](https://github.com/databricks/koalas) - Pandas API on Apache Spark. Makes data scientists more productive when interacting with big data.\n  * [Ludwig](https://github.com/uber/ludwig) - Allows users to train and test deep learning models without the need to write code.\n  * [MLNotify](https://github.com/aporia-ai/mlnotify) - No need to keep checking your training, just one import line and you'll know the second it's done.\n  * [PyCaret](https://pycaret.org/) - Open source, low-code machine learning library in Python.\n  * [Sagify](https://github.com/Kenza-AI/sagify) - A CLI utility to train and deploy ML/DL models on AWS SageMaker.\n  * [Soopervisor](https://github.com/ploomber/soopervisor) - Export ML projects to Kubernetes (Argo workflows), Airflow, AWS Batch, and SLURM.\n  * [Soorgeon](https://github.com/ploomber/soorgeon) - Convert monolithic Jupyter notebooks into maintainable pipelines.\n  * [TrainGenerator](https://github.com/jrieke/traingenerator) - A web app to generate template code for machine learning.\n  * [Turi Create](https://github.com/apple/turicreate) - Simplifies the development of custom machine learning models.\n\n\n## Visual Analysis and Debugging\n[](https://github.com/kelvins/awesome-mlops/#visual-analysis-and-debugging)\n_Tools for performing visual analysis and debugging of ML/DL models._\n  * [Aporia](https://www.aporia.com/) - Observability with customized monitoring and explainability for ML models.\n  * [Arize](https://www.arize.com/) - A free end-to-end ML observability and model monitoring platform.\n  * [Evidently](https://github.com/evidentlyai/evidently) - Interactive reports to analyze ML models during validation or production monitoring.\n  * [Fiddler](https://www.fiddler.ai/) - Monitor, explain, and analyze your AI in production.\n  * [Manifold](https://github.com/uber/manifold) - A model-agnostic visual debugging tool for machine learning.\n  * [NannyML](https://github.com/NannyML/nannyml) - Algorithm capable of fully capturing the impact of data drift on performance.\n  * [Netron](https://github.com/lutzroeder/netron) - Visualizer for neural network, deep learning, and machine learning models.\n  * [Opik](https://github.com/comet-ml/opik) - Evaluate, test, and ship LLM applications with a suite of observability tools.\n  * [Phoenix](https://phoenix.arize.com) - MLOps in a Notebook for troubleshooting and fine-tuning generative LLM, CV, and tabular models.\n  * [Radicalbit](https://github.com/radicalbit/radicalbit-ai-monitoring/) - The open source solution for monitoring your AI models in production.\n  * [Superwise](https://www.superwise.ai) - Fully automated, enterprise-grade model observability in a self-service SaaS platform.\n  * [Whylogs](https://github.com/whylabs/whylogs) - The open source standard for data logging. Enables ML monitoring and observability.\n  * [Yellowbrick](https://github.com/DistrictDataLabs/yellowbrick) - Visual analysis and diagnostic tools to facilitate machine learning model selection.\n\n\n## Workflow Tools\n[](https://github.com/kelvins/awesome-mlops/#workflow-tools)\n_Tools and frameworks to create workflows or pipelines in the machine learning context._\n  * [Argo](https://github.com/argoproj/argo) - Open source container-native workflow engine for orchestrating parallel jobs on Kubernetes.\n  * [Automate Studio](https://www.veritone.com/applications/automate-studio/) - Rapidly build & deploy AI-powered workflows.\n  * [Couler](https://github.com/couler-proj/couler) - Unified interface for constructing and managing workflows on different workflow engines.\n  * [dstack](https://github.com/dstackai/dstack) - An open-core tool to automate data and training workflows.\n  * [Flyte](https://flyte.org/) - Easy to create concurrent, scalable, and maintainable workflows for machine learning.\n  * [Hamilton](https://github.com/dagworks-inc/hamilton) - A scalable general purpose micro-framework for defining dataflows.\n  * [Kale](https://github.com/kubeflow-kale/kale) - Aims at simplifying the Data Science experience of deploying Kubeflow Pipelines workflows.\n  * [Kedro](https://github.com/quantumblacklabs/kedro) - Library that implements software engineering best-practice for data and ML pipelines.\n  * [Luigi](https://github.com/spotify/luigi) - Python module that helps you build complex pipelines of batch jobs.\n  * [Metaflow](https://metaflow.org/) - Human-friendly lib that helps scientists and engineers build and manage data science projects.\n  * [MLRun](https://github.com/mlrun/mlrun) - Generic mechanism for data scientists to build, run, and monitor ML tasks and pipelines.\n  * [Orchest](https://github.com/orchest/orchest/) - Visual pipeline editor and workflow orchestrator with an easy to use UI and based on Kubernetes.\n  * [Ploomber](https://github.com/ploomber/ploomber) - Write maintainable, production-ready pipelines. Develop locally, deploy to the cloud.\n  * [Prefect](https://docs.prefect.io/) - A workflow management system, designed for modern infrastructure.\n  * [VDP](https://github.com/instill-ai/vdp) - An open-source tool to seamlessly integrate AI for unstructured data into the modern data stack.\n  * [Wordware](https://www.wordware.ai) - A web-hosted IDE where non-technical domain experts can build task-specific AI agents.\n  * [ZenML](https://github.com/maiot-io/zenml) - An extensible open-source MLOps framework to create reproducible pipelines.\n\n\n# Resources\n[](https://github.com/kelvins/awesome-mlops/#resources)\nWhere to discover new tools and discuss about existing ones.\n## Articles\n[](https://github.com/kelvins/awesome-mlops/#articles)\n  * [A Tour of End-to-End Machine Learning Platforms](https://databaseline.tech/a-tour-of-end-to-end-ml-platforms/) (Databaseline)\n  * [Continuous Delivery for Machine Learning](https://martinfowler.com/articles/cd4ml.html) (Martin Fowler)\n  * [Machine Learning Operations (MLOps): Overview, Definition, and Architecture](https://arxiv.org/abs/2205.02302) (arXiv)\n  * [MLOps Roadmap: A Complete MLOps Career Guide](https://www.scaler.com/blog/mlops-roadmap/) (Scaler Blogs)\n  * [MLOps: Continuous delivery and automation pipelines in machine learning](https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning) (Google)\n  * [MLOps: Machine Learning as an Engineering Discipline](https://towardsdatascience.com/ml-ops-machine-learning-as-an-engineering-discipline-b86ca4874a3f) (Medium)\n  * [Rules of Machine Learning: Best Practices for ML Engineering](https://developers.google.com/machine-learning/guides/rules-of-ml) (Google)\n  * [The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf) (Google)\n  * [What Is MLOps?](https://blogs.nvidia.com/blog/2020/09/03/what-is-mlops/) (NVIDIA)\n\n\n## Books\n[](https://github.com/kelvins/awesome-mlops/#books)\n  * [Beginning MLOps with MLFlow](https://www.amazon.com/Beginning-MLOps-MLFlow-SageMaker-Microsoft/dp/1484265483) (Apress)\n  * [Building Machine Learning Pipelines](https://www.oreilly.com/library/view/building-machine-learning/9781492053187) (O'Reilly)\n  * [Building Machine Learning Powered Applications](https://www.oreilly.com/library/view/building-machine-learning/9781492045106) (O'Reilly)\n  * [Deep Learning in Production](https://www.amazon.com/gp/product/6180033773) (AI Summer)\n  * [Designing Machine Learning Systems](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956) (O'Reilly)\n  * [Engineering MLOps](https://www.packtpub.com/product/engineering-mlops/9781800562882) (Packt)\n  * [Implementing MLOps in the Enterprise](https://www.oreilly.com/library/view/implementing-mlops-in/9781098136574) (O'Reilly)\n  * [Introducing MLOps](https://www.oreilly.com/library/view/introducing-mlops/9781492083283) (O'Reilly)\n  * [Kubeflow for Machine Learning](https://www.oreilly.com/library/view/kubeflow-for-machine/9781492050117) (O'Reilly)\n  * [Kubeflow Operations Guide](https://www.oreilly.com/library/view/kubeflow-operations-guide/9781492053262) (O'Reilly)\n  * [Machine Learning Design Patterns](https://www.oreilly.com/library/view/machine-learning-design/9781098115777) (O'Reilly)\n  * [Machine Learning Engineering in Action](https://www.manning.com/books/machine-learning-engineering-in-action) (Manning)\n  * [ML Ops: Operationalizing Data Science](https://www.oreilly.com/library/view/ml-ops-operationalizing/9781492074663) (O'Reilly)\n  * [MLOps Engineering at Scale](https://www.manning.com/books/mlops-engineering-at-scale) (Manning)\n  * [MLOps Lifecycle Toolkit](https://link.springer.com/book/10.1007/978-1-4842-9642-4) (Apress)\n  * [Practical Deep Learning at Scale with MLflow](https://www.packtpub.com/product/practical-deep-learning-at-scale-with-mlflow/9781803241333) (Packt)\n  * [Practical MLOps](https://www.oreilly.com/library/view/practical-mlops/9781098103002) (O'Reilly)\n  * [Production-Ready Applied Deep Learning](https://www.packtpub.com/product/production-ready-applied-deep-learning/9781803243665) (Packt)\n  * [Reliable Machine Learning](https://www.oreilly.com/library/view/reliable-machine-learning/9781098106218) (O'Reilly)\n  * [The Machine Learning Solutions Architect Handbook](https://www.packtpub.com/product/the-machine-learning-solutions-architect-handbook/9781801072168) (Packt)\n\n\n## Events\n[](https://github.com/kelvins/awesome-mlops/#events)\n  * [apply() - The ML data engineering conference](https://www.applyconf.com/)\n  * [MLOps Conference - Keynotes and Panels](https://www.youtube.com/playlist?list=PLH8M0UOY0uy6d_n3vEQe6J_gRBUrISF9m)\n  * [MLOps World: Machine Learning in Production Conference](https://mlopsworld.com/)\n  * [NormConf - The Normcore Tech Conference](https://normconf.com/)\n  * [Stanford MLSys Seminar Series](https://mlsys.stanford.edu/)\n\n\n## Other Lists\n[](https://github.com/kelvins/awesome-mlops/#other-lists)\n  * [Applied ML](https://github.com/eugeneyan/applied-ml)\n  * [Awesome AutoML Papers](https://github.com/hibayesian/awesome-automl-papers)\n  * [Awesome AutoML](https://github.com/windmaple/awesome-AutoML)\n  * [Awesome Data Science](https://github.com/academic/awesome-datascience)\n  * [Awesome DataOps](https://github.com/kelvins/awesome-dataops)\n  * [Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning)\n  * [Awesome Game Datasets](https://github.com/leomaurodesenv/game-datasets) (includes AI content)\n  * [Awesome Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)\n  * [Awesome MLOps](https://github.com/visenger/awesome-mlops)\n  * [Awesome Production Machine Learning](https://github.com/EthicalML/awesome-production-machine-learning)\n  * [Awesome Python](https://github.com/vinta/awesome-python)\n  * [Deep Learning in Production](https://github.com/ahkarami/Deep-Learning-in-Production)\n\n\n## Podcasts\n[](https://github.com/kelvins/awesome-mlops/#podcasts)\n  * [Kubernetes Podcast from Google](https://kubernetespodcast.com/)\n  * [Machine Learning \u2013 Software Engineering Daily](https://podcasts.google.com/?feed=aHR0cHM6Ly9zb2Z0d2FyZWVuZ2luZWVyaW5nZGFpbHkuY29tL2NhdGVnb3J5L21hY2hpbmUtbGVhcm5pbmcvZmVlZC8)\n  * [MLOps.community](https://podcasts.google.com/?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8xNzRjYjFiOC9wb2RjYXN0L3Jzcw)\n  * [Pipeline Conversation](https://podcast.zenml.io/)\n  * [Practical AI: Machine Learning, Data Science](https://changelog.com/practicalai)\n  * [This Week in Machine Learning & AI](https://twimlai.com/)\n  * [True ML Talks](https://www.youtube.com/playlist?list=PL4-eEhdXDO5F9Myvh41EeUh7oCgzqFRGk)\n\n\n## Slack\n[](https://github.com/kelvins/awesome-mlops/#slack)\n  * [Kubeflow Workspace](https://kubeflow.slack.com/#/)\n  * [MLOps Community Wokspace](https://mlops-community.slack.com)\n\n\n## Websites\n[](https://github.com/kelvins/awesome-mlops/#websites)\n  * [Feature Stores for ML](http://featurestore.org/)\n  * [Made with ML](https://github.com/GokuMohandas/Made-With-ML)\n  * [ML-Ops](https://ml-ops.org/)\n  * [MLOps Community](https://mlops.community/)\n  * [MLOps Guide](https://mlops-guide.github.io/)\n  * [MLOps Now](https://mlopsnow.com)\n\n\n# Contributing\n[](https://github.com/kelvins/awesome-mlops/#contributing)\nAll contributions are welcome! Please take a look at the [contribution guidelines](https://github.com/kelvins/awesome-mlops/blob/main/CONTRIBUTING.md) first.\n## About\n\ud83d\ude0e A curated list of awesome MLOps tools \n### Topics\n[ data-science ](https://github.com/topics/data-science \"Topic: data-science\") [ machine-learning ](https://github.com/topics/machine-learning \"Topic: machine-learning\") [ awesome ](https://github.com/topics/awesome \"Topic: awesome\") [ ai ](https://github.com/topics/ai \"Topic: ai\") [ ml ](https://github.com/topics/ml \"Topic: ml\") [ machine-learning-engineering ](https://github.com/topics/machine-learning-engineering \"Topic: machine-learning-engineering\") [ mle ](https://github.com/topics/mle \"Topic: mle\") [ mlops ](https://github.com/topics/mlops \"Topic: mlops\")\n### Resources\n[ Readme ](https://github.com/kelvins/awesome-mlops/#readme-ov-file)\n### Code of conduct\n[ Code of conduct ](https://github.com/kelvins/awesome-mlops/#coc-ov-file)\n[ Activity](https://github.com/kelvins/awesome-mlops/activity)\n### Stars\n[ **4.4k** stars](https://github.com/kelvins/awesome-mlops/stargazers)\n### Watchers\n[ **91** watching](https://github.com/kelvins/awesome-mlops/watchers)\n### Forks\n[ **604** forks](https://github.com/kelvins/awesome-mlops/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fkelvins%2Fawesome-mlops&report=kelvins+%28user%29)\n##  [Contributors 67](https://github.com/kelvins/awesome-mlops/graphs/contributors)\n  * [ ![@kelvins](https://avatars.githubusercontent.com/u/1009397?s=64&v=4) ](https://github.com/kelvins)\n  * [ ![@mheriyanto](https://avatars.githubusercontent.com/u/22278148?s=64&v=4) ](https://github.com/mheriyanto)\n  * [ ![@jwmueller](https://avatars.githubusercontent.com/u/1390638?s=64&v=4) ](https://github.com/jwmueller)\n  * [ ![@gleydson404](https://avatars.githubusercontent.com/u/9268819?s=64&v=4) ](https://github.com/gleydson404)\n  * [ ![@emmanuelraj7](https://avatars.githubusercontent.com/u/17778830?s=64&v=4) ](https://github.com/emmanuelraj7)\n  * [ ![@Oxid15](https://avatars.githubusercontent.com/u/31771351?s=64&v=4) ](https://github.com/Oxid15)\n  * [ ![@akashg3627](https://avatars.githubusercontent.com/u/67226124?s=64&v=4) ](https://github.com/akashg3627)\n  * [ ![@romaup](https://avatars.githubusercontent.com/u/95645411?s=64&v=4) ](https://github.com/romaup)\n  * [ ![@anayden](https://avatars.githubusercontent.com/u/17383?s=64&v=4) ](https://github.com/anayden)\n  * [ ![@idomic](https://avatars.githubusercontent.com/u/11596985?s=64&v=4) ](https://github.com/idomic)\n  * [ ![@safoinme](https://avatars.githubusercontent.com/u/34200873?s=64&v=4) ](https://github.com/safoinme)\n  * [ ![@anmorgan24](https://avatars.githubusercontent.com/u/86856445?s=64&v=4) ](https://github.com/anmorgan24)\n  * [ ![@Paras-96](https://avatars.githubusercontent.com/u/169983983?s=64&v=4) ](https://github.com/Paras-96)\n  * [ ![@ixaxaar](https://avatars.githubusercontent.com/u/144122?s=64&v=4) ](https://github.com/ixaxaar)\n\n\n[+ 53 contributors](https://github.com/kelvins/awesome-mlops/graphs/contributors)\n## Languages\n  * [ Python 100.0% ](https://github.com/kelvins/awesome-mlops/search?l=python)\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary\n\nA curated list of **awesome MLOps tools** covering various categories such as **AutoML**, **CI/CD**, **Data Management**, **Model Serving**, and more. It includes resources for articles, books, events, and podcasts, aimed at enhancing machine learning operations and collaboration.\n```"
  },
  {
    "instruction": "![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61f958575ff62d320d46701e_closeicon.png)\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\nKindly note that during the maintenance window, app.hopsworks.ai will not be accessible.\n5\n[View the Changes](https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes)\n![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61f958575ff62d320d46701e_closeicon.png)\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\nKindly note that during the maintenance window, app.hopsworks.ai will not be accessible.\n5\n[View the Changes](https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes)\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\n[Contact](https://www.hopsworks.ai/contact/main)[Login](https://app.hopsworks.ai)[![Github Mark](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6418216a570b0da3d471661a_icons8-slack-new.svg)](https://join.slack.com/t/public-hopsworks/shared_invite/zt-1uf21vitz-rhHKNdIf8GEiOf1EJ6Wzsw)[![Github Mark](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/62261cde4669f63d3880938d_github.svg)](https://github.com/logicalclocks/hopsworks)[![linkedin logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/company/hopsworks/)[![Twitter icon](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/66a346ea27ec6d7c0e354747_icons8-twitter%20\\(1\\).svg)](https://twitter.com/hopsworks)\n[![Untitled UI logotext](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6202a13e7cafec5553703f6b_logo.svg)![Logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/666c3cc1cfc4741e6b2d9fe6_untitled-ui-logo.png)](https://www.hopsworks.ai/)\nProduct\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/5fad49f715203ed9d66fc1b0_Hops%20Icon%20Green.png)Hopsworks EnterpriseFull edition of Hopsworks, high levels of SLAs and support.](https://www.hopsworks.ai/try)[IntegrationsLeverage your existing data sources and tools.](https://www.hopsworks.ai/integrations)[ExamplesGet up and running on new features and techniques.](https://www.hopsworks.ai/hopsworks-examples)[FAQAll you need to know about Hopsworks.](https://www.hopsworks.ai/frequently-asked-questions)\n[Hopsworks On-PremisesManage everything securely within your data center.](https://www.hopsworks.ai/product-capabilities/feature-store-on-premises)[Performance & High AvailabilityHighest performance requirements in the industry.](https://www.hopsworks.ai/product-capabilities/operational-performance-and-high-availability)[Feature Engineering in PythonPython-first collaborative environment.](https://www.hopsworks.ai/product-capabilities/feature-engineering-in-python)[Other capabilitiesRead about our extended platform capabilities.](https://www.hopsworks.ai/product-capabilities)\nSolutions\nFor your Team\n[Technical Stakeholders](https://www.hopsworks.ai/solutions/teams)[Machine Learning Engineers](https://www.hopsworks.ai/solutions/teams?tab=ml-engineers)[Data Engineers](https://www.hopsworks.ai/solutions/teams?tab=data-engineers)[Data Scientists](https://www.hopsworks.ai/solutions/teams?tab=data-scientists)[DevOps](https://www.hopsworks.ai/solutions/teams?tab=devops)[Architects](https://www.hopsworks.ai/solutions/teams?tab=architects)[Non-Technical Stakeholders](https://www.hopsworks.ai/solutions/teams?tab=non-technical)[Project Managers ](https://www.hopsworks.ai/solutions/teams?tab=non-technical)\nFor your Industry\n[Defense and Law Enforcement](https://www.hopsworks.ai/solutions/industry/defense-and-law-enforcement)\n[FSI](https://www.hopsworks.ai/solutions/industry/fsi)\n[Online Retail & E-commerce](https://www.hopsworks.ai/solutions/industry/online-retail-and-e-commerce)\n[Public Sector](https://www.hopsworks.ai/solutions/industry/public-sector)\n[Research and Healthcare](https://www.hopsworks.ai/solutions/industry/research-and-healthcare)\n[iGaming](https://www.hopsworks.ai/solutions/industry/i-gaming)\n[All Solutions](https://www.hopsworks.ai/solutions/all)\nUse Cases\n[Generative AI](https://www.hopsworks.ai/use-case/fine-tuning-llms-rag-for-genai)[Real-time Fraud Detection](https://www.hopsworks.ai/use-case/realtime-fraud-detection)[Hopsworks Medical Copilot](https://www.hopscopilot.com/)\n[CustomersExplore how our customers leverage Hopsworks.](https://www.hopsworks.ai/customers)\n[Pricing](https://www.hopsworks.ai/pricing)[Blog](https://www.hopsworks.ai/blog)[Pricing](https://www.hopsworks.ai/pricing)[Blog](https://www.hopsworks.ai/blog)\nResources\n[MLOps DictionaryComprehensive terminology guide for ML solutions.](https://www.hopsworks.ai/mlops-dictionary)[DocumentationDetailed information to help you effectively utilize Hopsworks.](https://docs.hopsworks.ai/latest/)[Research PapersDiscover how our research is driving innovation.](https://www.hopsworks.ai/research-papers)[CommunityJoin our community and get all your questions answered. ](https://community.hopsworks.ai/)\n[EventsOnline & Offline sessions and workshops. ](https://www.hopsworks.ai/events)[AcademyEverything about ML Systems, and the Hopsworks platform.](https://www.hopsworks.ai/academy)[Feature Store ComparisonIn-depth comparisons of feature stores highlighting key features.](https://www.hopsworks.ai/product-comparison/sagemaker)[FAQ: EU AI ActA complete guide to The EU AI Act.](https://www.hopsworks.ai/faq-eu-ai-act)\nCompany\n[About usLearn more about our team. ](https://www.hopsworks.ai/about-us)[NewsThe latest industry news, updates and info.](https://www.hopsworks.ai/news)[Security & ComplianceRobust security and compliance with industry standards.](https://www.hopsworks.ai/security-compliance)\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/66a0b13c473a71304470c35a_oreilly_logo_mark_red.svg)Book](https://www.hopsworks.ai/lp/oreilly-book-building-ml-systems-with-a-feature-store)[Benchmarks](https://www.hopsworks.ai/index#performance)\n[// 5-minute Interviews](https://www.hopsworks.ai/blog-categories/5-minute-interviews)\n[// AI](https://www.hopsworks.ai/blog-categories/ai)\n[// Benchmark](https://www.hopsworks.ai/blog-categories/benchmark)\n[// Data Engineering](https://www.hopsworks.ai/blog-categories/data-engineering)\n[// Data Science](https://www.hopsworks.ai/blog-categories/data-science)\n[// Feature Store](https://www.hopsworks.ai/blog-categories/feature-stores)\n[// MLOps](https://www.hopsworks.ai/blog-categories/mlops)\n[// SIGMOD 2024](https://www.hopsworks.ai/blog-categories/sigmod-2024)\n[![The 10 Fallacies of MLOps](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bdbdb52730076471457479_MLOps%20Fallacies%20Blog.png)MLOpsFebruary 25, 202520 minReadThe 10 Fallacies of MLOpsMLOps fallacies can slow AI deployment and add complexity. This blog breaks down 10 common misconceptions, their impact, and how to avoid them to build scalable, production-ready AI systems efficient.Jim Dowling](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)\n[Migrating from AWS to a European Cloud - How We Cut Costs by 62%This post describes how we successfully migrated our serverless offering from AWS US-East to OVHCloud North America, reducing our monthly spend from $8,000 to $3,000 with no loss in service quality. Jim Dowling](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)\n[Migrating Hopsworks to KubernetesNearly a year ago, the Hopsworks team embarked on a journey to migrate its infrastructure to Kubernetes. In this article we describe three main pillars of our Kubernetes migration.Javier Cabrera](https://www.hopsworks.ai/post/migrating-hopsworks-to-kubernetes)\n[Introducing the AI LakehouseWe describe the capabilities that need to be added to Lakehouse to make it an AI Lakehouse that can support building and operating AI-enabled batch and real-time applications as well LLM applications.Jim Dowling](https://www.hopsworks.ai/post/the-ai-lakehouse)\n### // SIGMOD 2024\nA series of articles describing concepts and results from the [**SIGMOD 2024 research paper on the Hopsworks Feature Store**](https://www.hopsworks.ai/research-papers/the-hopsworks-feature-store-for-machine-learning)\n[View all posts](https://www.hopsworks.ai/blog/)\n[![Modularity and Composability for AI Systems with AI Pipelines and Shared Storage](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/667acae0ae637d5bb9a72ea2_modularity%20%26%20composability.png)](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)[Feature StoreJune 25, 202425 minReadModularity and Composability for AI Systems with AI Pipelines and Shared StorageWe present a unified software architecture for batch, real-time, and LLM AI systems that is based on a shared storage layer and a decomposition of machine learning pipelines. ](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)\nJim Dowling\n[![The Taxonomy for Data Transformations in AI Systems](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/668d97122ea716ed1633a7f7_PR%20Blog%202_Taxonomy.png)](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)[Feature StoreJuly 1, 202430 minReadThe Taxonomy for Data Transformations in AI SystemsThis article introduces a taxonomy for data transformations in AI applications that is fundamental for any AI system that wants to reuse feature data in more than one model. ](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)\nManu Joseph\n[![The Journey from Star Schema to Snowflake Schema in the Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66869f1f4f75a7d91c67775f_PR%20Blog%203_snowflake%20schema.png)](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store)[Feature StoreJuly 8, 202414 minReadThe Journey from Star Schema to Snowflake Schema in the Feature StoreIn this article we introduce the snowflake schema data model for feature stores, and show how it helps you include more features to make better predictions ](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store)\nDavit Bzhalava\n[![The Feature Store Makes Your Data Warehouse Easy to Use for AI](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/668bf0ab60826eb58e0d6af1_PR%20Blog%204_data%20warehouse.png)](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai)[Feature StoreJuly 10, 202424 minReadThe Feature Store Makes Your Data Warehouse Easy to Use for AIIn this article, we cover the added value of a feature store over a data warehouse when managing offline data for AI. ](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai)\nJim Dowling\n[![From Lakehouse to AI Lakehouse with a Python-Native Query Engine](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/669fa7eaa10b0dd15c46ab80_Lakehouse%20PQE%20Blog.png)](https://www.hopsworks.ai/post/from-lakehouse-to-ai-lakehouse-with-a-python-native-query-engine)[Data EngineeringJuly 23, 20249 minReadFrom Lakehouse to AI Lakehouse with a Python-Native Query EngineRead how Hopsworks generates temporal queries from Python code, and how a native query engine built on Arrow can massively outperform JDBC/ODBC APIs.](https://www.hopsworks.ai/post/from-lakehouse-to-ai-lakehouse-with-a-python-native-query-engine)\nJim Dowling\n[![RonDB: A Real-Time Database for Real-Time AI Systems](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b207bd4cf6636717910f8b_Real%20time%20Database%20Blog.png)](https://www.hopsworks.ai/post/rondb-a-real-time-database-for-real-time-ai-systems)[SIGMOD 2024July 25, 20248 minReadRonDB: A Real-Time Database for Real-Time AI SystemsLearn more about how Hopsworks (RonDB) outperforms AWS Sagemaker and GCP Vertex in latency for real-time AI databases, based on a peer-reviewed SIGMOD 2024 benchmark.](https://www.hopsworks.ai/post/rondb-a-real-time-database-for-real-time-ai-systems)\nMikael Ronstr\u00f6m\n[![Reproducible Data for the AI Lakehouse](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b1d5bb06a34c527b352841_reproducible%20data.png)](https://www.hopsworks.ai/post/reproducible-data-for-the-ai-lakehouse)[SIGMOD 2024August 6, 202413 minReadReproducible Data for the AI LakehouseWe present how Hopsworks leverages its time-travel capabilities for feature groups to support reproducible creation of training datasets using metadata.](https://www.hopsworks.ai/post/reproducible-data-for-the-ai-lakehouse)\nJim Dowling\n## Explore by Category\n[SIGMOD 2024](https://www.hopsworks.ai/blog-categories/sigmod-2024)\n[AI](https://www.hopsworks.ai/blog-categories/ai)\n[5-minute Interviews](https://www.hopsworks.ai/blog-categories/5-minute-interviews)\n[Benchmark](https://www.hopsworks.ai/blog-categories/benchmark)\n[Data Science](https://www.hopsworks.ai/blog-categories/data-science)\n[Data Engineering](https://www.hopsworks.ai/blog-categories/data-engineering)\n[![The 10 Fallacies of MLOps](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bdbdb52730076471457479_MLOps%20Fallacies%20Blog.png)](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)[MLOpsFebruary 25, 202520 minReadThe 10 Fallacies of MLOpsMLOps fallacies can slow AI deployment and add complexity. This blog breaks down 10 common misconceptions, their impact, and how to avoid them to build scalable, production-ready AI systems efficient.](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)\nJim Dowling\n[![Hopsworks AI Lakehouse: The Power of Integrated MLOps Components](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bc4ae4464caed2e2bff646_AI%20lakehouse%20Blog.png)](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)[AIFebruary 24, 20259 minReadHopsworks AI Lakehouse: The Power of Integrated MLOps ComponentsHopsworks AI Lakehouse simplifies MLOps by eliminating integration overhead with a pre-integrated, modular platform that connects seamlessly to existing AI ecosystems. ](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)\nRik Van Bruggen\n### // Latest Posts\n[![Migrating from AWS to a European Cloud - How We Cut Costs by 62%](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d4012dfee4417ef189aa94_AWS%20to%20OVH%20blog%20v2.png)](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)[Data EngineeringMarch 11, 202510minReadMigrating from AWS to a European Cloud - How We Cut Costs by 62%This post describes how we successfully migrated our serverless offering from AWS US-East to OVHCloud North America, reducing our monthly spend from $8,000 to $3,000 with no loss in service quality. ](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)\nJim Dowling\n[![Hopsworks AI Lakehouse: The Power of Integrated MLOps Components](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bc4ae4464caed2e2bff646_AI%20lakehouse%20Blog.png)](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)[AIFebruary 24, 20259 minReadHopsworks AI Lakehouse: The Power of Integrated MLOps ComponentsHopsworks AI Lakehouse simplifies MLOps by eliminating integration overhead with a pre-integrated, modular platform that connects seamlessly to existing AI ecosystems. ](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)\nRik Van Bruggen\n[![Unlocking the Power of AI in Government](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/679b54ad11fe440d84e8ece6_Government%20ai.png)](https://www.hopsworks.ai/post/the-power-of-ai-in-government)[AIJanuary 31, 20259 minReadUnlocking the Power of AI in GovernmentUnlock the transformative power of AI for government, enabling agencies to navigate the challenges, realize the benefits, and build a future-oriented public sector. ](https://www.hopsworks.ai/post/the-power-of-ai-in-government)\nRik Van Bruggen\n### // Feature Store\n[![](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d9dab74eb40a787c6b7c5e_why%20copy.jpg)](https://www.hopsworks.ai/post/why-do-you-need-a-feature-store)[Feature StoreAugust 11, 202320 minReadWhy Do You Need a Feature Store? Discover the power of feature stores in modern machine learning systems and how they bridge the gap between model development and production. ](https://www.hopsworks.ai/post/why-do-you-need-a-feature-store)\nLex Avstreikh\n[![](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65113a725a93dc40f6d74c85_duckdb_thumbnail.png)](https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb)[Feature StoreAugust 9, 202313 minReadFaster reading from the Lakehouse to Python with DuckDB/ArrowFlightIn this article, we outline how we leveraged ArrowFlight with DuckDB to build a new service that massively improves the performance of Python clients reading from lakehouse data in the Feature Store](https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb)\nTill D\u00f6hmen\n[![](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61bb72f4fbcc74f4c86eb3f8_61b8aed0674af6312e2c05b2_missingdatalayer.jpg)](https://www.hopsworks.ai/post/feature-store-the-missing-data-layer-in-ml-pipelines)[Feature StoreFebruary 2, 202330 minReadFeature Store: The missing data layer for Machine Learning pipelines?In this blog, we discuss the state-of-the-art in data management and machine learning pipelines (within the wider field of MLOps) and present the first open-source feature store, Hopsworks.](https://www.hopsworks.ai/post/feature-store-the-missing-data-layer-in-ml-pipelines)\nJim Dowling\n[View More](https://www.hopsworks.ai/blog-categories/feature-stores)\n### // MLOps\n[![The 10 Fallacies of MLOps](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bdbdb52730076471457479_MLOps%20Fallacies%20Blog.png)](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)[MLOpsFebruary 25, 202520 minReadThe 10 Fallacies of MLOpsMLOps fallacies can slow AI deployment and add complexity. This blog breaks down 10 common misconceptions, their impact, and how to avoid them to build scalable, production-ready AI systems efficient.](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)\nJim Dowling\n[![From MLOps to ML Systems with Feature/Training/Inference Pipelines](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65019f581267ff943c3931a6_mlops%20to%20ml%20systems_thumbnail.png)](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)[MLOpsSeptember 13, 202325 minReadFrom MLOps to ML Systems with Feature/Training/Inference PipelinesWe explain a new framework for ML systems as three independent ML pipelines: feature pipelines, training pipelines, and inference pipelines, creating a unified MLOps architecture.](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)\nJim Dowling\n[![MLOps vs. DevOps: Best Practices, Challenges and Differences](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d355fb6ba1878a9afd9066_Mlops%20Devops%20Blog.png)](https://www.hopsworks.ai/post/mlops-vs-devops-best-practices-challenges-and-differences)[MLOpsJuly 11, 202317ReadMLOps vs. DevOps: Best Practices, Challenges and DifferencesExplore the convergence of MLOps and DevOps. Learn about their purposes, differences, and areas of integration and discover best practices, challenges, and their future potential. ](https://www.hopsworks.ai/post/mlops-vs-devops-best-practices-challenges-and-differences)\nPrithivee Ramalingam\n[View More](https://www.hopsworks.ai/blog-categories/mlops)\n### // Data Engineering\n[![Migrating from AWS to a European Cloud - How We Cut Costs by 62%](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d4012dfee4417ef189aa94_AWS%20to%20OVH%20blog%20v2.png)](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)[Data EngineeringMarch 11, 202510minReadMigrating from AWS to a European Cloud - How We Cut Costs by 62%This post describes how we successfully migrated our serverless offering from AWS US-East to OVHCloud North America, reducing our monthly spend from $8,000 to $3,000 with no loss in service quality. ](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)\nJim Dowling\n[![Pandas2 and Polars for Feature Engineering](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/652709e8e9ee4bd6e7245f2d_pandas%26polars_thumbnail.png)](https://www.hopsworks.ai/post/pandas2-and-polars-for-feature-engineering)[Data EngineeringNovember 6, 202320 minReadPandas2 and Polars for Feature EngineeringWe review Python libraries, such as Pandas, Pandas2 and Polars, for Feature Engineering, evaluate their performance and explore how they power machine learning use cases. ](https://www.hopsworks.ai/post/pandas2-and-polars-for-feature-engineering)\nHaziqa Sajid\n[![Feature Engineering with Apache Airflow](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d3a7213fc689589547090e_airflow.png)](https://www.hopsworks.ai/post/feature-engineering-with-apache-airflow)[Data EngineeringSeptember 4, 202318 minReadFeature Engineering with Apache AirflowUnlock the power of Apache Airflow in the context of feature engineering. We will delve into building a feature pipeline using Airflow, focusing on two tasks: feature binning and aggregations.](https://www.hopsworks.ai/post/feature-engineering-with-apache-airflow)\nPrithivee Ramalingam\n[View More](https://www.hopsworks.ai/blog-categories/data-engineering)\n### // Data Science\n[![Facebook Prophet for Time-Series Machine Learning](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/651eac8c99616d72197da06a_facebook%20prophet_thumbnail.png)](https://www.hopsworks.ai/post/facebook-prophet-for-time-series-machine-learning)[Data ScienceOctober 5, 202317minReadFacebook Prophet for Time-Series Machine LearningTime-series data consists of records in chronological order and is crucial for forecasting trends and patterns. In this blog, we take a look at how to use Facebook Prophet for time-series forecasting.](https://www.hopsworks.ai/post/facebook-prophet-for-time-series-machine-learning)\nKais Laribi \n[![Elasticsearch is dead, long live Open Distro for Elasticsearch](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61e697708af5d9c46a0395a6_DFS.png)](https://www.hopsworks.ai/post/open-distro-for-elasticsearch-dynamic-support-hopsworks)[Data ScienceJanuary 14, 202113 minReadElasticsearch is dead, long live Open Distro for ElasticsearchIn this blog, we describe how we leverage the authentication and authorization support in Open Distro for Elasticsearch to make elasticsearch a project-based multi-tenant service in Hopsworks.](https://www.hopsworks.ai/post/open-distro-for-elasticsearch-dynamic-support-hopsworks)\nMahmoud Ismail\n[![Deep Learning for Anti-Money Laundering with a feature store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61b9d4f49a1b41665ee189ef_005%20\\(1\\).png)](https://www.hopsworks.ai/post/ai-deep-learning-for-fraud-and-aml)[Data ScienceNovember 27, 201918 minReadDeep Learning for Anti-Money Laundering with a feature storeDeep learning is now the state-of-the-art technique for identifying financial transactions suspected of money laundering. It delivers a lower number of false positives and with higher accuracy.](https://www.hopsworks.ai/post/ai-deep-learning-for-fraud-and-aml)\nJim Dowling\n[View More](https://www.hopsworks.ai/blog-categories/data-science)\n### // 5-minute Interviews\n[![5-minute interview Tales Marra](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66ffe16434403e12ef589291_5-min%20interviews%20\\(1\\).png)](https://www.hopsworks.ai/post/5-minute-interview-tales-marra-assessfirst)[5-minute InterviewsOctober 4, 20245 minRead5-minute interview Tales MarraNext up in our lineup is Tales Marra, Machine Learning Engineer at AssessFirst. ](https://www.hopsworks.ai/post/5-minute-interview-tales-marra-assessfirst)\nHopsworks Team\n[![5-minute interview Niels Nagl\u00e9 and Joop Snijder](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66f6a05da8e52f53288eb8d1_5-min%20interviews%20\\(1\\).png)](https://www.hopsworks.ai/post/5-minute-interview-niels-nagle-and-joop-snijder-info-support)[5-minute InterviewsSeptember 27, 20245 min Read5-minute interview Niels Nagl\u00e9 and Joop SnijderThis week Rik met with Niels Nagl\u00e9, Area Lead Data & AI at Info Support, and Joop Snijder, CTO at Aigency, for a quick chat. ](https://www.hopsworks.ai/post/5-minute-interview-niels-nagle-and-joop-snijder-info-support)\nHopsworks Team\n[![5-minute interview David Colls](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66d8618ae791fd129c5d1a30_5-min%20interviews%20\\(1\\).png)](https://www.hopsworks.ai/post/5-minute-interview-with-david-colls-nextdata)[5-minute InterviewsSeptember 4, 20246 minRead5-minute interview David CollsJoin Rik as he chats with David Colls, Founding Data Scientist at Nextdata, about Data & AI, the evolution of machine learning, and effective tech practices.](https://www.hopsworks.ai/post/5-minute-interview-with-david-colls-nextdata)\nHopsworks Team\n[View More](https://www.hopsworks.ai/blog-categories/5-minute-interviews)\n### // All Categories\n[![Migrating from AWS to a European Cloud - How We Cut Costs by 62%](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d4012dfee4417ef189aa94_AWS%20to%20OVH%20blog%20v2.png)](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)[Data EngineeringMarch 11, 202510minReadMigrating from AWS to a European Cloud - How We Cut Costs by 62%This post describes how we successfully migrated our serverless offering from AWS US-East to OVHCloud North America, reducing our monthly spend from $8,000 to $3,000 with no loss in service quality. ](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)\nJim Dowling\n[![The 10 Fallacies of MLOps](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bdbdb52730076471457479_MLOps%20Fallacies%20Blog.png)](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)[MLOpsFebruary 25, 202520 minReadThe 10 Fallacies of MLOpsMLOps fallacies can slow AI deployment and add complexity. This blog breaks down 10 common misconceptions, their impact, and how to avoid them to build scalable, production-ready AI systems efficient.](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)\nJim Dowling\n[![Hopsworks AI Lakehouse: The Power of Integrated MLOps Components](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bc4ae4464caed2e2bff646_AI%20lakehouse%20Blog.png)](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)[AIFebruary 24, 20259 minReadHopsworks AI Lakehouse: The Power of Integrated MLOps ComponentsHopsworks AI Lakehouse simplifies MLOps by eliminating integration overhead with a pre-integrated, modular platform that connects seamlessly to existing AI ecosystems. ](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)\nRik Van Bruggen\n[![Unlocking the Power of AI in Government](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/679b54ad11fe440d84e8ece6_Government%20ai.png)](https://www.hopsworks.ai/post/the-power-of-ai-in-government)[AIJanuary 31, 20259 minReadUnlocking the Power of AI in GovernmentUnlock the transformative power of AI for government, enabling agencies to navigate the challenges, realize the benefits, and build a future-oriented public sector. ](https://www.hopsworks.ai/post/the-power-of-ai-in-government)\nRik Van Bruggen\n[![Optimizing AI Costs](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/679b3d691b2ec03b7e664e84_optimizing%20AI%20costs.png)](https://www.hopsworks.ai/post/optimizing-ai-costs)[AIJanuary 30, 20259 minReadOptimizing AI CostsThis post will explore several key strategies to mitigate AI related costs and highlight how Hopsworks can facilitate these optimizations.](https://www.hopsworks.ai/post/optimizing-ai-costs)\nRik Van Bruggen\n[![Revolutionizing Telecom with AI-powered Customer and Employee Experiences in 2025](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67978dddb5d0422694708b94_telecom%20ai.png)](https://www.hopsworks.ai/post/revolutionizing-telecom-with-ai-powered-customer-and-employee-experiences-in-2025)[AIJanuary 27, 202510 minReadRevolutionizing Telecom with AI-powered Customer and Employee Experiences in 2025The ever changing telecommunications industry is on the verge of another major transformation, driven by a convergence of powerful trends. Here are some of the biggest AI trends in Telecom in 2025. ](https://www.hopsworks.ai/post/revolutionizing-telecom-with-ai-powered-customer-and-employee-experiences-in-2025)\nRik Van Bruggen\n[![How AI Will Redefine Retail in 2025](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/677f9c450ce2d25e39b3485a_retail%20ai.png)](https://www.hopsworks.ai/post/how-ai-will-redefine-retail-in-2025)[AIJanuary 9, 20258 minReadHow AI Will Redefine Retail in 2025In this blog post, we wanted to share a less technical, more business-level perspective on AI in retail and how retailers can adapt to a changing e-commerce landscape in 2025. ](https://www.hopsworks.ai/post/how-ai-will-redefine-retail-in-2025)\nRik Van Bruggen\n[![Amazon FSx for NetApp ONTAP interoperability test in a Hopsworks 4.x Deployment](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6777a9f7b803f755bcb9aec0_netapp.png)](https://www.hopsworks.ai/post/amazon-fsx-for-netapp-ontap-interoperability-test-in-a-hopsworks-4-x-deployment)[BenchmarkJanuary 3, 202523 minReadAmazon FSx for NetApp ONTAP interoperability test in a Hopsworks 4.x DeploymentBy following this tutorial, you can evaluate the interoperability between Hopsworks 4.x and Amazon FSx for NetApp ONTAP](https://www.hopsworks.ai/post/amazon-fsx-for-netapp-ontap-interoperability-test-in-a-hopsworks-4-x-deployment)\nJavier Cabrera\n[![Breaking Down FSI Regulations for AI in 2025](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/675fe310a23998da10110fbb_FSI%20regulations.png)](https://www.hopsworks.ai/post/breaking-down-fsi-regulations-for-ai-in-2025)[AIDecember 13, 202419 minReadBreaking Down FSI Regulations for AI in 2025This blog explores how Hopsworks can support FSIs in addressing key regulations, including DORA, AMLA, PSD3, and more. ](https://www.hopsworks.ai/post/breaking-down-fsi-regulations-for-ai-in-2025)\nRik Van Bruggen\n[![Hopsworks PKI: The Unseen Hero](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6757f688e7e67c915a6abd08_Hopsworks%20PKI.png)](https://www.hopsworks.ai/post/hopsworks-pki-the-unseen-hero)[Feature StoreDecember 9, 202416 minReadHopsworks PKI: The Unseen HeroIn this article we explore how our Public Key Infrastructure has changed over the years coming to its current form, a Kubernetes first-class citizen. ](https://www.hopsworks.ai/post/hopsworks-pki-the-unseen-hero)\nAntonios Kouzoupis\n[![Air-gapped Installations in Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/673c9f546015f6cf7d115de0_air%20gapped.png)](https://www.hopsworks.ai/post/air-gapped-installations-in-hopsworks)[Data EngineeringNovember 19, 202416 minReadAir-gapped Installations in HopsworksIn this tutorial, we walk you through how we install air-gapped environments in Hopsworks and how we extended this process to test air-gapped installations as well.](https://www.hopsworks.ai/post/air-gapped-installations-in-hopsworks)\nJavier Cabrera\n[![Migrating Hopsworks to Kubernetes](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67163b4a1b6ac639d242a23a_Hopsworks%20to%20Kubernetes.png)](https://www.hopsworks.ai/post/migrating-hopsworks-to-kubernetes)[Feature StoreOctober 21, 202412 minReadMigrating Hopsworks to KubernetesNearly a year ago, the Hopsworks team embarked on a journey to migrate its infrastructure to Kubernetes. In this article we describe three main pillars of our Kubernetes migration.](https://www.hopsworks.ai/post/migrating-hopsworks-to-kubernetes)\nJavier Cabrera\n[![A Year of Insights with the MLOps & LLMOps Dictionary](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66f65ada28726b20f5cb3b5f_dictionary%20thumbnail.png)](https://www.hopsworks.ai/post/a-year-of-insights-with-the-mlops-llmops-dictionary)[MLOpsSeptember 26, 202410 minReadA Year of Insights with the MLOps & LLMOps DictionaryIn this blog post, we have selected the 25 most-read dictionary entries from the MLOps and LLMOps Dictionary to highlight key trends and lessons in AI.](https://www.hopsworks.ai/post/a-year-of-insights-with-the-mlops-llmops-dictionary)\nCarolin Svenberg \n[![Introducing the AI Lakehouse](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66d6c9970cfc8086e7e1046b_The%20AI%20Lakehouse.png)](https://www.hopsworks.ai/post/the-ai-lakehouse)[Feature StoreSeptember 2, 202430 minReadIntroducing the AI LakehouseWe describe the capabilities that need to be added to Lakehouse to make it an AI Lakehouse that can support building and operating AI-enabled batch and real-time applications as well LLM applications.](https://www.hopsworks.ai/post/the-ai-lakehouse)\nJim Dowling\n[![Reproducible Data for the AI Lakehouse](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b1d5bb06a34c527b352841_reproducible%20data.png)](https://www.hopsworks.ai/post/reproducible-data-for-the-ai-lakehouse)[SIGMOD 2024August 6, 202413 minReadReproducible Data for the AI LakehouseWe present how Hopsworks leverages its time-travel capabilities for feature groups to support reproducible creation of training datasets using metadata.](https://www.hopsworks.ai/post/reproducible-data-for-the-ai-lakehouse)\nJim Dowling\n[![RonDB: A Real-Time Database for Real-Time AI Systems](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b207bd4cf6636717910f8b_Real%20time%20Database%20Blog.png)](https://www.hopsworks.ai/post/rondb-a-real-time-database-for-real-time-ai-systems)[SIGMOD 2024July 25, 20248 minReadRonDB: A Real-Time Database for Real-Time AI SystemsLearn more about how Hopsworks (RonDB) outperforms AWS Sagemaker and GCP Vertex in latency for real-time AI databases, based on a peer-reviewed SIGMOD 2024 benchmark.](https://www.hopsworks.ai/post/rondb-a-real-time-database-for-real-time-ai-systems)\nMikael Ronstr\u00f6m\n[![From Lakehouse to AI Lakehouse with a Python-Native Query Engine](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/669fa7eaa10b0dd15c46ab80_Lakehouse%20PQE%20Blog.png)](https://www.hopsworks.ai/post/from-lakehouse-to-ai-lakehouse-with-a-python-native-query-engine)[Data EngineeringJuly 23, 20249 minReadFrom Lakehouse to AI Lakehouse with a Python-Native Query EngineRead how Hopsworks generates temporal queries from Python code, and how a native query engine built on Arrow can massively outperform JDBC/ODBC APIs.](https://www.hopsworks.ai/post/from-lakehouse-to-ai-lakehouse-with-a-python-native-query-engine)\nJim Dowling\n[![The Feature Store Makes Your Data Warehouse Easy to Use for AI](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/668bf0ab60826eb58e0d6af1_PR%20Blog%204_data%20warehouse.png)](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai)[Feature StoreJuly 10, 202424 minReadThe Feature Store Makes Your Data Warehouse Easy to Use for AIIn this article, we cover the added value of a feature store over a data warehouse when managing offline data for AI. ](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai)\nJim Dowling\n[![The Journey from Star Schema to Snowflake Schema in the Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66869f1f4f75a7d91c67775f_PR%20Blog%203_snowflake%20schema.png)](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store)[Feature StoreJuly 8, 202414 minReadThe Journey from Star Schema to Snowflake Schema in the Feature StoreIn this article we introduce the snowflake schema data model for feature stores, and show how it helps you include more features to make better predictions ](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store)\nDavit Bzhalava\n[![The Taxonomy for Data Transformations in AI Systems](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/668d97122ea716ed1633a7f7_PR%20Blog%202_Taxonomy.png)](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)[Feature StoreJuly 1, 202430 minReadThe Taxonomy for Data Transformations in AI SystemsThis article introduces a taxonomy for data transformations in AI applications that is fundamental for any AI system that wants to reuse feature data in more than one model. ](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)\nManu Joseph\n[![Modularity and Composability for AI Systems with AI Pipelines and Shared Storage](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/667acae0ae637d5bb9a72ea2_modularity%20%26%20composability.png)](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)[Feature StoreJune 25, 202425 minReadModularity and Composability for AI Systems with AI Pipelines and Shared StorageWe present a unified software architecture for batch, real-time, and LLM AI systems that is based on a shared storage layer and a decomposition of machine learning pipelines. ](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)\nJim Dowling\n[![Building a Cheque Fraud Detection and Explanation AI System using a fine-tuned LLM](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/663dcc9e88ea5f92535220cf_Makerspace%20cheque%20fraud.png)](https://www.hopsworks.ai/post/building-a-cheque-fraud-detection-and-explanation-ai-system-using-a-fine-tuned-llm)[Data EngineeringMay 13, 202410 minReadBuilding a Cheque Fraud Detection and Explanation AI System using a fine-tuned LLMThe third edition of the LLM Makerspace dived into an example of an LLM system for detecting check fraud.](https://www.hopsworks.ai/post/building-a-cheque-fraud-detection-and-explanation-ai-system-using-a-fine-tuned-llm)\nJim Dowling\n[![Feature Pipelines in Production with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6630f43b3472fc184b558f94_feature%20pipelines.png)](https://www.hopsworks.ai/post/feature-pipelines-in-production-with-hopsworks)[MLOpsApril 30, 20243 minReadFeature Pipelines in Production with HopsworksIn this post, we will look at how to put feature pipelines into production using Hopsworks. ](https://www.hopsworks.ai/post/feature-pipelines-in-production-with-hopsworks)\nFabio Buso\n[![Build your Value Case for Feature Store Implementations](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6628b049bd8bd1759cec38c7_value%20case%20v2.png)](https://www.hopsworks.ai/post/build-your-value-case-for-feature-store-implementations)[AIApril 24, 20249 minReadBuild your Value Case for Feature Store ImplementationsWe explore how to develop a compelling value case that quantify the benefits and costs associated with implementing a feature store.](https://www.hopsworks.ai/post/build-your-value-case-for-feature-store-implementations)\nRik Van Bruggen\n[![Job Scheduling & Orchestration using Hopsworks and Airflow](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/661f828bcf5d65c8431cb023_orchestration.png)](https://www.hopsworks.ai/post/job-scheduling-orchestration-using-hopsworks-and-airflow)[Data EngineeringApril 17, 202410 minReadJob Scheduling & Orchestration using Hopsworks and AirflowThis article covers the different aspects of Job Scheduling in Hopsworks including how simple jobs can be scheduled through the Hopsworks UI by non-technical users](https://www.hopsworks.ai/post/job-scheduling-orchestration-using-hopsworks-and-airflow)\nEhsan Heydari\n[![Build Your Own Private PDF Search Tool ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66447bbb6d7e1ff8947d02c1_Makerspace%20blogs%20pdfai.png)](https://www.hopsworks.ai/post/build-your-own-pdf-ai-using-both-rag-and-fine-tuning-in-one-platform)[Data EngineeringApril 11, 20246 minReadBuild Your Own Private PDF Search Tool A summary from our LLM Makerspace event where we built our own PDF Search Tool using RAG and fine-tuning in one platform. Follow along the journey to build a LLM application from scratch. ](https://www.hopsworks.ai/post/build-your-own-pdf-ai-using-both-rag-and-fine-tuning-in-one-platform)\nJim Dowling\n[![Build Vs Buy: For Machine Learning/AI Feature Stores](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66168d151c36e53c9c40b93a_build%20vs%20buy.png)](https://www.hopsworks.ai/post/build-versus-buy-machine-learning)[AIApril 10, 202417 minReadBuild Vs Buy: For Machine Learning/AI Feature StoresOn the decision of building versus buying a feature store there are strategic and technical components to consider as it impacts both cost and technological debt.](https://www.hopsworks.ai/post/build-versus-buy-machine-learning)\nRik Van Bruggen\n[![Unlocking the Power of Function Calling with LLMs](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6617d0fd1c284b69172c8478_Makerspace%20blogs.png)](https://www.hopsworks.ai/post/unlocking-the-power-of-function-calling-with-llms)[Data EngineeringApril 2, 20247 minReadUnlocking the Power of Function Calling with LLMsThis is a summary of our latest LLM Makerspace event where we pulled back the curtain on a exciting paradigm in AI \u2013 function calling with LLMs.](https://www.hopsworks.ai/post/unlocking-the-power-of-function-calling-with-llms)\nJim Dowling\n[![Doubling Down on Open Source: How RonDB Upholds the Principles Redis Left Behind](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66014ad6f7a779228f92f926_RonDB%20vs%20Redis.png)](https://www.hopsworks.ai/post/doubling-down-on-open-source-how-rondb-upholds-the-principles-redis-left-behind)[Feature StoreMarch 25, 20249 minReadDoubling Down on Open Source: How RonDB Upholds the Principles Redis Left BehindRedis will no longer be open source. Our own project, RonDB, will continue being open source in order to uphold the principles that keeps the technology advancing. ](https://www.hopsworks.ai/post/doubling-down-on-open-source-how-rondb-upholds-the-principles-redis-left-behind)\nMikael Ronstr\u00f6m\n[![The Enterprise Journey to introducing a Software Factory for AI Systems](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6613cabcbb4ee391296e4978_software%20factory.png)](https://www.hopsworks.ai/post/the-enterprise-journey-to-introducing-a-software-factory-for-ai-systems)[AIMarch 21, 202419 minReadThe Enterprise Journey to introducing a Software Factory for AI SystemsIn this article we describe the software factory approach to building and maintaining AI systems.](https://www.hopsworks.ai/post/the-enterprise-journey-to-introducing-a-software-factory-for-ai-systems)\nJim Dowling\n[![GenAI comes to Hopsworks with Vector Similarity Search](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65f838173d7ea02b2b9d8c66_vector%20similairty%20search.png)](https://www.hopsworks.ai/post/genai-comes-to-hopsworks-with-vector-similarity-search)[Feature StoreMarch 20, 20248 minReadGenAI comes to Hopsworks with Vector Similarity SearchHopsworks has added support for approximate nearest neighbor (ANN) indexing and vector similarity search for vector embeddings stored in its feature store. ](https://www.hopsworks.ai/post/genai-comes-to-hopsworks-with-vector-similarity-search)\nKenneth Mak \n[![Delta Lake comes to Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65e7352423a296a8a2cf90a1_delta%20lake.png)](https://www.hopsworks.ai/post/delta-lake-comes-to-hopsworks)[Feature StoreMarch 5, 20248 minReadDelta Lake comes to HopsworksHopsworks has added support for Delta Lake to accelerate our mission to build the Python-Native Data for AI platform. ](https://www.hopsworks.ai/post/delta-lake-comes-to-hopsworks)\nJim Dowling\n[![Federated Data with the Hopsworks Feature Query Service](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65cf5ee3c3dbaa97a035cb49_query%20service.png)](https://www.hopsworks.ai/post/federated-data-with-the-hopsworks-feature-query-service)[Feature StoreFebruary 14, 202414 minReadFederated Data with the Hopsworks Feature Query ServiceA tutorial of the Hopsworks Feature Query Service which efficiently queries and joins features from multiple platforms such as Snowflake, BigQuery and Hopsworks without data any duplication.](https://www.hopsworks.ai/post/federated-data-with-the-hopsworks-feature-query-service)\nSteffen Grohsschmiedt\n[![5 Machine Learning Myths Debunked](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65c348c2c06bb9ba20fe540c_ml%20myth%203.png)](https://www.hopsworks.ai/post/5-machine-learning-myths-debunked)[MLOpsFebruary 6, 20249 minRead5 Machine Learning Myths DebunkedThe rapid development pace in AI is the cause for a lot of misconceptions surrounding ML and MLOps. In this post we debunk a few common myths about MLOps, LLMs and machine learning in production. ](https://www.hopsworks.ai/post/5-machine-learning-myths-debunked)\nCarolin Svenberg \n[![F.A.I.R. Principles in Data for AI](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65bbc47fd2c5f8bdf2a09d5d_fair.png)](https://www.hopsworks.ai/post/fair-principles-in-data-for-ai)[Data ScienceFebruary 1, 20249 minReadF.A.I.R. Principles in Data for AIAt Hopsworks the F.A.I.R principles have been a cornerstone of our approach in designing a platform for managing machine learning data and infrastructure. ](https://www.hopsworks.ai/post/fair-principles-in-data-for-ai)\nLex Avstreikh\n[![Common Error Messages in Pandas](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65ae4ba61b3e0b423831dd3d_errors%20pandas.png)](https://www.hopsworks.ai/post/common-error-messages-in-pandas)[Data ScienceJanuary 18, 202420 minReadCommon Error Messages in PandasWe go through the most common errors messages in Pandas and offer solutions to these errors as well as provide efficiency tips for Pandas code.](https://www.hopsworks.ai/post/common-error-messages-in-pandas)\nHaziqa Sajid\n[![Multi-Region High Availability Comes to Feature Stores](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65f319435c8ec1a9dacb1e15_HA%20part%202.png)](https://www.hopsworks.ai/post/multi-region-high-availability-comes-to-feature-stores-part-2)[Feature StoreJanuary 16, 202413 minReadMulti-Region High Availability Comes to Feature StoresFollowing our previous blog, we expand on the architecture to fit a Tier 1 classification where all components of Hopsworks are replicated in a different geographical region. ](https://www.hopsworks.ai/post/multi-region-high-availability-comes-to-feature-stores-part-2)\nAntonios Kouzoupis\n[![Feature Store Benchmark Comparison: Hopsworks and Feast](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/658599385be1ffd71dcb0e21_feature%20store%20benchmarks.png)](https://www.hopsworks.ai/post/feature-store-benchmark-comparison-hopsworks-and-feast)[BenchmarkDecember 22, 202315minReadFeature Store Benchmark Comparison: Hopsworks and FeastA comparison of the online feature serving performance for Hopsworks and Feast feature stores, contrasting the approaches to building a feature store.](https://www.hopsworks.ai/post/feature-store-benchmark-comparison-hopsworks-and-feast)\nDhananjay Mukhedkar\n[![The Guardrails for High Risk AI Required by the EU AI Act](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65816c47c8bb5c198f11a318_EU%20AI%20part%202.png)](https://www.hopsworks.ai/post/the-guardrails-for-high-risk-ai-required-by-the-eu-ai-act)[AIDecember 19, 202313ReadThe Guardrails for High Risk AI Required by the EU AI ActAs a continuation to our last blog on the EU AI Act, this blog explores the implications of how Hopsworks machine learning systems can address the act's requirements for high risk AI applications.](https://www.hopsworks.ai/post/the-guardrails-for-high-risk-ai-required-by-the-eu-ai-act)\nRik Van Bruggen\n[![Multi-Region High Availability Comes to Feature Stores](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65a658a00f2c2063eba79913_HA%20part%201.png)](https://www.hopsworks.ai/post/multi-region-high-availability-comes-to-feature-stores)[MLOpsDecember 18, 202314 minReadMulti-Region High Availability Comes to Feature StoresLearn how Hopsworks achieves high availability for the Online and Offline Feature Store and a Metadata layer, making it an operational ML system both for on-premise and cloud infrastructure. ](https://www.hopsworks.ai/post/multi-region-high-availability-comes-to-feature-stores)\nAntonios Kouzoupis\n[![High Risk AI in the EU AI Act ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65797453a920f69c57852e7d_EU%20AI%20Act%2023.png)](https://www.hopsworks.ai/post/high-risk-ai-in-the-eu-ai-act)[AIDecember 13, 20239 minReadHigh Risk AI in the EU AI Act What is the corporate and societal significance of the EU AI Act and how does it impact organizations with high risk AI systems? ](https://www.hopsworks.ai/post/high-risk-ai-in-the-eu-ai-act)\nRik Van Bruggen\n[![From BI to AI: A Data-Driven Journey](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6578600892cbaf1d0142e6d0_Bi%20to%20AI%20Blog.png)](https://www.hopsworks.ai/post/from-bi-to-ai-a-data-driven-journey)[AIDecember 8, 20238 minReadFrom BI to AI: A Data-Driven JourneyData is evolving from traditional Business Intelligence to Artificial Intelligence and Machine Learning for predictive analytics, creating new requirements for how businesses operationalises.](https://www.hopsworks.ai/post/from-bi-to-ai-a-data-driven-journey)\nRik Van Bruggen\n[![Feature Engineering with DBT for Data Warehouses](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65703bfc6cc493cb5888a340_feature%20engineering%20with%20dbt_thumbnail.png)](https://www.hopsworks.ai/post/feature-engineering-with-dbt-for-data-warehouses)[Data EngineeringNovember 30, 202320 minReadFeature Engineering with DBT for Data WarehousesRead about the advantages of using DBT for data warehouses and how it's positioned as a preferred solution for many data analytics and engineering teams.](https://www.hopsworks.ai/post/feature-engineering-with-dbt-for-data-warehouses)\nKais Laribi \n[![What is MLOps?](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65648999d04c1ff190699f6e_What%20is%20MLOps%20blog.png)](https://www.hopsworks.ai/post/what-is-mlops)[MLOpsNovember 27, 202320 minReadWhat is MLOps?This blog explores MLOps principles, with a focus on versioning, and provides a practical example using Hopsworks for both data and model versioning. ](https://www.hopsworks.ai/post/what-is-mlops)\nHaziqa Sajid\n[![Pandas2 and Polars for Feature Engineering](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/652709e8e9ee4bd6e7245f2d_pandas%26polars_thumbnail.png)](https://www.hopsworks.ai/post/pandas2-and-polars-for-feature-engineering)[Data EngineeringNovember 6, 202320 minReadPandas2 and Polars for Feature EngineeringWe review Python libraries, such as Pandas, Pandas2 and Polars, for Feature Engineering, evaluate their performance and explore how they power machine learning use cases. ](https://www.hopsworks.ai/post/pandas2-and-polars-for-feature-engineering)\nHaziqa Sajid\n[![How to Build a Python Environment with Custom Docker Commands](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/653902a4e3c0de023af32e87_PythonEnvironment_thumbnail.png)](https://www.hopsworks.ai/post/how-to-build-a-python-environment-with-custom-docker-commands)[Feature StoreOctober 26, 20238 minReadHow to Build a Python Environment with Custom Docker CommandsWith the latest version of Hopsworks we introduce new capabilities such as running custom bash commands and a improved UI which shows you the history of the python environment. ](https://www.hopsworks.ai/post/how-to-build-a-python-environment-with-custom-docker-commands)\nGibson Chikafa\n[![Machine Learning Embeddings as Features for Models](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/652fa0827a2f054fc4f5d726_ML%20embeddings_thumbnail.png)](https://www.hopsworks.ai/post/machine-learning-embeddings-as-features-for-models)[Data EngineeringOctober 9, 202313 minReadMachine Learning Embeddings as Features for ModelsDelve into the profound implications of machine learning embeddings, their diverse applications, and their crucial role in reshaping the way we interact with data.](https://www.hopsworks.ai/post/machine-learning-embeddings-as-features-for-models)\nPrithivee Ramalingam\n[![Facebook Prophet for Time-Series Machine Learning](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/651eac8c99616d72197da06a_facebook%20prophet_thumbnail.png)](https://www.hopsworks.ai/post/facebook-prophet-for-time-series-machine-learning)[Data ScienceOctober 5, 202317minReadFacebook Prophet for Time-Series Machine LearningTime-series data consists of records in chronological order and is crucial for forecasting trends and patterns. In this blog, we take a look at how to use Facebook Prophet for time-series forecasting.](https://www.hopsworks.ai/post/facebook-prophet-for-time-series-machine-learning)\nKais Laribi \n[![Bring Your Own Kafka Cluster to Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6516dbba2c0b331bc39da36b_thumbnail_byok.png)](https://www.hopsworks.ai/post/bring-your-own-kafka-cluster)[Feature StoreSeptember 25, 202310 minReadBring Your Own Kafka Cluster to HopsworksA tutorial of how to use our latest Bring Your Own Kafka (BYOK) capability in Hopsworks. It allows you to connect your existing Kafka clusters to your Hopsworks cluster.](https://www.hopsworks.ai/post/bring-your-own-kafka-cluster)\nRalfs Zangis\n[![From MLOps to ML Systems with Feature/Training/Inference Pipelines](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65019f581267ff943c3931a6_mlops%20to%20ml%20systems_thumbnail.png)](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)[MLOpsSeptember 13, 202325 minReadFrom MLOps to ML Systems with Feature/Training/Inference PipelinesWe explain a new framework for ML systems as three independent ML pipelines: feature pipelines, training pipelines, and inference pipelines, creating a unified MLOps architecture.](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)\nJim Dowling\n[![Feature Engineering with Apache Airflow](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d3a7213fc689589547090e_airflow.png)](https://www.hopsworks.ai/post/feature-engineering-with-apache-airflow)[Data EngineeringSeptember 4, 202318 minReadFeature Engineering with Apache AirflowUnlock the power of Apache Airflow in the context of feature engineering. We will delve into building a feature pipeline using Airflow, focusing on two tasks: feature binning and aggregations.](https://www.hopsworks.ai/post/feature-engineering-with-apache-airflow)\nPrithivee Ramalingam\n[![Automated Feature Engineering with FeatureTools](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d9eb5716ab472d0a7b7bc4_Featuretools%20thumbnail.png)](https://www.hopsworks.ai/post/automated-feature-engineering-with-featuretools)[Data EngineeringAugust 23, 202313 minReadAutomated Feature Engineering with FeatureToolsAn ML model\u2019s ability to learn and read data patterns largely depend on feature quality. With frameworks such as FeatureTools ML practitioners can automate the feature engineering process.](https://www.hopsworks.ai/post/automated-feature-engineering-with-featuretools)\nHaziqa Sajid\n[![Why Do You Need a Feature Store? ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d9dab74eb40a787c6b7c5e_why%20copy.jpg)](https://www.hopsworks.ai/post/why-do-you-need-a-feature-store)[Feature StoreAugust 11, 202320 minReadWhy Do You Need a Feature Store? Discover the power of feature stores in modern machine learning systems and how they bridge the gap between model development and production. ](https://www.hopsworks.ai/post/why-do-you-need-a-feature-store)\nLex Avstreikh\n[![Faster reading from the Lakehouse to Python with DuckDB/ArrowFlight](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65113a725a93dc40f6d74c85_duckdb_thumbnail.png)](https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb)[Feature StoreAugust 9, 202313 minReadFaster reading from the Lakehouse to Python with DuckDB/ArrowFlightIn this article, we outline how we leveraged ArrowFlight with DuckDB to build a new service that massively improves the performance of Python clients reading from lakehouse data in the Feature Store](https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb)\nTill D\u00f6hmen\n[![MLOps vs. DevOps: Best Practices, Challenges and Differences](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d355fb6ba1878a9afd9066_Mlops%20Devops%20Blog.png)](https://www.hopsworks.ai/post/mlops-vs-devops-best-practices-challenges-and-differences)[MLOpsJuly 11, 202317ReadMLOps vs. DevOps: Best Practices, Challenges and DifferencesExplore the convergence of MLOps and DevOps. Learn about their purposes, differences, and areas of integration and discover best practices, challenges, and their future potential. ](https://www.hopsworks.ai/post/mlops-vs-devops-best-practices-challenges-and-differences)\nPrithivee Ramalingam\n[![Building Feature Pipelines with Apache Flink](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d35592dae4e9246fb7c106_Apache%20Flink%20blog.png)](https://www.hopsworks.ai/post/building-feature-pipelines-with-apache-flink)[Data EngineeringJune 21, 20238 minReadBuilding Feature Pipelines with Apache FlinkFind out how to use Flink to compute real-time features and make them available to online models within seconds using Hopsworks. ](https://www.hopsworks.ai/post/building-feature-pipelines-with-apache-flink)\nFabio Buso\n[![Feature Engineering for Categorical Features with Pandas](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d354fcd29f6c64ab26eb92_Feature%20Engineering%20Blog.png)](https://www.hopsworks.ai/post/feature-engineering-for-categorical-features-with-pandas)[Data EngineeringJune 20, 202310 minReadFeature Engineering for Categorical Features with PandasExplore the power of feature engineering for categorical features using Pandas. Learn essential techniques for handling categorical variables, and creating new features.](https://www.hopsworks.ai/post/feature-engineering-for-categorical-features-with-pandas)\nPrithivee Ramalingam\n[![Hopsworks meets SOC2 Type II standards for data security and privacy](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6447ea626aa4aa2d0c97e786_SOC%202%20Certification%2000.png)](https://www.hopsworks.ai/post/hopsworks-meets-soc2-type-ii-standards-for-data-security)[AIApril 25, 20232 minReadHopsworks meets SOC2 Type II standards for data security and privacyHopsworks has successfully completed the AICPA Service Organization Control (SOC) 2 Type II audit.](https://www.hopsworks.ai/post/hopsworks-meets-soc2-type-ii-standards-for-data-security)\nCarolin Svenberg \n[![Hopsworks receives ISO 27001 Certification for Data Security](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64887cd64eb0d2a378319466_ISO%20Certification.png)](https://www.hopsworks.ai/post/hopsworks-receives-iso-27001-certification-for-data-security)[AIApril 4, 20232 minReadHopsworks receives ISO 27001 Certification for Data SecurityHopsworks has received an ISO 27001 certification, the globally recognized standard for establishing, implementing, maintaining, and continually improving an information security management system.](https://www.hopsworks.ai/post/hopsworks-receives-iso-27001-certification-for-data-security)\nCarolin Svenberg \n[![ROI of Feature Stores](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63aacdf3e93ff87bc5913656_ROI%20Feature%20Stores%20Blog.png)](https://www.hopsworks.ai/post/roi-of-feature-stores)[Feature StoreMarch 1, 20237 minReadROI of Feature StoresThis blog analyses the cost-benefits of Feature Stores for Machine Learning and estimates your return on investment with our Feature Store ROI Calculator.](https://www.hopsworks.ai/post/roi-of-feature-stores)\nJim Dowling\n[![Feature Store: The missing data layer for Machine Learning pipelines?](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61bb72f4fbcc74f4c86eb3f8_61b8aed0674af6312e2c05b2_missingdatalayer.jpg)](https://www.hopsworks.ai/post/feature-store-the-missing-data-layer-in-ml-pipelines)[Feature StoreFebruary 2, 202330 minReadFeature Store: The missing data layer for Machine Learning pipelines?In this blog, we discuss the state-of-the-art in data management and machine learning pipelines (within the wider field of MLOps) and present the first open-source feature store, Hopsworks.](https://www.hopsworks.ai/post/feature-store-the-missing-data-layer-in-ml-pipelines)\nJim Dowling\n[![Hopsworks 3.1 Product Updates: Feature Store & UI Improvements ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63dbadc63f7ca98a8a34b3bd_Hopsworks%203.1.png)](https://www.hopsworks.ai/post/hopsworks-3-1-product-updates)[February 2, 20234 minReadHopsworks 3.1 Product Updates: Feature Store & UI Improvements Read about Hopsworks 3.1 and the new improvements in the feature store (time-series splits for training data, support for managing thousands of models), stability and user-interface.](https://www.hopsworks.ai/post/hopsworks-3-1-product-updates)\nJim Dowling\n[![Optimize your MLOps Workflow with a Feature Store CI/CD and Github Actions](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/639add1f7195743d82f44ed0_CI_CD%20Blog.png)](https://www.hopsworks.ai/post/optimize-your-mlops-workflow-with-a-feature-store-ci-cd-and-github-actions)[MLOpsDecember 9, 202212 minReadOptimize your MLOps Workflow with a Feature Store CI/CD and Github ActionsIn this blog we present an end to end Git based workflow to test and deploy feature engineering, model training and inference pipelines. ](https://www.hopsworks.ai/post/optimize-your-mlops-workflow-with-a-feature-store-ci-cd-and-github-actions)\nFabio Buso\n[![Data Validation for Enterprise AI: Using Great Expectations with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d3575bfdbcc0984ed69ade_great%20expectations%20blog.png)](https://www.hopsworks.ai/post/data-validation-for-enterprise-ai-using-great-expectations-with-hopsworks)[MLOpsSeptember 21, 202210 minReadData Validation for Enterprise AI: Using Great Expectations with HopsworksLearn more about how Hopsworks stores both data and validation artifacts, enabling easy monitoring on the Feature Group UI page.](https://www.hopsworks.ai/post/data-validation-for-enterprise-ai-using-great-expectations-with-hopsworks)\nVictor Jouffrey\n[![How to use external data stores as an offline feature store in Hopsworks with Connector API](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6322eb50fc669956a18e9c55_Connector%20API_Blog%20Visual.png)](https://www.hopsworks.ai/post/external-data-stores-in-hopsworks-with-connector-api)[Data EngineeringSeptember 15, 20229 minReadHow to use external data stores as an offline feature store in Hopsworks with Connector APIIn this blog, we introduce Hopsworks Connector API that is used to mount a table in an external data source as an external feature group in Hopsworks. ](https://www.hopsworks.ai/post/external-data-stores-in-hopsworks-with-connector-api)\nDhananjay Mukhedkar\n[![Great Models Require Great MLOps: Using Weights & Biases with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63188873e12953561c68a8d3_Weights%20%26%20Biases.png)](https://www.hopsworks.ai/post/using-weights-biases-with-hopsworks)[MLOpsSeptember 7, 202215 minReadGreat Models Require Great MLOps: Using Weights & Biases with HopsworksDiscover how you can easily make the journey from ML models to putting prediction services in production by choosing best-of-breed technologies.](https://www.hopsworks.ai/post/using-weights-biases-with-hopsworks)\nMoritz Meister\n[![From Pandas to Features to Models to Predictions - A deep dive into the Hopsworks APIs](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6304d17d1bc96863f1833bed_pandatomodelstofeatures%20copy.jpg)](https://www.hopsworks.ai/post/a-deep-dive-into-the-hopsworks-apis)[Data EngineeringAugust 23, 202220 minReadFrom Pandas to Features to Models to Predictions - A deep dive into the Hopsworks APIsLearn how the Hopsworks feature store APIs work and what it takes to go from a Pandas DataFrame to features used by models for both training and inference.](https://www.hopsworks.ai/post/a-deep-dive-into-the-hopsworks-apis)\nFabio Buso\n[![Introducing the Serverless Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/62e7aead44765eaa5c619446_Serverless%20Blog.png)](https://www.hopsworks.ai/post/introducing-the-serverless-feature-store)[Feature StoreAugust 3, 20227 minReadIntroducing the Serverless Feature StoreHopsworks Serverless is the first serverless feature store for ML, allowing you to manage features and models seamlessly without worrying about scaling, configuration or management of servers.](https://www.hopsworks.ai/post/introducing-the-serverless-feature-store)\nJim Dowling\n[![Hopsworks 3.0: The Python-Centric Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d357d7f496d4ce9b950450_Python%20Centric%20Blog.gif)](https://www.hopsworks.ai/post/the-python-centric-feature-store)[Feature StoreJuly 26, 20229 minReadHopsworks 3.0: The Python-Centric Feature StoreHopsworks is the first feature store to extend its support from the traditional Big Data platforms to the Pandas-sized data realm, where Python reigns supreme. A new Python API is also provided.](https://www.hopsworks.ai/post/the-python-centric-feature-store)\nJim Dowling\n[![Hopsworks 3.0 - Connecting Python to the Modern Data Stack](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/62d9652fd44fb789460094f5_1200-630%20copy.jpg)](https://www.hopsworks.ai/post/hopsworks-3-0-connecting-python-to-the-modern-data-stack)[Feature StoreJuly 20, 20227 minReadHopsworks 3.0 - Connecting Python to the Modern Data StackHopsworks 3.0 is a new release focused on best-in-class Python support, Feature Views unifying Offline and Online read APIs to the Feature Store, Great Expectations support, KServe and a Model serving](https://www.hopsworks.ai/post/hopsworks-3-0-connecting-python-to-the-modern-data-stack)\nJim Dowling\n[![A Spark Join Operator for Point-in-Time Correct Joins](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/62bd5d438fd011e6686e59f5_Spark%20Join%20Blog.png)](https://www.hopsworks.ai/post/a-spark-join-operator-for-point-in-time-correct-joins)[BenchmarkJune 30, 202210 minReadA Spark Join Operator for Point-in-Time Correct JoinsIn this blog post we showcase the results of a study that examined point-in-time join optimization using Apache Spark in Hopsworks. ](https://www.hopsworks.ai/post/a-spark-join-operator-for-point-in-time-correct-joins)\nAxel Pettersson \n[![The EU AI Act: Time to Govern your AI or Turn it off](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/62b445273bb6c3d2a4b67854_EU%20AI%20ActBlog%20Thumbnail.png)](https://www.hopsworks.ai/post/the-eu-ai-act-time-to-govern-your-ai-or-turn-it-off)[AIJune 23, 202210 minReadThe EU AI Act: Time to Govern your AI or Turn it offAn introduction to the EU AI Act and how Feature Stores provide a great solution to the obligations imposed by the regulation.](https://www.hopsworks.ai/post/the-eu-ai-act-time-to-govern-your-ai-or-turn-it-off)\nGeoff Burne\n[![Feature Types for Machine Learning](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6290d72018708f098cfd1abf_featuretypes.png)](https://www.hopsworks.ai/post/feature-types-for-machine-learning)[Feature StoreMay 27, 202215 minReadFeature Types for Machine LearningProgrammers know data types, but what is a feature type to a programmer new to machine learning, given no mainstream programming language has native support for them?](https://www.hopsworks.ai/post/feature-types-for-machine-learning)\nJim Dowling\n[![Testing feature logic, transformations, and feature pipelines with pytest](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/62692b2da3a7260f0d57e36d_testing%20blog2.png)](https://www.hopsworks.ai/post/testing-feature-logic-transformations-and-feature-pipelines-with-pytest)[Data EngineeringApril 26, 202217 minReadTesting feature logic, transformations, and feature pipelines with pytestOperational machine learning requires the offline and online testing of both features and models. In this article, we show you how to design, build, and run test for features.](https://www.hopsworks.ai/post/testing-feature-logic-transformations-and-feature-pipelines-with-pytest)\nJim Dowling\n[![Hopsworks 2.5 Product Updates: Collaboration & Scalability](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6222098a11278365a58434d6_Blog%20Hops%202.5.png)](https://www.hopsworks.ai/post/hopsworks-2-5-product-updates)[March 4, 20223 minReadHopsworks 2.5 Product Updates: Collaboration & ScalabilityWe go through the new features and developments in Hopsworks 2.5 that will benefit open-source users and customers alike.](https://www.hopsworks.ai/post/hopsworks-2-5-product-updates)\nFabio Buso\n[![Model analysis on the What-If framework for TensorFlow on Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61e6717ebeb28447092d1030_WIF.png)](https://www.hopsworks.ai/post/model-analysis-what-if-framework-for-tensorflow)[MLOpsJanuary 17, 20225 minReadModel analysis on the What-If framework for TensorFlow on HopsworksWe introduce how to use the What-If Tool as a Jupyter plugin on Hopsworks to build better machine learning models by making it easier to ask counterfactual questions about your model\u2019s behaviour.](https://www.hopsworks.ai/post/model-analysis-what-if-framework-for-tensorflow)\nAnastasiia Andriievska \n[![How to Transform Snowflake Data into Features with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/637647a53a162c90114c1d6b_Snowflake%20Data%20Into%20Features.png)](https://www.hopsworks.ai/post/how-to-transform-snowflake-data-into-features-with-hopsworks)[Feature StoreDecember 7, 20218 minReadHow to Transform Snowflake Data into Features with HopsworksLearn how to connect Hopsworks to Snowflake and create features and make them available both offline in Snowflake and online in Hopsworks.](https://www.hopsworks.ai/post/how-to-transform-snowflake-data-into-features-with-hopsworks)\nFabio Buso\n[![Show me the code; how we linked notebooks to features](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/62221f7492811211efc66440_Show%20me%20the%20code%20Blog.png)](https://www.hopsworks.ai/post/show-me-the-code-how-we-linked-notebooks-to-features)[Data EngineeringNovember 23, 20213 minReadShow me the code; how we linked notebooks to featuresWe are introducing a new feature in Hopsworks UI - feature code preview - ability to view the notebook used to create a Feature Group or Training Dataset.](https://www.hopsworks.ai/post/show-me-the-code-how-we-linked-notebooks-to-features)\nJim Dowling\n[![Receiving Alerts in Slack/Email/PagerDuty from Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/637f7a009eebc51282fb86ea_Receiving%20Alerts%20Blog_.png)](https://www.hopsworks.ai/post/receiving-alerts-in-slack-email-pagerduty-from-hopsworks-support-for-managing-your-feature-store)[Feature StoreNovember 12, 20216 minReadReceiving Alerts in Slack/Email/PagerDuty from HopsworksLearn how to set up customized alerts in Hopsworks for different events that are triggered as part of the ingestion pipeline.](https://www.hopsworks.ai/post/receiving-alerts-in-slack-email-pagerduty-from-hopsworks-support-for-managing-your-feature-store)\nErmias Gebremeskel\n[![End-to-end Deep Learning Pipelines with Earth Observation Data in Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6380d0bc1484f157d512b6f7_Extreme%20Earth%20Blog.png)](https://www.hopsworks.ai/post/end-to-end-deep-learning-pipelines-with-earth-observation-data-in-hopsworks)[Data EngineeringOctober 8, 202110 minReadEnd-to-end Deep Learning Pipelines with Earth Observation Data in HopsworksIn this blog post we demonstrate how to build such a pipeline with real-world data in order to develop an iceberg classification model.](https://www.hopsworks.ai/post/end-to-end-deep-learning-pipelines-with-earth-observation-data-in-hopsworks)\nTheofilos Kakantousis\n[![Using an External Python Kafka Client to Interact with a Hopsworks Cluster](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63a56c868cfe910550c1c6fc_Guest%20Python%20Kafka%20Blog.png)](https://www.hopsworks.ai/post/using-an-external-python-kafka-client-to-interact-with-a-hopsworks-cluster)[MLOpsSeptember 27, 202118 minReadUsing an External Python Kafka Client to Interact with a Hopsworks ClusterLearn how to publish (write) and subscribe to (read) streams of events and how to interact with the schema registry and use Avro for data serialization.](https://www.hopsworks.ai/post/using-an-external-python-kafka-client-to-interact-with-a-hopsworks-cluster)\nAhmad Al-Shishtawy\n[![MLOps Wars: Versioned Feature Data with a Lakehouse](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/628cc10d7188da434fe09a13_MLOPS%20Wars%20Blog.png)](https://www.hopsworks.ai/post/mlops-wars-versioned-feature-data-with-a-lakehouse)[MLOpsAugust 3, 202112 minReadMLOps Wars: Versioned Feature Data with a LakehouseWith support to Apache Hudi, the Hopsworks Feature Store offers lakehouse capabilities to improve automated feature pipelines and training pipelines (MLOps).](https://www.hopsworks.ai/post/mlops-wars-versioned-feature-data-with-a-lakehouse)\nDavit Bzhalava\n[![AI Software Architecture for Copernicus Data with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63a56d1785e33d7aa02f1fff_Copernicus%20Data%20Blog.png)](https://www.hopsworks.ai/post/ai-software-architecture-for-copernicus-data-with-hopsworks)[Data EngineeringJuly 12, 202110 minReadAI Software Architecture for Copernicus Data with HopsworksHopsworks brings support for scale-out AI with the ExtremeEarth project which focuses on the most concerning issues of food security and sea mapping.](https://www.hopsworks.ai/post/ai-software-architecture-for-copernicus-data-with-hopsworks)\nTheofilos Kakantousis\n[![Hopsworks Online Feature Store: Fast Access to Feature Data for AI Applications](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6286332cf1f4378d630dea76_fast%20access%20features.jpg)](https://www.hopsworks.ai/post/hopsworks-online-feature-store-fast-access-to-feature-data-for-ai-applications)[Feature StoreJune 17, 202112 minReadHopsworks Online Feature Store: Fast Access to Feature Data for AI ApplicationsRead about how the Hopsworks Feature Store abstracts away the complexity of a dual database system, unifying feature access for online and batch applications. ](https://www.hopsworks.ai/post/hopsworks-online-feature-store-fast-access-to-feature-data-for-ai-applications)\nMoritz Meister\n[![How to build ML models with fastai and Jupyter in Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63aaaa8ef3af997b9065b714_How%20to%20build%20ML%20Models%20Blog.png)](https://www.hopsworks.ai/post/how-to-build-ml-models-with-fastai-and-jupyter-in-hopsworks)[Data EngineeringJune 4, 20216 minReadHow to build ML models with fastai and Jupyter in HopsworksThis tutorial gives an overview of how to work with Jupyter on the platform and train a state-of-the-art ML model using the fastai python library.](https://www.hopsworks.ai/post/how-to-build-ml-models-with-fastai-and-jupyter-in-hopsworks)\nRobin Andersson \n[![Scalable metadata: the new breed of file systems \\(em\\)powering big data companies](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63761d7bf7db427f0b5175d9_Scalable%20Metadada.png)](https://www.hopsworks.ai/post/scalable-metadata-the-new-breed-of-file-systems)[BenchmarkMay 31, 202120 minReadScalable metadata: the new breed of file systems (em)powering big data companiesHopsFS is an open-source scaleout metadata file system, but its primary use case is not Exabyte storage, rather customizable consistent metadata.](https://www.hopsworks.ai/post/scalable-metadata-the-new-breed-of-file-systems)\nJim Dowling\n[![Distributed ML Experiments on Databricks with Maggy](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63a57fa35752eb80df04abd0_Distributed%20ML%20Experiments%20Blog.png)](https://www.hopsworks.ai/post/distributed-ml-experiments-on-databricks-with-maggy)[MLOpsMay 13, 202110 minReadDistributed ML Experiments on Databricks with MaggyLearn how to train a ML model in a distributed fashion without reformatting our code on Databricks with Maggy, open source tool available on Hopsworks.](https://www.hopsworks.ai/post/distributed-ml-experiments-on-databricks-with-maggy)\nRiccardo Grigoletto\n[![How to manage Python libraries in Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63a5af136c31f26b2423a7a5_Python%20Libraries%20Blog.png)](https://www.hopsworks.ai/post/how-to-manage-python-libraries-in-hopsworks)[MLOpsMay 12, 20214 minReadHow to manage Python libraries in HopsworksThis tutorial will show an overview of how to install and manage Python libraries in the platform.](https://www.hopsworks.ai/post/how-to-manage-python-libraries-in-hopsworks)\nRobin Andersson \n[![From 100 to ZeRO: PyTorch and DeepSpeed ZeRO on any Spark Cluster with Maggy](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63a5ae6a68385c781aa1cf7e_DeepSpeed%20ZeRO%20Blog.png)](https://www.hopsworks.ai/post/pytorch-and-deepspeed-zero-on-any-spark-cluster-with-maggy)[MLOpsApril 27, 20217 minReadFrom 100 to ZeRO: PyTorch and DeepSpeed ZeRO on any Spark Cluster with MaggyUse open-source Maggy to write and debug PyTorch code on your local machine and run the code at scale without changing a single line in your program.](https://www.hopsworks.ai/post/pytorch-and-deepspeed-zero-on-any-spark-cluster-with-maggy)\nMoritz Meister\n[![Beyond Brainless AI with a Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63a5b57c6ccbdbe2a360ec7d_Brainless%20AI%20Blog.png)](https://www.hopsworks.ai/post/beyond-brainless-ai-with-a-feature-store)[MLOpsApril 27, 202110 minReadBeyond Brainless AI with a Feature StoreEvolve your models from stateless AI to Total Recall AI with the help of a Feature Store. ](https://www.hopsworks.ai/post/beyond-brainless-ai-with-a-feature-store)\nJim Dowling\n[![Detecting Financial Fraud Using GANs at Swedbank with Hopsworks and NVIDIA GPUs](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/638dc7c4ce20b724edd9d773_Nvidia%20Blog.png)](https://www.hopsworks.ai/post/detecting-financial-fraud-using-gans-at-swedbank-with-hopsworks-and-nvidia-gpus)[Feature StoreMarch 26, 202115 minReadDetecting Financial Fraud Using GANs at Swedbank with Hopsworks and NVIDIA GPUsRecently, one of Sweden\u2019s largest banks trained generative adversarial neural networks (GANs) using NVIDIA GPUs as part of its fraud and money-laundering prevention strategy.](https://www.hopsworks.ai/post/detecting-financial-fraud-using-gans-at-swedbank-with-hopsworks-and-nvidia-gpus)\nJim Dowling\n[![AI/ML needs a Key-Value store, and Redis is not up to it](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61eace23ecf46e36f195d5d1_REDIS.png)](https://www.hopsworks.ai/post/ai-ml-needs-a-key-value-store-and-redis-is-not-up-to-it)[BenchmarkFebruary 26, 202112 minReadAI/ML needs a Key-Value store, and Redis is not up to itSeeing how Redis is a popular open-source feature store with features significantly similar to RonDB, we compared the innards of RonDB\u2019s multithreading architecture to the commercial Redis products. ](https://www.hopsworks.ai/post/ai-ml-needs-a-key-value-store-and-redis-is-not-up-to-it)\nMikael Ronstr\u00f6m\n[![How to engineer and use Features in Azure ML Studio with the Hopsworks Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63aab17fc753bd66885fdf6e_Azure%20Ml%20Studio%20Blog.png)](https://www.hopsworks.ai/post/how-to-engineer-and-use-features-in-azure-ml-studio-with-hopsworks)[Feature StoreFebruary 25, 20218 minReadHow to engineer and use Features in Azure ML Studio with the Hopsworks Feature StoreLearn how to design and ingest features, browse existing features, create training datasets as DataFrames or as files on Azure Blob storage.](https://www.hopsworks.ai/post/how-to-engineer-and-use-features-in-azure-ml-studio-with-hopsworks)\nMoritz Meister\n[![How to transform Amazon Redshift data into features with Hopsworks Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63aabe5fcefc0ff23bdbe0aa_Amazon%20Redshift%20Blog.png)](https://www.hopsworks.ai/post/how-to-transform-amazon-redshift-data-into-features-with-hopsworks)[Feature StoreFebruary 9, 20219 minReadHow to transform Amazon Redshift data into features with Hopsworks Feature StoreConnect the Hopsworks Feature Store to Amazon Redshift to transform your data into features to train models and make predictions.](https://www.hopsworks.ai/post/how-to-transform-amazon-redshift-data-into-features-with-hopsworks)\nErmias Gebremeskel\n[![Elasticsearch is dead, long live Open Distro for Elasticsearch](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61e697708af5d9c46a0395a6_DFS.png)](https://www.hopsworks.ai/post/open-distro-for-elasticsearch-dynamic-support-hopsworks)[Data ScienceJanuary 14, 202113 minReadElasticsearch is dead, long live Open Distro for ElasticsearchIn this blog, we describe how we leverage the authentication and authorization support in Open Distro for Elasticsearch to make elasticsearch a project-based multi-tenant service in Hopsworks.](https://www.hopsworks.ai/post/open-distro-for-elasticsearch-dynamic-support-hopsworks)\nMahmoud Ismail\n[![HopsFS file system: 100X Times Faster than AWS S3](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61b8ded75115481bbce78769_faster%20file%20system.jpg)](https://www.hopsworks.ai/post/faster-than-aws-s3)[BenchmarkNovember 19, 202013 minReadHopsFS file system: 100X Times Faster than AWS S3Many developers believe S3 is the \"end of file system history\". It is impossible to build a file/object storage system on AWS that can compete with S3 on cost. But what if you could build on top of S3](https://www.hopsworks.ai/post/faster-than-aws-s3)\nMahmoud Ismail\n[![Feature Store for MLOps? Feature reuse means JOINs](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/637614127359609170ae02f8_FS%20for%20MLOps.png)](https://www.hopsworks.ai/post/feature-store-for-mlops-feature-reuse-means-joins)[Feature StoreOctober 23, 20207 minReadFeature Store for MLOps? Feature reuse means JOINsUse JOINs for feature reuse to save on infrastructure and the number of feature pipelines needed to maintain models in production.](https://www.hopsworks.ai/post/feature-store-for-mlops-feature-reuse-means-joins)\nJim Dowling\n[![ML Engineer Guide: Feature Store vs Data Warehouse](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61b9d4ec9a1b41068de189e7_005%20copy.png)](https://www.hopsworks.ai/post/feature-store-vs-data-warehouse)[Feature StoreOctober 8, 202013 minReadML Engineer Guide: Feature Store vs Data Warehouse The feature store is a data warehouse of features for machine learning (ML). Architecturally, it differs from the traditional data warehouse in that it is a dual-database.](https://www.hopsworks.ai/post/feature-store-vs-data-warehouse)\nJim Dowling\n[![One Function is All you Need: Machine Learning Experiments with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63aac419fdf77f36ae64b4c9_ML%20Experiments%20Blog.png)](https://www.hopsworks.ai/post/machine-learning-experiments-with-hopsworks)[MLOpsSeptember 30, 202016 minReadOne Function is All you Need: Machine Learning Experiments with HopsworksHopsworks supports machine learning experiments to track and distribute ML for free and with a built-in TensorBoard.](https://www.hopsworks.ai/post/machine-learning-experiments-with-hopsworks)\nRobin Andersson \n[![How we secure your data with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/627e76114c646d8045325fbc_Secure%20Data.png)](https://www.hopsworks.ai/post/how-we-secure-your-data-with-hopsworks)[Feature StoreJuly 16, 202015 minReadHow we secure your data with HopsworksIntegrate with third-party security standards and take advantage from our project-based multi-tenancy model to host data in one single shared cluster.](https://www.hopsworks.ai/post/how-we-secure-your-data-with-hopsworks)\nJim Dowling\n[![Untitled UI logotext](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/630e3413d3fafa0f79c52da2_hopsworks-logo%202022_white.svg)![Logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/666c3cc1cfc4741e6b2d9fe6_untitled-ui-logo.png)The AI Lakehouse](https://www.hopsworks.ai/)\nProduct\n[Hopsworks Enterprise](https://www.hopsworks.ai/try)[Capabilities](https://www.hopsworks.ai/product-capabilities)[Integrations](https://www.hopsworks.ai/integrations)[Examples](https://www.hopsworks.ai/hopsworks-examples)[Pricing](https://www.hopsworks.ai/pricing)[App Status](https://hopsworks.statuspage.io/)[FAQ](https://www.hopsworks.ai/frequently-asked-questions)\nSolutions\n[Industry & Team Solutions](https://www.hopsworks.ai/solutions/all)[Generative AI](https://www.hopsworks.ai/use-case/fine-tuning-llms-rag-for-genai)[Real-time Fraud Detection ](https://www.hopsworks.ai/use-case/realtime-fraud-detection)[Hopsworks Medical Copilot](https://www.hopscopilot.com/)[Customers](https://www.hopsworks.ai/customers)\nResources\n[Blog](https://www.hopsworks.ai/blog)[MLOps Dictionary](https://www.hopsworks.ai/mlops-dictionary)[Events](https://www.hopsworks.ai/events)[Documentation](https://docs.hopsworks.ai/latest/)[Academy](https://www.hopsworks.ai/academy)[Research Papers](https://www.hopsworks.ai/research-papers)[Feature Store Comparison](https://www.hopsworks.ai/product-comparison/sagemaker)[Community](https://community.hopsworks.ai/)[FAQ: EU AI Act](https://www.hopsworks.ai/faq-eu-ai-act)\nCompany\n[About Us](https://www.hopsworks.ai/about-us)[News](https://www.hopsworks.ai/news)[Security & Compliance](https://www.hopsworks.ai/security-compliance)[Contact Us](https://www.hopsworks.ai/contact/main)\nJoin our newsletter\n**Receive the latest product updates, upcoming events, and industry news.**\n\u00a9 Hopsworks 2025. All rights reserved. Various trademarks held by their respective owners.\n[](https://join.slack.com/t/public-hopsworks/shared_invite/zt-1uf21vitz-rhHKNdIf8GEiOf1EJ6Wzsw)[](https://github.com/logicalclocks/hopsworks)[](https://www.linkedin.com/company/hopsworks/)[](https://twitter.com/hopsworks)[](https://www.youtube.com/@hopsworks)\n[Privacy Policy](https://www.iubenda.com/privacy-policy/90800199 \"Privacy Policy \")\n[Cookie Policy](https://www.iubenda.com/privacy-policy/90800199/cookie-policy \"Cookie Policy \")\n[Terms and Conditions](https://www.iubenda.com/terms-and-conditions/90800199 \"Terms and Conditions \")\n",
    "answer": "```markdown\n### TL;DR Summary\nScheduled maintenance for Hopsworks on November 26, from 07:00 to 17:00 UTC. The app will be inaccessible during this time. For updates, visit [Hopsworks News](https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes).\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/coleam00/ai-agents-masterclass/tree/main/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fcoleam00%2Fai-agents-masterclass%2Ftree%2Fmain%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fcoleam00%2Fai-agents-masterclass%2Ftree%2Fmain%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=coleam00%2Fai-agents-masterclass) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/coleam00/ai-agents-masterclass/tree/main/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/coleam00/ai-agents-masterclass/tree/main/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/coleam00/ai-agents-masterclass/tree/main/) to refresh your session. Dismiss alert\n{{ message }}\n[ coleam00 ](https://github.com/coleam00) / **[ai-agents-masterclass](https://github.com/coleam00/ai-agents-masterclass) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Fcoleam00%2Fai-agents-masterclass) You must be signed in to change notification settings\n  * [ Fork 919 ](https://github.com/login?return_to=%2Fcoleam00%2Fai-agents-masterclass)\n  * [ Star  1.9k ](https://github.com/login?return_to=%2Fcoleam00%2Fai-agents-masterclass)\n\n\nFollow along with my AI Agents Masterclass videos! All of the code I create and use in this series on YouTube will be here for you to use and even build on top of! \n### License\n[ MIT license ](https://github.com/coleam00/ai-agents-masterclass/blob/main/LICENSE)\n[ 1.9k stars ](https://github.com/coleam00/ai-agents-masterclass/stargazers) [ 919 forks ](https://github.com/coleam00/ai-agents-masterclass/forks) [ Branches ](https://github.com/coleam00/ai-agents-masterclass/branches) [ Tags ](https://github.com/coleam00/ai-agents-masterclass/tags) [ Activity ](https://github.com/coleam00/ai-agents-masterclass/activity)\n[ Star  ](https://github.com/login?return_to=%2Fcoleam00%2Fai-agents-masterclass)\n[ Notifications ](https://github.com/login?return_to=%2Fcoleam00%2Fai-agents-masterclass) You must be signed in to change notification settings\n  * [ Code ](https://github.com/coleam00/ai-agents-masterclass)\n  * [ Issues 28 ](https://github.com/coleam00/ai-agents-masterclass/issues)\n  * [ Pull requests 9 ](https://github.com/coleam00/ai-agents-masterclass/pulls)\n  * [ Actions ](https://github.com/coleam00/ai-agents-masterclass/actions)\n  * [ Projects 0 ](https://github.com/coleam00/ai-agents-masterclass/projects)\n  * [ Security ](https://github.com/coleam00/ai-agents-masterclass/security)\n  * [ Insights ](https://github.com/coleam00/ai-agents-masterclass/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/coleam00/ai-agents-masterclass)\n  * [ Issues  ](https://github.com/coleam00/ai-agents-masterclass/issues)\n  * [ Pull requests  ](https://github.com/coleam00/ai-agents-masterclass/pulls)\n  * [ Actions  ](https://github.com/coleam00/ai-agents-masterclass/actions)\n  * [ Projects  ](https://github.com/coleam00/ai-agents-masterclass/projects)\n  * [ Security  ](https://github.com/coleam00/ai-agents-masterclass/security)\n  * [ Insights  ](https://github.com/coleam00/ai-agents-masterclass/pulse)\n\n\n# coleam00/ai-agents-masterclass\nmain\n[**1** Branch](https://github.com/coleam00/ai-agents-masterclass/branches)[**0** Tags](https://github.com/coleam00/ai-agents-masterclass/tags)\n[](https://github.com/coleam00/ai-agents-masterclass/branches)[](https://github.com/coleam00/ai-agents-masterclass/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[42 Commits](https://github.com/coleam00/ai-agents-masterclass/commits/main/)[](https://github.com/coleam00/ai-agents-masterclass/commits/main/)  \n[1-first-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/1-first-agent \"1-first-agent\")| [1-first-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/1-first-agent \"1-first-agent\")  \n[10-deploy-ai-agent-langserve](https://github.com/coleam00/ai-agents-masterclass/tree/main/10-deploy-ai-agent-langserve \"10-deploy-ai-agent-langserve\")| [10-deploy-ai-agent-langserve](https://github.com/coleam00/ai-agents-masterclass/tree/main/10-deploy-ai-agent-langserve \"10-deploy-ai-agent-langserve\")  \n[2-langchain-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/2-langchain-agent \"2-langchain-agent\")| [2-langchain-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/2-langchain-agent \"2-langchain-agent\")  \n[3-agent-ui](https://github.com/coleam00/ai-agents-masterclass/tree/main/3-agent-ui \"3-agent-ui\")| [3-agent-ui](https://github.com/coleam00/ai-agents-masterclass/tree/main/3-agent-ui \"3-agent-ui\")  \n[4-task-management-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/4-task-management-agent \"4-task-management-agent\")| [4-task-management-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/4-task-management-agent \"4-task-management-agent\")  \n[5-rag-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/5-rag-agent \"5-rag-agent\")| [5-rag-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/5-rag-agent \"5-rag-agent\")  \n[6-rag-task-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/6-rag-task-agent \"6-rag-task-agent\")| [6-rag-task-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/6-rag-task-agent \"6-rag-task-agent\")  \n[7-langgraph-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/7-langgraph-agent \"7-langgraph-agent\")| [7-langgraph-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/7-langgraph-agent \"7-langgraph-agent\")  \n[8-n8n-asana-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/8-n8n-asana-agent \"8-n8n-asana-agent\")| [8-n8n-asana-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/8-n8n-asana-agent \"8-n8n-asana-agent\")  \n[9-n8n-rag-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/9-n8n-rag-agent \"9-n8n-rag-agent\")| [9-n8n-rag-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/9-n8n-rag-agent \"9-n8n-rag-agent\")  \n[cost-saving-ai-router](https://github.com/coleam00/ai-agents-masterclass/tree/main/cost-saving-ai-router \"cost-saving-ai-router\")| [cost-saving-ai-router](https://github.com/coleam00/ai-agents-masterclass/tree/main/cost-saving-ai-router \"cost-saving-ai-router\")  \n[gohighlevel-saas-backend](https://github.com/coleam00/ai-agents-masterclass/tree/main/gohighlevel-saas-backend \"gohighlevel-saas-backend\")| [gohighlevel-saas-backend](https://github.com/coleam00/ai-agents-masterclass/tree/main/gohighlevel-saas-backend \"gohighlevel-saas-backend\")  \n[korvus-simple-rag](https://github.com/coleam00/ai-agents-masterclass/tree/main/korvus-simple-rag \"korvus-simple-rag\")| [korvus-simple-rag](https://github.com/coleam00/ai-agents-masterclass/tree/main/korvus-simple-rag \"korvus-simple-rag\")  \n[llama3-function-calling-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/llama3-function-calling-agent \"llama3-function-calling-agent\")| [llama3-function-calling-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/llama3-function-calling-agent \"llama3-function-calling-agent\")  \n[llm-agent-evaluation-framework](https://github.com/coleam00/ai-agents-masterclass/tree/main/llm-agent-evaluation-framework \"llm-agent-evaluation-framework\")| [llm-agent-evaluation-framework](https://github.com/coleam00/ai-agents-masterclass/tree/main/llm-agent-evaluation-framework \"llm-agent-evaluation-framework\")  \n[local-ai-packaged](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-ai-packaged \"local-ai-packaged\")| [local-ai-packaged](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-ai-packaged \"local-ai-packaged\")  \n[local-llm-tool-calling](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-llm-tool-calling \"local-llm-tool-calling\")| [local-llm-tool-calling](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-llm-tool-calling \"local-llm-tool-calling\")  \n[local-or-managed-llm-rag](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-or-managed-llm-rag \"local-or-managed-llm-rag\")| [local-or-managed-llm-rag](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-or-managed-llm-rag \"local-or-managed-llm-rag\")  \n[local-swarm-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-swarm-agent \"local-swarm-agent\")| [local-swarm-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-swarm-agent \"local-swarm-agent\")  \n[madlibs](https://github.com/coleam00/ai-agents-masterclass/tree/main/madlibs \"madlibs\")| [madlibs](https://github.com/coleam00/ai-agents-masterclass/tree/main/madlibs \"madlibs\")  \n[n8n-langchain-agent-advanced](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-langchain-agent-advanced \"n8n-langchain-agent-advanced\")| [n8n-langchain-agent-advanced](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-langchain-agent-advanced \"n8n-langchain-agent-advanced\")  \n[n8n-langchain-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-langchain-agent \"n8n-langchain-agent\")| [n8n-langchain-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-langchain-agent \"n8n-langchain-agent\")  \n[n8n-rag-pdfs-excel](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-rag-pdfs-excel \"n8n-rag-pdfs-excel\")| [n8n-rag-pdfs-excel](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-rag-pdfs-excel \"n8n-rag-pdfs-excel\")  \n[n8n-streamlit-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-streamlit-agent \"n8n-streamlit-agent\")| [n8n-streamlit-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-streamlit-agent \"n8n-streamlit-agent\")  \n[o1-ai-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/o1-ai-agent \"o1-ai-agent\")| [o1-ai-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/o1-ai-agent \"o1-ai-agent\")  \n[pydantic-ai](https://github.com/coleam00/ai-agents-masterclass/tree/main/pydantic-ai \"pydantic-ai\")| [pydantic-ai](https://github.com/coleam00/ai-agents-masterclass/tree/main/pydantic-ai \"pydantic-ai\")  \n[sql-ai-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/sql-ai-agent \"sql-ai-agent\")| [sql-ai-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/sql-ai-agent \"sql-ai-agent\")  \n[streamlit-chatbot](https://github.com/coleam00/ai-agents-masterclass/tree/main/streamlit-chatbot \"streamlit-chatbot\")| [streamlit-chatbot](https://github.com/coleam00/ai-agents-masterclass/tree/main/streamlit-chatbot \"streamlit-chatbot\")  \n[supabase-n8n-rag-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/supabase-n8n-rag-agent \"supabase-n8n-rag-agent\")| [supabase-n8n-rag-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/supabase-n8n-rag-agent \"supabase-n8n-rag-agent\")  \n[v0-agent-frontend](https://github.com/coleam00/ai-agents-masterclass/tree/main/v0-agent-frontend \"v0-agent-frontend\")| [v0-agent-frontend](https://github.com/coleam00/ai-agents-masterclass/tree/main/v0-agent-frontend \"v0-agent-frontend\")  \n[.gitattributes](https://github.com/coleam00/ai-agents-masterclass/blob/main/.gitattributes \".gitattributes\")| [.gitattributes](https://github.com/coleam00/ai-agents-masterclass/blob/main/.gitattributes \".gitattributes\")  \n[.gitignore](https://github.com/coleam00/ai-agents-masterclass/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/coleam00/ai-agents-masterclass/blob/main/.gitignore \".gitignore\")  \n[LICENSE](https://github.com/coleam00/ai-agents-masterclass/blob/main/LICENSE \"LICENSE\")| [LICENSE](https://github.com/coleam00/ai-agents-masterclass/blob/main/LICENSE \"LICENSE\")  \n[README.md](https://github.com/coleam00/ai-agents-masterclass/blob/main/README.md \"README.md\")| [README.md](https://github.com/coleam00/ai-agents-masterclass/blob/main/README.md \"README.md\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/coleam00/ai-agents-masterclass/tree/main/)\n  * [MIT license](https://github.com/coleam00/ai-agents-masterclass/tree/main/)\n\n\n[ ![AI Agents Masterclass](https://camo.githubusercontent.com/c6245efe5a6c4510b164c6e49f55ef6dcddf8f4fbfcd0999b73897d05a8497a2/68747470733a2f2f692e696d6775722e636f6d2f384772327042412e706e67) ](https://www.youtube.com/channel/UCMwVTLZIRRUyyVrkjDpn4pA)\nAI Agents Masterclass[](https://github.com/coleam00/ai-agents-masterclass/tree/main/#ai-agents-masterclass)\nArtificial Intelligence is the #1 thing for all developers to spend their time on now. The problem is, most developers aren't focusing on AI agents, which is the real way to unleash the full power of AI. This is why I'm creating this AI Agents Masterclass - so I can show YOU how to use AI agents to transform businesses and create incredibly powerful software like I've already done many times! Click the image or link above to go to the masterclass on YouTube. \n[**What are AI Agents?**](https://github.com/coleam00/ai-agents-masterclass/tree/main/#what-are-ai-agents) \u00b7 [**How this Repo Works**](https://github.com/coleam00/ai-agents-masterclass/tree/main/#how-this-repo-works) \u00b7 [**Instructions to Follow Along**](https://github.com/coleam00/ai-agents-masterclass/tree/main/#instructions-to-follow-along)\n## What are AI Agents?\n[](https://github.com/coleam00/ai-agents-masterclass/tree/main/#what-are-ai-agents)\nAI agents are simply Large Language Models that have been given the ability to interact with the outside world. They can do things like draft emails, book appointments in your CRM, create tasks in your task management software, and really anything you can dream of! I hope that everything I show here can really help you dream big and create incredible things with AI!\nAI agents can be very powerful without having to create a lot of code. That doesn't mean there isn't room though to create more complex applications to tie together many different agents to accomplish truly incredible things! That's where we'll be heading with this masterclass and I really look forward to it!\nBelow is a very basic diagram just to get an idea of what an AI agent looks like:\n[![Trainers Ally LangGraph graph](https://camo.githubusercontent.com/9fa1c160b31faee0870076572e7282e143586daa9497aec03adff8951b6d275a/68747470733a2f2f692e696d6775722e636f6d2f4368526f5638572e706e67)](https://camo.githubusercontent.com/9fa1c160b31faee0870076572e7282e143586daa9497aec03adff8951b6d275a/68747470733a2f2f692e696d6775722e636f6d2f4368526f5638572e706e67)\n## How this Repo Works\n[](https://github.com/coleam00/ai-agents-masterclass/tree/main/#how-this-repo-works)\nEach week there will be a new video for my AI Agents Masterclass! Each video will have its own folder in this repo, starting with [/1-first-agent/](https://github.com/coleam00/ai-agents-masterclass/blob/main/1-first-agent) for the first video in the masterclass where I create our very first AI agent!\nAny folder that starts with a number is for a masterclass video. The other folders are for other content on my YouTube channel. The other content goes very well with the masterclass series (think of it as supplemental material) which is why it is here too!\nThe code in each folder will be exactly what I used/created in the accompanying masterclass video.\n## Instructions to Follow Along\n[](https://github.com/coleam00/ai-agents-masterclass/tree/main/#instructions-to-follow-along)\nThe below instructions assume you already have Git, Python, and Pip installed. If you do not, you can install [Python + Pip from here](https://www.python.org/downloads/) and [Git from here](https://git-scm.com/).\nTo follow along with any of my videos, first clone this GitHub repository, open up a terminal, and change your directory to the folder for the current video you are watching (example: 1st video is [/1-first-agent/](https://github.com/coleam00/ai-agents-masterclass/blob/main/1-first-agent)).\nThe below instructions work on any OS - Windows, Linux, or Mac!\nYou will need to use the environment variables defined in the .env.example file in the folder (example for the first video: [`1-first-agent/.env.example`](https://github.com/coleam00/ai-agents-masterclass/blob/main/1-first-agent/.env.example)) to set up your API keys and other configuration. Turn the .env.example file into a `.env` file, and supply the necessary environment variables.\nAfter setting up the .env file, run the below commands to create a Python virtual environment and install the necessary Python packages to run the code from the masterclass. Creating a virtual environment is optional but recommended! Creating a virtual environment for the entire masterclass is a one time thing. Make sure to run the pip install for each video though!\n```\npython -m venv ai-agents-masterclass\n# On Windows:\n.\\ai-agents-masterclass\\Scripts\\activate\n# On MacOS/Linux: \nsource ai-agents-masterclass/bin/activate\ncd 1-first-agent (or whichever folder)\npip install -r requirements.txt\n```\n\nThen, you can execute the code in the folder with:\n```\npython [script name].py\n```\n\n## About\nFollow along with my AI Agents Masterclass videos! All of the code I create and use in this series on YouTube will be here for you to use and even build on top of! \n### Resources\n[ Readme ](https://github.com/coleam00/ai-agents-masterclass/tree/main/#readme-ov-file)\n### License\n[ MIT license ](https://github.com/coleam00/ai-agents-masterclass/tree/main/#MIT-1-ov-file)\n[ Activity](https://github.com/coleam00/ai-agents-masterclass/activity)\n### Stars\n[ **1.9k** stars](https://github.com/coleam00/ai-agents-masterclass/stargazers)\n### Watchers\n[ **80** watching](https://github.com/coleam00/ai-agents-masterclass/watchers)\n### Forks\n[ **919** forks](https://github.com/coleam00/ai-agents-masterclass/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fcoleam00%2Fai-agents-masterclass&report=coleam00+%28user%29)\n##  [Releases](https://github.com/coleam00/ai-agents-masterclass/releases)\nNo releases published\n##  [Packages 0](https://github.com/users/coleam00/packages?repo_name=ai-agents-masterclass)\nNo packages published \n##  [Contributors 2](https://github.com/coleam00/ai-agents-masterclass/graphs/contributors)\n  * [ ![@coleam00](https://avatars.githubusercontent.com/u/47287758?s=64&v=4) ](https://github.com/coleam00) [ **coleam00** Cole Medin ](https://github.com/coleam00)\n  * [ ![@jeffwiegner](https://avatars.githubusercontent.com/u/56127674?s=64&v=4) ](https://github.com/jeffwiegner) [ **jeffwiegner** ](https://github.com/jeffwiegner)\n\n\n## Languages\n  * [ Python 73.2% ](https://github.com/coleam00/ai-agents-masterclass/search?l=python)\n  * [ JavaScript 21.9% ](https://github.com/coleam00/ai-agents-masterclass/search?l=javascript)\n  * [ TypeScript 4.8% ](https://github.com/coleam00/ai-agents-masterclass/search?l=typescript)\n  * [ CSS 0.1% ](https://github.com/coleam00/ai-agents-masterclass/search?l=css)\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe **AI Agents Masterclass** by Cole Medin focuses on leveraging AI agents, which are Large Language Models (LLMs) capable of interacting with external systems to automate tasks like email drafting and CRM management. The repository contains code from weekly YouTube videos, guiding users to create and enhance AI agents with minimal coding. Follow along by setting up a Python environment and using provided scripts.\n\n**Tags:** Generative AI, LLMs, AI Agents\n```"
  },
  {
    "instruction": "[Skip to main content](https://arxiv.org/abs/2208.12242/#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/) > [cs](https://arxiv.org/list/cs/recent) > arXiv:2208.12242 \n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[ ![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg) ](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n## quick links\n  * [Login](https://arxiv.org/login)\n  * [Help Pages](https://info.arxiv.org/help)\n  * [About](https://info.arxiv.org/about)\n\n\n# Computer Science > Computer Vision and Pattern Recognition\n**arXiv:2208.12242** (cs) \n[Submitted on 25 Aug 2022 ([v1](https://arxiv.org/abs/2208.12242v1)), last revised 15 Mar 2023 (this version, v2)]\n# Title:DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\nAuthors:[Nataniel Ruiz](https://arxiv.org/search/cs?searchtype=author&query=Ruiz,+N), [Yuanzhen Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+Y), [Varun Jampani](https://arxiv.org/search/cs?searchtype=author&query=Jampani,+V), [Yael Pritch](https://arxiv.org/search/cs?searchtype=author&query=Pritch,+Y), [Michael Rubinstein](https://arxiv.org/search/cs?searchtype=author&query=Rubinstein,+M), [Kfir Aberman](https://arxiv.org/search/cs?searchtype=author&query=Aberman,+K)\nView a PDF of the paper titled DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation, by Nataniel Ruiz and 4 other authors\n[View PDF](https://arxiv.org/pdf/2208.12242)\n> Abstract:Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \"personalization\" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: [this https URL](https://dreambooth.github.io/)\nComments: | Published at CVPR 2023. Project page: [this https URL](https://dreambooth.github.io/)  \n---|---  \nSubjects: |  Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Machine Learning (cs.LG)  \nCite as: | [arXiv:2208.12242](https://arxiv.org/abs/2208.12242) [cs.CV]  \n(or  [arXiv:2208.12242v2](https://arxiv.org/abs/2208.12242v2) [cs.CV] for this version)   \n<https://doi.org/10.48550/arXiv.2208.12242> Focus to learn more arXiv-issued DOI via DataCite  \n## Submission history\nFrom: Nataniel Ruiz [[view email](https://arxiv.org/show-email/cd59910c/2208.12242)] **[[v1]](https://arxiv.org/abs/2208.12242v1)** Thu, 25 Aug 2022 17:45:49 UTC (15,973 KB) **[v2]** Wed, 15 Mar 2023 17:52:27 UTC (26,993 KB) \nFull-text links:\n## Access Paper:\nView a PDF of the paper titled DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation, by Nataniel Ruiz and 4 other authors\n  * [View PDF](https://arxiv.org/pdf/2208.12242)\n  * [TeX Source](https://arxiv.org/src/2208.12242)\n  * [Other Formats](https://arxiv.org/format/2208.12242)\n\n\n[ ![license icon](https://arxiv.org/icons/licenses/by-4.0.png) view license ](http://creativecommons.org/licenses/by/4.0/ \"Rights to this article\")\nCurrent browse context: \ncs.CV\n[< prev](https://arxiv.org/prevnext?id=2208.12242&function=prev&context=cs.CV \"previous in cs.CV \\(accesskey p\\)\") |  [next >](https://arxiv.org/prevnext?id=2208.12242&function=next&context=cs.CV \"next in cs.CV \\(accesskey n\\)\")\n[new](https://arxiv.org/list/cs.CV/new) |  [recent](https://arxiv.org/list/cs.CV/recent) | [2022-08](https://arxiv.org/list/cs.CV/2022-08)\nChange to browse by: \n[cs](https://arxiv.org/abs/2208.12242?context=cs) [cs.GR](https://arxiv.org/abs/2208.12242?context=cs.GR) [cs.LG](https://arxiv.org/abs/2208.12242?context=cs.LG)\n### References & Citations\n  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2208.12242)\n  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2208.12242)\n  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:2208.12242)\n\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citation Loading...\n## BibTeX formatted citation\n\u00d7\nloading...\nData provided by: \n### Bookmark\n[ ![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png) ](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2208.12242&description=DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation \"Bookmark on BibSonomy\") [ ![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png) ](https://reddit.com/submit?url=https://arxiv.org/abs/2208.12242&title=DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation \"Bookmark on Reddit\")\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\nConnected Papers Toggle\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\nLitmaps Toggle\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\nscite.ai Toggle\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_\nDagsHub Toggle\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\nGotitPub Toggle\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\nHuggingface Toggle\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\nLinks to Code Toggle\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\nScienceCast Toggle\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\nDemos\n# Demos\nReplicate Toggle\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\nSpaces Toggle\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\nSpaces Toggle\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\nCore recommender toggle\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n\nAbout arXivLabs \n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2208.12242) | [Disable MathJax](javascript:setMathjaxCookie\\(\\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n  * [About](https://info.arxiv.org/about)\n  * [Help](https://info.arxiv.org/help)\n\n\n  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)\n  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)\n\n\n  * [Copyright](https://info.arxiv.org/help/license/index.html)\n  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n\n  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n  * [arXiv Operational Status ](https://status.arxiv.org) Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n\n\n",
    "answer": "```markdown\n# TL;DR: DreamBooth\n\n**Authors:** Nataniel Ruiz et al.  \n**Published:** CVPR 2023  \n**Tags:** Generative AI, LLMs, Text-to-Image Models\n\n**Summary:** DreamBooth introduces a method for fine-tuning text-to-image diffusion models using a few images of a subject, enabling personalized image generation in diverse contexts. It preserves key features while allowing for recontextualization and artistic rendering. A new dataset and evaluation protocol are provided for subject-driven generation.\n```"
  },
  {
    "instruction": "[RunPod](https://www.runpod.io/)\n[Pricing](https://www.runpod.io/gpu-instance/pricing)[Serverless](https://www.runpod.io/serverless-gpu)[Blog](https://blog.runpod.io)[Docs](https://docs.runpod.io)\n[Contact sales](https://meetings.hubspot.com/runpodsales/runpod)[Sign Up](https://www.runpod.io/console/signup)[Login](https://www.runpod.io/console/login)\nAll in one cloud.\nTrain, fine-tune and deploy AImodels with RunPod.\n[Get started](https://www.runpod.io/console/signup)\nRunPod works with Startups, Academic Institutions, and Enterprises.\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2750%27%20height=%2760%27/%3e)![opencv logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2790%27%20height=%2753%27/%3e)![replika logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27152%27%20height=%2737%27/%3e)![datasciencedojo logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2735%27/%3e)![jina logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27130%27%20height=%2723%27/%3e)![defined ai logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27120%27%20height=%2744%27/%3e)![otovo logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2790%27%20height=%2735%27/%3e)![abzu logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27150%27%20height=%2727%27/%3e)![aftershoot logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27120%27%20height=%2733%27/%3e)![krnl logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2750%27%20height=%2760%27/%3e)![opencv logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2790%27%20height=%2753%27/%3e)![replika logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27152%27%20height=%2737%27/%3e)![datasciencedojo logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2735%27/%3e)![jina logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27130%27%20height=%2723%27/%3e)![defined ai logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27120%27%20height=%2744%27/%3e)![otovo logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2790%27%20height=%2735%27/%3e)![abzu logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27150%27%20height=%2727%27/%3e)![aftershoot logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27120%27%20height=%2733%27/%3e)![krnl logo](https://www.runpod.io/)\n1\nDevelop\nGlobally distributed GPU cloud for your AI workloads\nDeploy any GPU workload seamlessly, so you can focus less on infrastructure and more on running ML models.\nPyTorch\nID: twnw98clgxxf2z\n$2.89/hour\n200 GB Disk: 200 GB Pod Volume\nVolume Path: /workspace\n1 x H100 PCIe\n9 vCPU 50 GB RAM\nCA\n8654 Mbps\n938 Mbps\n963 MBps\n0\n2025-03-30T17:42:45.806Z\ncreate pod network\n1\n2025-03-30T17:42:46.806Z\ncreate 20GB network volume\n2\n2025-03-30T17:42:47.806Z\ncreate container runpod/pytorch:3.10-2.0.0-117\n3\n2025-03-30T17:42:48.806Z\n3.10-2.0.0-117 Pulling from runpod/pytorch\n4\n2025-03-30T17:42:49.806Z\nDigest: sha256:2dbf81dd888d383620a486f83ad2ff47540c6cb5e02a61e74b8db03a715488d6\n5\n2025-03-30T17:42:50.806Z\nStatus: Image is up to date for runpod/pytorch:3.10-2.0.0-117\n6\n2025-03-30T17:42:51.806Z\nstart container\n## Spin up a GPU pod in seconds\nit's a pain to having to wait upwards of 10 minutes for your pods to spin up - we've cut the cold-boot time down to milliseconds, so you can start building within seconds of deploying your pods.\n[Spin up a pod](https://www.runpod.io/console/deploy)\n## Choose from 50+ templates ready out-of-the-box, or bring your own custom container.\nGet setup instantly with PyTorch, Tensorflow, or any other preconfigured environment you might need for your machine learning workflow. Along with managed and community templates, we also let you configure your own template to fit your deployment needs.\n[Browse templates](https://www.runpod.io/console/explore)\nPyTorch\n[Deploy](https://www.runpod.io/console/explore/runpod-torch-v220)\nTensorflow\n[Deploy](https://www.runpod.io/console/explore/runpod-tensorflow)\nDocker\n[Deploy](https://www.runpod.io/console/explore/runpod-kobold-united)\n![runpod logo](https://www.runpod.io/static/svg/runpod-template-logo.svg)\nRunpod\n[Deploy](https://www.runpod.io/console/explore/runpod-desktop)\n## Powerful & Cost-Effective GPUsfor Every Workload\n[See all GPUs](https://www.runpod.io/console/deploy)\nThousands of GPUs across 30+ Regions\nDeploy any container on Secure Cloud. Public and private image repos are supported. Configure your environment the way you want.\nZero fees for ingress/egress\nGlobal interoperability\n99.99% Uptime\n$0.05/GB/month Network Storage\n![amd](https://www.runpod.io/static/images/companies/amd.svg)\nStarting from $2.49/hr\nMI300X\n192GB VRAM\n283GB RAM\n24 vCPUs\n$2.49/hr\nSecure Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $1.99/hr\nH100 PCIe\n80GB VRAM\n188GB RAM\n16 vCPUs\n$2.39/hr\nSecure Cloud\n$1.99/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $1.19/hr\nA100 PCIe\n80GB VRAM\n117GB RAM\n8 vCPUs\n$1.64/hr\nSecure Cloud\n$1.19/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $1.89/hr\nA100 SXM\n80GB VRAM\n125GB RAM\n16 vCPUs\n$1.89/hr\nSecure Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.44/hr\nA40\n48GB VRAM\n50GB RAM\n9 vCPUs\n$0.44/hr\nSecure Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.99/hr\nL40\n48GB VRAM\n94GB RAM\n8 vCPUs\n$0.99/hr\nSecure Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.79/hr\nL40S\n48GB VRAM\n62GB RAM\n12 vCPUs\n$0.86/hr\nSecure Cloud\n$0.79/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.33/hr\nRTX A6000\n48GB VRAM\n50GB RAM\n8 vCPUs\n$0.59/hr\nSecure Cloud\n$0.33/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.16/hr\nRTX A5000\n24GB VRAM\n25GB RAM\n8 vCPUs\n$0.29/hr\nSecure Cloud\n$0.16/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.34/hr\nRTX 4090\n24GB VRAM\n29GB RAM\n6 vCPUs\n$0.69/hr\nSecure Cloud\n$0.34/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.22/hr\nRTX 3090\n24GB VRAM\n24GB RAM\n4 vCPUs\n$0.43/hr\nSecure Cloud\n$0.22/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.20/hr\nRTX A4000 Ada\n20GB VRAM\n31GB RAM\n4 vCPUs\n$0.38/hr\nSecure Cloud\n$0.20/hr\nCommunity Cloud\n2\nScale\nScale ML inferencewith Serverless\nRun your AI models with autoscaling, job queueing and sub 250ms cold start time.\n[Deploy Now](https://www.runpod.io/console/serverless)\nAutoscale in seconds\nRespond to user demand in real time with GPU workers thatscale from 0 to 100s in seconds.\nFlex \nWorkers\nActive \nWorkers\n10 GPUs\n6:24AM\n100 GPUs\n11:34AM\n20 GPUs\n1:34PM\nUsage Analytics\nReal-time usage analytics for your endpoint with metrics on completed and failed requests. Useful for endpoints that have fluctuating usage profiles throughout the day.\n[See the console ](https://www.runpod.io/console/serverless)\nActive\nRequests\nCompleted:\n2,277\nRetried:\n21\nFailed:\n9\nExecution Time\nTotal:\n1,420s\nP70:\n8s\nP90:\n19s\nP98:\n22s\nExecution Time Analytics\nDebug your endpoints with detailed metrics on execution time. Useful for hosting models that have varying execution times, like large language models. You can also monitor delay time, cold start time, cold start count, GPU utilization, and more.\n[See the console ](https://www.runpod.io/console/serverless)\nReal-Time Logs\nGet descriptive, real-time logs to show you exactly what's happening across your active and flex GPU workers at all times.\n[See the console ](https://www.runpod.io/console/serverless)\nworker logs -- zsh\n2024-03-15T19:56:00.8264895Z INFO | Started job db7c792024-03-15T19:56:03.2667597Z0% | | 0/28 [00:00<?, ?it/s]12% |\u2588\u2588 | 4/28 [00:00<00:01, 12.06it/s]38% |\u2588\u2588\u2588\u2588 | 12/28 [00:00<00:01, 12.14it/s]77% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 22/28 [00:01<00:00, 12.14it/s]100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28/28 [00:02<00:00, 12.13it/s]2024-03-15T19:56:04.7438407Z INFO | Completed job db7c79 in 2.9s2024-03-15T19:57:00.8264895Z INFO | Started job ea1r142024-03-15T19:57:03.2667597Z0% | | 0/28 [00:00<?, ?it/s]15% |\u2588\u2588 | 4/28 [00:00<00:01, 12.06it/s]41% |\u2588\u2588\u2588\u2588 | 12/28 [00:00<00:01, 12.14it/s]80% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 22/28 [00:01<00:00, 12.14it/s]100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28/28 [00:02<00:00, 12.13it/s]2024-03-15T19:57:04.7438407Z INFO | Completed job ea1r14 in 2.9s2024-03-15T19:58:00.8264895Z INFO | Started job gn3a252024-03-15T19:58:03.2667597Z0% | | 0/28 [00:00<?, ?it/s]18% |\u2588\u2588 | 4/28 [00:00<00:01, 12.06it/s]44% |\u2588\u2588\u2588\u2588 | 12/28 [00:00<00:01, 12.14it/s]83% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 22/28 [00:01<00:00, 12.14it/s]100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28/28 [00:02<00:00, 12.13it/s]2024-03-15T19:58:04.7438407Z INFO | Completed job gn3a25 in 2.9s\n![cloud image everything header](https://www.runpod.io/static/images/everything-clouds.png)\nEverything your app needs. All in \none cloud.\n99.99%\nguaranteed uptime\n10PB+\nnetwork storage\n6,811,142,348\nrequests\nAI Inference\nWe handle millions of inference requests a day. Scale your machine learning inference while keeping costs low with RunPod serverless.\nAI Training\nRun machine learning training tasks that can take up to 7 days. Train on our available NVIDIA H100s and A100s or reserve AMD MI300Xs and AMD MI250s a year in advance.\nAutoscale\nServerless GPU workers scale from 0 to n with 8+ regions distributed globally. You only pay when your endpoint receives and processes a request.\nBring Your Own Container\nDeploy any container on our AI cloud. Public and private image repositories are supported. Configure your environment the way you want.\nZero Ops Overhead\nRunPod handles all the operational aspects of your infrastructure from deploying to scaling. You bring the models, let us handle the ML infra.\nNetwork Storage\nServerless workers can access network storage volume backed by NVMe SSD with up to 100Gbps network throughput. 100TB+ storage size is supported, contact us if you need 1PB+.\nEasy-to-use CLI\nUse our CLI tool to automatically hot reload local changes while developing, and deploy on Serverless when you\u2019re done tinkering.\nSecure & Compliant\nRunPod AI Cloud is built on enterprise-grade GPUs with world-class compliance and security to best serve your machine learning models.\nLightning Fast Cold-Start\nWith Flashboot, watch your cold-starts drop to sub 250 milliseconds. No more waiting for GPUs to warm up when usage is unpredictable.\nCertifications\nRunPod has obtained SOC2 Type 1 Certification as of February 2025. Our data center partners maintain leading compliance standards (including HIPAA, SOC2, and ISO 27001)\n![gpu background](https://www.runpod.io/static/images/soc2-light.webp)\nLaunch your AI application in minutes\nStart building with the most cost-effective platform for developing and scaling machine learning models.\n[Get started](https://www.runpod.io/console/signup)\nProducts\n[Secure Cloud](https://www.runpod.io/console/gpu-secure-cloud)[Community Cloud](https://www.runpod.io/console/gpu-cloud)[Serverless](https://www.runpod.io/console/serverless)\nResources\n[Docs](https://docs.runpod.io/overview)[FAQ](https://docs.runpod.io/faq)[Blog](https://blog.runpod.io)[Become a Host](https://www.runpod.io/console/host/docs/faq)[GPU Benchmarks](https://www.runpod.io/dir/gpu-benchmarks)[GPU Models](https://www.runpod.io/dir/gpu-models)[Articles](https://www.runpod.io/articles)[AI FAQ](https://www.runpod.io/ai-faq)[LLM Models](https://www.runpod.io/dir/llm-models)\nCompany\n[About](https://www.runpod.io/about)[Careers](https://job-boards.greenhouse.io/runpod)[Compliance](https://www.runpod.io/compliance)[Cookie Policy](https://www.runpod.io/legal/cookie-policy)[Disclaimer](https://www.runpod.io/legal/disclaimer)[Privacy Policy](https://www.runpod.io/legal/privacy-policy)[Terms of Service](https://www.runpod.io/legal/terms-of-service)\nContact\n[Contact Us](https://contact.runpod.io)[Discord](https://discord.gg/cUpRmau42V)help@runpod.ioreferrals@runpod.iopress@runpod.io\n[](https://github.com/runpod)[](https://discord.gg/cUpRmau42V)[](https://twitter.com/runpod_io)[](https://www.instagram.com/runpod.io)\n[RunPod](https://www.runpod.io/)\nCopyright \u00a9 2025. All rights reserved.\n",
    "answer": "```markdown\n## TL;DR Summary\n\nRunPod offers a cloud platform for training, fine-tuning, and deploying AI models with global GPU access. Key features include fast pod deployment, autoscaling, and support for various ML frameworks. It ensures 99.99% uptime and zero operational overhead, making it ideal for startups and enterprises. \n\n**Tags:** Generative AI, LLMs, AI Training, AI Inference\n```"
  },
  {
    "instruction": "[Skip to content](https://towardsdatascience.com/create-mixtures-of-experts-with-mergekit-11b318c99562/#wp--skip-link--target)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nThe world\u2019s leading publication for data science, AI, and ML professionals.\nSign in\nSign out\n[Contributor Portal](https://contributor.insightmediagroup.io/)\n  * [Latest](https://towardsdatascience.com/latest/)\n  * [Editor\u2019s Picks](https://towardsdatascience.com/tag/editors-pick/)\n  * [Deep Dives](https://towardsdatascience.com/tag/deep-dives/)\n  * [Contribute](https://towardsdatascience.com/questions-96667b06af5/)\n  * [Newsletter](https://newsletter.towardsdatascience.com/subscription-to-the-newsletter)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\n\n\nToggle Mobile Navigation\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [X](https://x.com/TDataScience)\n\n\nToggle Search\nSearch\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\n# Create Mixtures of Experts with MergeKit\nCombine multiple models into a single MoE \n[Maxime Labonne](https://towardsdatascience.com/author/mlabonne/)\nMar 27, 2024\n10 min read\nShare \n![Image by author](https://towardsdatascience.com/wp-content/uploads/2024/03/1kJ4ALMDzNaq9W88ZtVnhiQ.jpeg)Image by author\nThanks to the release of Mixtral, the **Mixture of Experts** (MoE) architecture has become popular in recent months. This architecture offers an interesting tradeoff: higher performance at the cost of increased VRAM usage. While Mixtral and other MoE architectures are pre-trained from scratch, another method of creating MoE has recently appeared. Thanks to Arcee\u2019s [MergeKit](https://github.com/arcee-ai/mergekit) library, we now have a new way of creating MoEs by ensembling several pre-trained models. These are often referred to as **frankenMoEs** or **MoErges** to distinguish them from the pre-trained MoEs.\nIn this article, we will detail how the MoE architecture works and how frankenMoEs are created. Finally, we will make our [own frankenMoE](https://huggingface.co/mlabonne/Beyonder-4x7B-v3) with MergeKit and evaluate it on several benchmarks. The code is available on Google Colab in a wrapper called [LazyMergeKit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y).\nSpecial thanks to [Charles Goddard](https://github.com/cg123), the creator of MergeKit, for proofreading this article.\n## \ud83d\udd00 Introduction to MoEs\nA Mixture of Experts is an architecture designed for improved efficiency and performance. It uses multiple specialized subnetworks, known as \"**experts**.\" Unlike dense models, where the entire network is activated, MoEs only activate relevant experts based on the input. This results in faster training and more efficient inference.\nThere are two components at the core of an MoE model:\n  1. **Sparse MoE Layers** : These replace the dense feed-forward network layers in the transformer architecture. Each MoE layer contains several experts, and only a subset of these experts are engaged for a given input.\n  2. **Gate Network or Router** : This component determines which tokens are processed by which experts, ensuring that each part of the input is handled by the most suitable expert(s).\n\n\nIn the following example, we show how a Mistral-7B block is transformed into an MoE block with a sparse MoE layer (feedforward network 1, 2, and 3) and a router. This example represents an MoE with three experts, where two are currently engaged (FFN 1 and FFN 3).\n![Image by author](https://towardsdatascience.com/wp-content/uploads/2024/03/1gGQxuStcGfSS_Boy9rKvAw.png)Image by author\nMoEs also come with their own set of challenges, especially in terms of fine-tuning and memory requirements. The fine-tuning process can be difficult due to the model\u2019s complexity, with the need to **balance expert usage** during training to properly train the gating weights to select the most relevant ones. In terms of memory, even though only a fraction of the total parameters are used during inference, the entire model, including all experts, needs to be **loaded into memory** , which requires high VRAM capacity.\nMore specifically, there are two essential parameters when it comes to MoEs:\n  * **Number of experts** (`num_local_experts`): This determines the total number of experts in the architecture (e.g., 8 for Mixtral). The higher the number of experts, the higher the VRAM usage.\n  * **Number of experts/token** (`num_experts_per_tok`): This determines the number of experts that are engaged for each token and each layer (e.g., 2 for Mixtral). There is a tradeoff between a high number of experts per token for accuracy (but diminishing returns) vs. a low number for fast training and inference.\n\n\nHistorically, MoEs have underperformed dense models. However, the release of [Mixtral-8x7B](https://arxiv.org/abs/2401.04088) in December 2023 shook things up and showed impressive performance for its size. Additionally, GPT-4 is also rumored to be an MoE, which would make sense as it would be a lot cheaper to run and train for OpenAI compared to a dense model. In addition to these recent excellent MoEs, we now have a new way of creating MoEs with MergeKit: frankenMoEs, also called MoErges.\n## \ud83e\udddf\u200d\u2642\ufe0f True MoEs vs. frankenMoEs\nThe main difference between true MoEs and frankenMoEs is how they\u2019re trained. In the case of true MoEs, the experts and the router are trained jointly. In the case of frankenMoEs, we upcycle existing models and initialize the router afterward.\nIn other words, we copy the weights of the layer norm and self-attention layers from a base model, and then copy the weights of the FFN layers found in each expert. This means that besides the FFNs, all the other parameters are shared. This explains why Mixtral-8x7B with eight experts doesn\u2019t have 8*7 = 56B parameters, but about 45B. This is also why using two experts per token gives the inference speed (FLOPs) of a 12B dense model instead of 14B.\nFrankenMoEs are about selecting the most relevant experts and initializing them properly. MergeKit currently implements three ways of initializing the routers:\n  1. **[Random](https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L139-L142)** : Random weights. Be careful when using it as the same experts might be selected every time (it requires further fine-tuning or `num_local_experts = num_experts_per_tok`, which means you don\u2019t need any routing).\n  2. **[Cheap embed](https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L91C1-L109C37)** : It uses the raw embeddings of the input tokens directly and applies the same transformation across all layers. This method is computationally inexpensive and suitable for execution on less powerful hardware.\n  3. **[Hidden](https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L70-L88)** : It creates hidden representations of a list of positive and negative prompts by extracting them from the last layer of the LLM. They are averaged and normalized to initialize the gates. More information about it is available on [Charles Goddard\u2019s blog](https://goddard.blog/posts/clown-moe/).\n\n\nAs you can guess, the \"hidden\" initialization is the most efficient to correctly route the tokens to the most relevant experts. In the next section, we will create our own frankenMoE using this technique.\n## \ud83d\udcbb Creating a frankenMoE\nTo create our frankenMoE, we need to select `n` experts. In this case, we will rely on Mistral-7B thanks to its popularity and relatively small size. However, eight experts like in Mixtral is quite a lot, as we need to fit all of them in memory. For efficiency, I\u2019ll only use four experts in this example, with two of them engaged for each token and each layer. In this case, we will end up with a model with 24.2B parameters instead of 4*7 = 28B parameters.\nHere, our goal is to create a well-rounded model that can do pretty much everything: write stories, explain articles, code in Python, etc. We can decompose this requirement into four tasks and select the best expert for each of them. This is how I decomposed it:\n  * **Chat model** : a general-purpose model that is used in most interactions. I used [mlabonne/AlphaMonarch-7B](https://huggingface.co/mlabonne/AlphaMonarch-7B), which perfectly satisfies the requirements.\n  * **Code model** : a model capable of generating good code. I don\u2019t have a lot of experience with Mistral-7B-based code models, but I found [beowolx/CodeNinja-1.0-OpenChat-7B](https://huggingface.co/beowolx/CodeNinja-1.0-OpenChat-7B) particularly good compared to others.\n  * **Math model** : math is tricky for LLMs, which is why we want a model specialized in math. Thanks to its high MMLU and GMS8K scores, I chose [mlabonne/NeuralDaredevil-7B](https://huggingface.co/mlabonne/NeuralDaredevil-7B) for this purpose.\n  * **Role-play model** : The goal of this model is to write high-quality stories and conversations. I selected [SanjiWatsuki/Kunoichi-DPO-v2\u20137B](https://huggingface.co/SanjiWatsuki/Kunoichi-DPO-v2-7B) because of its good reputation and high MT-Bench score (8.51 vs. 8.30 for Mixtral).\n\n\nNow that we\u2019ve identified the experts we want to use, we can create the YAML configuration that MergeKit will use to create our frankenMoE. This uses the mixtral branch of MergeKit. You can find more information about how to write the configuration [on this page](https://github.com/arcee-ai/mergekit/blob/mixtral/docs/moe.md). Here is our version:\n```\nbase_model: mlabonne/AlphaMonarch-7B\nexperts:\n - source_model: mlabonne/AlphaMonarch-7B\n  positive_prompts:\n  - \"chat\"\n  - \"assistant\"\n  - \"tell me\"\n  - \"explain\"\n  - \"I want\"\n - source_model: beowolx/CodeNinja-1.0-OpenChat-7B\n  positive_prompts:\n  - \"code\"\n  - \"python\"\n  - \"javascript\"\n  - \"Programming\"\n  - \"algorithm\"\n - source_model: SanjiWatsuki/Kunoichi-DPO-v2-7B\n  positive_prompts:\n  - \"storywriting\"\n  - \"write\"\n  - \"scene\"\n  - \"story\"\n  - \"character\"\n - source_model: mlabonne/NeuralDaredevil-7B\n  positive_prompts:\n  - \"reason\"\n  - \"math\"\n  - \"mathematics\"\n  - \"solve\"\n  - \"count\"\n```\n\nFor each expert, I provide five basic positive prompts. You can be a bit fancier and write entire sentences if you want. The best strategy consists of using real prompts that should trigger a particular expert. You can also add negative prompts to do the opposite.\nOnce this is ready, you can save your configuration as `config.yaml`. In the same folder, we will download and install the [mergekit](https://github.com/arcee-ai/mergekit) library (mixtral branch).\n```\ngit clone -b mixtral https://github.com/arcee-ai/mergekit.git\ncd mergekit &amp;&amp; pip install -e .\npip install -U transformers\n```\n\nIf your computer has enough RAM (roughly 24\u201332 GB of RAM), you can run the following command:\n```\nmergekit-moe config.yaml merge --copy-tokenizer\n```\n\nIf you don\u2019t have enough RAM, you can shard the models instead as follows (it will take longer):\n```\nmergekit-moe config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle\n```\n\nThis command automatically downloads the experts and creates the frankenMoE in the `merge` directory. For the `hidden` gate mode, you can also use the `--load-in-4bit` and `--load-in-8bit` options to compute hidden states with lower precision.\nAlternatively, you can copy your configuration into [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y), a wrapper I made to simplify model merging. In this Colab notebook, you can input your model name, select the `mixtral` branch, specify your Hugging Face username/token, and run the cells. After creating your frankenMoE, it will also upload it to the Hugging Face Hub with a nicely formatted model card.\nI called my model [Beyonder-4x7B-v3](https://huggingface.co/mlabonne/Beyonder-4x7B-v3) and created [GGUF versions](https://huggingface.co/mlabonne/Beyonder-4x7B-v3-GGUF) of it using [AutoGGUF](https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu#scrollTo=fD24jJxq7t3k). If you can\u2019t run GGUF versions on your local machine, you can also perform inference using this [Colab notebook](https://colab.research.google.com/drive/1SIfwhpLttmoZxT604LGVXDOI9UKZ_1Aq?usp=sharing).\nTo get a good overview of its capabilities, it has been evaluated on three different benchmarks: Nous\u2019 benchmark suite, EQ-Bench, and the Open LLM Leaderboard. This model is not designed to excel in traditional benchmarks, as the code and role-playing models generally do not apply to those contexts. Nonetheless, it performs remarkably well thanks to strong general-purpose experts.\n**Nous** : Beyonder-4x7B-v3 is one of the best models on Nous\u2019 benchmark suite (evaluation performed using [LLM AutoEval](https://github.com/mlabonne/llm-autoeval)) and significantly outperforms the v2. See the entire leaderboard [here](https://huggingface.co/spaces/mlabonne/Yet_Another_LLM_Leaderboard).\n![](https://towardsdatascience.com/wp-content/uploads/2024/03/0MTWS0BgqX17yq9qa.png)\n**EQ-Bench** : It\u2019s also the best 4x7B model on the [EQ-Bench leaderboard](https://eqbench.com/), outperforming older versions of ChatGPT and Llama-2\u201370b-chat. Beyonder is very close to Mixtral-8x7B-Instruct-v0.1 and Gemini Pro, which are (supposedly) much bigger models.\n![](https://towardsdatascience.com/wp-content/uploads/2024/03/0ebG79Y254SJp58KG.png)\n**Open LLM Leaderboard** : Finally, it\u2019s also a strong performer on the Open LLM Leaderboard, significantly outperforming the v2 model.\n![](https://towardsdatascience.com/wp-content/uploads/2024/03/0RpN_fYTFfzxvSJ66.png)\nOn top of these quantitative evaluations, I recommend checking the model\u2019s outputs in a more qualitative way using a GGUF version on [LM Studio](https://lmstudio.ai/). A common way of testing these models is to gather a private set of questions and check their outputs. With this strategy, I found that Beyonder-4x7B-v3 is quite robust to changes in the user and system prompts compared to other models, including AlphaMonarch-7B. This is pretty cool as it improves the usefulness of the model in general.\nFrankenMoEs are a promising but still experimental approach. The trade-offs, like higher VRAM demand and slower inference speeds, can make it challenging to see their advantage over simpler merging techniques like SLERP or DARE TIES. Especially, when you use frankenMoEs with just two experts, they might not perform as well as if you had simply merged the two models. However, frankenMoEs excel in preserving knowledge, which can result in stronger models, as demonstrated by Beyonder-4x7B-v3. With the right hardware, these drawbacks can be effectively mitigated.\n## Conclusion\nIn this article, we introduced the Mixture of Experts architecture. Unlike traditional MoEs that are trained from scratch, MergeKit facilitates the creation of MoEs by ensembling experts, offering an innovative approach to improving model performance and efficiency. We detailed the process of creating a frankenMoE with MergeKit, highlighting the practical steps involved in selecting and combining different experts to produce a high-quality MoE.\nThanks for reading this article. I encourage you to try to make your own FrankenMoEs using [LazyMergeKit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y): select a few models, create your config based Beyonder\u2019s, and run the notebook to create your own models! If you liked this article, please follow me on [Hugging Face](https://huggingface.co/mlabonne) and X/Twitter [@maximelabonne](https://twitter.com/maximelabonne).\n## References\n  * [Mixtral of Experts](https://arxiv.org/abs/2401.04088) by Jiang et al. (2023)\n  * [Mixture of Experts for Clowns](https://goddard.blog/posts/clown-moe/) by Charles Goddard (2023)\n  * [Mixture of Experts Explained](https://huggingface.co/blog/moe) by Sanseviero et al. (2023)\n  * [Adaptive Mixture of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf) by Jacobs et al. (1991)\n  * [Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints](https://arxiv.org/abs/2212.05055) by Komatsuzaki et al. (2022)\n\n\n_Learn more about machine learning and support my work with one click \u2013 become a Medium member here:_\n> [**Join Medium with my referral link \u2013 Maxime Labonne**](https://medium.com/@mlabonne/membership)\nWritten By\nMaxime Labonne\n[See all from Maxime Labonne](https://towardsdatascience.com/author/mlabonne/)\nTopics:\n[Artificial Intelligence](https://towardsdatascience.com/tag/artificial-intelligence/), [Data Science](https://towardsdatascience.com/tag/data-science/), [Editors Pick](https://towardsdatascience.com/tag/editors-pick/), [Large Language Models](https://towardsdatascience.com/tag/large-language-models/), [Programming](https://towardsdatascience.com/tag/programming/)\nShare this article:\n  * [ Share on Facebook  ](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-mixtures-of-experts-with-mergekit-11b318c99562%2F&title=Create%20Mixtures%20of%20Experts%20with%20MergeKit)\n  * [ Share on LinkedIn  ](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-mixtures-of-experts-with-mergekit-11b318c99562%2F&title=Create%20Mixtures%20of%20Experts%20with%20MergeKit)\n  * [ Share on X  ](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-mixtures-of-experts-with-mergekit-11b318c99562%2F&text=Create%20Mixtures%20of%20Experts%20with%20MergeKit)\n\n\n## Related Articles\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/0c09RmbCCpfjAbSMq.png)\n## [Implementing Convolutional Neural Networks in TensorFlow](https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nStep-by-step code guide to building a Convolutional Neural Network \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nAugust 20, 2024\n6 min read\n  * ## [What Do Large Language Models \u201cUnderstand\u201d?](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA deep dive on the meaning of understanding and how it applies to LLMs \n[Tarik Dzekman](https://towardsdatascience.com/author/tarikdzekman/)\nAugust 21, 2024\n31 min read\n  * ![Photo by Krista Mangulsone on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/0GyVVTbgotH-DhGPH-scaled.jpg)\n## [How to Forecast Hierarchical Time Series](https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA beginner\u2019s guide to forecast reconciliation \n[Dr. Robert K\u00fcbler](https://towardsdatascience.com/author/dr-robert-kuebler/)\nAugust 20, 2024\n13 min read\n  * ![Photo by davisuko on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/1bAABgtZtAIG5YW1oEjW3pA-scaled.jpeg)\n## [Hands-on Time Series Anomaly Detection using Autoencoders, with Python](https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nHere\u2019s how to use Autoencoders to detect signals with anomalies in a few lines of\u2026 \n[Piero Paialunga](https://towardsdatascience.com/author/piero-paialunga/)\nAugust 21, 2024\n12 min read\n  * ## [Solving a Constrained Project Scheduling Problem with Quantum Annealing](https://towardsdatascience.com/solving-a-constrained-project-scheduling-problem-with-quantum-annealing-d0640e657a3b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nSolving the resource constrained project scheduling problem (RCPSP) with D-Wave\u2019s hybrid constrained quadratic model (CQM) \n[Luis Fernando P\u00c9REZ ARMAS, Ph.D.](https://towardsdatascience.com/author/luisfernandopa1212/)\nAugust 20, 2024\n29 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2023/02/1VEUgT5T4absnTqBMOEuNig.png)\n## [Back To Basics, Part Uno: Linear Regression and Cost Function](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nAn illustrated guide on essential machine learning concepts \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nFebruary 3, 2023\n6 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/1kM8tfYcdaoccB1HX71YDig.png)\n## [Must-Know in Statistics: The Bivariate Normal Projection Explained](https://towardsdatascience.com/must-know-in-statistics-the-bivariate-normal-projection-explained-ace7b2f70b5b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nDerivation and practical examples of this powerful concept \n[Luigi Battistoni](https://towardsdatascience.com/author/lu-battistoni/)\nAugust 14, 2024\n7 min read\n  * ![Photo by Jess Bailey on Unsplash](https://towardsdatascience.com/wp-content/uploads/2022/09/11tHmNYFaWWtWG5I7bNeN6g-scaled.jpeg)\n## [How to Make the Most of Your Experience as a TDS Author](https://towardsdatascience.com/how-to-make-the-most-of-your-experience-as-a-tds-author-b1e056be63f1/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nA quick guide to our resources and FAQ \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nSeptember 13, 2022\n4 min read\n  * ![Photo by Alex Geerts on Unsplash](https://towardsdatascience.com/wp-content/uploads/2020/11/0BF38u2sw4WQdaMLS-scaled.jpg)\n## [Our Columns](https://towardsdatascience.com/our-columns-53501f74c86d/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nColumns on TDS are carefully curated collections of posts on a particular idea or category\u2026 \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nNovember 14, 2020\n4 min read\n\n\n  * [YouTube](https://www.youtube.com/c/TowardsDataScience)\n  * [X](https://x.com/TDataScience)\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [Threads](https://www.threads.net/@towardsdatascience)\n  * [Bluesky](https://bsky.app/profile/towardsdatascience.com)\n\n\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nYour home for data science and Al. The world\u2019s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\n\u00a9  Insight Media Group, LLC 2025 \n  * [About](https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/)\n  * [Privacy Policy](https://towardsdatascience.com/privacy-policy/)\n  * [Terms of Use](https://towardsdatascience.com/website-terms-of-use/)\n\n\n[Towards Data Science is now independent!](https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/)\nCookies Settings\n## Sign up to our newsletter\nEmail address*\nFirst name*\nLast name*\nJob title*\nJob level*\nPlease SelectC-LevelVP/DirectorManager/SupervisorMid Level or Senior Non-Managerial StaffEntry Level/Junior StaffFreelancer/ContractorStudent/InternOther\nCompany name*\n  * I consent to receive newsletters and other communications from Towards Data Science publications.*\n\n\n![Company Logo](https://cdn.cookielaw.org/logos/static/ot_company_logo.png)\n## Privacy Preference Center\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer. [More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nAllow All\n###  Manage Consent Preferences\n#### Functional Cookies\nFunctional Cookies Active\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\n#### Strictly Necessary Cookies\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n#### Performance Cookies\nPerformance Cookies Active\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n#### Targeting Cookies\nTargeting Cookies Active\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nBack Button\n### Cookie List\nSearch Icon\nFilter Icon\nClear\ncheckbox label label\nApply Cancel\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nReject All Confirm My Choices\n[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-consent/)\nSome areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe article discusses the Mixture of Experts (MoE) architecture, highlighting the new MergeKit library for creating frankenMoEs by combining pre-trained models. It details the process of building a frankenMoE, evaluates the Beyonder-4x7B-v3 model on various benchmarks, and emphasizes the trade-offs in VRAM usage and performance.\n```"
  },
  {
    "instruction": "# [eugeneyan](https://eugeneyan.com/)\n  * [Start Here](https://eugeneyan.com/start-here/ \"Start Here\")\n  * [Writing](https://eugeneyan.com/writing/ \"Writing\")\n  * [Speaking](https://eugeneyan.com/speaking/ \"Speaking\")\n  * [Prototyping](https://eugeneyan.com/prototyping/ \"Prototyping\")\n  * [About](https://eugeneyan.com/about/ \"About\")\n\n\n# Patterns for Building LLM-based Systems & Products\n[ [llm](https://eugeneyan.com/tag/llm/) [engineering](https://eugeneyan.com/tag/engineering/) [production](https://eugeneyan.com/tag/production/) [\ud83d\udd25](https://eugeneyan.com/tag/\ud83d\udd25/) ]  \u00b7 66 min read \n> Discussions on [HackerNews](https://news.ycombinator.com/item?id=36965993), [Twitter](https://twitter.com/eugeneyan/status/1686531758701899776), and [LinkedIn](https://www.linkedin.com/posts/eugeneyan_patterns-for-building-llm-based-systems-activity-7092300473981927424-_wVo)\n\u201cThere is a large class of problems that are easy to imagine and build demos for, but extremely hard to make products out of. For example, self-driving: It\u2019s easy to demo a car self-driving around a block, but making it into a product takes a decade.\u201d - [Karpathy](https://twitter.com/eugeneyan/status/1672692174704766976)\nThis write-up is about practical patterns for integrating large language models (LLMs) into systems & products. We\u2019ll build on academic research, industry resources, and practitioner know-how, and distill them into key ideas and practices.\nThere are seven key patterns. They\u2019re also organized along the spectrum of improving performance vs. reducing cost/risk, and closer to the data vs. closer to the user.\n  * [Evals](https://eugeneyan.com/writing/llm-patterns/#evals-to-measure-performance): To measure performance\n  * [RAG](https://eugeneyan.com/writing/llm-patterns/#retrieval-augmented-generation-to-add-knowledge): To add recent, external knowledge\n  * [Fine-tuning](https://eugeneyan.com/writing/llm-patterns/#fine-tuning-to-get-better-at-specific-tasks): To get better at specific tasks\n  * [Caching](https://eugeneyan.com/writing/llm-patterns/#caching-to-reduce-latency-and-cost): To reduce latency & cost\n  * [Guardrails](https://eugeneyan.com/writing/llm-patterns/#guardrails-to-ensure-output-quality): To ensure output quality\n  * [Defensive UX](https://eugeneyan.com/writing/llm-patterns/#defensive-ux-to-anticipate--handle-errors-gracefully): To anticipate & manage errors gracefully\n  * [Collect user feedback](https://eugeneyan.com/writing/llm-patterns/#collect-user-feedback-to-build-our-data-flywheel): To build our data flywheel\n\n\n(Also see this addendum on [how to match these LLM patterns to potential problems](https://eugeneyan.com/writing/llm-problems/).)\n![Image](https://eugeneyan.com/assets/llm-patterns-og.png)\nLLM patterns: From data to user, from defensive to offensive ([see connections between patterns](https://eugeneyan.com/writing/llm-patterns/#conclusion))\n## Evals: To measure performance\nEvaluations are a set of measurements used to assess a model\u2019s performance on a task. They include benchmark data and metrics. From a [HackerNews comment](https://news.ycombinator.com/item?id=36789901):\n> How important evals are to the team is a major differentiator between folks rushing out hot garbage and those seriously building products in the space.\n### Why evals?\nEvals enable us to measure how well our system or product is doing and detect any regressions. (A system or product can be made up of multiple components such as LLMs, prompt templates, retrieved context, and parameters like temperature.) A representative set of evals takes us a step towards measuring system changes at scale. Without evals, we would be flying blind, or would have to visually inspect LLM outputs with each change.\n### More about evals\n**There are many benchmarks in the field of language modeling**. Some notable ones are:\n  * **[MMLU](https://arxiv.org/abs/2009.03300)** : A set of 57 tasks that span elementary math, US history, computer science, law, and more. To perform well, models must possess extensive world knowledge and problem-solving ability.\n  * **[EleutherAI Eval](https://github.com/EleutherAI/lm-evaluation-harness)** : Unified framework to test models via zero/few-shot settings on 200 tasks. Incorporates a large number of evals including BigBench, MMLU, etc.\n  * **[HELM](https://arxiv.org/abs/2211.09110)** : Instead of specific tasks and metrics, HELM offers a comprehensive assessment of LLMs by evaluating them across domains. Metrics include accuracy, calibration, robustness, fairness, bias, toxicity, etc. Tasks include Q&A, information retrieval, summarization, text classification, etc.\n  * **[AlpacaEval](https://github.com/tatsu-lab/alpaca_eval)** : Automated evaluation framework which measures how often a strong LLM (e.g., GPT-4) prefers the output of one model over a reference model. Metrics include win rate, bias, latency, price, variance, etc. Validated to have high agreement with 20k human annotations.\n\n\nWe can group metrics into two categories: context-dependent or context-free.\n  * **Context-dependent** : These take context into account. They\u2019re often proposed for a specific task; repurposing them for other tasks will require some adjustment.\n  * **Context-free** : These aren\u2019t tied to the context when evaluating generated output; they only compare the output with the provided gold references. As they\u2019re task agnostic, they\u2019re easier to apply to a wide variety of tasks.\n\n\nTo get a better sense of these metrics (and their potential shortfalls), we\u2019ll explore a few of the commonly used metrics such as BLEU, ROUGE, BERTScore, and MoverScore.\n**[BLEU](https://dl.acm.org/doi/10.3115/1073083.1073135) (Bilingual Evaluation Understudy)** is a precision-based metric: It counts the number of n-grams in the generated output that also show up in the reference, and then divides it by the total number of words in the output. It\u2019s predominantly used in machine translation and remains a popular metric due to its cost-effectiveness.\nFirst, precision for various values of \\\\(n\\\\) is computed:\n\\\\[\\text{precision}_n = \\frac{\\sum_{p \\in \\text{output}} \\sum_{\\text{n-gram} \\in p} \\text{Count}_{\\text{clip}} (\\text{n-gram})}{\\sum_{p \\in \\text{output}} \\sum_{\\text{n-gram} \\in p} \\text{Count}(\\text{n-gram})}\\\\] \n\\\\(Count_{clip}(\\text{n-gram})\\\\) is clipped by the maximum number of times an n-gram appears in any corresponding reference sentence.\n\\\\[\\text{Count}_{\\text{clip}}(n\\text{-gram}) = \\min \\left(\\text{matched } n\\text{-gram count}, \\max_{r \\in R} \\left(n\\text{-gram count in } r\\right)\\right)\\\\] \nOnce we\u2019ve computed precision at various \\\\(n\\\\), a final BLEU-N score is computed as the geometric mean of all the \\\\(precision_n\\\\) scores.\nHowever, since precision relies solely on n-grams and doesn\u2019t consider the length of the generated output, an output containing just one unigram of a common word (like a stop word) would achieve perfect precision. This can be misleading and encourage outputs that contain fewer words to increase BLEU scores. To counter this, a brevity penalty is added to penalize excessively short sentences.\n\\\\[BP = \\begin{cases} 1 & \\text{if } |p| > |r| \\\\\\ e^{1-\\frac{|r|}{|p|}} & \\text{otherwise} \\end{cases}\\\\] \nThus, the final formula is:\n\\\\[\\text{BLEU-N} = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} W_n \\log(\\text{precision}_n)\\right)\\\\] \n**[ROUGE](https://aclanthology.org/W04-1013/) (Recall-Oriented Understudy for Gisting Evaluation)**: In contrast to BLEU, ROUGE is recall-oriented. It counts the number of words in the reference that also occur in the output. It\u2019s typically used to assess automatic summarization tasks.\nThere are several ROUGE variants. ROUGE-N is most similar to BLEU in that it also counts the number of matching n-grams between the output and the reference.\n\\\\[\\text{ROUGE-N} = \\frac{\\sum_{s_r \\in \\text{references}} \\sum_{n\\text{-gram} \\in s_r} \\text{Count}_{\\text{match}} (n\\text{-gram})}{\\sum_{s_r \\in \\text{references}} \\sum_{n\\text{-gram} \\in s_r} \\text{Count} (n\\text{-gram})}\\\\] \nOther variants include:\n  * ROUGE-L: This measures the longest common subsequence (LCS) between the output and the reference. It considers sentence-level structure similarity and zeros in on the longest series of co-occurring in-sequence n-grams.\n  * ROUGE-S: This measures the skip-bigram between the output and reference. Skip-bigrams are pairs of words that maintain their sentence order regardless of the words that might be sandwiched between them.\n\n\n**[BERTScore](https://arxiv.org/abs/1904.09675)** is an embedding-based metric that uses cosine similarity to compare each token or n-gram in the generated output with the reference sentence. There are three components to BERTScore:\n  * Recall: Average cosine similarity between each token in the reference and its closest match in the generated output.\n  * Precision: Average cosine similarity between each token in the generated output and its nearest match in the reference.\n  * F1: Harmonic mean of recall and precision\n\n\\\\[Recall_{\\text{BERT}} = \\frac{1}{|r|} \\sum_{i \\in r} \\max_{j \\in p} \\vec{i}^T \\vec{j}, \\quad Precision_{\\text{BERT}} = \\frac{1}{|p|} \\sum_{j \\in p} \\max_{i \\in r} \\vec{i}^T \\vec{j}\\\\] \\\\[\\text{BERTscore} = F_{\\text{BERT}} = \\frac{2 \\cdot P_{\\text{BERT}} \\cdot R_{\\text{BERT}}}{P_{\\text{BERT}} + R_{\\text{BERT}}}\\\\] \nBERTScore is useful because it can account for synonyms and paraphrasing. Simpler metrics like BLEU and ROUGE can\u2019t do this due to their reliance on exact matches. BERTScore has been shown to have better correlation for tasks such as image captioning and machine translation.\n**[MoverScore](https://arxiv.org/abs/1909.02622)** also uses contextualized embeddings to compute the distance between tokens in the generated output and reference. But unlike BERTScore, which is based on one-to-one matching (or \u201chard alignment\u201d) of tokens, MoverScore allows for many-to-one matching (or \u201csoft alignment\u201d).\n![BERTScore \\(left\\) vs. MoverScore \\(right\\)](https://eugeneyan.com/assets/mover-score.jpg)\nBERTScore (left) vs. MoverScore (right; [source](https://arxiv.org/abs/1909.02622))\nMoverScore enables the mapping of semantically related words in one sequence to their counterparts in another sequence. It does this by solving a constrained optimization problem that finds the minimum effort to transform one text into another. The idea is to measure the distance that words would have to move to convert one sequence to another.\nHowever, there are several pitfalls to using these conventional benchmarks and metrics.\nFirst, there\u2019s **poor correlation between these metrics and human judgments.** BLEU, ROUGE, and others have had [negative correlation with how humans evaluate fluency](https://arxiv.org/abs/2008.12009). They also showed moderate to less correlation with human adequacy scores. In particular, BLEU and ROUGE have [low correlation with tasks that require creativity and diversity](https://arxiv.org/abs/2303.16634).\nSecond, these metrics often have **poor adaptability to a wider variety of tasks**. Adopting a metric proposed for one task to another is not always prudent. For example, exact match metrics such as BLEU and ROUGE are a poor fit for tasks like abstractive summarization or dialogue. Since they\u2019re based on n-gram overlap between output and reference, they don\u2019t make sense for a dialogue task where a wide variety of responses are possible. An output can have zero n-gram overlap with the reference but yet be a good response.\nThird, these metrics have **poor reproducibility**. Even for the same metric, [high variance is reported across different studies](https://arxiv.org/abs/2008.12009), possibly due to variations in human judgment collection or metric parameter settings. Another study of [ROUGE scores](https://aclanthology.org/2023.acl-long.107/) across 2,000 studies found that scores were hard to reproduce, difficult to compare, and often incorrect because evals were often conducted with untested, incorrect ROUGE implementations.\n![Dimensions of model evaluations with ROUGE](https://eugeneyan.com/assets/rogue-scores.jpg)\nDimensions of model evaluations with ROUGE ([source](https://aclanthology.org/2023.acl-long.107/))\nAnd even with recent benchmarks such as MMLU, **the same model can get significantly different scores based on the eval implementation**. [Huggingface compared the original MMLU implementation](https://huggingface.co/blog/evaluating-mmlu-leaderboard) with the HELM and EleutherAI implementations and found that the same example could have different prompts across various providers.\n![Different prompts for the same question across MMLU implementations](https://eugeneyan.com/assets/mmlu-prompt.jpg)\nDifferent prompts for the same question across MMLU implementations ([source](https://huggingface.co/blog/evaluating-mmlu-leaderboard))\nFurthermore, the evaluation approach differed across all three benchmarks:\n  * Original MMLU: Compares predicted probabilities on the answers only (A, B, C, D)\n  * HELM: Uses the next token probabilities from the model and picks the token with the highest probability, even if it\u2019s _not_ one of the options.\n  * EleutherAI: Computes probability of the full answer sequence (i.e., a letter followed by the answer text) for each answer. Then, pick answer with highest probability.\n\n\n![Different eval for the same question across MMLU implementations](https://eugeneyan.com/assets/mmlu-eval.jpg)\nDifferent eval for the same question across MMLU implementations ([source](https://huggingface.co/blog/evaluating-mmlu-leaderboard))\nAs a result, even for the same eval, both absolute scores and model ranking can fluctuate widely depending on eval implementation. This means that model metrics aren\u2019t truly comparable\u2014even for the same eval\u2014unless the eval\u2019s implementation is identical down to minute details like prompts and tokenization. Similarly, the author of QLoRA found MMLU overly sensitive and concluded: \u201c[do not work with/report or trust MMLU scores](https://twitter.com/Tim_Dettmers/status/1673446047266504704)\u201d.\nBeyond conventional evals such as those mentioned above, **an emerging trend is to use a strong LLM as a reference-free metric** to evaluate generations from other LLMs. This means we may not need human judgments or gold references for evaluation.\n**[G-Eval](https://arxiv.org/abs/2303.16634) is a framework that applies LLMs** with Chain-of-Though (CoT) and a form-filling paradigm to **evaluate LLM outputs**. First, they provide a task introduction and evaluation criteria to an LLM and ask it to generate a CoT of evaluation steps. Then, to evaluate coherence in news summarization, they concatenate the prompt, CoT, news article, and summary and ask the LLM to output a score between 1 to 5. Finally, they use the probabilities of the output tokens from the LLM to normalize the score and take their weighted summation as the final result.\n![Overview of G-Eval](https://eugeneyan.com/assets/geval.jpg)\nOverview of G-Eval ([source](https://arxiv.org/abs/2303.16634))\nThey found that GPT-4 as an evaluator had a high Spearman correlation with human judgments (0.514), outperforming all previous methods. It also outperformed traditional metrics on aspects such as coherence, consistency, fluency, and relevance. On topical chat, it did better than traditional metrics such as ROUGE-L, BLEU-4, and BERTScore across several criteria such as naturalness, coherence, engagingness, and groundedness.\n**The[Vicuna](https://arxiv.org/abs/2306.05685) paper adopted a similar approach.** They start by defining eight categories (writing, roleplay, extraction, reasoning, math, coding, STEM, and humanities/social science) before developing 10 questions for each category. Next, they generated answers from five chatbots: LLaMA, Alpaca, ChatGPT, Bard, and Vicuna. Finally, they asked GPT-4 to rate the quality of the answers based on helpfulness, relevance, accuracy, and detail.\nOverall, they found that GPT-4 not only provided consistent scores but could also give detailed explanations for those scores. Under the single answer grading paradigm, GPT-4 had higher agreement with humans (85%) than the humans had amongst themselves (81%). This suggests that GPT-4\u2019s judgment aligns closely with the human evaluators.\n**[QLoRA](https://arxiv.org/abs/2305.14314) also used an LLM to evaluate another LLM\u2019s output.** They asked GPT-4 to rate the performance of various models against gpt-3.5-turbo on the Vicuna benchmark. Given the responses from gpt-3.5-turbo and another model, GPT-4 was prompted to score both out of 10 and explain its ratings. They also measured performance via direct comparisons between models, simplifying the task to a three-class rating scheme that included ties.\nTo validate the automated evaluation, they collected human judgments on the Vicuna benchmark. Using Mechanical Turk, they enlisted two annotators for comparisons to gpt-3.5-turbo, and three annotators for pairwise comparisons. They found that human and GPT-4 ranking of models were largely in agreement, with a Spearman rank correlation of 0.55 at the model level. This provides an additional data point suggesting that LLM-based automated evals could be a cost-effective and reasonable alternative to human evals.\n### How to apply evals?\n**Building solid evals should be the starting point** for any LLM-based system or product (as well as conventional machine learning systems).\nUnfortunately, classical metrics such as BLEU and ROUGE don\u2019t make sense for more complex tasks such as abstractive summarization or dialogue. Furthermore, we\u2019ve seen that benchmarks like MMLU (and metrics like ROUGE) are sensitive to how they\u2019re implemented and measured. And to be candid, unless your LLM system is studying for a school exam, using MMLU as an eval [doesn\u2019t quite make sense](https://twitter.com/Tim_Dettmers/status/1680782418335367169).\nThus, instead of using off-the-shelf benchmarks, we can **start by collecting a set of task-specific evals** (i.e., prompt, context, expected outputs as references). These evals will then guide prompt engineering, model selection, fine-tuning, and so on. And as we update our systems, we can run these evals to quickly measure improvements or regressions. Think of it as Eval Driven Development (EDD).\nIn addition to the evaluation dataset, we **also need useful metrics**. They help us distill performance changes into a single number that\u2019s comparable across eval runs. And if we can simplify the problem, we can choose metrics that are easier to compute and interpret.\nThe simplest task is probably classification: If we\u2019re using an LLM for classification-like tasks (e.g., toxicity detection, document categorization) or extractive QA without dialogue, we can rely on standard classification metrics such as recall, precision, PRAUC, etc. If our task has no correct answer but we have references (e.g., machine translation, extractive summarization), we can rely on reference metrics based on matching (BLEU, ROUGE) or semantic similarity (BERTScore, MoverScore).\nHowever, these metrics may not work for more open-ended tasks such as abstractive summarization, dialogue, and others. But collecting human judgments can be slow and expensive. Thus, we may opt to lean on **automated evaluations via a strong LLM**.\nRelative to human judgments which are typically noisy (due to differing biases among annotators), LLM judgments tend to be less noisy (as the bias is more systematic) but more biased. Nonetheless, since we\u2019re aware of these biases, we can mitigate them accordingly:\n  * Position bias: LLMs tend to favor the response in the first position. To mitigate this, we can evaluate the same pair of responses twice while swapping their order. If the same response is preferred in both orders, we mark it as a win; else, it\u2019s a tie.\n  * Verbosity bias: LLMs tend to favor longer, wordier responses over more concise ones, even if the latter is clearer and of higher quality. A possible solution is to ensure that comparison responses are similar in length.\n  * Self-enhancement bias: LLMs have a slight bias towards their own answers. [GPT-4 favors itself with a 10% higher win rate while Claude-v1 favors itself with a 25% higher win rate.](https://arxiv.org/abs/2306.05685) To counter this, don\u2019t use the same LLM for evaluation tasks.\n\n\nAnother tip: Rather than asking an LLM for a direct evaluation (via giving a score), try giving it a reference and asking for a comparison. This helps with reducing noise.\nFinally, sometimes the best eval is human eval aka vibe check. (Not to be confused with the poorly named code evaluation benchmark [HumanEval](https://arxiv.org/abs/2107.03374).) As mentioned in the [Latent Space podcast with MosaicML](https://www.latent.space/p/mosaic-mpt-7b#details) (34th minute):\n> The vibe-based eval cannot be underrated. \u2026 One of our evals was just having a bunch of prompts and watching the answers as the models trained and see if they change. Honestly, I don\u2019t really believe that any of these eval metrics capture what we care about. One of our prompts was \u201csuggest games for a 3-year-old and a 7-year-old to play\u201d and that was a lot more valuable to see how the answer changed during the course of training. \u2014 Jonathan Frankle\nAlso see this [deep dive into evals](https://eugeneyan.com/writing/abstractive/) for abstractive summarization. It covers reference, context, and preference-based metrics, and also discusses hallucination detection.\n## Retrieval-Augmented Generation: To add knowledge\nRetrieval-Augmented Generation (RAG) fetches relevant data from outside the foundation model and enhances the input with this data, providing richer context to improve output.\n### Why RAG?\nRAG helps reduce hallucination by grounding the model on the retrieved context, thus increasing factuality. In addition, it\u2019s cheaper to keep retrieval indices up-to-date than to continuously pre-train an LLM. This cost efficiency makes it easier to provide LLMs with access to recent data via RAG. Finally, if we need to update or remove data such as biased or toxic documents, it\u2019s more straightforward to update the retrieval index (compared to fine-tuning or prompting an LLM not to generate toxic outputs).\nIn short, RAG applies mature and simpler ideas from the field of information retrieval to support LLM generation. In a [recent Sequoia survey](https://www.sequoiacap.com/article/llm-stack-perspective/), 88% of respondents believe that retrieval will be a key component of their stack.\n### More about RAG\nBefore diving into RAG, it helps to have a basic understanding of text embeddings. (Feel free to skip this section if you\u2019re familiar with the subject.)\nA text embedding is a **compressed, abstract representation of text data** where text of arbitrary length can be represented as a fixed-size vector of numbers. It\u2019s usually learned from a corpus of text such as Wikipedia. Think of them as a universal encoding for text, where **similar items are close to each other while dissimilar items are farther apart**.\nA good embedding is one that does well on a downstream task, such as retrieving similar items. Huggingface\u2019s [Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard) scores various models on diverse tasks such as classification, clustering, retrieval, summarization, etc.\nQuick note: While we mainly discuss text embeddings here, embeddings can take many modalities. For example, [CLIP](https://arxiv.org/abs/2103.00020) is multimodal and embeds images and text in the same space, allowing us to find images most similar to an input text. We can also [embed products based on user behavior](https://eugeneyan.com/writing/search-query-matching/#supervised-techniques-improves-modeling-of-our-desired-event) (e.g., clicks, purchases) or [graph relationships](https://eugeneyan.com/writing/search-query-matching/#self-supervised-techniques-no-need-for-labels).\n**RAG has its roots in open-domain Q &A.** An early [Meta paper](https://arxiv.org/abs/2005.04611) showed that retrieving relevant documents via TF-IDF and providing them as context to a language model (BERT) improved performance on an open-domain QA task. They converted each task into a cloze statement and queried the language model for the missing token.\nFollowing that, **[Dense Passage Retrieval (DPR)](https://arxiv.org/abs/2004.04906)** showed that using dense embeddings (instead of a sparse vector space such as TF-IDF) for document retrieval can outperform strong baselines like Lucene BM25 (65.2% vs. 42.9% for top-5 accuracy.) They also showed that higher retrieval precision translates to higher end-to-end QA accuracy, highlighting the importance of upstream retrieval.\nTo learn the DPR embedding, they fine-tuned two independent BERT-based encoders on existing question-answer pairs. The passage encoder (\\\\(E_p\\\\)) embeds text passages into vectors while the query encoder (\\\\(E_q\\\\)) embeds questions into vectors. The query embedding is then used to retrieve \\\\(k\\\\) passages that are most similar to the question.\nThey trained the encoders so that the dot-product similarity makes a good ranking function, and optimized the loss function as the negative log-likelihood of the positive passage. The DPR embeddings are optimized for maximum inner product between the question and relevant passage vectors. The goal is to learn a vector space such that pairs of questions and their relevant passages are close together.\nFor inference, they embed all passages (via \\\\(E_p\\\\)) and index them in FAISS offline. Then, given a question at query time, they compute the question embedding (via \\\\(E_q\\\\)), retrieve the top \\\\(k\\\\) passages via approximate nearest neighbors, and provide it to the language model (BERT) that outputs the answer to the question.\n**[Retrieval Augmented Generation (RAG)](https://arxiv.org/abs/2005.11401)** , from which this pattern gets its name, highlighted the downsides of pre-trained LLMs. These include not being able to expand or revise memory, not providing insights into generated output, and hallucinations.\nTo address these downsides, they introduced RAG (aka semi-parametric models). Dense vector retrieval serves as the non-parametric component while a pre-trained LLM acts as the parametric component. They reused the DPR encoders to initialize the retriever and build the document index. For the LLM, they used BART, a 400M parameter seq2seq model.\n![Overview of Retrieval Augmented Generation](https://eugeneyan.com/assets/rag.jpg)\nOverview of Retrieval Augmented Generation ([source](https://arxiv.org/abs/2005.11401))\nDuring inference, they concatenate the input with the retrieved document. Then, the LLM generates \\\\(\\text{token}_i\\\\) based on the original input, the retrieved document, and the previous \\\\(i-1\\\\) tokens. For generation, they proposed two approaches that vary in how the retrieved passages are used to generate output.\nIn the first approach, RAG-Sequence, the model uses the same document to generate the complete sequence. Thus, for \\\\(k\\\\) retrieved documents, the generator produces an output for each document. Then, the probability of each output sequence is marginalized (sum the probability of each output sequence in \\\\(k\\\\) and weigh it by the probability of each document being retrieved). Finally, the output sequence with the highest probability is selected.\nOn the other hand, RAG-Token can generate each token based on a _different_ document. Given \\\\(k\\\\) retrieved documents, the generator produces a distribution for the next output token for each document before marginalizing (aggregating all the individual token distributions.). The process is then repeated for the next token. This means that, for each token generation, it can retrieve a different set of \\\\(k\\\\) relevant documents based on the original input _and_ previously generated tokens. Thus, documents can have different retrieval probabilities and contribute differently to the next generated token.\n[**Fusion-in-Decoder (FiD)**](https://arxiv.org/abs/2007.01282) also uses retrieval with generative models for open-domain QA. It supports two methods for retrieval, BM25 (Lucene with default parameters) and DPR. FiD is named for how it performs fusion on the retrieved documents in the decoder only.\n![Overview of Fusion-in-Decoder](https://eugeneyan.com/assets/fid.jpg)\nOverview of Fusion-in-Decoder ([source](https://arxiv.org/abs/2007.01282))\nFor each retrieved passage, the title and passage are concatenated with the question. These pairs are processed independently in the encoder. They also add special tokens such as `question:`, `title:`, and `context:` before their corresponding sections. The decoder attends over the concatenation of these retrieved passages.\nBecause it processes passages independently in the encoder, it can scale to a large number of passages as it only needs to do self-attention over one context at a time. Thus, compute grows linearly (instead of quadratically) with the number of retrieved passages, making it more scalable than alternatives such as RAG-Token. Then, during decoding, the decoder processes the encoded passages jointly, allowing it to better aggregate context across multiple retrieved passages.\n[**Retrieval-Enhanced Transformer (RETRO)**](https://arxiv.org/abs/2112.04426) adopts a similar pattern where it combines a frozen BERT retriever, a differentiable encoder, and chunked cross-attention to generate output. What\u2019s different is that RETRO does retrieval throughout the entire pre-training stage, and not just during inference. Furthermore, they fetch relevant documents based on chunks of the input. This allows for finer-grained, repeated retrieval during generation instead of only retrieving once per query.\nFor each input chunk (\\\\(C_u\\\\)), the \\\\(k\\\\) retrieved chunks \\\\(RET(C_u)\\\\) are fed into an encoder. The output is the encoded neighbors \\\\(E^{j}_{u}\\\\) where \\\\(E^{j}_{u} = \\text{Encoder}(\\text{RET}(C_{u})^{j}, H_{u}) \\in \\mathbb{R}^{r \\times d_{0}}\\\\). Here, each chunk encoding is conditioned on \\\\(H_u\\\\) (the intermediate activations) and the activations of chunk \\\\(C_u\\\\) through cross-attention layers. In short, the encoding of the retrieved chunks depends on the attended activation of the input chunk. \\\\(E^{j}_{u}\\\\) is then used to condition the generation of the next chunk.\n![Overview of RETRO](https://eugeneyan.com/assets/retro.jpg)\nOverview of RETRO ([source](https://arxiv.org/abs/2112.04426))\nDuring retrieval, RETRO splits the input sequence into chunks of 64 tokens. Then, it finds text similar to the _previous_ chunk to provide context to the _current_ chunk. The retrieval index consists of two contiguous chunks of tokens, \\\\(N\\\\) and \\\\(F\\\\). The former is the neighbor chunk (64 tokens) which is used to compute the key while the latter is the continuation chunk (64 tokens) in the original document.\nRetrieval is based on approximate \\\\(k\\\\)-nearest neighbors via \\\\(L_2\\\\) distance (euclidean) on BERT embeddings. (Interesting departure from the usual cosine or dot product similarity.) The retrieval index, built on SCaNN, can query a 2T token database in 10ms.\nThey also demonstrated how to RETRO-fit existing baseline models. By freezing the pre-trained weights and only training the chunked cross-attention and neighbor encoder parameters (< 10% of weights for a 7B model), they can enhance transformers with retrieval while only requiring 6M training sequences (3% of pre-training sequences). RETRO-fitted models were able to surpass the performance of baseline models and achieve performance close to that of RETRO trained from scratch.\n![Performance from RETRO-fitting a pre-trained model](https://eugeneyan.com/assets/retrofit.jpg)\nPerformance from RETRO-fitting a pre-trained model ([source](https://arxiv.org/abs/2112.04426))\n**[Internet-augmented LMs](https://arxiv.org/abs/2203.05115)** proposes using a humble \u201coff-the-shelf\u201d search engine to augment LLMs. First, they retrieve a set of relevant documents via Google Search. Since these retrieved documents tend to be long (average length 2,056 words), they chunk them into paragraphs of six sentences each. Finally, they embed the question and paragraphs via TF-IDF and applied cosine similarity to rank the most relevant paragraphs for each query.\n![Overview of internet-augmented LLMs](https://eugeneyan.com/assets/internet-llm.jpg)\nOverview of internet-augmented LLMs ([source](https://arxiv.org/abs/2203.05115))\nThe retrieved paragraphs are used to condition the LLM via few-shot prompting. They adopt the conventional \\\\(k\\\\)-shot prompting (\\\\(k=15\\\\)) from closed-book QA (only providing question-answer pairs) and extend it with an evidence paragraph, such that each context is an evidence, question, and answer triplet.\nFor the generator, they used Gopher, a 280B parameter model trained on 300B tokens. For each question, they generated four candidate answers based on each of the 50 retrieved paragraphs. Finally, they select the best answer by estimating the answer probability via several methods including direct inference, RAG, noisy channel inference, and Product-of-Experts (PoE). PoE consistently performed the best.\nRAG has also been **applied to non-QA tasks such as code generation**. While **[CodeT5+](https://arxiv.org/abs/2305.07922)** can be used as a standalone generator, when combined with RAG, it significantly outperforms similar models in code generation.\nTo assess the impact of RAG on code generation, they evaluate the model in three settings:\n  * Retrieval-based: Fetch the top-1 code sample as the prediction\n  * Generative-only: Output code based on the decoder only\n  * Retrieval-augmented: Append top-1 code sample to encoder input before code generation via the decoder.\n\n\n![>Overview of RAG for CodeT5+](https://eugeneyan.com/assets/codet5.jpg)\nOverview of RAG for CodeT5+ ([source](https://arxiv.org/abs/2305.07922))\nAs a qualitative example, they showed that retrieved code provides crucial context (e.g., use `urllib3` for an HTTP request) and guides the generative process towards more correct predictions. In contrast, the generative-only approach returns incorrect output that only captures the concepts of \u201cdownload\u201d and \u201ccompress\u201d.\n**What if we don\u2019t have relevance judgments for query-passage pairs?** Without them, we would not be able to train the bi-encoders that embed the queries and documents in the same embedding space where relevance is represented by the inner product. **[Hypothetical document embeddings (HyDE)](https://arxiv.org/abs/2212.10496)** suggests a solution.\n![Overview of HyDE](https://eugeneyan.com/assets/hyde.jpg)\nOverview of HyDE ([source](https://arxiv.org/abs/2212.10496))\nGiven a query, HyDE first prompts an LLM, such as InstructGPT, to generate a hypothetical document. Then, an unsupervised encoder, such as Contriver, encodes the document into an embedding vector. Finally, the inner product is computed between the _hypothetical_ document and the corpus, and the most similar _real_ documents are retrieved.\nThe expectation is that the encoder\u2019s dense bottleneck serves as a lossy compressor and the extraneous, non-factual details are excluded via the embedding. This reframes the relevance modeling problem from a representation learning task to a generation task.\n### How to apply RAG\nFrom experience with [Obsidian-Copilot](https://eugeneyan.com/writing/obsidian-copilot/), I\u2019ve found that hybrid retrieval (traditional search index + embedding-based search) works better than either alone. There, I complemented classical retrieval (BM25 via OpenSearch) with semantic search (`e5-small-v2`).\nWhy not embedding-based search only? While it\u2019s great in many instances, there are situations where it falls short, such as:\n  * Searching for a person or object\u2019s name (e.g., Eugene, Kaptir 2.0)\n  * Searching for an acronym or phrase (e.g., RAG, RLHF)\n  * Searching for an ID (e.g., `gpt-3.5-turbo`, `titan-xlarge-v1.01`)\n\n\nBut keyword search has its limitations too. It only models simple word frequencies and doesn\u2019t capture semantic or correlation information. Thus, it doesn\u2019t deal well with synonyms or hypernyms (i.e., words that represent a generalization). This is where combining it with semantic search is complementary.\nIn addition, with a conventional search index, we can use metadata to refine results. For example, we can use date filters to prioritize newer documents or narrow our search to a specific time period. And if the search is related to e-commerce, filters on average rating or categories are helpful. Finally, having metadata is handy for downstream ranking, such as prioritizing documents that are cited more, or boosting products by their sales volume.\n**With regard to embeddings** , the seemingly popular approach is to use [`text-embedding-ada-002`](https://openai.com/blog/new-and-improved-embedding-model). Its benefits include ease of use via an API and not having to maintain our own embedding infra or self-host embedding models. Nonetheless, personal experience and anecdotes from others suggest there are better alternatives for retrieval.\nThe OG embedding approaches include Word2vec and [fastText](https://fasttext.cc). FastText is an open-source, lightweight library that enables users to leverage pre-trained embeddings or train new embedding models. It comes with pre-trained embeddings for 157 languages and is extremely fast, even without a GPU. It\u2019s my go-to for early-stage proof of concepts.\nAnother good baseline is [sentence-transformers](https://github.com/UKPLab/sentence-transformers). It makes it simple to compute embeddings for sentences, paragraphs, and even images. It\u2019s based on workhorse transformers such as BERT and RoBERTa and is available in more than 100 languages.\nMore recently, instructor models have shown SOTA performance. During training, these models prepend the task description to the text. Then, when embedding new text, we simply have to describe the task to get task-specific embeddings. (Not that different from instruction tuning for embedding models IMHO.)\nAn example is the [E5](https://arxiv.org/abs/2212.03533) family of models. For open QA and information retrieval, we simply prepend documents in the index with `passage:`, and prepend queries with `query:`. If the task is symmetric (e.g., semantic similarity, paraphrase retrieval) or if we want to use embeddings as features (e.g., classification, clustering), we just use the `query:` prefix.\nThe [Instructor](https://arxiv.org/abs/2212.09741) model takes it a step further, allowing users to customize the prepended prompt: \u201cRepresent the `domain` `task_type` for the `task_objective`:\u201d For example, \u201cRepresent the Wikipedia document for retrieval:\u201d. (The domain and task objective are optional). This brings the concept of prompt tuning into the field of text embedding.\nFinally, as of Aug 1st, the top embedding model on the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) is the [GTE](https://huggingface.co/thenlper/gte-large) family of models by Alibaba DAMO Academy. The top performing model\u2019s size is half of the next best model `e5-large-v2` (0.67GB vs 1.34GB). In 2nd position is `gte-base` with a model size of only 0.22GB and embedding dimension of 768. (H/T [Nirant](https://twitter.com/NirantK).)\nTo retrieve documents with low latency at scale, we use approximate nearest neighbors (ANN). It optimizes for retrieval speed and returns the approximate (instead of exact) top \\\\(k\\\\) most similar neighbors, trading off a little accuracy loss for a large speed up.\nANN embedding indices are data structures that let us do ANN searches efficiently. At a high level, they build partitions over the embedding space so we can quickly zoom in on the specific space where the query vector is. Some popular techniques include:\n  * [Locality Sensitive Hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) (LSH): The core idea is to create hash functions so that similar items are likely to end up in the same hash bucket. By only needing to check the relevant buckets, we can perform ANN queries efficiently.\n  * [Facebook AI Similarity Search](https://github.com/facebookresearch/faiss) (FAISS): It uses a combination of quantization and indexing for efficient retrieval, supports both CPU and GPU, and can handle billions of vectors due to its efficient use of memory.\n  * [Hierarchical Navigable Small Worlds](https://github.com/nmslib/hnswlib) (HNSW): Inspired by \u201csix degrees of separation\u201d, it builds a hierarchical graph structure that embodies the small world phenomenon. Here, most nodes can be reached from any other node via a minimum number of hops. This structure allows HNSW to initiate queries from broader, coarser approximations and progressively narrow the search at lower levels.\n  * [Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/master/scann) (ScaNN): It has a two-step process. First, coarse quantization reduces the search space. Then, fine-grained search is done within the reduced set. Best recall/latency trade-off I\u2019ve seen.\n\n\nWhen evaluating an ANN index, some factors to consider include:\n  * Recall: How does it fare against exact nearest neighbors?\n  * Latency/throughput: How many queries can it handle per second?\n  * Memory footprint: How much RAM is required to serve an index?\n  * Ease of adding new items: Can new items be added without having to reindex all documents (LSH) or does the index need to be rebuilt (ScaNN)?\n\n\nNo single framework is better than all others in every aspect. Thus, start by defining your functional and non-functional requirements before benchmarking. Personally, I\u2019ve found ScaNN to be outstanding in the recall-latency trade-off (see benchmark graph [here](https://eugeneyan.com/writing/real-time-recommendations/#how-to-design-and-implement-an-mvp)).\n## Fine-tuning: To get better at specific tasks\nFine-tuning is the process of taking a pre-trained model (that has already been trained with a vast amount of data) and further refining it on a specific task. The intent is to harness the knowledge that the model has already acquired during its pre-training and apply it to a specific task, usually involving a smaller, task-specific, dataset.\nThe term \u201cfine-tuning\u201d is used loosely and can refer to several concepts such as:\n  * **Continued pre-training** : With domain-specific data, apply the same pre-training regime (next token prediction, masked language modeling) on the base model.\n  * **Instruction fine-tuning** : The pre-trained (base) model is fine-tuned on examples of instruction-output pairs to follow instructions, answer questions, be waifu, etc.\n  * **Single-task fine-tuning** : The pre-trained model is honed for a narrow and specific task such as toxicity detection or summarization, similar to BERT and T5.\n  * **Reinforcement learning with human feedback (RLHF)** : This combines instruction fine-tuning with reinforcement learning. It requires collecting human preferences (e.g., pairwise comparisons) which are then used to train a reward model. The reward model is then used to further fine-tune the instructed LLM via RL techniques such as proximal policy optimization (PPO).\n\n\nWe\u2019ll mainly focus on single-task and instruction fine-tuning here.\n### Why fine-tuning?\nFine-tuning an open LLM is becoming an increasingly viable alternative to using a 3rd-party, cloud-based LLM for several reasons.\n**Performance & control:** Fine-tuning can improve the performance of an off-the-shelf base model, and may even surpass a 3rd-party LLM. It also provides greater control over LLM behavior, resulting in a more robust system or product. Overall, fine-tuning enables us to build products that are differentiated from simply using 3rd-party or open LLMs.\n**Modularization:** Single-task fine-tuning lets us to use an army of smaller models that each specialize on their own tasks. Via this setup, a system can be modularized into individual models for tasks like content moderation, extraction, summarization, etc. Also, given that each model only has to focus on a narrow set of tasks, we can get around the alignment tax, where fine-tuning a model on one task reduces performance on other tasks.\n**Reduced dependencies:** By fine-tuning and hosting our own models, we can reduce legal concerns about proprietary data (e.g., PII, internal documents and code) being exposed to external APIs. It also gets around constraints that come with 3rd-party LLMs such as rate-limiting, high costs, or overly restrictive safety filters. By fine-tuning and hosting our own LLMs, we can ensure data doesn\u2019t leave our network, and can scale throughput as needed.\n### More about fine-tuning\nWhy do we need to fine-tune a _base_ model? At the risk of oversimplifying, base models are primarily optimized to predict the next word based on the corpus they\u2019re trained on. Hence, they aren\u2019t naturally adept at following instructions or answering questions. When posed a question, they tend to respond with more questions. Thus, we perform instruction fine-tuning so they learn to respond appropriately.\nHowever, fine-tuning isn\u2019t without its challenges. First, we **need a significant volume of demonstration data**. For instance, in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), they used 13k instruction-output samples for supervised fine-tuning, 33k output comparisons for reward modeling, and 31k prompts without human labels as input for RLHF.\nFurthermore, fine-tuning comes with an alignment tax\u2014the process can lead to **lower performance on certain critical tasks**. (There\u2019s no free lunch after all.) The same InstructGPT paper found that RLHF led to performance regressions (relative to the GPT-3 base model) on public NLP tasks like SQuAD, HellaSwag, and WMT 2015 French to English. (A workaround is to have several smaller, specialized models that excel at narrow tasks.)\nFine-tuning is similar to the concept of transfer learning. As defined in Wikipedia: \u201cTransfer learning is a technique in machine learning in which knowledge learned from a task is re-used to boost performance on a related task.\u201d Several years ago, transfer learning made it easy for me to apply ResNet models trained on ImageNet to [classify fashion products](https://eugeneyan.com/writing/image-categorization-is-now-live/) and [build image search](https://eugeneyan.com/writing/image-search-is-now-live/).\n**[ULMFit](https://arxiv.org/abs/1801.06146)** is one of the earlier papers to apply transfer learning to text. They established the protocol of self-supervised pre-training (on unlabeled data) followed by fine-tuning (on labeled data). They used AWS-LSTM, an LSTM variant with dropout at various gates.\n![Overview of ULMFit](https://eugeneyan.com/assets/ulmfit.jpg)\nOverview of ULMFit ([source](https://arxiv.org/abs/1801.06146))\nDuring pre-training (next word prediction), the model is trained on wikitext-103 which contains 28.6 Wikipedia articles and 103M words. Then, during target task fine-tuning, the LM is fine-tuned with data from the domain of the specific task. Finally, during classifier fine-tuning, the model is augmented with two additional linear blocks and fine-tuned on the target classification tasks which includes sentiment analysis, question classification, and topic classification.\nSince then, the pre-training followed by fine-tuning paradigm has driven much progress in language modeling. **[Bidirectional Encoder Representations from Transformers (BERT; encoder only)](https://arxiv.org/abs/1810.04805)** was pre-trained on masked language modeling and next sentence prediction on English Wikipedia and BooksCorpus. It was then fine-tuned on task-specific inputs and labels for single-sentence classification, sentence pair classification, single-sentence tagging, and question & answering.\n![Overview of BERT](https://eugeneyan.com/assets/bert.jpg)\nOverview of BERT ([source](https://arxiv.org/abs/1810.04805))\n**[Generative Pre-trained Transformers (GPT; decoder only)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)** was first pre-trained on BooksCorpus via next token prediction. This was followed by single-task fine-tuning for tasks such as text classification, textual entailment, similarity, and Q&A. Interestingly, they found that including language modeling as an auxiliary objective helped the model generalize and converge faster during training.\n![Overview of GPT](https://eugeneyan.com/assets/gpt.jpg)\nOverview of GPT ([source](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pd))\n**[Text-to-text Transfer Transformer (T5; encoder-decoder)](https://arxiv.org/abs/1910.10683)** was pre-trained on the Colossal Clean Crawled Corpus (C4), a cleaned version of the Common Crawl from April 2019. It employed the same denoising objective as BERT, namely masked language modeling. It was then fine-tuned on tasks such as text classification, abstractive summarization, Q&A, and machine translation.\n![Overview of T5](https://eugeneyan.com/assets/t5.jpg)\nOverview of T5 ([source](https://arxiv.org/abs/1910.10683))\nBut unlike ULMFIt, BERT, and GPT which used different classifier heads for downstream tasks, T5 represented downstream tasks as text-to-text only. For example, a translation task would have input text starting with `Translation English to German:`, while a summarization task might start with `Summarize:` or `TL;DR:`. The prefix essentially became a hyperparameter (first instance of prompt engineering?) This design choice allowed them to use a single fine-tuned model across a variety of downstream tasks.\n**[InstructGPT](https://arxiv.org/abs/2203.02155)** expanded this idea of single-task fine-tuning to instruction fine-tuning. The base model was GPT-3, pre-trained on internet data including Common Crawl, WebText, Books, and Wikipedia. It then applied supervised fine-tuning on demonstrations of desired behavior (instruction and output). Next, it trained a reward model on the dataset of comparisons. Finally, it optimized the instructed model against the reward model via PPO, with this last stage focusing more on alignment than specific task performance.\n![Overview of fine-tuning steps in InstructGPT](https://eugeneyan.com/assets/instructgpt.jpg)\nOverview of fine-tuning steps in InstructGPT ([source](https://arxiv.org/abs/2203.02155))\nNext, let\u2019s move from fine-tuned models to fine-tuning techniques.\n**[Soft prompt tuning](https://arxiv.org/abs/2104.08691)** prepends a trainable tensor to the model\u2019s input embeddings, essentially creating a soft prompt. Unlike discrete text prompts, soft prompts can be learned via backpropagation, meaning they can be fine-tuned to incorporate signals from any number of labeled examples.\nNext, there\u2019s **[prefix tuning](https://arxiv.org/abs/2101.00190)**. Instead of adding a soft prompt to the model input, it prepends trainable parameters to the hidden states of all transformer blocks. During fine-tuning, the LM\u2019s original parameters are kept frozen while the prefix parameters are updated.\n![Overview of prefix-tuning](https://eugeneyan.com/assets/prefix.jpg)\nOverview of prefix-tuning ([source](https://arxiv.org/abs/2101.00190))\nThe paper showed that this achieved performance comparable to full fine-tuning despite requiring updates on just 0.1% of parameters. Moreover, in settings with limited data and involved extrapolation to new topics, it outperformed full fine-tuning. One hypothesis is that training fewer parameters helped reduce overfitting on smaller target datasets.\nThere\u2019s also the **[adapter](https://arxiv.org/abs/1902.00751)** technique. This method adds fully connected network layers twice to each transformer block, after the attention layer and after the feed-forward network layer. On GLUE, it\u2019s able to achieve within 0.4% of the performance of full fine-tuning by just adding 3.6% parameters per task.\n![Overview of adapters](https://eugeneyan.com/assets/adapter.jpg)\nOverview of adapters ([source](https://arxiv.org/abs/1902.00751))\n**[Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685)** is a technique where adapters are designed to be the product of two low-rank matrices. It was inspired by [Aghajanyan et al.](https://arxiv.org/abs/2012.13255) which showed that, when adapting to a specific task, pre-trained language models have a low intrinsic dimension and can still learn efficiently despite a random projection into a smaller subspace. Thus, LoRA hypothesized that weight updates during adaption also have low intrinsic rank.\n![Overview of LoRA](https://eugeneyan.com/assets/lora.jpg)\nOverview of LoRA ([source](https://arxiv.org/abs/2106.09685))\nSimilar to prefix tuning, they found that LoRA outperformed several baselines including full fine-tuning. Again, the hypothesis is that LoRA, thanks to its reduced rank, provides implicit regularization. In contrast, full fine-tuning, which updates all weights, could be prone to overfitting.\n**[QLoRA](https://arxiv.org/abs/2305.14314)** builds on the idea of LoRA. But instead of using the full 16-bit model during fine-tuning, it applies a 4-bit quantized model. It introduced several innovations such as 4-bit NormalFloat (to quantize models), double quantization (for additional memory savings), and paged optimizers (that prevent OOM errors by transferring data to CPU RAM when the GPU runs out of memory).\n![Overview of QLoRA](https://eugeneyan.com/assets/qlora.jpg)\nOverview of QLoRA ([source](https://arxiv.org/abs/2305.14314))\nAs a result, QLoRA reduces the average memory requirements for fine-tuning a 65B model from > 780GB memory to a more manageable 48B without degrading runtime or predictive performance compared to a 16-bit fully fine-tuned baseline.\n(Fun fact: During a meetup with Tim Dettmers, an author of QLoRA, he quipped that double quantization was \u201ca bit of a silly idea but works perfectly.\u201d Hey, if it works, it works.)\n### How to apply fine-tuning?\nThe first step is to **collect demonstration data/labels**. These could be for straightforward tasks such as document classification, entity extraction, or summarization, or they could be more complex such as Q&A or dialogue. Some ways to collect this data include:\n  * **Via experts or crowd-sourced human annotators** : While this is expensive and slow, it usually leads to higher-quality data with [good guidelines](https://eugeneyan.com/writing/labeling-guidelines/).\n  * **Via user feedback** : This can be as simple as asking users to select attributes that describe a product, rating LLM responses with thumbs up or down (e.g., ChatGPT), or logging which images users choose to download (e.g., Midjourney).\n  * **Query larger open models with permissive licenses** : With prompt engineering, we might be able to elicit reasonable demonstration data from a larger model (Falcon 40B Instruct) that can be used to fine-tune a smaller model.\n  * **Reuse open-source data** : If your task can be framed as a natural language inference (NLI) task, we could fine-tune a model to perform NLI using [MNLI data](https://cims.nyu.edu/~sbowman/multinli/). Then, we can continue fine-tuning the model on internal data to classify inputs as entailment, neutral, or contradiction.\n\n\nNote: Some LLM terms prevent users from using their output to develop other models.\n  * [OpenAI Terms of Use](https://openai.com/policies/terms-of-use) (Section 2c, iii): You may not use output from the Services to develop models that compete with OpenAI.\n  * [LLaMA 2 Community License Agreement](https://ai.meta.com/llama/license/) (Section 1b-v): You will not use the Llama Materials or any output or results of the Llama Materials to improve any other large language model (excluding Llama 2 or derivative works thereof).\n\n\nThe next step is to **define evaluation metrics**. We\u2019ve discussed this in a [previous section](https://eugeneyan.com/writing/llm-patterns/#evals-to-measure-performance-scalably).\nThen, **select a pre-trained model.** There are [several open LLMs with permissive licenses](https://github.com/eugeneyan/open-llms) to choose from. Excluding Llama 2 (since it isn\u2019t fully commercial use), Falcon-40B is known to be the best-performing model. Nonetheless, I\u2019ve found it unwieldy to fine-tune and serve in production given how heavy it is.\nInstead, I\u2019m inclined to use smaller models like the Falcon-7B. And if we can simplify and frame the task more narrowly, BERT (340M params), RoBERTA (355M params), and BART (406M params) are solid picks for classification and natural language inference tasks. Beyond that, Flan-T5 (770M and 3B variants) is a reliable baseline for translation, abstractive summarization, headline generation, etc.\nWe may also need to **update the model architecture** , such as when the pre-trained model\u2019s architecture doesn\u2019t align with the task. For example, we might need to update the classification heads on BERT or T5 to match our task. Tip: If the task is a simple binary classification task, NLI models can work out of the box. Entailment is mapped to positive, contradiction is mapped to negative, while the neural label can indicate uncertainty.\n**Then, pick a fine-tuning approach.** LoRA and QLoRA are good places to start. But if your fine-tuning is more intensive, such as continued pre-training on new domain knowledge, you may find full fine-tuning necessary.\n**Finally, basic hyperparameter tuning.** Generally, most papers focus on learning rate, batch size, and number of epochs (see LoRA, QLoRA). And if we\u2019re using LoRA, we might want to tune the rank parameter (though the QLoRA paper found that different rank and alpha led to similar results). Other hyperparameters include input sequence length, loss type (contrastive loss vs. token match), and data ratios (like the mix of pre-training or demonstration data, or the ratio of positive to negative examples, among others).\n## Caching: To reduce latency and cost\nCaching is a technique to store data that has been previously retrieved or computed. This way, future requests for the same data can be served faster. In the space of serving LLM generations, the popularized approach is to cache the LLM response keyed on the embedding of the input request. Then, for each new request, if a semantically similar request is received, we can serve the cached response.\nFor some practitioners, this sounds like \u201c[a disaster waiting to happen.](https://twitter.com/HanchungLee/status/1681146845186363392)\u201d I\u2019m inclined to agree. Thus, I think the key to adopting this pattern is figuring out how to cache safely, instead of solely depending on semantic similarity.\n### Why caching?\nCaching can significantly reduce latency for responses that have been served before. In addition, by eliminating the need to compute a response for the same input again and again, we can reduce the number of LLM requests and thus save cost. Also, there are certain use cases that do not support latency on the order of seconds. Thus, pre-computing and caching may be the only way to serve those use cases.\n### More about caching\nA cache is a high-speed storage layer that stores a subset of data that\u2019s accessed more frequently. This lets us serve these requests faster via the cache instead of the data\u2019s primary storage (e.g., search index, relational database). Overall, caching enables efficient reuse of previously fetched or computed data. (More about [caching](https://aws.amazon.com/caching/) and [best practices](https://aws.amazon.com/caching/best-practices/).)\nAn example of caching for LLMs is [GPTCache](https://github.com/zilliztech/GPTCache).\n![Overview of GPTCache](https://eugeneyan.com/assets/gptcache.jpg)\nOverview of GPTCache ([source](https://github.com/zilliztech/GPTCache))\nWhen a new request is received:\n  * Embedding generator: This embeds the request via various models such as OpenAI\u2019s `text-embedding-ada-002`, FastText, Sentence Transformers, and more.\n  * Similarity evaluator: This computes the similarity of the request via the vector store and then provides a distance metric. The vector store can either be local (FAISS, Hnswlib) or cloud-based. It can also compute similarity via a model.\n  * Cache storage: If the request is similar, the cached response is fetched and served.\n  * LLM: If the request isn\u2019t similar enough, it gets passed to the LLM which then generates the result. Finally, the response is served and cached for future use.\n\n\nRedis also shared a [similar example](https://www.youtube.com/live/9VgpXcfJYvw?feature=share&t=1517), mentioning that some teams go as far as precomputing all the queries they anticipate receiving. Then, they set a similarity threshold on which queries are similar enough to warrant a cached response.\n### How to apply caching?\n**We should start with having a good understanding of user request patterns**. This allows us to design the cache thoughtfully so it can be applied reliably.\nFirst, let\u2019s consider a non-LLM example. Imagine we\u2019re caching product prices for an e-commerce site. During checkout, is it safe to display the (possibly outdated) cached price? Probably not, since the price the customer sees during checkout should be the same as the final amount they\u2019re charged. Caching isn\u2019t appropriate here as we need to ensure consistency for the customer.\nNow, bringing it back to LLM responses. Imagine we get a request for a summary of \u201cMission Impossible 2\u201d that\u2019s semantically similar enough to \u201cMission Impossible 3\u201d. If we\u2019re looking up cache based on semantic similarity, we could serve the wrong response.\nWe also need to **consider if caching is effective for the usage pattern.** One way to quantify this is via the cache hit rate (percentage of requests served directly from the cache). If the usage pattern is uniformly random, the cache would need frequent updates. Thus, the effort to keep the cache up-to-date could negate any benefit a cache has to offer. On the other hand, if the usage follows a power law where a small proportion of unique requests account for the majority of traffic (e.g., search queries, product views), then caching could be an effective strategy.\nBeyond semantic similarity, we could also explore caching based on:\n  * **Item IDs:** This applies when we pre-compute [summaries of product reviews](https://www.cnbc.com/2023/06/12/amazon-is-using-generative-ai-to-summarize-product-reviews.html) or generate a summary for an entire movie trilogy.\n  * **Pairs of Item IDs:** Such as when we generate comparisons between two movies. While this appears to be \\\\(O(N^2)\\\\), in practice, a small number of combinations drive the bulk of traffic, such as comparison between popular movies in a series or genre.\n  * **Constrained input:** Such as variables like movie genre, director, or lead actor. For example, if a user is looking for movies by a specific director, we could execute a structured query and run it through an LLM to frame the response more eloquently. Another example is [generating code based on drop-down options](https://cheatlayer.com)\u2014if the code has been verified to work, we can cache it for reliable reuse.\n\n\nAlso, **caching doesn\u2019t only have to occur on-the-fly.** As Redis shared, we can pre-compute LLM generations offline or asynchronously before serving them. By serving from a cache, we shift the latency from generation (typically seconds) to cache lookup (milliseconds). Pre-computing in batch can also help reduce cost relative to serving in real-time.\nWhile the approaches listed here may not be as flexible as semantically caching on natural language inputs, I think it provides a good balance between efficiency and reliability.\n## Guardrails: To ensure output quality\nIn the context of LLMs, guardrails validate the output of LLMs, ensuring that the output doesn\u2019t just sound good but is also syntactically correct, factual, and free from harmful content. It also includes guarding against adversarial input.\n### Why guardrails?\nFirst, they help ensure that model outputs are reliable and consistent enough to use in production. For example, we may require output to be in a specific JSON schema so that it\u2019s machine-readable, or we need code generated to be executable. Guardrails can help with such syntactic validation.\nSecond, they provide an additional layer of safety and maintain quality control over an LLM\u2019s output. For example, to verify if the content generated is appropriate for serving, we may want to check that the output isn\u2019t harmful, verify it for factual accuracy, or ensure coherence with the context provided.\n### More about guardrails\n**One approach is to control the model\u2019s responses via prompts.** For example, Anthropic shared about prompts designed to guide the model toward generating responses that are [helpful, harmless, and honest](https://arxiv.org/abs/2204.05862) (HHH). They found that Python fine-tuning with the HHH prompt led to better performance compared to fine-tuning with RLHF.\n![Example of HHH prompt](https://eugeneyan.com/assets/hhh.jpg)\nExample of HHH prompt ([source](https://arxiv.org/abs/2204.05862))\n**A more common approach is to validate the output.** An example is the [Guardrails package](https://github.com/ShreyaR/guardrails). It allows users to add structural, type, and quality requirements on LLM outputs via Pydantic-style validation. And if the check fails, it can trigger corrective action such as filtering on the offending output or regenerating another response.\nMost of the validation logic is in [`validators.py`](https://github.com/ShreyaR/guardrails/blob/main/guardrails/validators.py). It\u2019s interesting to see how they\u2019re implemented. Broadly speaking, its validators fall into the following categories:\n  * Single output value validation: This includes ensuring that the output (i) is one of the predefined choices, (ii) has a length within a certain range, (iii) if numeric, falls within an expected range, and (iv) is a complete sentence.\n  * Syntactic checks: This includes ensuring that generated URLs are valid and reachable, and that Python and SQL code is bug-free.\n  * Semantic checks: This verifies that the output is aligned with the reference document, or that the extractive summary closely matches the source document. These checks can be done via cosine similarity or fuzzy matching techniques.\n  * Safety checks: This ensures that the generated output is free of inappropriate language or that the quality of translated text is high.\n\n\nNvidia\u2019s [NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) follows a similar principle but is designed to guide LLM-based conversational systems. Rather than focusing on syntactic guardrails, it emphasizes semantic ones. This includes ensuring that the assistant steers clear of politically charged topics, provides factually correct information, and can detect jailbreaking attempts.\nThus, NeMo\u2019s approach is somewhat different: Instead of using more deterministic checks like verifying if a value exists in a list or inspecting code for syntax errors, NeMo leans heavily on using another LLM to validate outputs (inspired by [SelfCheckGPT](https://arxiv.org/abs/2303.08896)).\nIn their example for fact-checking and preventing hallucination, they ask the LLM itself to check whether the most recent output is consistent with the given context. To fact-check, the LLM is queried if the response is true based on the documents retrieved from the knowledge base. To prevent hallucinations, since there isn\u2019t a knowledge base available, they get the LLM to generate multiple alternative completions which serve as the context. The underlying assumption is that if the LLM produces multiple completions that disagree with one another, the original completion is likely a hallucination.\nThe moderation example follows a similar approach: The response is screened for harmful and unethical content via an LLM. Given the nuance of ethics and harmful content, heuristics and conventional machine learning techniques fall short. Thus, an LLM is required for a deeper understanding of the intent and structure of dialogue.\nApart from using guardrails to verify the output of LLMs, we can also **directly steer the output to adhere to a specific grammar.** An example of this is Microsoft\u2019s [Guidance](https://github.com/microsoft/guidance). Unlike Guardrails which [imposes JSON schema via a prompt](https://github.com/ShreyaR/guardrails/blob/main/guardrails/constants.xml#L14), Guidance enforces the schema by injecting tokens that make up the structure.\nWe can think of Guidance as a domain-specific language for LLM interactions and output. It draws inspiration from [Handlebars](https://handlebarsjs.com), a popular templating language used in web applications that empowers users to perform variable interpolation and logical control.\nHowever, Guidance sets itself apart from regular templating languages by executing linearly. This means it maintains the order of tokens generated. Thus, by inserting tokens that are part of the structure\u2014instead of relying on the LLM to generate them correctly\u2014Guidance can dictate the specific output format. In their examples, they show how to [generate JSON that\u2019s always valid](https://github.com/microsoft/guidance#guaranteeing-valid-syntax-json-example-notebook), [generate complex output formats](https://github.com/microsoft/guidance#rich-output-structure-example-notebook) with multiple keys, ensure that LLMs [play the right roles](https://github.com/microsoft/guidance#role-based-chat-model-example-notebook), and have [agents interact with each other](https://github.com/microsoft/guidance#agents-notebook).\nThey also introduced a concept called [token healing](https://github.com/microsoft/guidance#token-healing-notebook), a useful feature that helps avoid subtle bugs that occur due to tokenization. In simple terms, it rewinds the generation by one token before the end of the prompt and then restricts the first generated token to have a prefix matching the last token in the prompt. This eliminates the need to fret about token boundaries when crafting prompts.\n### How to apply guardrails?\nThough the concept of guardrails for LLMs in industry is still nascent, there are a handful of immediately useful and practical strategies we can consider.\n**Structural guidance:** Apply guidance whenever possible. It provides direct control over outputs and offers a more precise method to ensure that output conforms to a specific structure or format.\n**Syntactic guardrails:** These include checking if categorical output is within a set of acceptable choices, or if numeric output is within an expected range. Also, if we generate SQL, these can verify its free from syntax errors and also ensure that all columns in the query match the schema. Ditto for generating code (e.g., Python, JavaScript).\n**Content safety guardrails:** These verify that the output has no harmful or inappropriate content. It can be as simple as checking against the [List of Dirty, Naughty, Obscene, and Otherwise Bad Words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words) or using [profanity detection](https://pypi.org/project/profanity-check/) models. (It\u2019s [common to run moderation classifiers on output](https://twitter.com/goodside/status/1685023251532320768).) More complex and nuanced output can rely on an LLM evaluator.\n**Semantic/factuality guardrails:** These confirm that the output is semantically relevant to the input. Say we\u2019re generating a two-sentence summary of a movie based on its synopsis. We can validate if the produced summary is semantically similar to the output, or have (another) LLM ascertain if the summary accurately represents the provided synopsis.\n**Input guardrails:** These limit the types of input the model will respond to, helping to mitigate the risk of the model responding to inappropriate or adversarial prompts which would lead to generating harmful content. For example, you\u2019ll get an error if you ask Midjourney to generate NSFW content. This can be as straightforward as comparing against a list of strings or using a moderation classifier.\n![An example of an input guardrail on Midjourney](https://eugeneyan.com/assets/input-guardrail.jpg)\nAn example of an input guardrail on Midjourney\n## Defensive UX: To anticipate & handle errors gracefully\nDefensive UX is a design strategy that acknowledges that bad things, such as inaccuracies or hallucinations, can happen during user interactions with machine learning or LLM-based products. Thus, the intent is to anticipate and manage these in advance, primarily by guiding user behavior, averting misuse, and handling errors gracefully.\n### Why defensive UX?\nMachine learning and LLMs aren\u2019t perfect\u2014they can produce inaccurate output. Also, they respond differently to the same input over time, such as search engines displaying varying results due to personalization, or LLMs generating diverse output on more creative, higher temperature, settings. This can violate the principle of consistency which advocates for a consistent UI and predictable behaviors.\nDefensive UX can help mitigate the above by providing:\n  * **Increased accessibility** : By helping users understand how ML/LLM features work and their limitations, defensive UX makes it more accessible and user-friendly.\n  * **Increased trust** : When users see that the feature can handle difficult scenarios gracefully and doesn\u2019t produce harmful output, they\u2019re likely to trust it more.\n  * **Better UX** : By designing the system and UX to handle ambiguous situations and errors, defensive UX paves the way for a smoother, more enjoyable user experience.\n\n\n### More about defensive UX\nTo learn more about defensive UX, we can look at Human-AI guidelines from Microsoft, Google, and Apple.\n**Microsoft\u2019s[Guidelines for Human-AI Interaction](https://www.microsoft.com/en-us/research/publication/guidelines-for-human-ai-interaction/)** is based on a survey of 168 potential guidelines. These were collected from internal and external industry sources, academic literature, and public articles. After combining guidelines that were similar, filtering guidelines that were too vague or too specific or not AI-specific, and a round of heuristic evaluation, they narrowed it down to 18 guidelines.\n![Guidelines for Human-AI interaction across the user journey](https://eugeneyan.com/assets/ms-guidelines.jpg)\nGuidelines for Human-AI interaction across the user journey ([source](https://www.microsoft.com/en-us/research/project/guidelines-for-human-ai-interaction/))\nThese guidelines follow a certain style: Each one is a succinct action rule of 3 - 10 words, beginning with a verb. Each rule is accompanied by a one-liner that addresses potential ambiguities. They are organized based on their likely application during user interaction:\n  * Initially: Make clear what the system can do (G1), make clear how well the system can do what it can do (G2)\n  * During interaction: Time services based on context (G3), mitigate social biases (G6)\n  * When wrong: Support efficient dismissal (G8), support efficient correction (G9)\n  * Over time: Learn from user behavior (G13), provide global controls (G17)\n\n\n**Google\u2019s[People + AI Guidebook](https://pair.withgoogle.com/guidebook/)** is rooted in data and insights drawn from Google\u2019s product team and academic research. In contrast to Microsoft\u2019s guidelines which are organized around the user, Google organizes its guidelines into concepts that a developer needs to keep in mind.\nThere are 23 patterns grouped around common questions that come up during the product development process, including:\n  * How do I get started with human-centered AI: Determine if the AI adds value, invest early in good data practices (e.g., evals)\n  * How do I onboard users to new AI features: Make it safe to explore, anchor on familiarity, automate in phases\n  * How do I help users build trust in my product: Set the right expectations, be transparent, automate more when the risk is low.\n\n\n**Apple\u2019s[Human Interface Guidelines for Machine Learning](https://developer.apple.com/design/human-interface-guidelines/machine-learning)** differs from the bottom-up approach of academic literature and user studies. Instead, its primary source is practitioner knowledge and experience. Thus, it doesn\u2019t include many references or data points, but instead focuses on Apple\u2019s longstanding design principles. This results in a unique perspective that distinguishes it from the other two guidelines.\nThe document focuses on how Apple\u2019s design principles can be applied to ML-infused products, emphasizing aspects of UI rather than model functionality. It starts by asking developers to consider the role of ML in their app and work backwards from the user experience. This includes questions such as whether ML is:\n  * Critical or complementary: For example, Face ID cannot work without ML but the keyboard can still work without QuickType.\n  * Proactive or reactive: Siri Suggestions are proactive while autocorrect is reactive.\n  * Dynamic or static: Recommendations are dynamic while object detection in Photos only improves with each iOS release.\n\n\nIt then delves into several patterns, split into inputs and outputs of a system. Inputs focus on explicit feedback, implicit feedback, calibration, and corrections. This section guides the design for how AI products request and process user data and interactions. Outputs focus on mistakes, multiple options, confidence, attribution, and limitations. The intent is to ensure the model\u2019s output is presented in a comprehensible and useful manner.\nThe differences between the three guidelines are insightful. Google has more emphasis on considerations for training data and model development, likely due to its engineering-driven culture. Microsoft has more focus on mental models, likely an artifact of the HCI academic study. Lastly, Apple\u2019s approach centers around providing a seamless UX, a focus likely influenced by its cultural values and principles.\n### How to apply defensive UX?\nHere are some patterns based on the guidelines above. (Disclaimer: I\u2019m not a designer.)\n**Set the right expectations.** This principle is consistent across all three guidelines:\n  * Microsoft: Make clear how well the system can do what it can do (help the user understand how often the AI system may make mistakes)\n  * Google: Set the right expectations (be transparent with your users about what your AI-powered product can and cannot do)\n  * Apple: Help people establish realistic expectations (describe the limitation in marketing material or within the feature\u2019s context)\n\n\nThis can be as simple as adding a brief disclaimer above AI-generated results, like those of Bard, or highlighting our app\u2019s limitations on its landing page, like how ChatGPT does it.\n![Example of a disclaimer on Google Bard results \\(Note: The code provided will not work.\\)](https://eugeneyan.com/assets/bard-disclaimer.png)\nExample of a disclaimer on Google Bard results (Note: `nrows` is not a valid argument.)\nBy being transparent about our product\u2019s capabilities and limitations, we help users calibrate their expectations about its functionality and output. While this may cause users to trust it less in the short run, it helps foster trust in the long run\u2014users are less likely to overestimate our product and subsequently face disappointment.\n**Enable efficient dismissal.** This is explicitly mentioned as Microsoft\u2019s Guideline 8: Support efficient dismissal (make it easy to dismiss or ignore undesired AI system services).\nFor example, if a user is navigating our site and a chatbot pops up asking if they need help, it should be easy for the user to dismiss the chatbot. This ensures the chatbot doesn\u2019t get in the way, especially on devices with smaller screens. Similarly, GitHub Copilot allows users to conveniently ignore its code suggestions by simply continuing to type. While this may reduce usage of the AI feature in the short term, it prevents it from becoming a nuisance and potentially reducing customer satisfaction in the long term.\n**Provide attribution.** This is listed in all three guidelines:\n  * Microsoft: Make clear why the system did what it did (enable the user to access an explanation of why the AI system behaved as it did)\n  * Google: Add context from human sources (help users appraise your recommendations with input from 3rd-party sources)\n  * Apple: Consider using attributions to help people distinguish among results\n\n\nCitations are becoming an increasingly common design element. Take BingChat for example. When we make a query, it includes citations, usually from reputable sources, in its responses. This not only shows where the information came from, but also allows users to assess the quality of the sources. Similarly, imagine we\u2019re using an LLM to explain why a user might like a product. Alongside the LLM-generated explanation, we could include a quote from an actual review or mention the product rating.\nContext from experts and the community also enhances user trust. For example, if a user is seeking recommendations for a hiking trail, mentioning that a suggested trail comes highly recommended by the relevant community can go a long way. It not only adds value to the recommendation but also helps users calibrate trust through the human connection.\n![Example of attribution via social proof](https://eugeneyan.com/assets/social-proof.jpg)\nExample of attribution via social proof ([source](https://pair.withgoogle.com/guidebook/patterns))\nFinally, Apple\u2019s guidelines include popular attributions such as \u201cBecause you\u2019ve read non-fiction\u201d, \u201cNew books by authors you\u2019ve read\u201d. These descriptors not only personalize the experience but also provide context, enhancing user understanding and trust.\n**Anchor on familiarity.** When introducing users to a new AI product or feature, it helps to guide them with familiar UX patterns and features. This makes it easier for users to focus on the main task and start to earn customer trust in our new product. Resist the temptation to showcase new and \u201cmagical\u201d features via exotic UI elements.\nAlong a similar vein, chat-based features are becoming more common due to ChatGPT\u2019s growing popularity. For example, chat with your docs, chat to query your data, chat to buy groceries. However, I [question whether chat is the right UX](https://eugeneyan.com/writing/llm-ux/) for most user experiences\u2014it just takes too much effort relative to the familiar UX of clicking on text and images.\nFurthermore, increasing user effort leads to higher expectations that are harder to meet. Netflix shared that users have [higher expectations for recommendations](https://slideslive.com/38934788/a-human-perspective-on-algorithmic-similarity?ref=folder-59726) that result from explicit actions such as search. In general, the more effort a user puts in (e.g., chat, search), the higher the expectations they have. Contrast this with lower-effort interactions such as scrolling over recommendations slates or clicking on a product.\nThus, while chat offers more flexibility, it also demands more user effort. Moreover, using a chat box is less intuitive as it lacks signifiers on how users can adjust the output. Overall, I think that sticking with a familiar and constrained UI makes it easier for users to navigate our product; chat should only be considered as a secondary or tertiary option.\n## Collect user feedback: To build our data flywheel\nGathering user feedback allows us to learn their preferences. Specific to LLM products, user feedback contributes to building evals, fine-tuning, and guardrails. If we think about it, data\u2014such as corpus for pre-training, expert-crafted demonstrations, human preferences for reward modeling\u2014is one of the few moats for LLM products. Thus, we want to be deliberately thinking about collecting user feedback when designing our UX.\nFeedback can be explicit or implicit. Explicit feedback is information users provide in response to a request by our product; implicit feedback is information we learn from user interactions without needing users to deliberately provide feedback.\n### Why collect user feedback\nUser feedback **helps our models improve**. By learning what users like, dislike, or complain about, we can improve our models to better meet their needs. It also allows us to **adapt to individual preferences**. Recommendation systems are a prime example. As users interact with items, we learn what they like and dislike and better cater to their tastes over time.\nIn addition, the feedback loop helps us **evaluate our system\u2019s overall performance**. While evals can help us measure model/system performance, user feedback offers a concrete measure of user satisfaction and product effectiveness.\n### How to collect user feedback\n**Make it easy for users to provide feedback.** This is echoed across all three guidelines:\n  * Microsoft: Encourage granular feedback (enable the user to provide feedback indicating their preferences during regular interaction with the AI system)\n  * Google: Let users give feedback (give users the opportunity for real-time teaching, feedback, and error correction)\n  * Apple: Provide actionable information your app can use to improve the content and experience it presents to people\n\n\nChatGPT is one such example. Users can indicate thumbs up/down on responses, or choose to regenerate a response if it\u2019s really bad or unhelpful. This is useful feedback on human preferences which can then be used to fine-tune LLMs.\nMidjourney is another good example. After images are generated, users can generate a new set of images (negative feedback), tweak an image by asking for a variation (positive feedback), or upscale and download the image (strong positive feedback). This enables Midjourney to gather rich comparison data on the outputs generated.\n![>Example of collecting user feedback as part of the UX](https://eugeneyan.com/assets/midjourney.jpg)\nExample of collecting user feedback as part of the UX\n**Consider implicit feedback too.** Implicit feedback is information that arises as users interact with our product. Unlike the specific responses we get from explicit feedback, implicit feedback can provide a wide range of data on user behavior and preferences.\nCopilot-like assistants are a prime example. Users indicate whether a suggestion was helpful by either wholly accepting it (strong positive feedback), accepting and making minor tweaks (positive feedback), or ignoring it (neutral/negative feedback). Alternatively, they may update the comment that led to the generated code, suggesting that the initial code generation didn\u2019t meet their needs.\nChatbots, such as ChatGPT and BingChat, are another example. How has daily usage changed over time? If the product is sticky, it suggests that users like it. Also, how long is the average conversation? This can be tricky to interpret: Is a longer conversation better because the conversation was engaging and fruitful? Or is it worse because it took the user longer to get what they needed?\n## Other patterns common in machine learning\nApart from the seven patterns above, there are other patterns in machine learning that are also relevant to LLM systems and products. They include:\n  * [Data flywheel](https://eugeneyan.com/writing/more-patterns/#data-flywheel-to-continuously-improve--build-a-moat): Continuous data collection improves the model and leads to a better user experience. This, in turn, promotes more usage which provides more data to further evaluate and fine-tune models, creating a virtuous cycle.\n  * [Cascade](https://eugeneyan.com/writing/more-patterns/#cascade-to-split-a-problem-into-smaller-problems): Rather than assigning a single, complex task to the LLM, we can simplify and break it down so it only has to handle tasks it excels at, such as reasoning or communicating eloquently. RAG is an example of this. Instead of relying on the LLM to retrieve and rank items based on its internal knowledge, we can augment LLMs with external knowledge and focus on applying the LLM\u2019s reasoning abilities.\n  * [Monitoring](https://eugeneyan.com/writing/practical-guide-to-maintaining-machine-learning/#monitor-models-for-misbehaviour-when-retraining): This helps demonstrate the value added by the AI system, or the lack of it. Someone shared an anecdote of running an LLM-based customer support solution in prod for two weeks before discontinuing it\u2014an A/B test showed that losses were 12x more when using an LLM as a substitute for their support team!\n\n\n(Read more about design patterns for [machine learning code](https://eugeneyan.com/writing/design-patterns/) and [systems](https://eugeneyan.com/writing/more-patterns/).)\nAlso, here\u2019s what others said:\n> Separation of concerns/task decomposition- having distinct prompts for distinct subtasks and chaining them together helps w attention and reliability (hurts latency). We were having trouble specifying a rigid output structure AND variable response content so we split up the tasks \u2014 [Erick Enriquez](https://twitter.com/generick_ez/status/1681153738822516736)\n> A few others that will be needed: role based access control: who can access what; security: if I\u2019m using a DB with an LLM, how do I ensure that I have the right security guards \u2014 [Krishna](https://twitter.com/ntkris/status/16812092400299991050)\n> Consistent output format: setting outputs to a standardized format such as JSON; Tool augmentation: offload tasks to more specialised, proven, reliable models \u2014 [Paul Tune](https://twitter.com/ptuls/status/1681284873741561857)\n> Security: mitigate cache poisoning, input validation, mitigate prompt injection, training data provenance, output with non-vulnerable code, mitigate malicious input aimed at influencing requests used by tools (AI Agent), mitigate denial of service (stress test llm), to name a few :) \u2014 [Anderson Darario](https://www.linkedin.com/feed/update/urn:li:activity:7087089908229558272?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7087089908229558272%2C7087224131292684288%29)\n> Another ux/ui related: incentivize users to provide feedback on generated answers (implicit or explicit). Implicit could be sth like copilot\u2019s ghost text style, if accepted with TAB, meaning positive feedback etc. \u2014 [Wen Yang](https://www.linkedin.com/feed/update/urn:li:activity:7087089908229558272?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7087089908229558272%2C7087149792660750336%29)\n> Great list. I would add consistency checks like self-consistency sampling, chaining and decomposition of tasks, and the emsembling of multiple model outputs. Applying each of these almost daily. [Dan White](https://www.threads.net/@dwhitena/post/Cu3BBaJtoyj/?igshid=OGQ5ZDc2ODk2ZA==)\n> Guardrails is super relevant for building analytics tools where llm is a translator from natural to programming language \u2014 [m_voitko](https://www.threads.net/@m_voitko/post/Cu1b4liNwCS/?igshid=OGQ5ZDc2ODk2ZA==)\n## Conclusion\nThis is the longest post I\u2019ve written by far. If you\u2019re still with me, thank you! I hope you found reading about these patterns helpful, and that the 2x2 below makes sense.\n![LLM patterns across the axis of data to user, and defensive to offensive.](https://eugeneyan.com/assets/llm-patterns.png)\nLLM patterns across the axis of data to user, and defensive to offensive.\nWe\u2019re still so early on the journey towards building LLM-based systems and products. Are there any other key patterns or resources? What have you found useful or not useful? I\u2019d love to hear your experience. **Please[reach out!](https://twitter.com/eugeneyan)**\n## References\nHendrycks, Dan, et al. [\u201cMeasuring massive multitask language understanding.\u201d](https://arxiv.org/abs/2009.03300) arXiv preprint arXiv:2009.03300 (2020).\nGao, Leo, et al. [\u201cA Framework for Few-Shot Language Model Evaluation.\u201d](https://github.com/EleutherAI/lm-evaluation-harness) v0.0.1, Zenodo, (2021), doi:10.5281/zenodo.5371628.\nLiang, Percy, et al. [\u201cHolistic evaluation of language models.\u201d](https://arxiv.org/abs/2211.09110) arXiv preprint arXiv:2211.09110 (2022).\nDubois, Yann, et al. [\u201cAlpacaFarm: A Simulation Framework for Methods That Learn from Human Feedback.\u201d](https://github.com/tatsu-lab/alpaca_eval) (2023)\nPapineni, Kishore, et al. [\u201cBleu: a method for automatic evaluation of machine translation.\u201d](https://dl.acm.org/doi/10.3115/1073083.1073135) Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002.\nLin, Chin-Yew. [\u201cRouge: A package for automatic evaluation of summaries.\u201d](https://aclanthology.org/W04-1013/) Text summarization branches out. 2004.\nZhang, Tianyi, et al. [\u201cBertscore: Evaluating text generation with bert.\u201d](https://arxiv.org/abs/1904.09675) arXiv preprint arXiv:1904.09675 (2019).\nZhao, Wei, et al. [\u201cMoverScore: Text generation evaluating with contextualized embeddings and earth mover distance.\u201d](https://arxiv.org/abs/1909.02622) arXiv preprint arXiv:1909.02622 (2019).\nSai, Ananya B., Akash Kumar Mohankumar, and Mitesh M. Khapra. [\u201cA survey of evaluation metrics used for NLG systems.\u201d](https://arxiv.org/abs/2008.12009) ACM Computing Surveys (CSUR) 55.2 (2022): 1-39.\nGrusky, Max. [\u201cRogue Scores.\u201d](https://aclanthology.org/2023.acl-long.107/) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.\nLiu, Yang, et al. [\u201cGpteval: Nlg evaluation using gpt-4 with better human alignment.\u201d](https://arxiv.org/abs/2303.16634) arXiv preprint arXiv:2303.16634 (2023).\nFourrier, Cl\u00e9mentine, et al. [\u201cWhat\u2019s going on with the Open LLM Leaderboard?\u201d](https://huggingface.co/blog/evaluating-mmlu-leaderboard#whats-going-on-with-the-open-llm-leaderboard) (2023).\nZheng, Lianmin, et al. [\u201cJudging LLM-as-a-judge with MT-Bench and Chatbot Arena.\u201d](https://arxiv.org/abs/2306.05685) arXiv preprint arXiv:2306.05685 (2023).\nDettmers, Tim, et al. [\u201cQlora: Efficient finetuning of quantized llms.\u201d](https://arxiv.org/abs/2305.14314) arXiv preprint arXiv:2305.14314 (2023).\nSwyx et al. [MPT-7B and The Beginning of Context=Infinity](https://www.latent.space/p/mosaic-mpt-7b#details) (2023).\nFradin, Michelle, Reeder, Lauren [\u201cThe New Language Model Stack\u201d](https://www.sequoiacap.com/article/llm-stack-perspective/) (2023).\nRadford, Alec, et al. [\u201cLearning transferable visual models from natural language supervision.\u201d](https://arxiv.org/abs/2103.00020) International conference on machine learning. PMLR, 2021.\nYan, Ziyou. [\u201cSearch: Query Matching via Lexical, Graph, and Embedding Methods.\u201d](https://eugeneyan.com/writing/search-query-matching/) eugeneyan.com, (2021).\nPetroni, Fabio, et al. [\u201cHow context affects language models\u2019 factual predictions.\u201d](https://arxiv.org/abs/2005.04611) arXiv preprint arXiv:2005.04611 (2020).\nKarpukhin, Vladimir, et al. [\u201cDense passage retrieval for open-domain question answering.\u201d](https://arxiv.org/abs/2004.04906) arXiv preprint arXiv:2004.04906 (2020).\nLewis, Patrick, et al. [\u201cRetrieval-augmented generation for knowledge-intensive nlp tasks.\u201d](https://arxiv.org/abs/2005.11401) Advances in Neural Information Processing Systems 33 (2020): 9459-9474.\nIzacard, Gautier, and Edouard Grave. [\u201cLeveraging passage retrieval with generative models for open domain question answering.\u201d](https://arxiv.org/abs/2007.01282) arXiv preprint arXiv:2007.01282 (2020).\nBorgeaud, Sebastian, et al. [\u201cImproving language models by retrieving from trillions of tokens.\u201d](https://arxiv.org/abs/2112.04426) International conference on machine learning. PMLR, (2022).\nLazaridou, Angeliki, et al. [\u201cInternet-augmented language models through few-shot prompting for open-domain question answering.\u201d](https://arxiv.org/abs/2203.05115) arXiv preprint arXiv:2203.05115 (2022).\nWang, Yue, et al. [\u201cCodet5+: Open code large language models for code understanding and generation.\u201d](https://arxiv.org/abs/2305.07922) arXiv preprint arXiv:2305.07922 (2023).\nGao, Luyu, et al. [\u201cPrecise zero-shot dense retrieval without relevance labels.\u201d](https://arxiv.org/abs/2212.10496) arXiv preprint arXiv:2212.10496 (2022).\nYan, Ziyou. [\u201cObsidian-Copilot: An Assistant for Writing & Reflecting.\u201d](https://eugeneyan.com/writing/obsidian-copilot/) eugeneyan.com, (2023).\nBojanowski, Piotr, et al. [\u201cEnriching word vectors with subword information.\u201d](https://arxiv.org/abs/1607.04606) Transactions of the association for computational linguistics 5 (2017): 135-146.\nReimers, Nils, and Iryna Gurevych. [\u201cMaking Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation.\u201d](https://arxiv.org/abs/2004.09813) Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, (2020).\nWang, Liang, et al. [\u201cText embeddings by weakly-supervised contrastive pre-training.\u201d](https://arxiv.org/abs/2212.03533) arXiv preprint arXiv:2212.03533 (2022).\nSu, Hongjin, et al. [\u201cOne embedder, any task: Instruction-finetuned text embeddings.\u201d](https://arxiv.org/abs/2212.09741) arXiv preprint arXiv:2212.09741 (2022).\nJohnson, Jeff, et al. [\u201cBillion-Scale Similarity Search with GPUs.\u201d](https://arxiv.org/abs/1702.08734) IEEE Transactions on Big Data, vol. 7, no. 3, IEEE, 2019, pp. 535\u201347.\nMalkov, Yu A., and Dmitry A. Yashunin. [\u201cEfficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.\u201d](https://arxiv.org/abs/1603.09320) IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 4, IEEE, 2018, pp. 824\u201336.\nGuo, Ruiqi, et al. [\u201cAccelerating Large-Scale Inference with Anisotropic Vector Quantization.\u201d](https://arxiv.org/abs/1908.10396.) International Conference on Machine Learning, (2020)\nOuyang, Long, et al. [\u201cTraining language models to follow instructions with human feedback.\u201d](https://arxiv.org/abs/2203.02155) Advances in Neural Information Processing Systems 35 (2022): 27730-27744.\nHoward, Jeremy, and Sebastian Ruder. [\u201cUniversal language model fine-tuning for text classification.\u201d](https://arxiv.org/abs/1801.06146) arXiv preprint arXiv:1801.06146 (2018).\nDevlin, Jacob, et al. [\u201cBert: Pre-training of deep bidirectional transformers for language understanding.\u201d](https://arxiv.org/abs/1810.04805) arXiv preprint arXiv:1810.04805 (2018).\nRadford, Alec, et al. [\u201cImproving language understanding with unsupervised learning.\u201d](https://openai.com/research/language-unsupervised) (2018).\nRaffel, Colin, et al. [\u201cExploring the limits of transfer learning with a unified text-to-text transformer.\u201d](https://arxiv.org/abs/1910.10683) The Journal of Machine Learning Research 21.1 (2020): 5485-5551.\nLester, Brian, Rami Al-Rfou, and Noah Constant. [\u201cThe power of scale for parameter-efficient prompt tuning.\u201d](https://arxiv.org/abs/2104.08691) arXiv preprint arXiv:2104.08691 (2021).\nLi, Xiang Lisa, and Percy Liang. [\u201cPrefix-tuning: Optimizing continuous prompts for generation.\u201d](https://arxiv.org/abs/2101.00190) arXiv preprint arXiv:2101.00190 (2021).\nHoulsby, Neil, et al. [\u201cParameter-efficient transfer learning for NLP.\u201d](https://arxiv.org/abs/1902.00751) International Conference on Machine Learning. PMLR, 2019.\nHu, Edward J., et al. [\u201cLora: Low-rank adaptation of large language models.\u201d](https://arxiv.org/abs/2106.09685) arXiv preprint arXiv:2106.09685 (2021).\nDettmers, Tim, et al. [\u201cQlora: Efficient finetuning of quantized llms.\u201d](https://arxiv.org/abs/2305.14314) arXiv preprint arXiv:2305.14314 (2023).\nWilliams, Adina, et al. [\u201cA Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.\u201d](https://cims.nyu.edu/~sbowman/multinli/) Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Association for Computational Linguistics, (2018).\n[GPTCache](https://github.com/zilliztech/GPTCache) (2023).\nBai, Yuntao, et al. [\u201cTraining a helpful and harmless assistant with reinforcement learning from human feedback.\u201d](https://arxiv.org/abs/2204.05862) arXiv preprint arXiv:2204.05862 (2022).\n[Guardrails](https://github.com/ShreyaR/guardrails) (2023)\n[NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) (2023)\nManakul, Potsawee, Adian Liusie, and Mark JF Gales. [\u201cSelfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.\u201d](https://arxiv.org/abs/2303.08896) arXiv preprint arXiv:2303.08896 (2023).\n[Guidance](https://github.com/microsoft/guidance) (2023).\nAmershi, Saleema, et al. [\u201cGuidelines for human-AI interaction.\u201d](https://www.microsoft.com/en-us/research/publication/guidelines-for-human-ai-interaction/) Proceedings of the 2019 chi conference on human factors in computing systems. 2019.\n[People + AI Guidebook](https://pair.withgoogle.com/guidebook/) (2023).\n[Human Interface Guidelines for Machine Learning](https://developer.apple.com/design/human-interface-guidelines/machine-learning) (2023).\nSchendel, Zachary A., Faraz Farzin, and Siddhi Sundar. [\u201cA Human Perspective on Algorithmic Similarity.\u201d](https://slideslive.com/38934788/a-human-perspective-on-algorithmic-similarity?ref=folder-59726) Proceedings of the 14th ACM Conference on Recommender Systems. 2020.\nIf you found this useful, please cite this write-up as:\n> Yan, Ziyou. (Jul 2023). Patterns for Building LLM-based Systems & Products. eugeneyan.com. https://eugeneyan.com/writing/llm-patterns/.\nor\n```\n@article{yan2023llm-patterns,\n title  = {Patterns for Building LLM-based Systems & Products},\n author = {Yan, Ziyou},\n journal = {eugeneyan.com},\n year  = {2023},\n month  = {Jul},\n url   = {https://eugeneyan.com/writing/llm-patterns/}\n}\n```\n\nShare on: \n![](https://eugeneyan.com/assets/icon-twitter.svg)\n![](https://eugeneyan.com/assets/icon-linkedin.svg)\n![](https://eugeneyan.com/assets/icon-facebook.svg)\n![](https://eugeneyan.com/assets/icon-mail.svg)\nBrowse related tags: [ [llm](https://eugeneyan.com/tag/llm/) [engineering](https://eugeneyan.com/tag/engineering/) [production](https://eugeneyan.com/tag/production/) [\ud83d\udd25](https://eugeneyan.com/tag/\ud83d\udd25/) ] \n[ ![](https://eugeneyan.com/assets/icon-search.svg)Search](https://eugeneyan.com/search/ \"Search\")\n[\u00ab Obsidian-Copilot: An Assistant for Writing & Reflecting](https://eugeneyan.com/writing/obsidian-copilot/) [How to Match LLM Patterns to Problems \u00bb](https://eugeneyan.com/writing/llm-problems/)\nJoin **10,300+** readers getting updates on machine learning, RecSys, LLMs, and engineering.\nGet email updates\n  * ![](https://eugeneyan.com/assets/icon-twitter.svg) [Twitter](https://twitter.com/eugeneyan \"Twitter\")\n  * ![](https://eugeneyan.com/assets/icon-linkedin.svg) [LinkedIn](https://www.linkedin.com/in/eugeneyan/ \"Linkedin\")\n  * ![](https://eugeneyan.com/assets/icon-github.svg) [GitHub](https://github.com/eugeneyan/ \"GitHub\")\n\n\nEugene Yan is a senior applied scientist at Amazon building machine learning systems that serve customers at scale. Previously, he led ML/AI at Alibaba, Lazada, and a Healthtech Series A. He writes and speaks about RecSys, LLMs, and engineering at [eugeneyan.com](https://eugeneyan.com/).\n\u00a9 Eugene Yan 2015 - 2025 \u2022 [Feedback](https://eugeneyan.com/site-feedback/) \u2022 [RSS](https://eugeneyan.com/rss/)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis document outlines seven key patterns for building LLM-based systems: Evals, RAG, Fine-tuning, Caching, Guardrails, Defensive UX, and Collecting User Feedback. These patterns help improve performance, reduce costs, and ensure output quality, guiding the integration of LLMs in products effectively.\n```"
  },
  {
    "instruction": "Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters\n[Lightning AI Studios: Never set up a local environment again \u2192](https://lightning.ai)\n[\u2190 Back to blog](https://lightning.ai/blog)\n#  Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters\nPosted on April 12, 2023 by [Sebastian Raschka](https://lightning.ai/pages/author/sebastian-raschka/ \"Posts by Sebastian Raschka\") - [Articles](https://lightning.ai/pages/category/community/article/), [Tutorials](https://lightning.ai/pages/category/community/tutorial/)\n#### Table of Contents\n  * [Finetuning Large Language Models](https://lightning.ai/pages/community/article/understanding-llama-adapters/#finetuning-large-language-models)\n  * [Parameter-Efficient Finetuning](https://lightning.ai/pages/community/article/understanding-llama-adapters/#parameter-efficient-finetuning)\n  * [Prompt Tuning And Prefix Tuning](https://lightning.ai/pages/community/article/understanding-llama-adapters/#prompt-tuning-and-prefix-tuning)\n  * [Adapters](https://lightning.ai/pages/community/article/understanding-llama-adapters/#adapters)\n  * [Extending Prefix Tuning and Adapters: LLaMA-Adapter](https://lightning.ai/pages/community/article/understanding-llama-adapters/#extending-prefix-tuning-and-adapters-llama-adapter)\n  * [Conclusion](https://lightning.ai/pages/community/article/understanding-llama-adapters/#conclusion)\n\n\n### Key takeaway\nLearn how popular parameter-efficient finetuning methods for LLM work: prefix tuning, adapters, and LLaMA-Adapter.\nIn the rapidly evolving field of artificial intelligence, utilizing large language models in an efficient and effective manner has become increasingly important.\nParameter-efficient finetuning stands at the forefront of this pursuit, allowing researchers and practitioners to reuse pretrained models while minimizing their computational and resource footprints. It also allows us to train AI models on a broader range of hardware, including devices with limited computational power, such as laptops, smartphones, and IoT devices. Lastly, with the increasing focus on environmental sustainability, parameter-efficient finetuning reduces the energy consumption and carbon footprint associated with training large-scale AI models.\nThis article explains the broad concept of finetuning and discusses popular parameter-efficient alternatives like prefix tuning and adapters. Finally, we will look at the recent LLaMA-Adapter method and see how we can use it in practice.\n## Finetuning Large Language Models\nSince GPT-2 ([Radford et al.](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)) and GPT-3 ([Brown et al.](https://arxiv.org/abs/2005.14165)), we have seen that generative large language models (LLMs) pretrained on a general text corpus are capable of in-context learning, which doesn\u2019t require us to further train or finetune pretrained LLMs if we want to perform specific or new tasks that the LLM wasn\u2019t explicitly trained on. Instead, we can directly provide a few examples of a target task via the input prompt, as illustrated in the example below.\n![In-context learning example.](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/in-context.png)\nIn-context learning is a valuable and user-friendly method for situations where direct access to the large language model (LLM) is limited, such as when interacting with the LLM through an API or user interface. However, if we have access to the LLM, adapting and finetuning it on a target task using data from a target domain usually leads to superior results. So, how can we adapt a model to a target task? There are three conventional approaches outlined in the figure below.\n![the three classic finetuning approaches](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/classic-flowchart.png)\nThese methods above are compatible with generative (decoder-style) models such as GPT and embedding-focused (encoder-style) models such as BERT. In contrast to these three approaches, in-context learning only applies to generative models. It\u2019s also worth highlighting that when we finetune generative models, we work with and build on the embeddings they create instead of the generated output texts.\n**Feature-based approach**\nIn the feature-based approach, we load a pretrained LLM and apply it to our target dataset. Here, we are particularly interested in generating the output embeddings for the training set, which we can use as input features to train a classification model. While this approach is particularly common for embedding-focused like BERT, we can also extract embeddings from generative GPT-style models (you can find an example in our blog post [here](https://lightning.ai/pages/blog/gradient-accumulation/)).\nThe classification model can then be a logistic regression model, a random forest, or XGBoost \u2014 whatever our hearts desire. (However, based on my experience, linear classifiers like logistic regression perform best here.)\nConceptually, we can illustrate the feature-based approach with the following code:\n```\n\nmodel = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n# ...\n# tokenize dataset\n# ...\n# generate embeddings\n@torch.inference_mode()\ndef get_output_embeddings(batch):\n  output = model(\n    batch[\"input_ids\"],\n    attention_mask=batch[\"attention_mask\"]\n  ).last_hidden_state[:, 0]\nreturn {\"features\": output}\ndataset_features = dataset_tokenized.map(\n get_output_embeddings, batched=True, batch_size=10)\nX_train = np.array(dataset_features[\"train\"][\"features\"])\ny_train = np.array(dataset_features[\"train\"][\"label\"])\nX_val = np.array(dataset_features[\"validation\"][\"features\"])\ny_val = np.array(dataset_features[\"validation\"][\"label\"])\nX_test = np.array(dataset_features[\"test\"][\"features\"])\ny_test = np.array(dataset_features[\"test\"][\"label\"])\n# train classifier\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\nprint(\"Training accuracy\", clf.score(X_train, y_train))\nprint(\"Validation accuracy\", clf.score(X_val, y_val))\nprint(\"test accuracy\", clf.score(X_test, y_test))\n\nExpandCopy\n\n```\n\n(Interested readers can find the full code example [here](https://github.com/rasbt/blog-finetuning-llama-adapters/blob/main/three-conventional-methods/1_distilbert-feature-extractor.ipynb).)\n**Finetuning I \u2014 Updating The Output Layers**\nA popular approach related to the feature-based approach described above is finetuning the output layers (we will refer to this approach as  _finetuning I_). Similar to the feature-based approach, we keep the parameters of the pretrained LLM frozen. We only train the newly added output layers, analogous to training a logistic regression classifier or small multilayer perceptron on the embedded features.\nIn code, this would look as follows:\n```\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n  \"distilbert-base-uncased\",\n   num_labels=2 # suppose target task is a binary classification task\n) \n# freeze all layers\nfor param in model.parameters():\n  param.requires_grad = False\n# then unfreeze the two last layers (output layers)\nfor param in model.pre_classifier.parameters():\n  param.requires_grad = True\nfor param in model.classifier.parameters():\n  param.requires_grad = True\n# finetune model\nlightning_model = CustomLightningModule(model)\ntrainer = L.Trainer(\n  max_epochs=3,\n  ...\n)\ntrainer.fit(\n model=lightning_model,\n train_dataloaders=train_loader,\n val_dataloaders=val_loader)\n# evaluate model\ntrainer.test(lightning_model, dataloaders=test_loader)\n\nExpandCopy\n\n```\n\n(Interested readers can find the complete code example [here](https://github.com/rasbt/blog-finetuning-llama-adapters/blob/main/three-conventional-methods/2_finetune-last-layers.ipynb).)\nIn theory, this approach should perform similarly well, in terms of modeling performance and speed, as the feature-based approach since we use the same frozen backbone model. However, since the feature-based approach makes it slightly easier to pre-compute and store the embedded features for the training dataset, the feature-based approach may be more convenient for specific practical scenarios.\n**Finetuning II \u2014 Updating All Layers**\nWhile the original BERT paper ([Devlin et al.](https://arxiv.org/abs/1810.04805)) reported that finetuning only the output layer can result in modeling performance comparable to finetuning all layers, which is substantially more expensive since more parameters are involved. For instance, a BERT base model has approximately 110 million parameters. However, the final layer of a BERT base model for binary classification consists of merely 1,500 parameters. Furthermore, the last two layers of a BERT base model account for 60,000 parameters \u2014 that\u2019s only around 0.6% of the total model size.\nOur mileage will vary based on how similar our target task and target domain is to the dataset the model was pretrained on. But in practice, finetuning all layers almost always results in superior modeling performance.\nSo, when optimizing the modeling performance, the gold standard for using pretrained LLMs is to update all layers (here referred to as finetuning II). Conceptually finetuning II is very similar to finetuning I. The only difference is that we do not freeze the parameters of the pretrained LLM but finetune them as well:\n```\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n  \"distilbert-base-uncased\",\n   num_labels=2 # suppose target task is a binary classification task\n) \n# don't freeze layers\n# for param in model.parameters():\n#  param.requires_grad = False\n# finetune model\nlightning_model = LightningModel(model)\ntrainer = L.Trainer(\n  max_epochs=3,\n  ...\n)\ntrainer.fit(\n model=lightning_model,\n train_dataloaders=train_loader,\n val_dataloaders=val_loader)\n# evaluate model\ntrainer.test(lightning_model, dataloaders=test_loader)\n\nExpandCopy\n\n```\n\n(Interested readers can find the complete code example here.)\nIf you are curious about some real-world results, the code snippets above were used to train a movie review classifier using a pretrained DistilBERT base model (you can access the code notebooks here): Feature-based approach with logistic regression: 83% test accuracy Finetuning I, updating the last 2 layers: 87% accuracy Finetuning II, updating all layers: 92% accuracy. These results are consistent with the general rule of thumb that finetuning more layers often results in better performance, but it comes with increased cost.\n![finetuning performance trade-offs](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/classic-performance.png)\n## **Parameter-Efficient Finetuning**\nIn the previous sections, we learned that finetuning more layers usually leads to better results. Now, the experiments above are based on a DistilBERT model, which is relatively small. What if we want to finetune larger models that only barely fit into GPU memory, for example, the latest generative LLMs? We can use the feature-based or finetuning I approach above, of course. But suppose we want to get a similar modeling quality as finetuning II?\nOver the years, researchers developed several techniques ([Lialin et al.](https://arxiv.org/abs/2303.15647)) to finetune LLM with high modeling performance while only requiring the training of only a small number of parameters. These methods are usually referred to as parameter-efficient finetuning techniques (PEFT).\nSome of the most widely used PEFT techniques are summarized in the figure below.\n![popular LLM finetuning methods](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/popular-methods.png)\nOne PEFT technique that recently made big waves is LLaMA-Adapter, which was proposed for Meta\u2019s popular LLaMA model ([Touvron et al.](https://arxiv.org/abs/2302.13971)) \u2014 however, while LLaMA-Adapter was proposed in the context of LLaMA, the idea is model-agnostic.\nTo understand how LLaMA-Adapter works, we have to take a (small) step back and review two related techniques called  _prefix tuning_ and  _adapters_ \u2014 LLaMA-Adapter ([Zhang et al.](https://arxiv.org/abs/2303.16199)) combines and extends both of these ideas.\nSo, in the remainder of this article, we will discuss the various concepts of prompt modifications to understand prefix tuning and adapter methods before we take a closer look at LLaMA-Adapter. (And we will save low-rank adaptation for a future article.)\n## **Prompt Tuning And Prefix Tuning**\nThe original concept of prompt tuning refers to techniques that vary the input prompt to achieve better modeling results. For example, suppose we are interested in translating an English sentence into German. We can ask the model in various different ways, as illustrated below.\n![an example of hard-prompting](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/hard-prompting.png)\nNow, this concept illustrated above is referred to as  _hard_ prompt tuning since we directly change the discrete input tokens, which are not differentiable. \nIn contrast to  _hard_ prompt tuning,  _soft_ prompt tuning concatenates the embeddings of the input tokens with a trainable tensor that can be optimized via backpropagation to improve the modeling performance on a target task. \nA specific flavor of prompt tuning is prefix tuning ([Li and Liang](https://arxiv.org/abs/2101.00190)). The idea in prefix tuning is to add a trainable tensor to each transformer block instead of only the input embeddings, as in  _soft_ prompt tuning. The following figure illustrates the difference between a regular transformer block and a transformer block modified with a prefix.\n![prefix-tuning for LLMs](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/prefix-tuning.png)\nNote that in the figure above, the \u201cfully connected layers\u201d refer to a small multilayer perceptron (two fully connected layers with a nonlinear activation function in-between). These fully connected layers embed the soft prompt in a feature space with the same dimensionality as the transformer-block input to ensure compatibility for concatenation. Using (Python) pseudo-code, we can illustrate the difference between a regular transformer block and a prefix-modified transformer block as follows:\n![transformer blog with prefix code](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/prefix-code.png)\nAccording to the original [prefix tuning](https://arxiv.org/abs/2101.00190) paper, prefix tuning achieves comparable modeling performance to finetuning all layers while only requiring the training of 0.1% of the parameters \u2014 the experiments were based on GPT-2 models. Moreover, in many cases, prefix tuning even outperformed the finetuning of all layers, which is likely because fewer parameters are involved, which helps reduce overfitting on smaller target datasets.\nLastly, to clarify the use of soft prompts during inference: after learning a soft prompt, we have to supply it as a prefix when performing the specific task we finetuned the model on. This allows the model to tailor its responses to that particular task. Moreover, we can have multiple soft prompts, each corresponding to a different task, and provide the appropriate prefix during inference to achieve optimal results for a particular task.\n## **Adapters**\nThe original  _adapter_ method ([Houlsby et al.](https://arxiv.org/abs/1902.00751)) is somewhat related to the aforementioned  _prefix tuning_ as they also add additional parameters to each transformer block. However, instead of prepending prefixes to the input embeddings, the adapter method adds adapter layers in two places, as illustrated in the figure below.\n![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/adapter-outline.png)\nAnd for readers who prefer (Python) pseudo-code, the adapter layer can be written as follows:\n![LLM Adapter Code](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/adapter.png)\nNote that the fully connected layers of the adapters are usually relatively small and have a bottleneck structure similar to autoencoders. Each adapter block\u2019s first fully connected layer projects the input down onto a low-dimensional representation. The second fully connected layer projects the input back into the input dimension. How is this parameter efficient? For example, assume the first fully connected layer projects a 1024-dimensional input down to 24 dimensions, and the second fully connected layer projects it back into 1024 dimensions. This means we introduced 1,024 x 24 + 24 x 1,024 = 49,152 weight parameters. In contrast, a single fully connected layer that reprojects a 1024-dimensional input into a 1,024-dimensional space would have 1,024 x 1024 = 1,048,576 parameters.\nAccording to the original [adapter paper](https://arxiv.org/abs/1902.00751), a BERT model trained with the adapter method reaches a modeling performance comparable to a fully finetuned BERT model while only requiring the training of 3.6% of the parameters.\nNow, the question is how the adapter method compares to prefix tuning. Based on the original [prefix tuning paper](https://arxiv.org/abs/2101.00190), the adapter method performed slightly worse than the prefix tuning method when 0.1% of the total number of model parameters were tuned. However, when the adapter method is used to tune 3% of the model parameters, the method ties with prefix tuning of 0.1% of the model parameters. So, we may conclude that the prefix tuning method is the more efficient of the two.\n## **Extending Prefix Tuning and Adapters: LLaMA-Adapter**\nExtending the ideas of prefix tuning and the original adapter method, researchers recently proposed LLaMA-Adapter ([Zhang et al.](https://arxiv.org/abs/2303.16199)), a parameter-efficient finetuning method for [LLaMA](https://github.com/facebookresearch/llama) (LLaMA is a popular GPT-alternative by Meta).\nLike  _prefix tuning_ , the LLaMA-Adapter method prepends tunable prompt tensors to the embedded inputs. It\u2019s worth noting that in the LLaMA-Adapter method, the prefix is learned and maintained within an embedding table rather than being provided externally. Each transformer block in the model has its own distinct learned prefix, allowing for more tailored adaptation across different model layers.\nIn addition, LLaMA-Adapter introduces a zero-initialized attention mechanism coupled with gating. The motivation behind this so-called  _zero-init_ attention and gating is that adapters and prefix tuning could potentially disrupt the linguistic knowledge of the pretrained LLM by incorporating randomly initialized tensors (prefix prompts or adapter layers), resulting in unstable finetuning and high loss values during initial training phases.\nAnother difference compared to prefix tuning and the original adapter method is that LLaMA-Adapter adds the learnable adaption prompts only to the  _L_ topmost transformer layers instead of all transformer layers. The authors argue that this approach enables more effective tuning of language representations focusing on higher-level semantic information.\nWhile the basic idea of the LLaMA adapter method is related to prefix tuning (prepending tunable soft prompts), there are some additional, subtle differences in how this is implemented. For instance, only a self-attention input\u2019s key and value sequences are modified via the tunable soft prompt. Then, depending on the gating factor (which is set to zero at the beginning of the training), the prefix-modified attention is either used or not. This concept is illustrated in the visualization below.\n![llama-adapter outline](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/llama-adapter.png)\nIn pseudo-code, we may express this as follows:\n![llama-adapter pseudo-code](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/llama-adapter-code-1.png)\nIn short, the differences between LLaMA-Adapter and regular prefix tuning are that LLaMA-Adapter only modifies the top (i.e., the first few) transformer blocks and introduces a gating mechanism to stabilize the training. While the researchers specifically experiment with LLaMA, their proposed Adapter method is a general method that can also be applied to other types of LLMs (like GPT).\nUsing the LLaMA-Adapter approach, the researchers were able to finetune a 7 billion parameter LLaMA model in only 1 hour (using eight A100 GPUs) on a dataset consisting of 52k instruction pairs. Furthermore, the finetuned LLaMA-Adapter model outperformed all other models compared in this study on question-answering tasks, while only 1.2 M parameters (the adapter layers) needed to be finetuned.\nIf you want to check out the LLaMA-Adapter method, you can find the original implementation on top of the GPL-licensed LLaMA code [here](https://github.com/ZrrSkywalker/LLaMA-Adapter).\nAlternatively, if your use cases are incompatible with the GPL license, which requires you to open source all derivative works under a similar license, check out the [Lit-LLaMA GitHub repository](https://github.com/Lightning-AI/lit-llama). Lit-LLaMA is a readable implementation of LLaMA on top of the Apache-licensed nanoGPT code, which has less restrictive licensing terms.\nSpecifically, if you are interested in finetuning a LLaMA model using the LLaMA-Apapter method, you can run the\n```\n\npython finetune_adapter.py\n\nExpandCopy\n\n```\n\nscript from the [Lit-LLaMA GitHub repository](https://github.com/Lightning-AI/lit-llama).\n## **Conclusion**\nFinetuning pre-trained large language models (LLMs) is an effective method to tailor these models to suit specific business requirements and align them with target domain data. This process involves adjusting the model parameters using a smaller dataset relevant to the desired domain, which enables the model to learn domain-specific knowledge and vocabulary.\nHowever, as LLMs are \u201clarge,\u201d updating multiple layers in a transformer model can be very expensive, so researchers started developing parameter-efficient alternatives.\nIn this article, we discussed several parameter-efficient alternatives to the conventional LLM finetuning mechanism. In particular, we covered prepending tunable soft prompts via prefix tuning and inserting additional adapter layers.\nFinally, we discussed the recent and popular LLaMA-Adapter method that prepends tunable soft prompts and introduces an additional gating mechanism to stabilize the training.\nIf you want to try this out in practice, check out [the Lit-LLaMA repository](https://github.com/Lightning-AI/lit-llama) \u2014 questions and suggestions for additional parameter-efficient finetuning methods are very welcome! (Preferably via the \ud83e\udd99[lit-llama channel on Discord](https://discord.com/invite/XncpTy7DSt)) \n**Acknowledgments**\nI want to thank Carlos Mocholi, Luca Antiga, and Adrian Waelchli for the constructive feedback to improve the clarity of this article.\n#### Table of Contents\n  * [Finetuning Large Language Models](https://lightning.ai/pages/community/article/understanding-llama-adapters/#finetuning-large-language-models)\n  * [Parameter-Efficient Finetuning](https://lightning.ai/pages/community/article/understanding-llama-adapters/#parameter-efficient-finetuning)\n  * [Prompt Tuning And Prefix Tuning](https://lightning.ai/pages/community/article/understanding-llama-adapters/#prompt-tuning-and-prefix-tuning)\n  * [Adapters](https://lightning.ai/pages/community/article/understanding-llama-adapters/#adapters)\n  * [Extending Prefix Tuning and Adapters: LLaMA-Adapter](https://lightning.ai/pages/community/article/understanding-llama-adapters/#extending-prefix-tuning-and-adapters-llama-adapter)\n  * [Conclusion](https://lightning.ai/pages/community/article/understanding-llama-adapters/#conclusion)\n\n\n#### Related Content\n[](https://lightning.ai/pages/community/announcements/lightning-ai-joins-ai-alliance-to-advance-open-safe-responsible-ai/)\n#####  Lightning AI Joins AI Alliance To Advance Open, Safe, Responsible AI\n[Read More](https://lightning.ai/pages/community/announcements/lightning-ai-joins-ai-alliance-to-advance-open-safe-responsible-ai/)\n[](https://lightning.ai/pages/community/tutorial/doubling-neural-network-finetuning-efficiency-with-16-bit-precision-techniques/)\n![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/bf16-featured-300x132.png)\n#####  Doubling Neural Network Finetuning Efficiency with 16-bit Precision Techniques\n[Read More](https://lightning.ai/pages/community/tutorial/doubling-neural-network-finetuning-efficiency-with-16-bit-precision-techniques/)\n[](https://lightning.ai/pages/community/lora-insights/)\n![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage7-300x177.png)\n#####  Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments\n[Read More](https://lightning.ai/pages/community/lora-insights/)\n![](https://t.co/1/i/adsct?=&bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%22%22%3A%22%22%7D&event_id=205f78f0-fe18-4c1e-95d2-bbc67046c355&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=bf4e31f2-1b6d-4b60-9bfb-4bc9b580a5db&tw_document_href=https%3A%2F%2Flightning.ai%2Fpages%2Fcommunity%2Farticle%2Funderstanding-llama-adapters%2F&tw_iframe_status=0&txn_id=p06ii&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?=&bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%22%22%3A%22%22%7D&event_id=205f78f0-fe18-4c1e-95d2-bbc67046c355&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=bf4e31f2-1b6d-4b60-9bfb-4bc9b580a5db&tw_document_href=https%3A%2F%2Flightning.ai%2Fpages%2Fcommunity%2Farticle%2Funderstanding-llama-adapters%2F&tw_iframe_status=0&txn_id=p06ii&type=javascript&version=2.3.31)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis article explores parameter-efficient finetuning methods for large language models (LLMs), focusing on prefix tuning, adapters, and the LLaMA-Adapter. These techniques minimize computational costs and environmental impact while enhancing model performance, enabling effective use on limited hardware.\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/triton-lang/triton/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Ftriton-lang%2Ftriton%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Ftriton-lang%2Ftriton%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=triton-lang%2Ftriton) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/triton-lang/triton/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/triton-lang/triton/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/triton-lang/triton/) to refresh your session. Dismiss alert\n{{ message }}\n[ triton-lang ](https://github.com/triton-lang) / **[triton](https://github.com/triton-lang/triton) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton) You must be signed in to change notification settings\n  * [ Fork 1.9k ](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton)\n  * [ Star  15k ](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton)\n\n\nDevelopment repository for the Triton language and compiler \n[triton-lang.org/](https://triton-lang.org/ \"https://triton-lang.org/\")\n### License\n[ MIT license ](https://github.com/triton-lang/triton/blob/main/LICENSE)\n[ 15k stars ](https://github.com/triton-lang/triton/stargazers) [ 1.9k forks ](https://github.com/triton-lang/triton/forks) [ Branches ](https://github.com/triton-lang/triton/branches) [ Tags ](https://github.com/triton-lang/triton/tags) [ Activity ](https://github.com/triton-lang/triton/activity)\n[ Star  ](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton)\n[ Notifications ](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton) You must be signed in to change notification settings\n  * [ Code ](https://github.com/triton-lang/triton)\n  * [ Issues 732 ](https://github.com/triton-lang/triton/issues)\n  * [ Pull requests 69 ](https://github.com/triton-lang/triton/pulls)\n  * [ Discussions ](https://github.com/triton-lang/triton/discussions)\n  * [ Actions ](https://github.com/triton-lang/triton/actions)\n  * [ Projects 1 ](https://github.com/triton-lang/triton/projects)\n  * [ Wiki ](https://github.com/triton-lang/triton/wiki)\n  * [ Security ](https://github.com/triton-lang/triton/security)\n  * [ Insights ](https://github.com/triton-lang/triton/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/triton-lang/triton)\n  * [ Issues  ](https://github.com/triton-lang/triton/issues)\n  * [ Pull requests  ](https://github.com/triton-lang/triton/pulls)\n  * [ Discussions  ](https://github.com/triton-lang/triton/discussions)\n  * [ Actions  ](https://github.com/triton-lang/triton/actions)\n  * [ Projects  ](https://github.com/triton-lang/triton/projects)\n  * [ Wiki  ](https://github.com/triton-lang/triton/wiki)\n  * [ Security  ](https://github.com/triton-lang/triton/security)\n  * [ Insights  ](https://github.com/triton-lang/triton/pulse)\n\n\n# triton-lang/triton\nmain\n[Branches](https://github.com/triton-lang/triton/branches)[Tags](https://github.com/triton-lang/triton/tags)\n[](https://github.com/triton-lang/triton/branches)[](https://github.com/triton-lang/triton/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[3,666 Commits](https://github.com/triton-lang/triton/commits/main/)[](https://github.com/triton-lang/triton/commits/main/)  \n[.github](https://github.com/triton-lang/triton/tree/main/.github \".github\")| [.github](https://github.com/triton-lang/triton/tree/main/.github \".github\")  \n[bin](https://github.com/triton-lang/triton/tree/main/bin \"bin\")| [bin](https://github.com/triton-lang/triton/tree/main/bin \"bin\")  \n[cmake](https://github.com/triton-lang/triton/tree/main/cmake \"cmake\")| [cmake](https://github.com/triton-lang/triton/tree/main/cmake \"cmake\")  \n[docs](https://github.com/triton-lang/triton/tree/main/docs \"docs\")| [docs](https://github.com/triton-lang/triton/tree/main/docs \"docs\")  \n[include](https://github.com/triton-lang/triton/tree/main/include \"include\")| [include](https://github.com/triton-lang/triton/tree/main/include \"include\")  \n[lib](https://github.com/triton-lang/triton/tree/main/lib \"lib\")| [lib](https://github.com/triton-lang/triton/tree/main/lib \"lib\")  \n[python](https://github.com/triton-lang/triton/tree/main/python \"python\")| [python](https://github.com/triton-lang/triton/tree/main/python \"python\")  \n[test](https://github.com/triton-lang/triton/tree/main/test \"test\")| [test](https://github.com/triton-lang/triton/tree/main/test \"test\")  \n[third_party](https://github.com/triton-lang/triton/tree/main/third_party \"third_party\")| [third_party](https://github.com/triton-lang/triton/tree/main/third_party \"third_party\")  \n[unittest](https://github.com/triton-lang/triton/tree/main/unittest \"unittest\")| [unittest](https://github.com/triton-lang/triton/tree/main/unittest \"unittest\")  \n[utils](https://github.com/triton-lang/triton/tree/main/utils \"utils\")| [utils](https://github.com/triton-lang/triton/tree/main/utils \"utils\")  \n[.clang-format](https://github.com/triton-lang/triton/blob/main/.clang-format \".clang-format\")| [.clang-format](https://github.com/triton-lang/triton/blob/main/.clang-format \".clang-format\")  \n[.editorconfig](https://github.com/triton-lang/triton/blob/main/.editorconfig \".editorconfig\")| [.editorconfig](https://github.com/triton-lang/triton/blob/main/.editorconfig \".editorconfig\")  \n[.git-blame-ignore-revs](https://github.com/triton-lang/triton/blob/main/.git-blame-ignore-revs \".git-blame-ignore-revs\")| [.git-blame-ignore-revs](https://github.com/triton-lang/triton/blob/main/.git-blame-ignore-revs \".git-blame-ignore-revs\")  \n[.gitignore](https://github.com/triton-lang/triton/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/triton-lang/triton/blob/main/.gitignore \".gitignore\")  \n[.pre-commit-config.yaml](https://github.com/triton-lang/triton/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/triton-lang/triton/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")  \n[CMakeLists.txt](https://github.com/triton-lang/triton/blob/main/CMakeLists.txt \"CMakeLists.txt\")| [CMakeLists.txt](https://github.com/triton-lang/triton/blob/main/CMakeLists.txt \"CMakeLists.txt\")  \n[CONTRIBUTING.md](https://github.com/triton-lang/triton/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/triton-lang/triton/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")  \n[LICENSE](https://github.com/triton-lang/triton/blob/main/LICENSE \"LICENSE\")| [LICENSE](https://github.com/triton-lang/triton/blob/main/LICENSE \"LICENSE\")  \n[Makefile](https://github.com/triton-lang/triton/blob/main/Makefile \"Makefile\")| [Makefile](https://github.com/triton-lang/triton/blob/main/Makefile \"Makefile\")  \n[README.md](https://github.com/triton-lang/triton/blob/main/README.md \"README.md\")| [README.md](https://github.com/triton-lang/triton/blob/main/README.md \"README.md\")  \n[RELEASE.md](https://github.com/triton-lang/triton/blob/main/RELEASE.md \"RELEASE.md\")| [RELEASE.md](https://github.com/triton-lang/triton/blob/main/RELEASE.md \"RELEASE.md\")  \n[pyproject.toml](https://github.com/triton-lang/triton/blob/main/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/triton-lang/triton/blob/main/pyproject.toml \"pyproject.toml\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/triton-lang/triton/)\n  * [MIT license](https://github.com/triton-lang/triton/)\n\n\n[![Triton logo](https://camo.githubusercontent.com/94edaa23faf33f9f7e76384487d4480ebd006ce791e63aba1fdbe0cf4adc06df/68747470733a2f2f6c68352e676f6f676c6575736572636f6e74656e742e636f6d2f777a514b457354466b72674e514f394a6a68474835774676736c4a723173614c744c614a5f613646705f674e454e70767433564737426d7a74776e67553968464a615534435077476977316f7051744476546b4c72785752624f5f613132512d70644553574867746d686549486350624f4c355a4d43345453694a566535747931773d7733353137)](https://camo.githubusercontent.com/94edaa23faf33f9f7e76384487d4480ebd006ce791e63aba1fdbe0cf4adc06df/68747470733a2f2f6c68352e676f6f676c6575736572636f6e74656e742e636f6d2f777a514b457354466b72674e514f394a6a68474835774676736c4a723173614c744c614a5f613646705f674e454e70767433564737426d7a74776e67553968464a615534435077476977316f7051744476546b4c72785752624f5f613132512d70644553574867746d686549486350624f4c355a4d43345453694a566535747931773d7733353137)\n**`Documentation`**| **`Nightly Wheels`**  \n---|---  \n[![Documentation](https://github.com/triton-lang/triton/actions/workflows/documentation.yml/badge.svg)](https://triton-lang.org/) | [![Wheels](https://github.com/triton-lang/triton/actions/workflows/wheels.yml/badge.svg)](https://github.com/triton-lang/triton/actions/workflows/wheels.yml)  \n# Triton\n[](https://github.com/triton-lang/triton/#triton)\nThis is the development repository of Triton, a language and compiler for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.\nThe foundations of this project are described in the following MAPL2019 publication: [Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations](http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf). Please consider citing this work if you use Triton!\nThe [official documentation](https://triton-lang.org) contains installation instructions and tutorials. See also these third-party [Triton puzzles](https://github.com/srush/Triton-Puzzles), which can all be run using the Triton interpreter -- no GPU required.\n# Quick Installation\n[](https://github.com/triton-lang/triton/#quick-installation)\nYou can install the latest stable release of Triton from pip:\n```\npip install triton\n```\n\nBinary wheels are available for CPython 3.9-3.13.\n# Enabling Blackwell Support\n[](https://github.com/triton-lang/triton/#enabling-blackwell-support)\nThe main branch now features support for NVIDIA Blackwell GPUs using 5th generation tensor cores. To enable this, you will need two additional steps:\n  1. Build a pre-release PyTorch from source with CUDA 12.8\n  2. Build triton from the latest source\n\n\nFirst, to build pytorch you need to have CUDA 12.8 installed locally. If not, follow the [instructions for your platform](https://developer.nvidia.com/cuda-downloads)\n```\n# Clone and checkout pytorch 2.6 release candidate\ngit clone https://github.com/pytorch/pytorch\ncd pytorch\ngit checkout v2.6.0-rc9\ngit submodule sync\ngit submodule update --init --recursive -j 8\n# Install build dependencies (assumes you already have a system compiler)\npip install -r requirements.txt\npip install mkl-static mkl-include wheel\n# Build PyTorch (will take a long time)\nexport CUDA_HOME=/usr/local/cuda-12.8\nexport CUDA_PATH=$CUDA_HOME\nexport TORCH_CUDA_ARCH_LIST=Blackwell\npython setup.py develop\n# Optional, package build into a wheel to install on other machines.\npython setup.py bdist_wheel\nls dist # Wheel should be output in this directory\n```\n\nNote that if you use the domain libraries (`torchvision`, `torchtext`, `torchaudio`, etc.) these will need to be built from source as well, otherwise their custom PyTorch extensions will not work.\nFinally, follow the instructions below to install triton from source.\n# Install from source\n[](https://github.com/triton-lang/triton/#install-from-source)\n```\ngit clone https://github.com/triton-lang/triton.git\ncd triton\npip install -r python/requirements.txt # build-time dependencies\npip install -e python\n```\n\nOr with a virtualenv:\n```\ngit clone https://github.com/triton-lang/triton.git\ncd triton\npython -m venv .venv --prompt triton\nsource .venv/bin/activate\npip install -r python/requirements.txt # build-time dependencies\npip install -e python\n```\n\n# Building with a custom LLVM\n[](https://github.com/triton-lang/triton/#building-with-a-custom-llvm)\nTriton uses LLVM to generate code for GPUs and CPUs. Normally, the Triton build downloads a prebuilt LLVM, but you can also build LLVM from source and use that.\nLLVM does not have a stable API, so the Triton build will not work at an arbitrary LLVM version.\n  1. Find the version of LLVM that Triton builds against. Check `cmake/llvm-hash.txt` to see the current version. For example, if it says: 49af6502c6dcb4a7f7520178bd14df396f78240c\nThis means that the version of Triton you have builds against [LLVM](https://github.com/llvm/llvm-project) 49af6502.\n  2. `git checkout` LLVM at this revision. Optionally, make additional modifications to LLVM.\n  3. [Build LLVM](https://llvm.org/docs/CMake.html). For example, you might run\n```\n$ cd $HOME/llvm-project # your clone of LLVM.\n$ mkdir build\n$ cd build\n$ cmake -G Ninja -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON ../llvm -DLLVM_ENABLE_PROJECTS=\"mlir;llvm;lld\" -DLLVM_TARGETS_TO_BUILD=\"host;NVPTX;AMDGPU\"\n$ ninja\n\n```\n\n  4. Grab a snack, this will take a while.\n  5. Build Triton as above, but set the following environment variables.\n```\n# Modify as appropriate to point to your LLVM build.\n$ export LLVM_BUILD_DIR=$HOME/llvm-project/build\n$ cd <triton install>\n$ LLVM_INCLUDE_DIRS=$LLVM_BUILD_DIR/include \\\n LLVM_LIBRARY_DIR=$LLVM_BUILD_DIR/lib \\\n LLVM_SYSPATH=$LLVM_BUILD_DIR \\\n pip install -e python\n\n```\n\n\n\n# Tips for building\n[](https://github.com/triton-lang/triton/#tips-for-building)\n  * Set `TRITON_BUILD_WITH_CLANG_LLD=true` as an environment variable to use clang and lld. lld in particular results in faster builds.\n  * Set `TRITON_BUILD_WITH_CCACHE=true` to build with ccache.\n  * Set `TRITON_HOME=/some/path` to change the location of the `.triton` directory where Triton's cache is located and downloads are stored during the build. By default, this is the user's home directory. It can be changed anytime.\n  * If you're running out of memory when building Triton, specify the `MAX_JOBS` environment variable (to the `pip install -e python` command) to limit the number of jobs.\n  * Pass `--no-build-isolation` to `pip install` to make nop builds faster. Without this, every invocation of `pip install` uses a different symlink to cmake, and this forces ninja to rebuild most of the `.a` files.\n  * vscode intellisense has some difficulty figuring out how to build Triton's C++ (probably because, in our build, users don't invoke cmake directly, but instead use setup.py). Teach vscode how to compile Triton as follows.\n    * Do a local build. Run command `pip install -e python`\n    * Get the full path to the `compile_commands.json` file produced by the build: `find python/build -name 'compile_commands.json' | xargs readlink -f`. You might get a full path similar to `/Users/{username}/triton/python/build/cmake.macosx-11.1-arm64-cpython-3.12/compile_commands.json`\n    * In vscode, install the [C/C++ extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools), then open the command palette (`Shift + Command + P` on Mac, or `Shift + Ctrl + P` on Windows/Linux) and open `C/C++: Edit Configurations (UI)`.\n    * Open \"Advanced Settings\" and paste the full path to `compile_commands.json` into the \"Compile Commands\" textbox.\n\n\n# Running tests\n[](https://github.com/triton-lang/triton/#running-tests)\nThere currently isn't a turnkey way to run all the Triton tests, but you can follow the following recipe.\n```\n# One-time setup. Note this will reinstall local Triton because torch\n# overwrites it with the public version.\n$ make dev-install\n# To run all tests (requires a GPU)\n$ make test\n# Or, to run tests without a gpu\n$ make test-nogpu\n```\n\n# Tips for hacking\n[](https://github.com/triton-lang/triton/#tips-for-hacking)\nFor detailed instructions on how to debug Triton's frontend, please refer to this [tutorial](https://triton-lang.org/main/programming-guide/chapter-3/debugging.html). The following includes additional tips for hacking on Triton's backend.\n**Helpful environment variables**\n  * `MLIR_ENABLE_DUMP=1` dumps the IR before every MLIR pass Triton runs, for all kernels. Use `MLIR_ENABLE_DUMP=kernelName` to dump for a specific kernel only.\n    * Triton cache can interfere with the dump. In cases where `MLIR_ENABLE_DUMP=1` does not work, try cleaning your triton cache: `rm -r ~/.triton/cache/*`\n  * `MLIR_DUMP_PATH` specifies where `MLIR_ENABLE_DUMP` will dump to. If unset will dump to stderr.\n  * `LLVM_IR_ENABLE_DUMP=1` dumps the IR before every pass run over the LLVM IR.\n  * `TRITON_REPRODUCER_PATH=<reproducer_path>` will generate an MLIR reproducer file at `<reproducer_path>` before each MLIR compiler stage. If any of the stages fail, `<reproducer_path>` will be a local MLIR reproducer captured right before the failing pass.\n  * `TRITON_INTERPRET=1` uses the Triton interpreter instead of running on the GPU. You can insert Python breakpoints in your kernel code!\n  * `TRITON_ENABLE_LLVM_DEBUG=1` passes `-debug` to LLVM, printing a lot of debugging information to stdout. If this is too noisy, run with just `TRITON_LLVM_DEBUG_ONLY` instead to limit the output.\nAn alternative way to reduce output noisiness is running with `LLVM_IR_ENABLE_DUMP=1`, extract the IR before the LLVM pass of interest, and then run LLVM's `opt` standalone, perhaps passing `-debug-only=foo` on the command line.\n  * `TRITON_LLVM_DEBUG_ONLY=<comma-separated>` is the equivalent of LLVM's `-debug-only` command-line option. This limits the LLVM debug output to specific pass or component names (which are specified using `#define DEBUG_TYPE` throughout LLVM and Triton) in order to allow the debug output to be less noisy. `TRITON_LLVM_DEBUG_ONLY` allows for one or more comma separated values to be specified (eg `TRITON_LLVM_DEBUG_ONLY=\"tritongpu-remove-layout-conversions\"` or `TRITON_LLVM_DEBUG_ONLY=\"tritongpu-remove-layout-conversions,regalloc\"`).\n  * `TRITON_ENABLE_ASAN=1` invokes the LLVM address sanitizer for memory leak and out of bounds access detection. Currently only supported on the AMD backend. This must be run using the ASAN libraries documented [here](https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/using-gpu-sanitizer.html).\nWhen enabling the address sanitizer it is recommended to disable various memory caching strategies both within the ROCm stack and PyTorch. This will give the address sanitizer the best chance at finding the memory fault where it originates. See this [test](https://github.com/triton-lang/triton/blob/main/third_party/amd/python/test/test_address_sanitizer.py) for more details.\n  * `USE_IR_LOC={ttir,ttgir}` reparses the IR such that the location information will be the line number of the IR file with that particular extension, instead of line number of the python file. This can provide a direct mapping from the IR to llir/ptx. When used with performance tools, it can provide a breakdown on IR instructions.\n  * `TRITON_PRINT_AUTOTUNING=1` prints out the best autotuning config and total time spent for each kernel after autotuning is complete.\n  * `DISABLE_LLVM_OPT` will disable llvm optimizations for make_llir and make_ptx if its value is true when parsing as Bool. Otherwise, it will be parsed as a list of flags to disable llvm optimizations. One usage case is `DISABLE_LLVM_OPT=\"disable-lsr\"` Loop strength reduction is known to cause up to 10% performance changes for certain kernels with register pressure.\n  * `TRITON_ALWAYS_COMPILE=1` forces to compile kernels regardless of cache hit.\n  * `MLIR_ENABLE_TIMING` dumps the timing information for each MLIR pass.\n  * `LLVM_ENABLE_TIMING` dumps the timing information for each LLVM pass.\n  * `TRITON_DEFAULT_FP_FUSION` overrides the default behavior of allowing fp fusion (mul+add->fma).\n  * `MLIR_ENABLE_DIAGNOSTICS=<comma-separated>` controls diagnostic emission in MLIR. Options are: `warnings`, `remarks`, `stacktraces`, `operations`. Use comma-separated values to customize output. For example, `MLIR_ENABLE_DIAGNOSTICS=remarks,operations` enables remarks and IR operations, while `MLIR_ENABLE_DIAGNOSTICS=warnings,stacktraces` enables warnings with stacktraces. By default, only errors are shown. Setting `warnings` includes errors and warnings; `remarks` includes errors, warnings, and remarks.\n  * `MLIR_ENABLE_REMARK` is deprecated. Please use `MLIR_ENABLE_DIAGNOSTICS=remarks`.\n  * `TRITON_KERNEL_DUMP` enables the dumping of the IR from each compilation stage and the final ptx/amdgcn.\n  * `TRITON_DUMP_DIR` specifies the directory to save the dumped IR and ptx/amdgcn when `TRITON_KERNEL_DUMP` is set to 1.\n  * `TRITON_KERNEL_OVERRIDE` enables the override of the compiled kernel with a user-specified IR/ptx/amdgcn at the beginning of each compilation stage.\n  * `TRITON_OVERRIDE_DIR` specifies the directory from which to load the IR/ptx/amdgcn files when `TRITON_KERNEL_OVERRIDE` is set to 1.\n  * `TRITON_F32_DEFAULT` sets the default input precision of `tl.dot` when using 32-bit floats, which can be either `ieee`, `tf32`, or `tf32x3`.\n  * `TRITON_FRONT_END_DEBUGGING=1` disables exception wrapping when an error occurs in the compiler frontend, allowing the full stack trace to be seen.\n\n\n**Kernel Override Steps**\n```\nexport TRITON_ALWAYS_COMPILE=1\nexport TRITON_KERNEL_DUMP=1\nexport TRITON_DUMP_DIR=<dump_dir>\nexport TRITON_KERNEL_OVERRIDE=1\nexport TRITON_OVERRIDE_DIR=<override_dir>\n# Step 1: Run the kernel once to dump kernel's IRs and ptx/amdgcn in $TRITON_DUMP_DIR\n# Step 2: Copy $TRITON_DUMP_DIR/<kernel_hash> to $TRITON_OVERRIDE_DIR\n# Step 3: Delete the stages that you do not want to override and modify the stage you do want to override\n# Step 4: Run the kernel again to see the overridden result\n```\n\n# Changelog\n[](https://github.com/triton-lang/triton/#changelog)\nVersion 2.0 is out! New features include:\n  * Many, many bug fixes\n  * Performance improvements\n  * Backend rewritten to use MLIR\n  * Support for kernels that contain back-to-back matmuls (e.g., flash attention)\n\n\n# Contributing\n[](https://github.com/triton-lang/triton/#contributing)\nCommunity contributions are more than welcome, whether it be to fix bugs or to add new features at [github](https://github.com/triton-lang/triton/). For more detailed instructions, please visit our [contributor's guide](https://github.com/triton-lang/triton/blob/main/CONTRIBUTING.md).\n# Compatibility\n[](https://github.com/triton-lang/triton/#compatibility)\nSupported Platforms:\n  * Linux\n\n\nSupported Hardware:\n  * NVIDIA GPUs (Compute Capability 8.0+)\n  * AMD GPUs (ROCm 6.2+)\n  * Under development: CPUs\n\n\n# Development Container (Dev Container)\n[](https://github.com/triton-lang/triton/#development-container-dev-container)\n**Dev Containers** for the Triton project are available from the [triton-dev-containers repository](https://github.com/redhat-et/triton-dev-containers)\n### Key Benefits:\n[](https://github.com/triton-lang/triton/#key-benefits)\n  * **Consistency** : All developers can work with the same development environment, ensuring uniform behavior across different systems.\n  * **Isolation** : The container prevents potential conflicts with software installed on your local machine.\n  * **Portability** : Easily share the development environment with team members, minimizing onboarding time and setup issues.\n\n\n### How to Use the Dev Container:\n[](https://github.com/triton-lang/triton/#how-to-use-the-dev-container)\nFor detailed instructions on how to use the dev containers please see the [dev container user guide](https://github.com/redhat-et/triton-dev-containers/blob/main/.devcontainer/devcontainer.md)\n## About\nDevelopment repository for the Triton language and compiler \n[triton-lang.org/](https://triton-lang.org/ \"https://triton-lang.org/\")\n### Resources\n[ Readme ](https://github.com/triton-lang/triton/#readme-ov-file)\n### License\n[ MIT license ](https://github.com/triton-lang/triton/#MIT-1-ov-file)\n[ Activity](https://github.com/triton-lang/triton/activity)\n[ Custom properties](https://github.com/triton-lang/triton/custom-properties)\n### Stars\n[ **15k** stars](https://github.com/triton-lang/triton/stargazers)\n### Watchers\n[ **196** watching](https://github.com/triton-lang/triton/watchers)\n### Forks\n[ **1.9k** forks](https://github.com/triton-lang/triton/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Ftriton-lang%2Ftriton&report=triton-lang+%28user%29)\n##  [Releases](https://github.com/triton-lang/triton/releases)\n[ 11 tags ](https://github.com/triton-lang/triton/tags)\n##  [Packages 0](https://github.com/orgs/triton-lang/packages?repo_name=triton)\nNo packages published \n##  [Used by 58.8k](https://github.com/triton-lang/triton/network/dependents)\n[\n  * ![@mshmygel](https://avatars.githubusercontent.com/u/177831504?s=64&v=4)\n  * ![@PrithvirajG](https://avatars.githubusercontent.com/u/67669857?s=64&v=4)\n  * ![@FhG-IISB-MKI](https://avatars.githubusercontent.com/u/205148278?s=64&v=4)\n  * ![@mlvc-lab](https://avatars.githubusercontent.com/u/51532443?s=64&v=4)\n  * ![@Bigkatoan](https://avatars.githubusercontent.com/u/63138206?s=64&v=4)\n  * ![@3078139041](https://avatars.githubusercontent.com/u/103644827?s=64&v=4)\n  * ![@Varshaa-08](https://avatars.githubusercontent.com/u/202936892?s=64&v=4)\n  * ![@OpenRobotLab](https://avatars.githubusercontent.com/u/127282590?s=64&v=4)\n\n+ 58,787  ](https://github.com/triton-lang/triton/network/dependents)\n##  [Contributors 397](https://github.com/triton-lang/triton/graphs/contributors)\n  * [ ![@ptillet](https://avatars.githubusercontent.com/u/614788?s=64&v=4) ](https://github.com/ptillet)\n  * [ ![@Jokeren](https://avatars.githubusercontent.com/u/2306281?s=64&v=4) ](https://github.com/Jokeren)\n  * [ ![@ThomasRaoux](https://avatars.githubusercontent.com/u/24322263?s=64&v=4) ](https://github.com/ThomasRaoux)\n  * [ ![@antiagainst](https://avatars.githubusercontent.com/u/487928?s=64&v=4) ](https://github.com/antiagainst)\n  * [ ![@jlebar](https://avatars.githubusercontent.com/u/150663?s=64&v=4) ](https://github.com/jlebar)\n  * [ ![@peterbell10](https://avatars.githubusercontent.com/u/13238737?s=64&v=4) ](https://github.com/peterbell10)\n  * [ ![@zahimoud](https://avatars.githubusercontent.com/u/128723247?s=64&v=4) ](https://github.com/zahimoud)\n  * [ ![@pawelszczerbuk](https://avatars.githubusercontent.com/u/153013546?s=64&v=4) ](https://github.com/pawelszczerbuk)\n  * [ ![@lezcano](https://avatars.githubusercontent.com/u/3291265?s=64&v=4) ](https://github.com/lezcano)\n  * [ ![@Mogball](https://avatars.githubusercontent.com/u/15016832?s=64&v=4) ](https://github.com/Mogball)\n  * [ ![@anmyachev](https://avatars.githubusercontent.com/u/45976948?s=64&v=4) ](https://github.com/anmyachev)\n  * [ ![@zhanglx13](https://avatars.githubusercontent.com/u/3949632?s=64&v=4) ](https://github.com/zhanglx13)\n  * [ ![@daadaada](https://avatars.githubusercontent.com/u/29946339?s=64&v=4) ](https://github.com/daadaada)\n  * [ ![@binarman](https://avatars.githubusercontent.com/u/3631351?s=64&v=4) ](https://github.com/binarman)\n\n\n[+ 383 contributors](https://github.com/triton-lang/triton/graphs/contributors)\n## Languages\n  * [ MLIR 41.9% ](https://github.com/triton-lang/triton/search?l=mlir)\n  * [ C++ 34.5% ](https://github.com/triton-lang/triton/search?l=c%2B%2B)\n  * [ Python 22.8% ](https://github.com/triton-lang/triton/search?l=python)\n  * [ CMake 0.6% ](https://github.com/triton-lang/triton/search?l=cmake)\n  * [ C 0.1% ](https://github.com/triton-lang/triton/search?l=c)\n  * [ Makefile 0.1% ](https://github.com/triton-lang/triton/search?l=makefile)\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary\n\nTriton is an open-source language and compiler for efficient custom Deep-Learning primitives, aiming for higher productivity than CUDA. It supports NVIDIA and AMD GPUs, with installation via pip. Key features include Blackwell GPU support, MLIR backend, and community contributions are encouraged. \n\n[Documentation](https://triton-lang.org) for installation and usage is available.\n```"
  },
  {
    "instruction": "[April 3: DBOS User Group Online Meeting!](https://www.dbos.dev/dbos-user-group-meeting)\n[![DBOS - Logo](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67241465129b580e4b924af4_Vectors-Wrapper.svg)](https://www.dbos.dev/)\nProducts\n[DBOS TransactOpen source durable execution library](https://www.dbos.dev/dbos-transact)[DBOS CloudDeploy with a click, scale to millions](https://www.dbos.dev/dbos-cloud)\nUse Cases\n[Durable Data PipelinesDurable, observable, scalable by default](https://www.dbos.dev/solutions/data-pipeline-platform)[Agentic AI WorkflowsCrash-proof AI automation](https://www.dbos.dev/solutions/agentic-ai-platform)[Cron JobsFree cron job hosting](https://www.dbos.dev/solutions/cron-hosting)[Cloud ModernizationCloud-enable or replatform legacy systems](https://www.dbos.dev/solutions/cloud-modernization)\n[Pricing](https://www.dbos.dev/dbos-pricing)[Docs](https://docs.dbos.dev/)\nResources\n[About DBOSSee our story, meet our team.](https://www.dbos.dev/about)[BlogThe latest on cloud & reliable backends.](https://www.dbos.dev/blog)[VideosDBOS concepts and best practices](https://www.dbos.dev/videos)\n[Start your project](https://console.dbos.dev/login-redirect)\n[Login](https://console.dbos.dev/login-redirect)\n[](https://github.com/dbos-inc/dbos-transact-py)[](https://discord.com/invite/jsmC6pXGgX)[Login](https://console.dbos.dev/launch)\n[Start your project](https://docs.dbos.dev/quickstart)\n# Reliable backends,built effortlessly\nReliable backends,built effortlessly\nDBOS provides an open-source library for building reliable and observable programs.Add a few annotations to your program to make it resilient to any failure.\n[Start your project](https://docs.dbos.dev/quickstart)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/6730407c420dae5eaf176275_Glow.svg)\n[Durable execution](https://www.dbos.dev/#w-tabs-0-data-w-pane-0)[Event-driven processing](https://www.dbos.dev/#w-tabs-0-data-w-pane-1)[Cron jobs](https://www.dbos.dev/#w-tabs-0-data-w-pane-2)[Data pipelines](https://www.dbos.dev/#w-tabs-0-data-w-pane-3)[Reliable AI agents](https://www.dbos.dev/#w-tabs-0-data-w-pane-4)[Webhooks & notifications](https://www.dbos.dev/#w-tabs-0-data-w-pane-5)\n## Durable workflow orchestration\nWrite your business logic in normal code, with branches, loops, subtasks, and retries. The DBOS Transact library makes it resilient to any failure.\n[See an example](https://docs.dbos.dev/python/tutorials/workflow-tutorial)\n@DBOS.workflow()def checkout_workflow(items): order = create_order() reserve_inventory(order, items) payment_status = process_payment(order, items) if payment_status == 'paid': fulfill_order(order) else: undo_reserve_inventory(order, items) cancel_order(order)\n## Event-driven processing\nConsume Kafka messages exactly-once, no need to worry about timeouts or offsets.\n[See an example](https://docs.dbos.dev/python/tutorials/kafka-integration)\n@DBOS.kafka_consumer(config,[\"alerts-topic\"])@DBOS.workflow()def process_kafka_alerts(msg: KafkaMessage): alerts = msg.value.decode() for alert in alerts: respond_to_alert(alert)\n## Cron jobs made easy\nSchedule your durable workflows to run exactly once per time interval.\n[See an example](https://docs.dbos.dev/python/examples/cron-starter)\n@DBOS.scheduled(\"0 * * * *\") # Run once an hour@DBOS.workflow()def run_hourly(scheduled_time: datetime, actual_time: datetime): results = search_hackernews(\"serverless\") for comment, url in results: post_to_slack(comment, url)\n## Resilient data pipelines\nBuild data pipelines that are reliable and observable by default.DBOS durable queues guarantee all your tasks complete.\n[See an example](https://docs.dbos.dev/python/examples/document-detective)\nqueue = Queue(\"indexing_queue\")@DBOS.workflow()def indexing_workflow(urls: List[HttpUrl]): handles: List[WorkflowHandle] = [] for url in urls: handle = queue.enqueue(index_document, url) handles.append(handle) indexed_pages = 0 for handle in handles: indexed_pages += handle.get_result() logger.info(f\"Indexed {len(urls)} documents totaling {indexed_pages} pages\")\n## Reliable AI agents\nEnhance your AI workflows with DBOS, build reliable AI agents with automatic retries and no limit on how long they can run for.\n[See an example](https://docs.dbos.dev/python/examples/reliable-ai-agent)\n@DBOS.step(retries_allowed=True, max_attempts=2)def process_refund(item, reason): return f\"Processed refund for item {item}, because {reason}\"@DBOS.step(retries_allowed=True, max_attempts=3)def apply_discount(amount): return f\"Applied discount of {amount}%\"refunds_agent = Agent( name=\"Refunds Agent\", instructions=\"Help the user with a refund. If the reason is that it was too expensive, offer the user a refund code.\", functions=[process_refund, apply_discount],)\n## Webhooks & notifications\nEffortlessly mix synchronous webhook code with asynchronous event processing. Reliably wait weeks or months for events, then use idempotency and durable execution to process them exactly once.\n[See an example](https://docs.dbos.dev/python/examples/rag-slackbot)\n@slackapp.message()def handle_message(request: BoltRequest) -> None: event_id = request.body[\"event_id\"] with SetWorkflowID(event_id):DBOS.start_workflow(message_workflow, request.body[\"event\"])\n\"I love the design of DBOS. If you're a gamer, it's like having a \u201csave point\u201d in your programs. If a function fails, a new function can start, picking up at the last checkpoint.\"\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e21d345ceb6064084225f8_Paul-Copplestone.jpeg)\nPaul Copplestone\nCEO & Co-Founder, Supabase.com\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/678ac6f8c8e18a35ad94d208_supabase-logo-wordmark-dark.png)\n\"With DBOS, developers can build applications in days that now take months with conventional approaches.\"\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/673ab74082767acb2d4c191c_Matei-Zaharia.png)\nMatei Zaharia\nCo-Founder, Databricks\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/673ab683b32cf03625b9fa9b_databricks-logo.png)\n\u201cWhat took us 2 months to build using a labyrinth of AWS resources took just 2 days with DBOS.\u201d\n![Thomas McNally Portrait](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/6724211a9ec6f0803fe3cdd8_TMcN.webp)\nThomas McNally\nVP Technology, TMG.io\n![tmg Logo](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/6729b592490a06a163cae7c3_TMG.png)\n\"DBOS isn't kidding when they say they make serverless simple. Their claims about speed and ease are right on.\"\n![Vince Fulco - Portrait](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/675b7ab0c18c12e6de867bb0_vince-fulco.jpeg)\nVince Fulco\nCo-Founder, Bighire.io\n![Bighire.io Logo](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/6729b592b8ea54af0f836896_Bighire.io.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a448eddd3bf98456f5530b_playfix-Logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a4478dc32b9c82f6d90472_Pin-Out-Logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a44180c41e3f2060b782a6_Strobe-Alert-logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a440a450b5c5639417bb2a_Digital-Underground-Technology-logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a434bf698fe4c1cea87770_Ravenvolt-logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a430264b81088f346238d4_Pearstop-logo-white.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/673d27061af0558b20254b3e_zeep-chat.svg)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67242fbc49dd3e04041ba3db_Bighire.io.svg)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67242fd05d5d91b38b2f3a9a_TMG.svg)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a448eddd3bf98456f5530b_playfix-Logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a4478dc32b9c82f6d90472_Pin-Out-Logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a44180c41e3f2060b782a6_Strobe-Alert-logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a440a450b5c5639417bb2a_Digital-Underground-Technology-logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a434bf698fe4c1cea87770_Ravenvolt-logo.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67a430264b81088f346238d4_Pearstop-logo-white.png)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/673d27061af0558b20254b3e_zeep-chat.svg)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67242fbc49dd3e04041ba3db_Bighire.io.svg)\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67242fd05d5d91b38b2f3a9a_TMG.svg)\n## Make code durable and observable in minutes\nDurable & observable in minutes, not months.\n### Open Source DBOS Transact\nLightweight open source durable execution library.\nSimply add a few annotations to your program to make it resilient to any failure. \nAbstract away the complexity of dealing with crashes, network outages, flaky endpoints, and infrastructure nuances.\nBuilt-in observability\u2013emit OpenTelemetry traces and monitor the execution of your workflows in real time.\n[DBOS Transact Quickstart](https://docs.dbos.dev/quickstart)\n## Reliability at any scale, anywhere\nTooling and hosting to make your DBOS Transact production deployments a success, on any platform.\n[![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e0ea70f549dd14d7092573_DBOS-Conductor-Workflows.webp)](https://www.dbos.dev/dbos-transact)\n### DBOS Pro \u2014 Run DBOS in production, effortlessly\nTooling to operate DBOS Transact applications anywhere.\n  * Manage application deployment, versioning, scaling \n  * Automatically detect and seamlessly recover interrupted workflows\n  * View and manage your workflows from anywhere\n\n[Explore DBOS Pro](https://www.dbos.dev/dbos-transact)\n[![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e450bf176b1e7bec8a7ea5_DBOS-Cloud-Serverless-Compute-Price-Performance.png)](https://www.dbos.dev/dbos-cloud)\n### DBOS Cloud \u2014 Durable app hosting\nA seriously fast serverless platform for DBOS Transact applications. \n  * 25x better price-performance than AWS Lambda + Step Functions\n  * Automatic app restart / resume with exactly-once processing\n  * Deploy with a click, scale to millions. \n\n[Explore DBOS Cloud](https://www.dbos.dev/dbos-cloud)\n## The fastest path to production-ready\nThe fastest path to production-ready\n### Rooted in cutting-edge research >\nBased on years of MIT and Stanford research, brought to you by the creators of Postgres, Apache Spark, and Chaos Engineering.\n### Works with your infrastructure >\nDBOS is open source and standards-based. It fits your dev and production environments like a glove.\n### Your data is safe >\nThe safety and privacy of your code and data is a top priority for DBOS.\n![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e1bb3fb99125a28f3abdbd_DBOS-Creators-of-PostgreSQL-Apache-Spark-MIT-Stanford.png)![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e0db56b795eb7cff2fa7c9_DBOS-Standards-Based.png)![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e0d43fffdc94caccbabbab_DBOS-SOC-GDPR-HIPAA.png)\n[Start your project](https://docs.dbos.dev/quickstart)\n## What you can build with DBOS\nMake backends more durable, cost-efficient, and easier to build.\n### [Document PipelineBuild a reliable and scalable document ingestion pipeline for a RAG-based chat agent.Learn more](https://docs.dbos.dev/python/examples/document-detective)\n### [Fault-Tolerant CheckoutUse durable workflows to build an online storefront that's resilient to any failure.Learn more](https://docs.dbos.dev/python/examples/widget-store)\n### [Stock TrackerTrack stock prices and receive alerts when they cross a threshold.Learn more](https://docs.dbos.dev/python/examples/stock-tracker)\n### [AI-Powered SlackbotUse DBOS and LlamaIndex to build an AI-powered Slackbot that answers questions.Learn more](https://docs.dbos.dev/python/examples/rag-slackbot)\n### [LLM-Powered ChatbotBuild a chatbot with DBOS, and deploy it 50x cheaper than AWS.Learn more](https://docs.dbos.dev/python/examples/chatbot)\n## Start for Free\nUse the open source DBOS Transact library, free forever.Try DBOS Pro fee for 30 days.\n[DBOS Pro Free Trial](https://console.dbos.dev/launch)\n[Browse pricing plans](https://www.dbos.dev/dbos-pricing)\n[![Gartner Cool Vendor 2024 - Badge](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/6724211a9ec6f0803fe3cdea_Gartner-Cool-Vendor-Badge-DBOS.jpg)2024 Gartner\u00ae Cool Vendor\u2122Enabling Efficient Cloud Operations](https://www.dbos.dev/blog/dbos-2024-gartner-cool-vendor)\n![DBOS Transact Workflow Example](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/673a7fab98fc4326f533ad6c_dbos-transact-workflow-hero.svg)![](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67e0e8b22b675cf0d1563159_DBOS-Conductor-Workflow-Management%20\\(2\\).jpg)\n## FAQs\n### Will DBOS add support for other languages?\n+\nDBOS applications are developed with the open source DBOS Transact framework, available for TypeScript and Python. We will implement the DBOS Transact framework in other languages based on popular demand.Please join the [DBOS Community on Discord](https://discord.gg/jsmC6pXGgX) and let us know what language(s) you'd like us to support!\n### What happens if my monthly usage exceeds my subscription settings?\n+\nIf your usage exceeds the your current DBOS subscription limits, we will not interrupt your service. What we will do is notify you via email that you've exceeded your usage limits. In response to the email, you can either reduce your usage or increase your subscription usage limits.\n### Can I run DBOS applications on-premises?\n+\nYes, you can run applications built with the open source DBOS Transact library on any Linux, MacOS, or Windows platform. Manage them with DBOS Pro.\n\u200d\n## Follow the future of cloud architecture\n[![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6789880d179631f2c2752989_DBOS-Transact-2-TypeScript.jpg)Announcing V2.0 of DBOS Transact for TypeScriptDBOS ArchitectureJanuary 16, 2025](https://www.dbos.dev/blog/dbos-transact-v2-typescript)\n[![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67327c6f1e5a6763385d906f_66cab52f9f89ac4f354dd5eb_How-to-add-observability-to-your-application.jpeg)How to Make Your Apps Observable by DefaultHow ToAugust 25, 2024](https://www.dbos.dev/blog/how-to-make-your-apps-observable-by-default)\n[![](https://cdn.prod.website-files.com/672411cbf038560468c9e68f/67e5be8ba3eabdb025ac8b28_Introducing-DBOS-Conductor.jpg)Durable Execution Anywhere - Introducing DBOS ConductorDBOS ArchitectureMarch 27, 2025](https://www.dbos.dev/blog/introducing-dbos-conductor)\n#### Improve your knowledge of durable application architecture.\n  * Forward-thinking, monthly updates.\n  * Perfect for devs building on cloud architecture.\n  * Advice on creating high-performing applications.\n\n\nYou've successfully subscribed. Stay tuned!\nOops! Please enter a valid email and try again.\nOnly valuable insights, no spam. Unsubscribe in a single click.\n### Build your reliable backend. Effortlessly.\n[Contact sales](https://www.dbos.dev/contact)\n[Start your project](https://docs.dbos.dev/quickstart)\n![DBOS - Logo](https://cdn.prod.website-files.com/672411cbf038560468c9e63b/67241465129b580e4b924af4_Vectors-Wrapper.svg)\nDBOS radically simplifies cloud application devops and deployment.\n[](https://www.linkedin.com/company/dbos-inc/)[](https://github.com/dbos-inc)[](https://discord.com/invite/jsmC6pXGgX)[](https://twitter.com/DBOS_Inc)[](https://www.dbos.dev/contact)\nProducts\n[DBOS Cloud](https://www.dbos.dev/dbos-cloud)[DBOS Transact](https://www.dbos.dev/dbos-transact)[Pricing Plans](https://www.dbos.dev/dbos-pricing)[Contact Us](https://www.dbos.dev/contact)\nSolutions\n[Cron Job Platform](https://www.dbos.dev/solutions/cron-hosting)[Durable AI Workflows](https://www.dbos.dev/solutions/agentic-ai-platform)[Durable Data Pipelines](https://www.dbos.dev/solutions/data-pipeline-platform)[Cloud Modernization](https://www.dbos.dev/solutions/cloud-modernization)\nDevelopers\n[Docs](https://docs.dbos.dev/)[Quickstart Guide](https://docs.dbos.dev/quickstart)[Examples](https://docs.dbos.dev/examples)[Tutorials](https://docs.dbos.dev/cloud-tutorials/application-management)\nCompany\n[About Us](https://www.dbos.dev/about)[Privacy Policy](https://www.dbos.dev/privacy)[Terms of Service](https://www.dbos.dev/terms-of-service)[Cookies](https://www.dbos.dev/)\nCopyright \u00a9 DBOS, Inc. 2025\n",
    "answer": "```markdown\n# TL;DR Summary\n\nDBOS offers an open-source library, DBOS Transact, for building reliable, durable applications with features like event-driven processing, cron jobs, and resilient data pipelines. It simplifies cloud deployment, enhances observability, and is backed by research from MIT and Stanford. Users can start for free and scale effortlessly.\n```"
  },
  {
    "instruction": "\u2728 New course! Enroll in [Vibe Coding 101 with Replit](https://bit.ly/4l8QyZ5)\n[](https://www.deeplearning.ai/)\n  * [Explore Courses](https://www.deeplearning.ai/courses/)\n  * [AI Newsletter](https://www.deeplearning.ai/the-batch/)\n    * [The Batch](https://www.deeplearning.ai/the-batch/)\n    * [Andrew's Letter](https://www.deeplearning.ai/the-batch/tag/letters/)\n    * [Data Points](https://www.deeplearning.ai/the-batch/tag/data-points/)\n    * [ML Research](https://www.deeplearning.ai/the-batch/tag/research/)\n    * [Blog](https://www.deeplearning.ai/blog/)\n  * [Community](https://www.deeplearning.ai/community/)\n    * [Forum](https://community.deeplearning.ai/)\n    * [Events](https://www.deeplearning.ai/events/)\n    * [Ambassadors](https://www.deeplearning.ai/ambassador/)\n    * [Ambassador Spotlight](https://www.deeplearning.ai/blog/category/ambassador-spotlight/)\n  * [Resources](https://www.deeplearning.ai/resources/)\n  * [Company](https://www.deeplearning.ai/about/)\n    * [About](https://www.deeplearning.ai/about/)\n    * [Careers](https://www.deeplearning.ai/careers/)\n    * [Contact](https://www.deeplearning.ai/contact/)\n\n[Log In](https://bit.ly/3RB9T8a)\n  * [Overview](https://www.deeplearning.ai/short-courses/reinforcement-learning-from-human-feedback/?utm_campaign=googlecloud2-launch&utm_medium=email&_hsmi=286365341&_hsenc=p2ANqtz-_23ROKwApqG01Fp8FT_ml2dc2BpqEK2fqGAzcDcB8QXYh2BXCkrKWGu9jjiC9MHS8FoYOn77Pmwc0l_VvgT6foQaXobw&utm_content=286365341&utm_source=hs_email/#overview)\n  * [Course Outline](https://www.deeplearning.ai/short-courses/reinforcement-learning-from-human-feedback/?utm_campaign=googlecloud2-launch&utm_medium=email&_hsmi=286365341&_hsenc=p2ANqtz-_23ROKwApqG01Fp8FT_ml2dc2BpqEK2fqGAzcDcB8QXYh2BXCkrKWGu9jjiC9MHS8FoYOn77Pmwc0l_VvgT6foQaXobw&utm_content=286365341&utm_source=hs_email/#course-outline)\n  * [Instructors](https://www.deeplearning.ai/short-courses/reinforcement-learning-from-human-feedback/?utm_campaign=googlecloud2-launch&utm_medium=email&_hsmi=286365341&_hsenc=p2ANqtz-_23ROKwApqG01Fp8FT_ml2dc2BpqEK2fqGAzcDcB8QXYh2BXCkrKWGu9jjiC9MHS8FoYOn77Pmwc0l_VvgT6foQaXobw&utm_content=286365341&utm_source=hs_email/#instructors)\n\n\n![](https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F12%2FMainGraphic_2070x1080-1.png&w=1080&q=75)\n  * [All Courses](https://www.deeplearning.ai/courses/)\n  * [Short Courses](https://www.deeplearning.ai/courses/?courses_date_desc%5BrefinementList%5D%5Bcourse_type%5D%5B0%5D=Short%20Courses)\n  * Reinforcement Learning from Human Feedback\n\n\nShort CourseIntermediate1 Hour 16 Minutes\n# Reinforcement Learning from Human Feedback\nInstructor: Nikita Namjoshi\n[![Google Cloud](https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/12/e5f1055b-da31-41d3-b251-57e79e4dfd5c.png)](https://cloud.google.com/)\n[Enroll for Free](https://learn.deeplearning.ai/reinforcement-learning-from-human-feedback?utm_source=home&utm_medium=course-landing-page&utm_campaign=hero-cta-button)\n  * [All Courses](https://www.deeplearning.ai/courses/)\n  * [Short Courses](https://www.deeplearning.ai/courses/?courses_date_desc%5BrefinementList%5D%5Bcourse_type%5D%5B0%5D=Short%20Courses)\n  * Reinforcement Learning from Human Feedback\n\n\n![](https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F12%2FYouTube-Thumbnails-14.png&w=3840&q=75)\n  * Intermediate\n  * 1 Hour 16 Minutes\n  * 6 Video Lessons\n  * 4 Code Examples\n  * Instructor: Nikita Namjoshi\n  *     * ![Google Cloud](https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F06%2Fgoogle-cloud.png&w=48&q=75)\nGoogle Cloud\n\n\n## What you'll learn\n  * Get a conceptual understanding of Reinforcement Learning from Human Feedback (RLHF), as well as the datasets needed for this technique\n  * Fine-tune the Llama 2 model using RLHF with the open source Google Cloud Pipeline Components Library\n  * Evaluate tuned model performance against the base model with evaluation methods\n\n\n## About this course\nLarge language models (LLMs) are trained on human-generated text, but additional methods are needed to align an LLM with human values and preferences.\nReinforcement Learning from Human Feedback (RLHF) is currently the main method for aligning LLMs with human values and preferences. RLHF is also used for further tuning a base LLM to align with values and preferences that are specific to your use case. \nIn this course, you will gain a conceptual understanding of the RLHF training process, and then practice applying RLHF to tune an LLM. You will: \n  * Explore the two datasets that are used in RLHF training: the \u201cpreference\u201d and \u201cprompt\u201d datasets.\n  * Use the open source Google Cloud Pipeline Components Library, to fine-tune the Llama 2 model with RLHF.\n  * Assess the tuned LLM against the original base model by comparing loss curves and using the \u201cSide-by-Side (SxS)\u201d method.\n\n\n## Who should join?\nAnyone with intermediate Python knowledge who\u2019s interested in learning about using the Reinforcement Learning from Human Feedback technique. \n## Course Outline\n6 Lessons\u30fb4 Code Examples\n  * Introduction\nVideo\u30fb4 mins\n  * How does RLHF work\nVideo\u30fb12 mins\n  * Datasets for RL training\nVideo with code examples\u30fb9 mins\n  * Tune an LLM with RLHF\nVideo with code examples\u30fb24 mins\n  * Evaluate the tuned model\nVideo with code examples\u30fb22 mins\n  * Google Cloud Setup\nCode examples\u30fb1 min\n  * Conclusion\nVideo\u30fb4 mins\n\n\n## Instructor\n![Nikita Namjoshi](https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F09%2F1673997679119.jpeg&w=256&q=75)\n### Nikita Namjoshi\nDeveloper Advocate at [Google Cloud](https://cloud.google.com/)\n  * [](https://www.linkedin.com/in/nikitanamjoshi/)\n\n\n## Reinforcement Learning from Human Feedback\n  * Intermediate\n  * 1 Hour 16 Minutes\n  * 6 Video Lessons\n  * 4 Code Examples\n  * Instructor: Nikita Namjoshi\n  *     * ![Google Cloud](https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F06%2Fgoogle-cloud.png&w=48&q=75)\nGoogle Cloud\n\n\n[Enroll for Free](https://learn.deeplearning.ai/reinforcement-learning-from-human-feedback?utm_source=home&utm_medium=course-landing-page&utm_campaign=summary-cta-button)\nCourse access is free for a limited time during the DeepLearning.AI learning platform beta!\n[Enroll for Free](https://learn.deeplearning.ai/reinforcement-learning-from-human-feedback?utm_source=home&utm_medium=course-landing-page&utm_campaign=promotional-banner-cta-button)\n## Want to learn more about Generative AI?\nKeep learning with updates on curated AI news, courses, and events, as well as Andrew\u2019s thoughts from DeepLearning.AI! \n[Enroll for Free](https://learn.deeplearning.ai/reinforcement-learning-from-human-feedback?utm_source=home&utm_medium=course-landing-page&utm_campaign=summary-cta-button)\n  * [Courses](https://www.deeplearning.ai/courses/)\n  * [The Batch](https://www.deeplearning.ai/the-batch/)\n  * [Community](https://www.deeplearning.ai/community/)\n  * [Careers](https://www.deeplearning.ai/careers/)\n  * [About](https://www.deeplearning.ai/about/)\n\n\n[](https://www.facebook.com/1027125564106325)[](https://www.instagram.com/deeplearningai)[](https://twitter.com/deeplearningai)[](https://www.linkedin.com/company/18246783)[](https://www.youtube.com/c/Deeplearningai)\n",
    "answer": "```markdown\n## TL;DR Summary\n\nEnroll in the **Reinforcement Learning from Human Feedback** course by **Nikita Namjoshi** at DeepLearning.AI. Learn to fine-tune LLMs like Llama 2 using RLHF, evaluate model performance, and understand key datasets. Ideal for those with intermediate Python skills. \n\n[Enroll for Free](https://learn.deeplearning.ai/reinforcement-learning-from-human-feedback?utm_source=home&utm_medium=course-landing-page&utm_campaign=summary-cta-button)\n```"
  },
  {
    "instruction": "[Skip to content](https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557/#wp--skip-link--target)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nThe world\u2019s leading publication for data science, AI, and ML professionals.\nSign in\nSign out\n[Contributor Portal](https://contributor.insightmediagroup.io/)\n  * [Latest](https://towardsdatascience.com/latest/)\n  * [Editor\u2019s Picks](https://towardsdatascience.com/tag/editors-pick/)\n  * [Deep Dives](https://towardsdatascience.com/tag/deep-dives/)\n  * [Contribute](https://towardsdatascience.com/questions-96667b06af5/)\n  * [Newsletter](https://newsletter.towardsdatascience.com/subscription-to-the-newsletter)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\n\n\nToggle Mobile Navigation\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [X](https://x.com/TDataScience)\n\n\nToggle Search\nSearch\n[ ChatGPT ](https://towardsdatascience.com/category/artificial-intelligence/chatgpt/)\n# Visualize your RAG Data \u2013 Evaluate your Retrieval-Augmented Generation System with Ragas\nHow to use UMAP dimensionality reduction for Embeddings to show multiple evaluation Questions and their relationships to source documents\u2026 \n[Markus Stoll](https://towardsdatascience.com/author/markus-stoll/)\nMar 3, 2024\n15 min read\nShare \nRetrieval-Augmented Generation (RAG) adds a retrieval step to the workflow of an LLM, enabling it to query relevant data from additional sources like private documents when responding to questions and queries [1]. This workflow does not require costly training or fine-tuning of LLMs on the additional documents. The documents are split into snippets, which are then indexed, often using a compact ML-generated vector representation (embedding). Snippets with similar content will be in proximity to each other in this embedding space.\nThe RAG application projects the user-provided questions into the embedding space to retrieve relevant document snippets based on their distance to the question. The LLM can use the retrieved information to answer the query and to substantiate its conclusion by presenting the snippets as references.\n![Animation of the iterations of a UMAP \\[3\\] dimensionality reduction for Wikipedia Formula One articles in the embedding space with manually labeled clusters - created by the author.](https://towardsdatascience.com/wp-content/uploads/2024/03/1peWTe1A-MqeROT_Jdof_Cw.gif)Animation of the iterations of a UMAP [3] dimensionality reduction for Wikipedia Formula One articles in the embedding space with manually labeled clusters \u2013 created by the author.\nThe evaluation of a RAG application is challenging [2]. Different approaches exist: on one hand, there are methods where the answer as ground truth must be provided by the developer; on the other hand, the answer (and the question) can also be generated by another LLM. One of the largest open-source systems for LLM-supported answering is Ragas [4](Retrieval-Augmented Generation Assessment), which provides\n  * Methods for generating test data based on the documents and\n  * Evaluations based on different metrics for evaluating retrieval and generation steps one-by-one and end-to-end.\n\n\nIn this article, you will learn\n  * How to briefly build a RAG system for Formula One (see the previous article [Visualize your RAG Data \u2013 EDA for Retrieval-Augmented Generation](https://medium.com/itnext/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f) for detailed descriptions)\n  * Generate questions and answers\n  * Evaluate the RAG system with [Ragas](https://github.com/explodinggradients/ragas)\n  * Most importantly how to visualize the results with [Renumics Spotlight ](https://github.com/Renumics/spotlight)and interpret the results.\n\n\n> _The [code is available at Github](https://github.com/Renumics/renumics-rag/blob/main/notebooks/visualize_rag_tutorial_qs.ipynb)_\n## Get your environment ready\nStart a notebook and install the required python packages\n```\n!pip install langchain langchain-openai chromadb renumics-spotlight\n%env OPENAI_API_KEY=<your-api-key>\n```\n\nThis tutorial uses the following python packages:\n  * **[Langchain](https://github.com/langchain-ai/langchain)** : A framework to integrate language models and RAG components, making the setup process smoother.\n  * **[Renumics-Spotlight](https://github.com/Renumics/spotlight)** : A visualization tool to interactively explore unstructured ML datasets.\n  * **[Ragas](https://github.com/explodinggradients/ragas)** : a framework that helps you evaluate your RAG pipelines\n\n\n_Disclaimer: The author of this article is also one of the developers of Spotlight._\n## Prepare documents and embeddings for the dataset\nYou can use your own RAG Application, skip to the next part to learn how to evaluate, extract and visualize.\nOr you can use the RAG application from the [last article](https://medium.com/itnext/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f) with [our prepared dataset of all Formula One articles of Wikipedia](https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/docs.zip). There you can also insert your own Documents into a \u2018docs/\u2019 subfolder.\n> This dataset is based on articles from [Wikipedia](https://www.wikipedia.org/) and is licensed under the Creative Commons Attribution-ShareAlike License. The original articles and a list of authors can be found on the respective Wikipedia pages.\nNow you can use Langchain\u2019s `DirectoryLoader` to load all files from the docs subdirectory and split the documents in snippets using the `RecursiveCharacterTextSpliter`. With `OpenAIEmbeddings` you can create embeddings and store them in a `ChromaDB` as vector store. For the Chain itself you can use LangChains `ChatOpenAI` and a `ChatPromptTemplate`.\nThe[ linked code](https://github.com/Renumics/rag-demo/blob/main/notebooks/visualize_rag_tutorial_qs.ipynb) for this article contains all necessary steps and you can find a detailed description of all steps above in [the last article](https://medium.com/itnext/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f).\nOne important point is, that you should use a hash function to create ids for snippets in `ChromaDB`. This allows to find the embeddings in the db if you only have the document with its content and metadata. This makes it possible to skip documents that already exist in the database.\n```\nimport hashlib\nimport json\nfrom langchain_core.documents import Document\ndef stable_hash_meta(doc: Document) -> str:\n  \"\"\"\n  Stable hash document based on its metadata.\n  \"\"\"\n  return hashlib.sha1(json.dumps(doc.metadata, sort_keys=True).encode()).hexdigest()\n...\nsplits = text_splitter.split_documents(docs)\nsplits_ids = [\n  {\"doc\": split, \"id\": stable_hash_meta(split.metadata)} for split in splits\n]\nexisting_ids = docs_vectorstore.get()[\"ids\"]\nnew_splits_ids = [split for split in splits_ids if split[\"id\"] not in existing_ids]\ndocs_vectorstore.add_documents(\n  documents=[split[\"doc\"] for split in new_splits_ids],\n  ids=[split[\"id\"] for split in new_splits_ids],\n)\ndocs_vectorstore.persist()\n```\n\n## Evaluation Questions\nFor a common topic like Formula One, one can also use [ChatGPT](https://towardsdatascience.com/tag/chatgpt/ \"ChatGPT\") directly to generate general questions. In this article, four methods of question generation are used:\n  * **GPT4** : 30 questions were generated using ChatGPT 4 with the following prompt \"Write 30 question about Formula one\" \u2013 Random Example: \"Which Formula 1 team is known for its prancing horse logo?\"\n  * **GPT3.5:** Another 199 question were generated with ChatGPT 3.5 with the following prompt \"Write 100 question about Formula one\" and repeating \"Thanks, write another 100 please\" \u2013 Example: \"\"Which driver won the inaugural Formula One World Championship in 1950?\"\n  * **Ragas_GPT4** : 113 questions were generated using Ragas. Ragas utilizes the documents again and its own embedding model to construct a vector database, which is then used to generate questions with GPT4. \u2013 Example: \"Can you tell me more about the performance of the Jordan 198 Formula One car in the 1998 World Championship?\"\n  * **Rags_GPT3.5** : 226 additional questions were generated with Ragas \u2013 here we use GPT3.5 \u2013 Example: \"What incident occurred at the 2014 Belgian Grand Prix that led to Hamilton\u2019s retirement from the race?\"\n\n```\nfrom ragas.testset import TestsetGenerator\ngenerator = TestsetGenerator.from_default(\n  openai_generator_llm=\"gpt-3.5-turbo-16k\", \n  openai_filter_llm=\"gpt-3.5-turbo-16k\"\n)\ntestset_ragas_gpt35 = generator.generate(docs, 100)\n```\n\nThe questions and answers were not reviewed or modified in any way. All questions are combined in a single dataframe with the columns `id`, `question`, `ground_truth`, `question_by` and `answer`.\n![](https://towardsdatascience.com/wp-content/uploads/2024/03/1R_74K0-_SJXyTxq6ovAcWg.png)\nNext, the questions will be posed to the RAG system. For over 500 questions, this can take some time and incur costs. If you ask the questions row-by-row, you can pause and continue the process or recover from a crash without losing the results so far:\n```\nfor i, row in df_questions_answers.iterrows():\n  if row[\"answer\"] is None or pd.isnull(row[\"answer\"]):\n    response = rag_chain.invoke(row[\"question\"])\n    df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response[\n      \"answer\"\n    ]\n    df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = [\n      stable_hash_meta(source_document.metadata)\n      for source_document in response[\"source_documents\"]\n    ]\n\n```\n\nNot only is the answer stored but also the source IDs of the retrieved document snippets, and their text content as context:\n![](https://towardsdatascience.com/wp-content/uploads/2024/03/1umlKv7Qf9SSLzRslT2r0Qw.png)\nAdditionally, the embeddings for all questions are generated and stored in the dataframe as well. This allows for visualizing them alongside the documents.\n## Evaluation with Ragas\n[Ragas](https://github.com/explodinggradients/ragas) provides metrics for evaluating each component of your RAG pipeline in isolation and end-to-end metrics for overall performance:\n  1. **Context Precision:** Uses the `question` and retrieved `contexts` to measure the signal-to-noise ratio.\n  2. **Context Relevancy:** Measures the relevance of the retrieved context to the question, calculated using the `question` and `contexts`.\n  3. **Context Recall:** Based on the `ground truth` and `contexts` to check if all relevant information for the answer is retrieved.\n  4. **Faithfulness:** Utilizes the `contexts` and `answer` to measure how factually accurate the generated answer is.\n  5. **Answer Relevance:** Computed using the `question` and `answer` to assess the relevance of the generated answer to the question (does not consider factuality).\n  6. **Answer Semantic Similarity:** Evaluated using the `ground truth` and `answer` to assess the semantic resemblance between the generated and the correct answer.\n  7. **Answer Correctness:** Relies on the `ground truth` and `answer` to measure the accuracy and alignment of the generated answer with the correct one.\n  8. **Aspect Critique:** Involves analyzing the `answer` to evaluate submissions based on predefined or custom aspects such as correctness or harmfulness.\n\n\nFor now, we focus on the end-to-end metric of answer correctness. The column names and content in the dataframe are copied and adapted to meet the naming and formatting requirements according to the Ragas API:\n```\n# prepare the dataframe for evaluation\ndf_qa_eval = df_questions_answers.copy()\n# adapt the ground truth to the ragas naming and format\ndf_qa_eval.rename(columns={\"ground_truth\": \"ground_truths\"}, inplace=True)\ndf_qa_eval[\"ground_truths\"] = [\n  [gt] if not isinstance(gt, list) else gt for gt in df_qa_eval[\"ground_truths\"]\n]\n```\n\nThis again can take some time and even more money than just querying your RAG system. Let\u2019s apply the evaluation row-by-row to be able to recover from a crash without losing the results so far:\n```\n# evaluate the answer correctness if not already done\nfields = [\"question\", \"answer\", \"contexts\", \"ground_truths\"]\nfor i, row in df_qa_eval.iterrows():\n  if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n    evaluation_result = evaluate(\n      Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n      [answer_correctness],\n    )\n    df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n      \"answer_correctness\"\n    ]\n\n```\n\nAfterwards, you can store the results in the `df_questions_answer` dataframe:\n```\ndf_questions_answers[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]\n```\n\n## Prepare visualization\nTo include the document snippets in the visualization, we add references from documents to questions that used the document as a source. Additionally, the count of questions referencing a document is stored:\n```\n# Explode 'source_documents' so each document ID is in its own row alongside the question ID\ndf_questions_exploded = df_qa_eval.explode(\"source_documents\")\n# Group by exploded 'source_documents' (document IDs) and aggregate\nagg = (\n  df_questions_exploded.groupby(\"source_documents\")\n  .agg(\n    num_questions=(\"id\", \"count\"), # Count of questions referencing the document\n    question_ids=(\n      \"id\",\n      lambda x: list(x),\n    ), # List of question IDs referencing the document\n  )\n  .reset_index()\n  .rename(columns={\"source_documents\": \"id\"})\n)\n# Merge the aggregated information back into df_documents\ndf_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n# Use apply to replace NaN values with empty lists for 'question_ids'\ndf_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n  lambda x: x if isinstance(x, list) else []\n)\n# Replace NaN values in 'num_questions' with 0\ndf_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)\n```\n\nNow concatenate the dataframe of questions with the dataframe of the documents\n```\ndf = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n```\n\nAdditionally, let\u2019s prepare some different UMAP [3] mappings. You could do much the same in the Spotlight GUI later, but doing it upfront can save time.\n  * umap_all: UMAP with fit and transform applied on all document and question embeddings\n  * umap_questions: UMAP with fit applied on questions embeddings only and transform applied on both\n  * umap_docs: UMAP with fit applied on document embeddings only and transform applied on both\n\n\nWe prepare each of the UMAP transformations like this:\n```\n\numap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit\numap_all = umap.transform(df[\"embedding\"].values.tolist())\ndf[\"umap\"] = umap_all.tolist()\n\n```\n\nAnother interesting metric for each of the document snippets is the distance between its embeddings and the embeddings of the nearest question\n```\nquestion_embeddings = np.array(df[df[\"question\"].notna()][\"embedding\"].tolist())\ndf[\"nearest_question_dist\"] = [ # brute force, could be optimized using ChromaDB\n  np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings)])\n  for doc_emb in df[\"embedding\"].values\n]\n```\n\nThis metric can be helpful to find documents that are not referenced by questions.\n![](https://towardsdatascience.com/wp-content/uploads/2024/03/1YTRUXZmd0iX8kyPIdUUnlg.png)\n## Visualize results\nIf you skipped the previous steps, you can download the dataframe and load it with:\n```\nimport pandas as pd\ndf = pd.read_parquet(\"df_f1_rag_docs_and_questions.parquet\")\n```\n\nand start [Renumics Spotlight](https://github.com/Renumics/spotlight) to visualize it with:\n```\nfrom renumics import spotlight\nspotlight.show(df)\nspotlight.show(\n  df,\n  layout=\"/home/markus/Downloads/layout_rag_1.json\",\n  dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n)\n```\n\nIt will open a new browser window:\n![Formula One documents and evaluation questions statistics and similarity maps - created by the author with Renumics Spotlight](https://towardsdatascience.com/wp-content/uploads/2024/03/1IMbva0pP8RAVhoY4dVbjLg.png)Formula One documents and evaluation questions statistics and similarity maps \u2013 created by the author with [Renumics Spotlight](https://github.com/Renumics/spotlight)\nOn the top left side, you can see a **table of all questions and all document** snippets. You can use the \"visible columns\" button to control which columns of the dataframe are shown in the table. It is useful to create a filter directly that selects only the questions to be able to turn the questions on and off in the visualizations: Select all questions and and then create a filter using the \"Create filter from selected row\" button.\nTo the right of the table, the `answer correctness` **is displayed as a metric** across all questions. Below there are two **histograms** ; the left one shows the distribution of `answer correctness` divided into the different methods of question generation. The right one shows the distribution of methods of question generation. Here, it is advisable to create a filter for the questions using the filter button to display only the selected rows (the questions) if needed.\nOn the right side, there are **two similarity maps.** The first one uses the `umap_questions` column and shows the questions and documents based on the transformation applied only to the questions. It is helpful for viewing the distribution of questions independently from the associated documents because this approach allows analysts to identify patterns or clusters within the questions themselves.\nThe second similarity map shows the questions and documents based on the transformation applied only to the documents (`umap_docs`). It is useful for viewing the questions in the context of their associated documents. A similarity map that simultaneously transforms questions and documents has proven to be less helpful with a larger number of questions, as more or fewer questions get clustered together and tend to be separated from the documents. Therefore, this representation is omitted here.\n![Formula One evaluation questions statistics and similarity maps - created by the author with Renumics Spotlight](https://towardsdatascience.com/wp-content/uploads/2024/03/11wZrAj60hiw1T3RVnCuBtA.png)Formula One evaluation questions statistics and similarity maps \u2013 created by the author with [Renumics Spotlight](https://github.com/Renumics/spotlight)\n### Document Embedding Similarity Map: Observations\nIn the similarity map `umap_docs`, you can identify areas in the embedding space of the documents that have no neighboring questions. It is even better recognized when selecting `nearest_question_dist` for coloring.\n![Similarity map of Formula One documents and questions \\(highlighted\\) - created by the author with Renumics Spotlight](https://towardsdatascience.com/wp-content/uploads/2024/03/1cMGNPnnBa9Bn7BJ05SzxBw.png)Similarity map of Formula One documents and questions (highlighted) \u2013 created by the author with [Renumics Spotlight](https://github.com/Renumics/spotlight)\nSome clusters can be identified, including snippets that contain only headings or tabular data containing only numbers page by page, whose meaning is lost during splitting. Additionally, many Wikipedia-specific text additions that contain no relevant information, such as links to other languages or editing notes, form clusters with no neighboring questions.\nRemoving the noise in form of Wikipedia-related text is very simple when using the Wikipedia API. It is probably not particularly necessary, as it mainly costs some space \u2013 it is not expected that the RAG result will be particularly worsened by it. However, data contained in large tables are hardly captured by the RAG system and it could ne benifical to extract these using advanced pre-processing methods for Table Extraction and to connect them to the RAG system.\nAnother point that you can observe in the `umap_docs` similarity map is how the questions from different sources are distributed.\n![](https://towardsdatascience.com/wp-content/uploads/2024/03/1IH7z3J4yUmU0C_SruxnDkg.png) ![Left: Questions generated from ChatGPT \\(GPT-3.5 and GPT-4\\), Right: Questions generated with ragas using GPT-3.5 and GPT-4 - created by the author with Renumics Spotlight](https://towardsdatascience.com/wp-content/uploads/2024/03/1K4bADgDmSAr5t4t4r9VImQ.png)Left: Questions generated from ChatGPT (GPT-3.5 and GPT-4), Right: Questions generated with ragas using GPT-3.5 and GPT-4 \u2013 created by the author with [Renumics Spotlight](https://github.com/Renumics/spotlight)\nThe questions that were directly generated by ChatGPT (GPT-3.5, GPT-4) are located in a more confined area in the center, whereas the questions generated with ragas based on the documents cover a larger area.\n### Answer correctness histogram\nThe histogram can be used as a starting point to get an initial impression of the global statistics of the data. Overall, across all questions, the `answer correctness` is 0.45. For the questions created without ragas, it is 0.36, and for questions with ragas, it is 0.52. It was expected that the system would perform better for questions generated by ragas, as these questions are based on the available data, whereas the questions directly generated by ChatGPT could come from all the data with which ChatGPT was trained.\n![Histogram of the answer correctness colored by the source of the question - created by the author](https://towardsdatascience.com/wp-content/uploads/2024/03/1GsLBsg7uwTrw-AzvO4BHmw.png)Histogram of the answer correctness colored by the source of the question \u2013 created by the author\nA quick, random manual review of some of the questions/answers and ground truth shows that in the interval of`answer correctness`0.3\u20130.4, most questions were still correctly answered according to the ground truth. In the interval 0.2\u20130.3, many incorrect answers are present. In the interval 0.1\u20130.2, most answers are incorrect. Notably, almost all questions in this range came from GPT-3.5. The two questions in this interval generated with GPT-4 were answered correctly even though they received an `answer correctness` of below 0.2.\n### Questions Embedding Similarity Map: Observations\nThe Questions Embedding Similarity Map can be helpful to dig deeper into `answer correctness` by examining clusters of similar questions that may cause similar problems.\n  * **Cluster \"Term for driver/process/cars\":** average `answer correctness` 0.23: Answers often not precise enough. E.g., Chassis tuning vs. Chassis flexing or brake tuning vs. brake bias adjustment. It is questionable whether these types of questions are suitable for evaluating the system, as it seems very difficult to judge the answers.\n  * **Cluster \"Terms for fuel strategy:\"** average `answer correctness`0.44, similar to the global`answer correctness`.\n  * **Cluster \"Names of tracks\":** average `answer correctness` 0.49, similar to the global `answer correctnes`.\n  * **Cluster \"Who holds the record for\u2026\"** : average `answer correctness` 0.44, similar to the global `answer correctness`.\n  * **Cluster \"Win championship with\u2026\"** : average `answer correctnes` 0.26 \u2013 looks challenging. Questions with many conditions, e.g., \"Who is the only driver to win the Formula One World Championship with a British racing license, driving for an Italian team with an American engine.\" Extended RAG methods like Multi Query might help improve here.\n  * **Cluster \"Who is the only driver to win\u2026 with a car bearing the number \"** : average `answer correctness` 0.23 \u2013 looks like GPT-3.5 was lazy here, repeating the same question with different numbers, even though most ground truth entries are wrong!\n\n![Similarity map of Formula One questions \\(highlighted\\) and documents - created by the author](https://towardsdatascience.com/wp-content/uploads/2024/03/1Yc03cpSEFlJoZSBPIpMkiQ.png)Similarity map of Formula One questions (highlighted) and documents \u2013 created by the author\n## Conclusion\nIn conclusion, utilizing UMAP-based visualizations offers a interesting approach to dig deeper than just analyzing global metrics. The document embedding similarity map gives a good overview, illustrating the clustering of similar documents and their relation to evaluation questions. The question similarity map reveals patterns that allow the differentiation and analysis of questions in conjunction with quality metrics to enable insight generation. Follow the Visualize results section to apply the visualization on your evaluation strategy \u2013 what insights will you uncover?\n_I am a professional with expertise in creating advanced software solutions for the interactive exploration of unstructured data. I write about unstructured data and use powerful visualization tools to analyze and make informed decisions._\n## References\n[1] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang: [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997) (2024), arxiv\n[2] Yixuan Tang, Yi Yang: [MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries](https://arxiv.org/abs/2401.15391) (2021), arXiv\n[3] Leland McInnes, John Healy, James Melville: [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426) (2018), arXiv\n[4] Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schockaert: [RAGAS: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217) (2023), arXiv\nWritten By\nMarkus Stoll\n[See all from Markus Stoll](https://towardsdatascience.com/author/markus-stoll/)\nTopics:\n[ChatGPT](https://towardsdatascience.com/tag/chatgpt/), [Data Science](https://towardsdatascience.com/tag/data-science/), [Editors Pick](https://towardsdatascience.com/tag/editors-pick/), [Large Language Models](https://towardsdatascience.com/tag/large-language-models/), [Machine Learning](https://towardsdatascience.com/tag/machine-learning/)\nShare this article:\n  * [ Share on Facebook  ](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557%2F&title=Visualize%20your%20RAG%20Data%20%E2%80%93%20Evaluate%20your%20Retrieval-Augmented%20Generation%20System%20with%20Ragas)\n  * [ Share on LinkedIn  ](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557%2F&title=Visualize%20your%20RAG%20Data%20%E2%80%93%20Evaluate%20your%20Retrieval-Augmented%20Generation%20System%20with%20Ragas)\n  * [ Share on X  ](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fvisualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557%2F&text=Visualize%20your%20RAG%20Data%20%E2%80%93%20Evaluate%20your%20Retrieval-Augmented%20Generation%20System%20with%20Ragas)\n\n\n## Related Articles\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/0c09RmbCCpfjAbSMq.png)\n## [Implementing Convolutional Neural Networks in TensorFlow](https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nStep-by-step code guide to building a Convolutional Neural Network \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nAugust 20, 2024\n6 min read\n  * ## [What Do Large Language Models \u201cUnderstand\u201d?](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA deep dive on the meaning of understanding and how it applies to LLMs \n[Tarik Dzekman](https://towardsdatascience.com/author/tarikdzekman/)\nAugust 21, 2024\n31 min read\n  * ![Photo by Krista Mangulsone on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/0GyVVTbgotH-DhGPH-scaled.jpg)\n## [How to Forecast Hierarchical Time Series](https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA beginner\u2019s guide to forecast reconciliation \n[Dr. Robert K\u00fcbler](https://towardsdatascience.com/author/dr-robert-kuebler/)\nAugust 20, 2024\n13 min read\n  * ![Photo by davisuko on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/1bAABgtZtAIG5YW1oEjW3pA-scaled.jpeg)\n## [Hands-on Time Series Anomaly Detection using Autoencoders, with Python](https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nHere\u2019s how to use Autoencoders to detect signals with anomalies in a few lines of\u2026 \n[Piero Paialunga](https://towardsdatascience.com/author/piero-paialunga/)\nAugust 21, 2024\n12 min read\n  * ![Image from Canva.](https://towardsdatascience.com/wp-content/uploads/2024/08/1UAA9jQVdqMXnwzYiz8Q53Q.png)\n## [3 AI Use Cases (That Are Not a Chatbot)](https://towardsdatascience.com/3-ai-use-cases-that-are-not-a-chatbot-f4f328a2707a/)\n[ Machine Learning ](https://towardsdatascience.com/category/artificial-intelligence/machine-learning/)\nFeature engineering, structuring unstructured data, and lead scoring \n[Shaw Talebi](https://towardsdatascience.com/author/shawhin/)\nAugust 21, 2024\n7 min read\n  * ## [Solving a Constrained Project Scheduling Problem with Quantum Annealing](https://towardsdatascience.com/solving-a-constrained-project-scheduling-problem-with-quantum-annealing-d0640e657a3b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nSolving the resource constrained project scheduling problem (RCPSP) with D-Wave\u2019s hybrid constrained quadratic model (CQM) \n[Luis Fernando P\u00c9REZ ARMAS, Ph.D.](https://towardsdatascience.com/author/luisfernandopa1212/)\nAugust 20, 2024\n29 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2023/02/1VEUgT5T4absnTqBMOEuNig.png)\n## [Back To Basics, Part Uno: Linear Regression and Cost Function](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nAn illustrated guide on essential machine learning concepts \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nFebruary 3, 2023\n6 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/1kM8tfYcdaoccB1HX71YDig.png)\n## [Must-Know in Statistics: The Bivariate Normal Projection Explained](https://towardsdatascience.com/must-know-in-statistics-the-bivariate-normal-projection-explained-ace7b2f70b5b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nDerivation and practical examples of this powerful concept \n[Luigi Battistoni](https://towardsdatascience.com/author/lu-battistoni/)\nAugust 14, 2024\n7 min read\n  * ![Photo by Jess Bailey on Unsplash](https://towardsdatascience.com/wp-content/uploads/2022/09/11tHmNYFaWWtWG5I7bNeN6g-scaled.jpeg)\n## [How to Make the Most of Your Experience as a TDS Author](https://towardsdatascience.com/how-to-make-the-most-of-your-experience-as-a-tds-author-b1e056be63f1/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nA quick guide to our resources and FAQ \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nSeptember 13, 2022\n4 min read\n\n\n  * [YouTube](https://www.youtube.com/c/TowardsDataScience)\n  * [X](https://x.com/TDataScience)\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [Threads](https://www.threads.net/@towardsdatascience)\n  * [Bluesky](https://bsky.app/profile/towardsdatascience.com)\n\n\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nYour home for data science and Al. The world\u2019s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\n\u00a9  Insight Media Group, LLC 2025 \n  * [About](https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/)\n  * [Privacy Policy](https://towardsdatascience.com/privacy-policy/)\n  * [Terms of Use](https://towardsdatascience.com/website-terms-of-use/)\n\n\n[Towards Data Science is now independent!](https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/)\nCookies Settings\n## Sign up to our newsletter\nEmail address*\nFirst name*\nLast name*\nJob title*\nJob level*\nPlease SelectC-LevelVP/DirectorManager/SupervisorMid Level or Senior Non-Managerial StaffEntry Level/Junior StaffFreelancer/ContractorStudent/InternOther\nCompany name*\n  * I consent to receive newsletters and other communications from Towards Data Science publications.*\n\n\n![Company Logo](https://cdn.cookielaw.org/logos/static/ot_company_logo.png)\n## Privacy Preference Center\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer. [More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nAllow All\n###  Manage Consent Preferences\n#### Functional Cookies\nFunctional Cookies Active\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\n#### Strictly Necessary Cookies\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n#### Performance Cookies\nPerformance Cookies Active\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n#### Targeting Cookies\nTargeting Cookies Active\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nBack Button\n### Cookie List\nSearch Icon\nFilter Icon\nClear\ncheckbox label label\nApply Cancel\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nReject All Confirm My Choices\n[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-consent/)\nSome areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis article discusses evaluating Retrieval-Augmented Generation (RAG) systems using UMAP for visualizing embeddings. It highlights the use of Ragas for generating questions and assessing answer correctness, revealing insights into the performance of different question generation methods, particularly those using Ragas outperforming ChatGPT-based ones.\n```"
  },
  {
    "instruction": "[![Ahead of AI](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)](https://magazine.sebastianraschka.com/)\n# [![Ahead of AI](https://substackcdn.com/image/fetch/e_trim:10:white/e_trim:10:transparent/h_72,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe553b9ca-3e8c-442b-969a-22556b86d1e5_448x212.png)](https://magazine.sebastianraschka.com/)\nSubscribeSign in\n#### Share this post\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1655aff8-c9bd-4a93-9e72-f1911359a667_1646x1090.jpeg)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Encoder And Decoder LLMs](https://substack.com/home/post/p-128842439?utm_campaign=post&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n![](https://substackcdn.com/image/fetch/w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg)\nDiscover more from Ahead of AI\nAhead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\nOver 95,000 subscribers\nSubscribe\nBy subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).\nAlready have an account? Sign in\n# Understanding Encoder And Decoder LLMs\n[![](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg)](https://substack.com/@rasbt)\n[Sebastian Raschka, PhD](https://substack.com/@rasbt)\nJun 17, 2023\n163\n#### Share this post\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1655aff8-c9bd-4a93-9e72-f1911359a667_1646x1090.jpeg)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Encoder And Decoder LLMs](https://substack.com/home/post/p-128842439?utm_campaign=post&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n[5](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comments)10\n[Share](javascript:void\\(0\\))\nSeveral people asked me to dive a bit deeper into large language model (LLM) jargon and explain some of the more technical terms we nowadays take for granted. This includes references to \"encoder-style\" and \"decoder-style\" LLMs. What do these terms mean? \nLet's get to it:  _**What are the differences between encoder- and decoder-based language transformers?**_\n## **Encoder- And Decoder-Style Transformers**\nFundamentally, both encoder- and decoder-style architectures use the same self-attention layers to encode word tokens. However, the main difference is that encoders are designed to learn embeddings that can be used for various predictive modeling tasks such as classification. In contrast, decoders are designed to generate new texts, for example, answering user queries.\n**The original transformer**\nThe original transformer architecture (_[Attention Is All You Need, 2017](https://arxiv.org/abs/1706.03762)_), which was developed for English-to-French and English-to-German language translation, utilized both an encoder and a decoder, as illustrated in the figure below.\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png)Illustration of the original transformer architecture proposed in [Attention Is All You Need, 2017](https://arxiv.org/abs/1706.03762)\nIn the figure above, the input text (that is, the sentences of the text that is to be translated) is first tokenized into individual word tokens, which are then encoded via an embedding layer before it enters the encoder part. Then, after adding a positional encoding vector to each embedded word, the embeddings go through a multi-head self-attention layer. The multi-head attention layer is followed by an \"Add & normalize\" step, which performs a layer normalization and adds the original embeddings via a skip connection (also known as a residual or shortcut connection). Finally, after entering a \"fully connected layer,\" which is a small multilayer perceptron consisting of two fully connected layers with a nonlinear activation function in between, the outputs are again added and normalized before they are passed to a multi-head self-attention layer of the decoder part.\nThe decoder part in the figure above has a similar overall structure as the encoder part. The key difference is that the inputs and outputs are different. The encoder receives the input text that is to be translated, and the decoder generates the translated text.\n**Encoders**\nThe encoder part in the original transformer, illustrated in the preceding figure, is responsible for understanding and extracting the relevant information from the input text. It then outputs a continuous representation (embedding) of the input text that is passed to the decoder. Finally, the decoder generates the translated text (target language) based on the continuous representation received from the encoder.\nOver the years, various encoder-only architectures have been developed based on the encoder module of the original transformer model outlined above. Notable examples include BERT (_[Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018](https://arxiv.org/abs/1810.04805)_) and RoBERTa (_[A Robustly Optimized BERT Pretraining Approach, 2018](https://arxiv.org/abs/1907.11692)_).\nBERT (**B** idirectional **E** ncoder **R** epresentations from **T** ransformers) is an encoder-only architecture based on the Transformer's encoder module. The BERT model is pretrained on a large text corpus using masked language modeling (illustrated in the figure below) and next-sentence prediction tasks.\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8fc5971-d59b-4d5a-bb2a-d8ac6d2aa23a_2022x1224.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8fc5971-d59b-4d5a-bb2a-d8ac6d2aa23a_2022x1224.png)Illustration of the masked language modeling pretraining objective used in BERT-style transformers.\nThe main idea behind masked language modeling is to mask (or replace) random word tokens in the input sequence and then train the model to predict the original masked tokens based on the surrounding context.\nNext to the masked language modeling pretraining task illustrated in the figure above, the next-sentence prediction task asks the model to predict whether the original document's sentence order of two randomly shuffled sentences is correct. For example, two sentences, in random order, are separated by the [SEP] token:\n  * [CLS] Toast is a simple yet delicious food [SEP] It\u2019s often served with butter, jam, or honey.\n  * [CLS] It\u2019s often served with butter, jam, or honey. [SEP] Toast is a simple yet delicious food.\n\n\nThe [CLS] token is a placeholder token for the model, prompting the model to return a _True_ or _False_ label indicating whether the sentences are in the correct order or not.\nThe masked language and next-sentence pretraining objectives (which are a form of self-supervised learning, as discussed in Chapter 2) allow BERT to learn rich contextual representations of the input texts, which can then be finetuned for various downstream tasks like sentiment analysis, question-answering, and named entity recognition.\nRoBERTa (**R** obustly **o** ptimized **BERT****a** pproach) is an optimized version of BERT. It maintains the same overall architecture as BERT but employs several training and optimization improvements, such as larger batch sizes, more training data, and eliminating the next-sentence prediction task. These changes resulted in RoBERTa achieving better performance on various natural language understanding tasks than BERT.\n**Decoders**\nComing back to the original transformer architecture outlined at the beginning of this section, the multi-head self-attention mechanism in the decoder is similar to the one in the encoder, but it is masked to prevent the model from attending to future positions, ensuring that the predictions for position _i_ can depend only on the known outputs at positions less than _i_. As illustrated in the figure below, the decoder generates the output word by word.\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c55ad5-cc86-4421-b0bc-bbd967409ca6_1162x1022.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c55ad5-cc86-4421-b0bc-bbd967409ca6_1162x1022.png)Illustration of the next-sentence prediction task used in the original transformer.\nThis masking (shown explicitly in the figure above, although it happens internally in the decoder's multi-head self-attention mechanism) is essential to maintain the autoregressive property of the transformer model during training and inference. The autoregressive property ensures that the model generates output tokens one at a time and uses previously generated tokens as context for generating the next word token.\nOver the years, researchers have built upon the original encoder-decoder transformer architecture and developed several decoder-only models that have proven to be highly effective in various natural language processing tasks. The most notable models include the GPT family.\nThe GPT (**G** enerative **P** re-trained **T** ransformer) series are decoder-only models pretrained on large-scale unsupervised text data and finetuned for specific tasks such as text classification, sentiment analysis, question-answering, and summarization. The GPT models, including GPT-2, (_[GPT-3 Language Models are Few-Shot Learners, 2020](https://arxiv.org/abs/2005.14165)_), and the more recent GPT-4, have shown remarkable performance in various benchmarks and are currently the most popular architecture for natural language processing.\nOne of the most notable aspects of GPT models is their emergent properties. Emergent properties refer to the abilities and skills that a model develops due to its next-word prediction pretraining. Even though these models were only taught to predict the next word, the pretrained models are capable of text summarization, translation, question answering, classification, and more. Furthermore, these models can perform new tasks without updating the model parameters via in-context learning, which is discussed in more detail in Chapter 18.\n**Encoder-decoder hybrids**\nNext to the traditional encoder and decoder architectures, there have been advancements in the development of new encoder-decoder models that leverage the strengths of both components. These models often incorporate novel techniques, pre-training objectives, or architectural modifications to enhance their performance in various natural language processing tasks. Some notable examples of these new encoder-decoder models include \n  * BART (_[Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension, 2019](https://arxiv.org/abs/1910.13461)_) \n  * and T5 (_[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, 2019](https://arxiv.org/abs/1910.10683)_).\n\n\nEncoder-decoder models are typically used for natural language processing tasks that involve understanding input sequences and generating output sequences, often with different lengths and structures. They are particularly good at tasks where there is a complex mapping between the input and output sequences and where it is crucial to capture the relationships between the elements in both sequences. Some common use cases for encoder-decoder models include text translation and summarization.\n**Terminology and jargon**\nAll of these methods, encoder-only, decoder-only, and encoder-decoder models, are sequence-to-sequence models (often abbreviated as _seq2seq_). Note that while we refer to BERT-style methods as encoder-only, the description _encoder-only_ may be misleading since these methods also _decode_ the embeddings into output tokens or text during pretraining.\nIn other words, both encoder-only and decoder-only architectures are \"decoding.\" However, the encoder-only architectures, in contrast to decoder-only and encoder-decoder architectures, are not decoding in an autoregressive fashion. Autoregressive decoding refers to generating output sequences one token at a time, conditioning each token on the previously generated tokens. Encoder-only models do not generate coherent output sequences in this manner. Instead, they focus on understanding the input text and producing task-specific outputs, such as labels or token predictions.\n**Conclusion**\nIn brief, encoder-style models are popular for learning embeddings used in classification tasks, encoder-decoder-style models are used in generative tasks where the output heavily relies on the input (for example, translation and summarization), and decoder-only models are used for other types of generative tasks including Q&A. Since the first transformer architecture emerged, hundreds of encoder-only, decoder-only, and encoder-decoder hybrids have been developed, as summarized in the figure below.\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcce3c437-4b9c-4d15-947d-7c177c9518e5_4258x5745.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcce3c437-4b9c-4d15-947d-7c177c9518e5_4258x5745.png)An overview of _**some**_ of the most popular large language transformers organized by architecture type and developers.\nWhile encoder-only models gradually lost in popularity, decoder-only models like GPT exploded in popularity thanks to breakthrough in text generation via GPT-3, ChatGPT, and GPT-4. However, encoder-only models are still very useful for training predictive models based on text embeddings versus generating texts.\n_This magazine is a personal passion project that does not offer direct compensation. However, for those who wish to support me, please consider purchasing a copy of[one of my books](https://sebastianraschka.com/books). If you find them insightful and beneficial, please feel free to recommend them to your friends and colleagues._\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27b6d27d-1052-4a4f-a419-a69cdd36704a_1118x454.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27b6d27d-1052-4a4f-a419-a69cdd36704a_1118x454.png)[Machine Learning with PyTorch and Scikit-Learn](https://www.amazon.com/Machine-Learning-PyTorch-Scikit-Learn-scikit-learn-ebook-dp-B09NW48MR1/dp/B09NW48MR1/), [Machine Learning Q and AI](https://nostarch.com/machine-learning-and-ai-beyond-basics), and [Build a Large Language Model (from Scratch)](http://mng.bz/M96o)\n**Your support means a great deal! Thank you!**\n#### Subscribe to Ahead of AI\nBy Sebastian Raschka \u00b7 Hundreds of paid subscribers\nAhead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\nSubscribe\nBy subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64769218-5ab1-4442-b194-623564737424_1080x1080.jpeg)](https://substack.com/profile/96251662-kak)\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F55c2183f-7755-4692-8063-59df2751f834_800x800.png)](https://substack.com/profile/85853406-sairam-sundaresan)\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc59205ed-f7b0-4c31-8810-1097c56a778f_1200x1600.jpeg)](https://substack.com/profile/1134131-sid-kapur)\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc95711cf-0604-4af6-a3e0-47be3b58a2dd_1364x1364.jpeg)](https://substack.com/profile/147444258-lukasz-ostrowski)\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa635f57b-3633-43d2-bc80-950afcaa42cb_144x144.png)](https://substack.com/profile/108287441-gabriel)\n163 Likes\u2219\n[10 Restacks](https://substack.com/note/p-128842439/restacks?utm_source=substack&utm_content=facepile-restacks)\n163\n#### Share this post\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1655aff8-c9bd-4a93-9e72-f1911359a667_1646x1090.jpeg)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Encoder And Decoder LLMs](https://substack.com/home/post/p-128842439?utm_campaign=post&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n[5](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comments)10\n[Share](javascript:void\\(0\\))\n#### Discussion about this post\nCommentsRestacks\n![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png)\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4daf1168-7d63-41f1-a70a-972a9c563d33_144x144.png)](https://substack.com/profile/11904497-richard-hackathorn?utm_source=comment)\n[Richard Hackathorn](https://substack.com/profile/11904497-richard-hackathorn?utm_source=substack-feed-item)\n[Dec 31, 2023](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comment/46346150 \"Dec 31, 2023, 12:06 PM\")Edited\nLiked by Sebastian Raschka, PhD\nThis LLM encoder/decoder stuff messes with my mind! There is something fundamental here that I'm not getting. HELP... \ud83e\udd14 I have been fascinated with autoencoders, which take an example from feature space and ENCODE it into point in latent space and then DECODE it back into a reconstructed example in feature space, thus allowing a reconstruction loss to be calculated. [ref: Python ML 3Ed, Chap 17] \n1) Should LLM decoders be called 'generators' like in GANs? \n2) That single line that connects LLM encoder to its decoder... Is that the same data that one receives as an embedding from the LLM API? \n3) For a decoder-only LLM, is its input always an embedding vector? Or, where do the model weights come from?\n4) Is it possible to take an LLM embedding, reconstruct its initial input, and calculate the reconstruction loss? If true, this would enable us to map the fine (manifold) structures in these mysterious LLM latent spaces. Loved your old examples of putting/removing smiles on celebrity faces. Like to find a few hallucinations lurking in LLM latent spaces! \ud83d\ude2e\nExpand full comment\n[Like (2)](javascript:void\\(0\\))ReplyShare\n[1 reply by Sebastian Raschka, PhD](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comment/46346150)\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce196789-7b5a-436a-bd65-1078805f1ee6_420x420.png)](https://substack.com/profile/185804816-viswa-kumar?utm_source=comment)\n[Viswa Kumar](https://substack.com/profile/185804816-viswa-kumar?utm_source=substack-feed-item)\n[Nov 7](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comment/76035897 \"Nov 7, 2024, 10:48 AM\")\nLiked by Sebastian Raschka, PhD\nI agree the term encoder / decoder is overloaded since almost all architectures would essentially perform the encoding / decoding as a function. Engineers are not good at naming not only vars after all \ud83e\udd23\nExpand full comment\n[Like (1)](javascript:void\\(0\\))ReplyShare\n[3 more comments...](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comments)\nTopLatestDiscussions\n[Understanding Reasoning LLMs](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)\n[Methods and Strategies for Building and Refining Reasoning Models](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)\nFeb 5 \u2022\n[Sebastian Raschka, PhD](https://substack.com/@rasbt)\n833\n#### Share this post\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Reasoning LLMs](https://substack.com/home/post/p-156484949?utm_campaign=post&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n[31](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms/comments)[](javascript:void\\(0\\))\n![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png)\n[Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models)\n[A Cross-Section of the Most Relevant Literature To Get Up to Speed](https://magazine.sebastianraschka.com/p/understanding-large-language-models)\nApr 16, 2023 \u2022\n[Sebastian Raschka, PhD](https://substack.com/@rasbt)\n886\n#### Share this post\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Large Language Models](https://substack.com/home/post/p-115060492?utm_campaign=post&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n[54](https://magazine.sebastianraschka.com/p/understanding-large-language-models/comments)[](javascript:void\\(0\\))\n![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png)\n[Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)\n[This article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama.](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)\nJan 14, 2024\n341\n#### Share this post\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69bfee26-ea3b-42a6-8a1a-6b8187852082_738x564.png)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs](https://substack.com/home/post/p-140464659?utm_campaign=post&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n[41](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention/comments)[](javascript:void\\(0\\))\n![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69bfee26-ea3b-42a6-8a1a-6b8187852082_738x564.png)\nSee all\nReady for more?\nSubscribe\n\u00a9 2025 Sebastian Raschka\n[Privacy](https://substack.com/privacy) \u2219 [Terms](https://substack.com/tos) \u2219 [Collection notice](https://substack.com/ccpa#personal-data-collected)\n[ Start Writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)\n[Substack](https://substack.com) is the home for great culture\n#### Share\n[](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder?utm_campaign=unknown&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n## Create your profile\nName (Required)HandleBioEmail (Required)\nSubscribe to the newsletter\nundefined subscriptions will be displayed on your profile (edit)\nSkip for now\nI agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).\nSave & Post Comment\n## Only paid subscribers can comment on this post\n[Subscribe](https://magazine.sebastianraschka.com/subscribe?simple=true&next=https%3A%2F%2Fmagazine.sebastianraschka.com%2Fp%2Funderstanding-encoder-and-decoder&utm_source=paywall&utm_medium=web&utm_content=128842439)\n[Already a paid subscriber? **Sign in**](https://substack.com/sign-in?redirect=%2Fp%2Funderstanding-encoder-and-decoder&for_pub=sebastianraschka&change_user=false)\n#### Check your email\nFor your security, we need to re-authenticate you.\nClick the link we sent to , or [click here to sign in](https://substack.com/sign-in?redirect=%2Fp%2Funderstanding-encoder-and-decoder&for_pub=sebastianraschka&with_password=true).\n",
    "answer": "```markdown\n# TL;DR: Understanding Encoder And Decoder LLMs\n\nThe article explains the differences between encoder and decoder architectures in large language models (LLMs). Encoders focus on learning embeddings for tasks like classification (e.g., BERT), while decoders generate text (e.g., GPT). Encoder-decoder hybrids like BART and T5 leverage both for tasks like translation and summarization. The evolution of these models reflects their applications in natural language processing.\n```"
  },
  {
    "instruction": "[Skip to main content](https://arxiv.org/abs/2203.02155/#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/) > [cs](https://arxiv.org/list/cs/recent) > arXiv:2203.02155 \n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[ ![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg) ](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n## quick links\n  * [Login](https://arxiv.org/login)\n  * [Help Pages](https://info.arxiv.org/help)\n  * [About](https://info.arxiv.org/about)\n\n\n# Computer Science > Computation and Language\n**arXiv:2203.02155** (cs) \n[Submitted on 4 Mar 2022]\n# Title:Training language models to follow instructions with human feedback\nAuthors:[Long Ouyang](https://arxiv.org/search/cs?searchtype=author&query=Ouyang,+L), [Jeff Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+J), [Xu Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+X), [Diogo Almeida](https://arxiv.org/search/cs?searchtype=author&query=Almeida,+D), [Carroll L. Wainwright](https://arxiv.org/search/cs?searchtype=author&query=Wainwright,+C+L), [Pamela Mishkin](https://arxiv.org/search/cs?searchtype=author&query=Mishkin,+P), [Chong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+C), [Sandhini Agarwal](https://arxiv.org/search/cs?searchtype=author&query=Agarwal,+S), [Katarina Slama](https://arxiv.org/search/cs?searchtype=author&query=Slama,+K), [Alex Ray](https://arxiv.org/search/cs?searchtype=author&query=Ray,+A), [John Schulman](https://arxiv.org/search/cs?searchtype=author&query=Schulman,+J), [Jacob Hilton](https://arxiv.org/search/cs?searchtype=author&query=Hilton,+J), [Fraser Kelton](https://arxiv.org/search/cs?searchtype=author&query=Kelton,+F), [Luke Miller](https://arxiv.org/search/cs?searchtype=author&query=Miller,+L), [Maddie Simens](https://arxiv.org/search/cs?searchtype=author&query=Simens,+M), [Amanda Askell](https://arxiv.org/search/cs?searchtype=author&query=Askell,+A), [Peter Welinder](https://arxiv.org/search/cs?searchtype=author&query=Welinder,+P), [Paul Christiano](https://arxiv.org/search/cs?searchtype=author&query=Christiano,+P), [Jan Leike](https://arxiv.org/search/cs?searchtype=author&query=Leike,+J), [Ryan Lowe](https://arxiv.org/search/cs?searchtype=author&query=Lowe,+R)\nView a PDF of the paper titled Training language models to follow instructions with human feedback, by Long Ouyang and 19 other authors\n[View PDF](https://arxiv.org/pdf/2203.02155)\n> Abstract:Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent. \nSubjects: |  Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)  \n---|---  \nCite as: | [arXiv:2203.02155](https://arxiv.org/abs/2203.02155) [cs.CL]  \n(or  [arXiv:2203.02155v1](https://arxiv.org/abs/2203.02155v1) [cs.CL] for this version)   \n<https://doi.org/10.48550/arXiv.2203.02155> Focus to learn more arXiv-issued DOI via DataCite  \n## Submission history\nFrom: Jan Leike [[view email](https://arxiv.org/show-email/f3b1678a/2203.02155)] **[v1]** Fri, 4 Mar 2022 07:04:42 UTC (1,047 KB) \nFull-text links:\n## Access Paper:\nView a PDF of the paper titled Training language models to follow instructions with human feedback, by Long Ouyang and 19 other authors\n  * [View PDF](https://arxiv.org/pdf/2203.02155)\n  * [TeX Source](https://arxiv.org/src/2203.02155)\n  * [Other Formats](https://arxiv.org/format/2203.02155)\n\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \"Rights to this article\")\nCurrent browse context: \ncs.CL\n[< prev](https://arxiv.org/prevnext?id=2203.02155&function=prev&context=cs.CL \"previous in cs.CL \\(accesskey p\\)\") |  [next >](https://arxiv.org/prevnext?id=2203.02155&function=next&context=cs.CL \"next in cs.CL \\(accesskey n\\)\")\n[new](https://arxiv.org/list/cs.CL/new) |  [recent](https://arxiv.org/list/cs.CL/recent) | [2022-03](https://arxiv.org/list/cs.CL/2022-03)\nChange to browse by: \n[cs](https://arxiv.org/abs/2203.02155?context=cs) [cs.AI](https://arxiv.org/abs/2203.02155?context=cs.AI) [cs.LG](https://arxiv.org/abs/2203.02155?context=cs.LG)\n### References & Citations\n  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2203.02155)\n  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2203.02155)\n  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:2203.02155)\n\n\n### [ 13 blog links](https://arxiv.org/tb/2203.02155)\n([what is this?](https://info.arxiv.org/help/trackback.html)) \n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citation Loading...\n## BibTeX formatted citation\n\u00d7\nloading...\nData provided by: \n### Bookmark\n[ ![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png) ](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2203.02155&description=Training language models to follow instructions with human feedback \"Bookmark on BibSonomy\") [ ![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png) ](https://reddit.com/submit?url=https://arxiv.org/abs/2203.02155&title=Training language models to follow instructions with human feedback \"Bookmark on Reddit\")\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\nConnected Papers Toggle\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\nLitmaps Toggle\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\nscite.ai Toggle\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_\nDagsHub Toggle\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\nGotitPub Toggle\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\nHuggingface Toggle\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\nLinks to Code Toggle\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\nScienceCast Toggle\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\nDemos\n# Demos\nReplicate Toggle\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\nSpaces Toggle\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\nSpaces Toggle\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\nCore recommender toggle\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n\nAbout arXivLabs \n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2203.02155) | [Disable MathJax](javascript:setMathjaxCookie\\(\\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n  * [About](https://info.arxiv.org/about)\n  * [Help](https://info.arxiv.org/help)\n\n\n  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)\n  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)\n\n\n  * [Copyright](https://info.arxiv.org/help/license/index.html)\n  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n\n  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n  * [arXiv Operational Status ](https://status.arxiv.org) Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n\n\n",
    "answer": "```markdown\n# TL;DR: Training Language Models with Human Feedback\n\nThe paper presents InstructGPT, a model fine-tuned with human feedback to better align with user intent. It shows that a smaller model (1.3B parameters) outperforms a larger one (175B parameters) in truthfulness and reduced toxicity, demonstrating the effectiveness of human feedback in training language models.\n\n**Tags:** Generative AI, LLMs, Human Feedback, InstructGPT\n```"
  },
  {
    "instruction": "[![VectorHub by Superlinked](https://superlinked.com/vectorhub/_next/static/media/vectorhub-logo.d71e47bd.svg)](https://superlinked.com/vectorhub/)\n[Building Blocks](https://superlinked.com/vectorhub/building-blocks)[Articles](https://superlinked.com/vectorhub/all-articles)[Contributing](https://superlinked.com/vectorhub/contributing/contributing)[VDB Comparison](https://superlinked.com/vector-db-comparison/)\n[---](https://github.com/superlinked/superlinked)\nSearch\nSubscribe\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nMenu\nImprove your vector search[![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow-primary.2ff1db1a.svg)Try Superlinked](https://github.com/superlinked/superlinked)\nHome\nManifesto\nMost Recent\nCompare Vector DBs\nContributeSubscribe\nSearch\nTable of Contents\nUnderstanding the different Data Modality / Types\nUnstructured Data\nMulti-Task Evaluation datasets\nStructured Data\nKeeping your retrieval stack up to date with Change Data Capture\nContributors\n  1. [Building blocks](https://superlinked.com/vectorhub/building-blocks)\n/\n  2. [Data sources](https://superlinked.com/vectorhub/building-blocks)\n/\n  3. [Data modality](https://superlinked.com/vectorhub/12-data-modality)\n\n\nPublication Date: November 27, 2023|\n#Data Sources\n|Update on Github\n# Data Modality\n## Understanding the different Data Modality / Types\nWhether your data is structured, unstructured, or hybrid is crucial when evaluating data sources. The nature of the data source your vector retrieval system uses shapes how that data should be managed and processed.\n### Unstructured Data\nUnstructured data encompasses a wide variety of information that doesn't adhere to a fixed structure or definition. This data type is often characterized by its raw, unordered, and noisy nature. Examples of unstructured data include natural language text, image, audio, and video data. Let's take a closer look at each type:\n**Text Data**\n  * **Example Data:** Social media posts, news articles, chat transcripts, product reviews.\n  * **Typical Formats:** Plain text, JSON, XML, HTML, PDF, CSV (for tabular text data).\n  * **Datasets:**\n    * Kaggle: [Sentiment Analysis on Movie Reviews](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)\n    * Hugging Face: [Text Classification](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=trending)\n\n\n**Image Data**\n  * **Example Data:** Photographs, medical images, satellite imagery, generative AI-created images.\n  * **Typical Formats:** JPEG, PNG, TIFF.\n  * **Datasets:**\n    * Kaggle: [CIFAR-10 - Object Recognition in Images](https://www.kaggle.com/c/cifar-10)\n    * Github: [Unsplash 4.8M Photos, Keywords, Searches](https://github.com/unsplash/datasets)\n\n\n**Audio Data**\n  * **Example Data:** Speech recordings, music, environmental sounds.\n  * **Typical Formats:** WAV, MP3, FLAC.\n  * **Datasets:**\n    * Kaggle: [Urban Sound Classification](https://www.kaggle.com/datasets/chrisfilo/urbansound8k)\n    * Hugging Face: [Audio Classification](https://huggingface.co/datasets?task_categories=task_categories:audio-classification&sort=trending)\n\n\n**Video Data**\n  * **Example Data:** Movie clips, surveillance footage, video streams.\n  * **Typical Formats:** MP4, AVI, MOV.\n  * **Datasets:**\n    * Kaggle: [YouTube-8M Segments - Video Classification](https://www.kaggle.com/c/youtube8m)\n    * Hugging Face: [Video Classification](https://huggingface.co/datasets?task_categories=task_categories:video-classification&sort=trending)\n\n\n#### Multi-Task Evaluation datasets\nWhen building information retrieval systems, choosing the right dataset for evaluation is key. As a rule, you should always do this using your company\u2019s or product\u2019s own data.\nHowever, there are also instances when using a 3rd party dataset for evaluation is possible, preferable, or even the only option. Most commonly, this occurs when: Your own data quality or volume is insufficient for robust evaluation. You want to standardize your evaluation results by running your system against a standardized evaluation dataset available in the market.\nLet\u2019s look at a couple of evaluation dataset examples for language models:\nGLUE (General Language Understanding Evaluation) and SuperGLUE (Super General Language Understanding Evaluation) are multi-task benchmark datasets designed to assess model performance across various NLP tasks.\n**GLUE** (General Language Understanding Evaluation)\n  * **Description** : GLUE contains diverse NLP tasks like sentiment analysis, text classification, and textual entailment. These tasks rely on semi-structured text input-output pairs rather than completely free-form text.\n  * **Format** : JSON/CSV with text snippets and corresponding labels.\n  * [Hugging Face GLUE Datasets](https://huggingface.co/datasets/glue)\n\n\n**SuperGLUE** (Super General Language Understanding Evaluation)\n  * **Description** : SuperGLUE introduces more complex language tasks like question answering and coreference resolution, which are also based on semi-structured text.\n  * **Format** : JSON/CSV with text inputs and labels.\n  * [Hugging Face SuperGLUE Datasets](https://huggingface.co/datasets/super_glue)\n\n\nWhile GLUE and SuperGLUE are useful for benchmarking language models, it would be inaccurate to describe them solely as unstructured text datasets, since many tasks involve semi-structured input-output pairs.\n### Structured Data\nAll major enterprises today run on structured data. Structured data adheres to predefined formats, organized categories, and a fixed set of data fields, making it easier to start working with. But, beware, even structured datasets may be of poor quality, with many missing values or poor schema compliance.\nHere are some examples of structured data types, links to example datasets, typical formats, and considerations relevant to each type of structured data:\n**Tabular Data**\n  * **Example Data:** Sales records, customer information, financial statements.\n  * **Typical Formats:** CSV, Excel spreadsheets, SQL databases.\n  * **Datasets:**\n    * [Kaggle Datasets](https://www.kaggle.com/datasets): Kaggle offers a wide range of structured datasets covering various domains.\n    * [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php): UCI's repository provides many structured datasets for machine learning.\n  * **Considerations:** When working with tabular data, consider data quality, missing values, and the choice of columns or features relevant to your analysis. You may need to preprocess data and address issues such as normalization and encoding of categorical variables.\n  * **Systems:** Structured data often lives in relational database management systems (RDBMS) like MySQL, PostgreSQL, or cloud-based solutions like AWS RDS.\n\n\n**Graph Data**\n  * **Example Data:** Social networks, organizational hierarchies, knowledge graphs.\n  * **Typical Formats:** Graph databases (e.g., Neo4j), edge-list or adjacency matrix representation.\n  * **Datasets:**\n    * [Stanford Network Analysis Project (SNAP)](http://snap.stanford.edu/data/): Offers a collection of real-world network datasets.\n    * [KONECT](http://konect.cc/networks/): Provides a variety of network datasets for research.\n  * **Considerations:** In graph data, consider the types of nodes, edges, and their attributes. Pay attention to graph algorithms for traversing, analyzing, and extracting insights from the graph structure.\n  * **Systems:** Graph data is often stored in graph databases like [Neo4j](https://neo4j.com/), [ArangoDB](https://github.com/arangodb/arangodb), or [Apollo](https://www.apollographql.com/), but it can also be represented using traditional RDBMS with specific schemas for relations.\n\n\n**Time Series Data**\n  * **Example Data:** Stock prices, weather measurements, sensor data.\n  * **Typical Formats:** CSV, JSON, time-series databases (e.g., InfluxDB).\n  * **Datasets:**\n    * [Federal Reserve Economic Data (FRED)](https://fred.stlouisfed.org/): Covers economic and research data from various countries, including the USA, Germany, and Japan.\n    * [The Google Trends Dataset](https://trends.google.com/trends/?ref=hackernoon.com)\n  * **Considerations:** Time series data requires dealing with temporal aspects, seasonality, trends, and handling irregularities. It may involve time-based feature engineering and modeling techniques, like [ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) or other sequential models, like [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory).\n  * **Systems:** Time series data can be stored in specialized time-series databases (e.g., InfluxDB, TimescaleDB, [KX](https://kx.com/)) or traditional databases with timestamp columns.\n\n\n**Spatial Data**\n  * **Example Data:** Geographic information, maps, GPS coordinates.\n  * **Typical Formats:** Shapefiles (SHP), GeoJSON, GPS coordinates in CSV.\n  * **Datasets:**\n    * [Natural Earth Data](https://www.naturalearthdata.com/): Offers free vector and raster map data.\n    * [OpenStreetMap (OSM) Data](https://www.openstreetmap.org/): Provides geospatial data for mapping and navigation.\n  * **Considerations:** Spatial data often involves geographic analysis, mapping, and visualization. Understanding coordinate systems, geospatial libraries, and map projections is important.\n  * **Systems:** Spatial data can be stored in specialized Geographic Information Systems (GIS) or in databases with spatial extensions (e.g., PostGIS for PostgreSQL).\n\n\n**Logs Data**\n  * **Example Data:** Some examples of different logs include: system event logs that monitor traffic to an application, detect issues, and record errors causing a system to crash, or user behaviour logs, which track actions a user takes on a website or when signed into a device.\n  * **Typical Formats:** [CLF](https://en.wikipedia.org/wiki/Common_Log_Format) or a custom text or binary file that contains ordered (timestamp action) pairs.\n  * **Datasets:**\n    * [loghub](https://github.com/logpai/loghub): A large collection of different system log datasets for AI-driven log analytics.\n  * **Considerations:** How long you want to save the log interactions and what you want to use them for \u2013 i.e. understanding where errors occur, defining \u201ctypical\u201d behaviour \u2013 are key considerations for processing this data. For further details on what to track and how, see this [Tracking Plan](https://segment.com/academy/collecting-data/how-to-create-a-tracking-plan/) course from Segment.\n  * **Systems:** There are plenty of log management tools, for example [Better Stack](https://betterstack.com/logs), which has a pipeline set up for ClickHouse, allowing real-time processing, or [Papertrail](https://www.papertrail.com/), which can ingest different syslogs txt log file formats from Apache, MySQL, and Ruby.\n\n\nEach of these structured data types comes with its own unique challenges and characteristics. In particular, paying attention to data quality and pre-processing is important to make choices aligned with your vector retrieval system.\n## Keeping your retrieval stack up to date with Change Data Capture\nIn any data retrieval system, a key requirement is ensuring the underlying representations (i.e. vector embeddings) accurately reflect the latest state of source data. As underlying data changes \u2013 e.g., product updates, user activities, sensor readings \u2013 corresponding vector representations must also be kept current.\nOne approach to updating your data is batch recomputation \u2013 periodically rebuilding all vectors from scratch as the new data piles up. But batch recomputation ignores incremental changes between batches.\n**Change Data Capture** (CDC) provides a more efficient alternative \u2013 capturing granular data changes as they occur and incrementally updating associated vectors. Using CDC, an e-commerce site, for example, can stream product catalog changes to update product vectors rapidly. Or, a real-time anomaly detection system can employ CDC to incorporate user account changes to adapt user vectors. As a result, CDC plays an integral role in keeping vectorized data aligned with changing realities.\nThe visualization below shows how CDC can be implemented within a streaming data retrieval system. A primary database emits CDC into a queue, which is then consumed like any other streaming data source:\n  1. **Primary Database** : \n     * The primary database, which can be MySQL, PostgreSQL, SQL Server, or other database management system, serves as the source of data changes.\n     * It continuously captures changes to its tables, including inserts, updates, and deletes, in its transaction logs.\n  2. **Change Data Capture (CDC)** : \n     * CDC technology, for example [Kebola](https://www.keboola.com/) or [Qlik Replicate](https://www.qlik.com/us/products/qlik-replicate), acts as the bridge between the primary database and your retrieval system, detecting and managing incremental changes at the data source.\n     * A dedicated **capture process** is responsible for reading these changes from the transaction log of the primary database.\n     * These changes are then organized and placed into corresponding **change tables** within the CDC system.\n     * These change tables store metadata about what data has changed and when those changes occurred.\n  3. **Queue (Message Broker)** : \n     * The CDC system efficiently transmits these organized changes into a message queue, which can be implemented using technologies like Apache Kafka or RabbitMQ.\n     * The message queue acts as a buffer and is crucial in ensuring the reliable delivery of change events to downstream consumers.\n  4. **Streaming Data Consumers** : \n     * Applications, analytics pipelines, and other data consumers subscribe to the message queue.\n     * Streaming data consumers actively consume change events in real-time, just as they would with any other streaming data source.\n     * These consumers treat CDC data as a continuous flow of real-time data, making it readily available for processing and analysis.\n\n![CDC with streaming data](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/building_blocks/data_sources/bb1-3.png)\n## Contributors\n  * [Daniel Svonava](https://www.linkedin.com/in/svonava/)\n  * [Paolo Perrone](https://www.linkedin.com/in/paoloperrone/)\n  * [Robert Turner, editor](https://robertturner.co/copyedit)\n\n\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nStay updated with VectorHub\nSubscribe\n![arrow](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nContinue Reading\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/research_agent_3a22caf3b8.png)\n#Superlinked\nSimplifying Complex Research with AI\nLearn how to build an AI research assistant that finds, summarizes, and answers questions about scie...\nMarch 24, 2025\n[![Superlinked](https://superlinked.com/vectorhub/_next/static/media/superlinked-logo.9ad3911a.svg)](https://superlinked.com/)\nSubscribe\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nBy subscribing, you agree to our [Terms and Conditions](https://www.superlinked.com/policies/terms-and-conditions).\nAbout\n[Company](https://superlinked.com/)[Careers](https://superlinked.notion.site/Work-at-Superlinked-541f2d74ce714c889a1c6dba6dd0dbf1)[Index](https://superlinked.com/vectorhub/siteindex)\nSupport\nContact Us[Terms of Use](https://superlinked.com/policies/terms-and-conditions)[Privacy Policy](https://superlinked.com/policies/privacy-policy)[Cookie Policy](https://superlinked.com/policies/cookie-policy)\nSocial\n[Github](https://github.com/superlinked/superlinked)[X (Twitter)](https://twitter.com/superlinked)[LinkedIn](https://www.linkedin.com/company/superlinked/)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe document discusses data modalities in vector retrieval systems, emphasizing the importance of structured vs. unstructured data. It highlights Change Data Capture (CDC) for real-time updates and provides examples of datasets for various data types. Key contributors include Daniel Svonava, Paolo Perrone, and Robert Turner.\n```"
  },
  {
    "instruction": "[![logo](https://www.philschmid.de/_next/image?url=%2Fstatic%2Flogo.png&w=48&q=75)Philschmid](https://www.philschmid.de/)\nSearch`\u2318k`\n[Blog](https://www.philschmid.de/)[Projects](https://www.philschmid.de/projects)[Newsletter](https://www.philschmid.de/cloud-attention)[About Me](https://www.philschmid.de/philipp-schmid)Toggle Menu\n# Deploy Llama 3 on Amazon SageMaker\nApril 18, 20249 minute read[View Code](https://github.com/philschmid/llm-sagemaker-sample/blob/main/notebooks/deploy-llama3.ipynb)\nEarlier today Meta released [Llama 3](https://huggingface.co/blog/llama3), the next iteration of the open-access Llama family. Llama 3 comes in two sizes: [8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) for efficient deployment and development on consumer-size GPU, and [70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-instruct) for large-scale AI native applications. Both come in base and instruction-tuned variants. In addition to the 4 models, a new version of Llama Guard was fine-tuned on Llama 3 8B and is released as Llama Guard 2 (safety fine-tune).\nIn this blog you will learn how to deploy [meta-llama/Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) model to Amazon SageMaker. We are going to use the Hugging Face LLM DLC is a purpose-built Inference Container to easily deploy LLMs in a secure and managed environment. The DLC is powered by [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) a scalelable, optimized solution for deploying and serving Large Language Models (LLMs). The Blog post also includes Hardware requirements for the different model sizes.\nIn the blog will cover how to:\n  1. [Setup development environment](https://www.philschmid.de/sagemaker-llama3/#1-setup-development-environment)\n  2. [Hardware requirements](https://www.philschmid.de/sagemaker-llama3/#2-hardware-requirements)\n  3. [Deploy Llama 3 70b to Amazon SageMaker](https://www.philschmid.de/sagemaker-llama3/#3-deploy-llama-3-to-amazon-sagemaker)\n  4. [Run inference and chat with the model](https://www.philschmid.de/sagemaker-llama3/#4-run-inference-and-chat-with-the-model)\n  5. [Benchmark llama 3 70B with llmperf](https://www.philschmid.de/sagemaker-llama3/#5-benchmark-llama-3-70b)\n  6. [Clean up](https://www.philschmid.de/sagemaker-llama3/#6-clean-up)\n\n\nLets get started!\n## [](https://www.philschmid.de/sagemaker-llama3/#1-setup-development-environment)1. Setup development environment\nWe are going to use the `sagemaker` python SDK to deploy Mixtral to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed.\n```\n!pip install \"sagemaker>=2.216.0\" --upgrade --quiet\n```\n\nIf you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n```\nimport sagemaker\nimport boto3\nsess = sagemaker.Session()\n# sagemaker session bucket -> used for uploading data, models and logs\n# sagemaker will automatically create this bucket if it not exists\nsagemaker_session_bucket=None\nif sagemaker_session_bucket is None and sess is not None:\n  # set to default bucket if a bucket name is not given\n  sagemaker_session_bucket = sess.default_bucket()\ntry:\n  role = sagemaker.get_execution_role()\nexcept ValueError:\n  iam = boto3.client('iam')\n  role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\nsess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\nprint(f\"sagemaker role arn: {role}\")\nprint(f\"sagemaker session region: {sess.boto_region_name}\")\n\n```\n\nCompared to deploying regular Hugging Face models we first need to retrieve the container uri and provide it to our `HuggingFaceModel` model class with a `image_uri` pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use the `get_huggingface_llm_image_uri` method provided by the `sagemaker` SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specified `backend`, `session`, `region`, and `version`. You can find the available versions [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)\n_Note: At the time of writing this blog post the latest version of the Hugging Face LLM DLC is not yet available via the`get_huggingface_llm_image_uri` method. We are going to use the raw container uri instead._\n```\n# COMMENT IN WHEN PR (https://github.com/aws/sagemaker-python-sdk/pull/4314) IS MERGED\n# from sagemaker.huggingface import get_huggingface_llm_image_uri\n# # retrieve the llm image uri\n# llm_image = get_huggingface_llm_image_uri(\n#  \"huggingface\",\n#  version=\"2.0.0\"\n# )\nllm_image = f\"763104351884.dkr.ecr.{sess.boto_region_name}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\"\n# print ecr image uri\nprint(f\"llm image uri: {llm_image}\")\n```\n\nllm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\n## [](https://www.philschmid.de/sagemaker-llama3/#2-hardware-requirements)2. Hardware requirements\nLlama 3 comes in 2 different sizes - 8B & 70B parameters. The hardware requirements will vary based on the model size deployed to SageMaker. Below is a set up minimum requirements for each model size we tested.\nModel| Instance Type| Quantization| # of GPUs per replica  \n---|---|---|---  \n[Llama 8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)| `(ml.)g5.2xlarge`| `-`| 1  \n[Llama 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)| `(ml.)g5.12xlarge`| `gptq / awq`| 8  \n[Llama 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)| `(ml.)g5.48xlarge`| `-`| 8  \n[Llama 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)| `(ml.)p4d.24xlarge`| `-`| 8  \n_Note: We haven't tested GPTQ or AWQ models yet._\n## [](https://www.philschmid.de/sagemaker-llama3/#3-deploy-llama-3-to-amazon-sagemaker)3. Deploy Llama 3 to Amazon SageMaker\nTo deploy [Llama 3 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) to Amazon SageMaker we create a `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type` etc. We will use a `p4d.24xlarge` instance type, which has 8 NVIDIA A100 GPUs and 320GB of GPU memory. Llama 3 70B instruct is a fine-tuned model for conversational AI this allows us to enable the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api) from TGI to interact with llama using the common OpenAI format of `messages`.\n```\n{\n \"messages\": [\n  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n  { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n ]\n}\n```\n\n_Note: Llama 3 is a gated model, please visit the[Model Card](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) and accept the license terms and acceptable use policy before submitting this form._\n```\nimport json\nfrom sagemaker.huggingface import HuggingFaceModel\n# sagemaker config\ninstance_type = \"ml.p4d.24xlarge\"\nhealth_check_timeout = 900\n# Define Model and Endpoint configuration parameter\nconfig = {\n 'HF_MODEL_ID': \"meta-llama/Meta-Llama-3-70B-Instruct\", # model_id from hf.co/models\n 'SM_NUM_GPUS': \"8\", # Number of GPU used per replica\n 'MAX_INPUT_LENGTH': \"2048\", # Max length of input text\n 'MAX_TOTAL_TOKENS': \"4096\", # Max length of the generation (including input text)\n 'MAX_BATCH_TOTAL_TOKENS': \"8192\", # Limits the number of tokens that can be processed in parallel during the generation\n 'MESSAGES_API_ENABLED': \"true\", # Enable the messages API\n 'HUGGING_FACE_HUB_TOKEN': \"<REPLACE WITH YOUR TOKEN>\"\n}\n# check if token is set\nassert config['HUGGING_FACE_HUB_TOKEN'] != \"<REPLACE WITH YOUR TOKEN>\", \"Please set your Hugging Face Hub token\"\n# create HuggingFaceModel with the image uri\nllm_model = HuggingFaceModel(\n role=role,\n image_uri=llm_image,\n env=config\n)\n\n```\n\nAfter we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.p4d.24xlarge` instance type. TGI will automatically distribute and shard the model across all GPUs.\n```\n# Deploy model to an endpoint\n# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\nllm = llm_model.deploy(\n initial_instance_count=1,\n instance_type=instance_type,\n container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n)\n\n```\n\nSageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes.\n## [](https://www.philschmid.de/sagemaker-llama3/#4-run-inference-and-chat-with-the-model)4. Run inference and chat with the model\nAfter our endpoint is deployed we can run inference on it. We will use the `predict` method from the `predictor` to run inference on our endpoint. We can inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. You can find supported parameters in the [here](https://huggingface.co/docs/text-generation-inference/messages_api).\nThe Messages API allows us to interact with the model in a conversational way. We can define the role of the message and the content. The role can be either `system`,`assistant` or `user`. The `system` role is used to provide context to the model and the `user` role is used to ask questions or provide input to the model.\n```\n{\n \"messages\": [\n  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n  { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n ]\n}\n```\n\n```\n# Prompt to generate\nmessages=[\n  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n  { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n ]\n# Generation arguments\nparameters = {\n  \"model\": \"meta-llama/Meta-Llama-3-70B-Instruct\", # placholder, needed\n  \"top_p\": 0.6,\n  \"temperature\": 0.9,\n  \"max_tokens\": 512,\n  \"stop\": [\"<|eot_id|>\"],\n}\n```\n\nOkay lets test it.\n```\nchat = llm.predict({\"messages\" :messages, **parameters})\nprint(chat[\"choices\"][0][\"message\"][\"content\"].strip())\n```\n\n## [](https://www.philschmid.de/sagemaker-llama3/#5-benchmark-llama-3-70b-with-llmperf)5. Benchmark llama 3 70B with llmperf\nWe successfully deployed Llama 3 70B to Amazon SageMaker and tested it. Now we want to benchmark the model to see how it performs. We will use a [llmperf](https://github.com/philschmid/llmperf) fork with support for `sagemaker`.\nFirst lets install the `llmperf` package.\n```\n!git clone https://github.com/philschmid/llmperf.git\n!pip install -e llmperf/\n```\n\nNow we can run the benchmark with the following command. We are going to benchmark using `25` concurrent users and max `500` requests. The benchmark will measure `first-time-to-token`, `latency (ms/token)` and `throughput (tokens/s)` full details can be found in the `results` folder\n_\ud83d\udea8Important\ud83d\udea8: This benchmark was initiatied from Europe, while the endpoint runs in us-east-1. This has significant impact on the`first-time-to-token` metric, since it includes the network communication. If you want to measure the `first-time-to-token` correctly, you need to run the benchmark on the same host or your production region._\n```\n# tell llmperf that we are using the messages api\n!MESSAGES_API=true python llmperf/token_benchmark_ray.py \\\n--model {llm.endpoint_name} \\\n--llm-api \"sagemaker\" \\\n--max-num-completed-requests 500 \\\n--timeout 600 \\\n--num-concurrent-requests 25 \\\n--results-dir \"results\"\n```\n\nLets parse the results and display them nicely.\n```\nimport glob\nimport json\n# Reads the summary.json file and prints the results\nwith open(glob.glob(f'results/*summary.json')[0], 'r') as file:\n  data = json.load(file)\nprint(\"Concurrent requests: 25\")\nprint(f\"Avg. Input token length: {data['mean_input_tokens']}\")\nprint(f\"Avg. Output token length: {data['mean_output_tokens']}\")\nprint(f\"Avg. First-Time-To-Token: {data['results_ttft_s_mean']*1000:.2f}ms\")\nprint(f\"Avg. Thorughput: {data['results_mean_output_throughput_token_per_s']:.2f} tokens/sec\")\nprint(f\"Avg. Latency: {data['results_inter_token_latency_s_mean']*1000:.2f}ms/token\")\n#  Concurrent requests: 25\n#  Avg. Input token length: 550\n#  Avg. Output token length: 150\n#  Avg. First-Time-To-Token: 1301.28ms\n#  Avg. Thorughput: 1116.25 tokens/sec\n#  Avg. Latency: 9.45ms/token\n```\n\nThats it! We successfully deployed, tested and benchmarked Llama 3 70B on Amazon SageMaker. The benchmark is not a full representation of the model performance, but it gives you a first good indication. If you plan to use the model in production, we recommend to run a longer and closer to your production benchmark, modify the number of replicas see ([Scale LLM Inference on Amazon SageMaker with Multi-Replica Endpoints](https://www.philschmid.de/sagemaker-multi-replicahttps://www.philschmid.de/sagemaker-multi-replica)) and most importantly test the model with your own data.\n## [](https://www.philschmid.de/sagemaker-llama3/#6-clean-up)6. Clean up\nTo clean up, we can delete the model and endpoint.\n```\nllm.delete_model()\nllm.delete_endpoint()\n```\n\nThanks for reading! If you have any questions, feel free to contact me on [Twitter](https://twitter.com/_philschmid) or [LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).\n[Philipp Schmid \u00a9 2025](https://www.philschmid.de/philipp-schmid)[Imprint](https://www.philschmid.de/imprint)[RSS Feed](https://www.philschmid.de/rss)\ntheme\nMail[Twitter](https://twitter.com/_philschmid)[LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/)[GitHub](https://github.com/philschmid)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nMeta released Llama 3, available in 8B and 70B sizes. This guide details deploying Llama 3 70B on Amazon SageMaker using the Hugging Face LLM DLC. It covers setup, hardware requirements, deployment, inference, benchmarking, and cleanup. Key insights include performance metrics and model interaction via the Messages API.\n```"
  },
  {
    "instruction": "\u2728 New course! Enroll in [Vibe Coding 101 with Replit](https://bit.ly/4l8QyZ5)\n[](https://www.deeplearning.ai/)\n  * [Explore Courses](https://www.deeplearning.ai/courses/)\n  * [AI Newsletter](https://www.deeplearning.ai/the-batch/)\n    * [The Batch](https://www.deeplearning.ai/the-batch/)\n    * [Andrew's Letter](https://www.deeplearning.ai/the-batch/tag/letters/)\n    * [Data Points](https://www.deeplearning.ai/the-batch/tag/data-points/)\n    * [ML Research](https://www.deeplearning.ai/the-batch/tag/research/)\n    * [Blog](https://www.deeplearning.ai/blog/)\n  * [Community](https://www.deeplearning.ai/community/)\n    * [Forum](https://community.deeplearning.ai/)\n    * [Events](https://www.deeplearning.ai/events/)\n    * [Ambassadors](https://www.deeplearning.ai/ambassador/)\n    * [Ambassador Spotlight](https://www.deeplearning.ai/blog/category/ambassador-spotlight/)\n  * [Resources](https://www.deeplearning.ai/resources/)\n  * [Company](https://www.deeplearning.ai/about/)\n    * [About](https://www.deeplearning.ai/about/)\n    * [Careers](https://www.deeplearning.ai/careers/)\n    * [Contact](https://www.deeplearning.ai/contact/)\n\n[Log In](https://bit.ly/3RB9T8a)\n  * [Overview](https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/#overview)\n  * [Course Outline](https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/#course-outline)\n  * [Instructors](https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/#instructors)\n\n\n![](https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F07%2FAsset-2%402x.webp&w=3840&q=90)\n  * [All Courses](https://www.deeplearning.ai/courses/)\n  * [Short Courses](https://www.deeplearning.ai/courses/?courses_date_desc%5BrefinementList%5D%5Bcourse_type%5D%5B0%5D=Short%20Courses)\n  * Large Language Models with Semantic Search\n\n\nShort CourseBeginner1 Hour 12 Minutes\n# Large Language Models with Semantic Search\nInstructors: Jay Alammar, Luis Serrano\n[![Cohere](https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/07/cohere.png)](https://cohere.com/)\n[Enroll for Free](https://learn.deeplearning.ai/large-language-models-semantic-search?utm_source=home&utm_medium=course-landing-page&utm_campaign=hero-cta-button)\n  * [All Courses](https://www.deeplearning.ai/courses/)\n  * [Short Courses](https://www.deeplearning.ai/courses/?courses_date_desc%5BrefinementList%5D%5Bcourse_type%5D%5B0%5D=Short%20Courses)\n  * Large Language Models with Semantic Search\n\n\n![](https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F07%2FLarge-Language-Models-with-Semantic-Search.png&w=3840&q=75)\n  * Beginner\n  * 1 Hour 12 Minutes\n  * 7 Video Lessons\n  * 5 Code Examples\n  * Instructors: Jay Alammar, Luis Serrano\n  *     * ![Cohere](https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F08%2FCohere.png&w=48&q=75)\nCohere\n\n\n## What you'll learn\n  * Enhance keyword search using Cohere Rerank\n  * Use embeddings to leverage dense retrieval, a powerful NLP tool\n  * Evaluate your effectiveness for further optimization\n\n\n## About this course\nKeyword search has been a common method for search for many years. But for content-rich websites like news media sites or online shopping platforms, the keyword search capability can be limiting. Incorporating large language models (LLMs) into your search can significantly enhance the user experience by allowing them to ask questions and find information in a much easier way.\nThis course teaches the techniques needed to leverage LLMs into search.\nThroughout the lessons, you\u2019ll explore key concepts like dense retrieval, which elevates the relevance of retrieved information, leading to improved search results beyond traditional keyword search, and reranking, which injects the intelligence of LLMs into your search system, making it faster and more effective. \nAfter completing the course, you will:\n  * Know how to implement basic keyword search, the underpinnings of many search systems before language models became accessible.\n  * Enhance keyword search with the rerank method, which ranks the best responses by relevance with the query.\n  * Implement dense retrieval through the use of embeddings, a powerful NLP tool, which uses the actual semantic meaning of the text to carry out search, and vastly improves results.\n  * Gain hands-on practice by working with large amounts of data and overcome challenges like varying search results and accuracy.\n  * Implement language model-powered search into your website or project.\n\n\n## Who should join?\nAnyone who has basic familiarity with Python and wants to get a deeper understanding of key technical foundations of LLMs, and learn to use semantic search.\n## Course Outline\n7 Lessons\u30fb5 Code Examples\n  * Introduction\nVideo\u30fb4 mins\n  * Keyword Search\nVideo with code examples\u30fb14 mins\n  * Embeddings\nVideo with code examples\u30fb9 mins\n  * Dense Retrieval\nVideo with code examples\u30fb20 mins\n  * ReRank\nVideo with code examples\u30fb10 mins\n  * Generating Answers\nVideo with code examples\u30fb12 mins\n  * Conclusion\nVideo\u30fb1 min\n\n\n## Instructors\n![Jay Alammar](https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F07%2F1604955719487.jpeg&w=256&q=75)\n### Jay Alammar\nDirector and Engineering Fellow at [Cohere ](https://cohere.com/)and co-author of [Hands-On Large Language Models](https://github.com/handsOnLLM/Hands-On-Large-Language-Models)\n  * [](https://www.linkedin.com/in/jalammar/)\n  * [](https://twitter.com/JayAlammar)\n  * [](https://jalammar.github.io/about/)\n\n\n![Luis Serrano](https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F01%2FLuis_Serrano.png&w=256&q=75)\n### Luis Serrano\nLead of Developer Relations at Cohere\n  * [](https://www.linkedin.com/in/luisgserrano/)\n  * [](https://twitter.com/SerranoAcademy)\n  * [](https://serrano.academy/)\n\n\n## Large Language Models with Semantic Search\n  * Beginner\n  * 1 Hour 12 Minutes\n  * 7 Video Lessons\n  * 5 Code Examples\n  * Instructors: Jay Alammar, Luis Serrano\n  *     * ![Cohere](https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F08%2FCohere.png&w=48&q=75)\nCohere\n\n\n[Enroll for Free](https://learn.deeplearning.ai/large-language-models-semantic-search?utm_source=home&utm_medium=course-landing-page&utm_campaign=summary-cta-button)\nCourse access is free for a limited time during the DeepLearning.AI learning platform beta!\n[Enroll for Free](https://learn.deeplearning.ai/large-language-models-semantic-search?utm_source=home&utm_medium=course-landing-page&utm_campaign=promotional-banner-cta-button)\n## Want to learn more about Generative AI?\nKeep learning with updates on curated AI news, courses, events, as well as Andrew\u2019s thoughts from DeepLearning.AI! \n[Enroll for Free](https://learn.deeplearning.ai/large-language-models-semantic-search?utm_source=home&utm_medium=course-landing-page&utm_campaign=summary-cta-button)\n  * [Courses](https://www.deeplearning.ai/courses/)\n  * [The Batch](https://www.deeplearning.ai/the-batch/)\n  * [Community](https://www.deeplearning.ai/community/)\n  * [Careers](https://www.deeplearning.ai/careers/)\n  * [About](https://www.deeplearning.ai/about/)\n\n\n[](https://www.facebook.com/1027125564106325)[](https://www.instagram.com/deeplearningai)[](https://twitter.com/deeplearningai)[](https://www.linkedin.com/company/18246783)[](https://www.youtube.com/c/Deeplearningai)\n",
    "answer": "```markdown\n## TL;DR Summary\n\n**Course:** Large Language Models with Semantic Search  \n**Instructors:** Jay Alammar, Luis Serrano  \n**Duration:** 1 Hour 12 Minutes  \n**Level:** Beginner\n\nLearn to enhance keyword search using LLMs and embeddings for improved search results. The course covers dense retrieval, reranking, and practical implementation for better user experience. Free enrollment available during beta.\n\n**Tags:** Generative AI, LLMs, NLP\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\n# \n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/1676334017493-63ead3a0d0b894bbc77b199c.png)](https://huggingface.co/mosaicml)\n[mosaicml](https://huggingface.co/mosaicml)\n/\n[mpt-30b](https://huggingface.co/mosaicml/mpt-30b)\nlike 342\nFollow\n![](https://cdn-avatars.huggingface.co/v1/production/uploads/1676334017493-63ead3a0d0b894bbc77b199c.png) Mosaic ML, Inc. 380\n[ Text Generation ](https://huggingface.co/models?pipeline_tag=text-generation)[ Transformers ](https://huggingface.co/models?library=transformers)[ PyTorch ](https://huggingface.co/models?library=pytorch)\n5 datasets\n[ mpt ](https://huggingface.co/models?other=mpt)[ Composer ](https://huggingface.co/models?other=Composer)[ MosaicML ](https://huggingface.co/models?other=MosaicML)[ llm-foundry ](https://huggingface.co/models?other=llm-foundry)[ StreamingDatasets ](https://huggingface.co/models?other=StreamingDatasets)[ custom_code ](https://huggingface.co/models?other=custom_code)[ text-generation-inference ](https://huggingface.co/models?other=text-generation-inference)\narxiv: 6 papers\nLicense: apache-2.0\n[ Model card ](https://huggingface.co/mosaicml/mpt-30b)[ Files Files and versions ](https://huggingface.co/mosaicml/mpt-30b/tree/main)[ Community 22 ](https://huggingface.co/mosaicml/mpt-30b/discussions)\nTrain \nDeploy \nUse this model \n  * [MPT-30B](https://huggingface.co/mosaicml/mpt-30b/#mpt-30b \"MPT-30B\")\n    * [Model Date](https://huggingface.co/mosaicml/mpt-30b/#model-date \"Model Date\")\n    * [Model License](https://huggingface.co/mosaicml/mpt-30b/#model-license \"Model License\")\n    * [Documentation](https://huggingface.co/mosaicml/mpt-30b/#documentation \"Documentation\")\n    * [How to Use](https://huggingface.co/mosaicml/mpt-30b/#how-to-use \"How to Use\")\n    * [Model Description](https://huggingface.co/mosaicml/mpt-30b/#model-description \"Model Description\")\n    * [Training Data](https://huggingface.co/mosaicml/mpt-30b/#training-data \"Training Data\")\n      * [Streaming Datasets](https://huggingface.co/mosaicml/mpt-30b/#streaming-datasets \"Streaming Datasets\")\n      * [Data Mix](https://huggingface.co/mosaicml/mpt-30b/#data-mix \"Data Mix\")\n      * [Training Configuration](https://huggingface.co/mosaicml/mpt-30b/#training-configuration \"Training Configuration\")\n    * [Limitations and Biases](https://huggingface.co/mosaicml/mpt-30b/#limitations-and-biases \"Limitations and Biases\")\n    * [MosaicML Platform](https://huggingface.co/mosaicml/mpt-30b/#mosaicml-platform \"MosaicML Platform\")\n    * [Disclaimer](https://huggingface.co/mosaicml/mpt-30b/#disclaimer \"Disclaimer\")\n    * [Citation](https://huggingface.co/mosaicml/mpt-30b/#citation \"Citation\")\n\n\n#  [ ](https://huggingface.co/mosaicml/mpt-30b/#mpt-30b) MPT-30B \nMPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by [MosaicML](https://www.mosaicml.com).\nMPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.\nMPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see [MPT-7B-StoryWriter](https://huggingface.co/mosaicml/mpt-7b-storywriter)), support for context-length extrapolation via [ALiBi](https://arxiv.org/abs/2108.12409), and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's [FasterTransformer](https://github.com/NVIDIA/FasterTransformer). The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPU\u2014either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.\nThis model uses the MosaicML LLM codebase, which can be found in the [llm-foundry repository](https://github.com/mosaicml/llm-foundry). It was trained by MosaicML\u2019s NLP team on the [MosaicML platform](https://www.mosaicml.com/training) for LLM pretraining, finetuning, and inference.\n###  [ ](https://huggingface.co/mosaicml/mpt-30b/#how-is-this-model-different) How is this model different? \nMPT-30B is:\n  * **Licensed for the possibility of commercial use** (unlike [LLaMA](https://arxiv.org/abs/2302.13971)).\n  * **Trained on a large amount of data** (1T tokens like [LLaMA](https://arxiv.org/abs/2302.13971) vs. 300B for [Pythia](https://github.com/EleutherAI/pythia), 300B for [OpenLLaMA](https://github.com/openlm-research/open_llama), and 800B for [StableLM](https://github.com/Stability-AI/StableLM)).\n  * **Prepared to handle extremely long inputs** thanks to [ALiBi](https://arxiv.org/abs/2108.12409).\n  * **Capable of fast training and inference** (via [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf) and [FasterTransformer](https://github.com/NVIDIA/FasterTransformer))\n  * **Equipped with highly efficient open-source training code** via the [llm-foundry repository](https://github.com/mosaicml/llm-foundry)\n\n\n###  [ ](https://huggingface.co/mosaicml/mpt-30b/#models-finetuned-off-mpt-30b) Models finetuned off MPT-30B: \nThe following models are finetuned on MPT-30B:\n  * [MPT-30B-Instruct](https://huggingface.co/mosaicml/mpt-30b-instruct): a model for long-form instruction following (especially summarization and question-answering). Built by finetuning MPT-30B on several carefully curated datasets.\n    * License: _CC-BY-SA-3.0_\n  * [MPT-30B-Chat](https://huggingface.co/mosaicml/mpt-30b-chat): a chatbot-like model for dialogue generation. Built by finetuning MPT-30B on [ShareGPT-Vicuna](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered), [Camel-AI](https://huggingface.co/camel-ai), [GPTeacher](https://github.com/teknium1/GPTeacher), [Guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco), [Baize](https://github.com/project-baize/baize-chatbot) and some generated datasets.\n    * License: _CC-By-NC-SA-4.0_\n    * [Demo on Hugging Face Spaces](https://huggingface.co/spaces/mosaicml/mpt-30b-chat)\n\n\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#model-date) Model Date \nJune 22, 2023\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#model-license) Model License \nApache-2.0\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#documentation) Documentation \n  * [Blog post: MPT-30B: Raising the bar for open-source foundation models](https://www.mosaicml.com/blog/mpt-30b)\n  * [Codebase (mosaicml/llm-foundry repo)](https://github.com/mosaicml/llm-foundry/)\n  * Questions: Feel free to contact us via the [MosaicML Community Slack](https://mosaicml.me/slack)!\n\n\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#how-to-use) How to Use \nThis model is best used with the MosaicML [llm-foundry repository](https://github.com/mosaicml/llm-foundry) for training and finetuning.\n```\nimport transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n 'mosaicml/mpt-30b',\n trust_remote_code=True\n)\n\n```\n\nNote: This model requires that `trust_remote_code=True` be passed to the `from_pretrained` method. This is because we use a custom `MPT` model architecture that is not yet part of the Hugging Face `transformers` package. `MPT` includes options for many training efficiency features such as [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf), [ALiBi](https://arxiv.org/abs/2108.12409), [QK LayerNorm](https://arxiv.org/abs/2010.04245), and more.\nTo use the optimized [triton implementation](https://github.com/openai/triton) of FlashAttention, you can load the model on GPU (`cuda:0`) with `attn_impl='triton'` and with `bfloat16` precision:\n```\nimport torch\nimport transformers\nname = 'mosaicml/mpt-30b'\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton' # change this to use triton-based FlashAttention\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n name,\n config=config,\n torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n trust_remote_code=True\n)\n\n```\n\nThe model was trained initially with a sequence length of 2048 with an additional pretraining stage for sequence length adapation up to 8192. However, ALiBi enables users to increase the maximum sequence length even further during finetuning and/or inference. For example:\n```\nimport transformers\nname = 'mosaicml/mpt-30b'\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 16384 # (input + output) tokens can now be up to 16384\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n name,\n config=config,\n trust_remote_code=True\n)\n\n```\n\nThis model was trained with the MPT-30B tokenizer which is identical to the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer.\n```\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('mosaicml/mpt-30b')\n\n```\n\nThe model can then be used, for example, within a text-generation pipeline.Note: when running Torch modules in lower precision, it is best practice to use the [torch.autocast context manager](https://pytorch.org/docs/stable/amp.html).\n```\nfrom transformers import pipeline\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n  inputs = tokenizer('Here is a recipe for vegan banana bread:\\n', return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(**inputs, max_new_tokens=100)\n  print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n# or using the HF pipeline\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n  print(\n    pipe('Here is a recipe for vegan banana bread:\\n',\n      max_new_tokens=100,\n      do_sample=True,\n      use_cache=True))\n\n```\n\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#model-description) Model Description \nThe architecture is a modification of a standard decoder-only transformer.\nThe model has been modified from a standard transformer in the following ways:\n  * It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)\n  * It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings\n  * It does not use biases\n\n\nHyperparameter | Value  \n---|---  \nn_parameters | 29.95B  \nn_layers | 48  \nn_heads | 64  \nd_model | 7168  \nvocab size | 50432  \nsequence length | 8192  \n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#training-data) Training Data \n###  [ ](https://huggingface.co/mosaicml/mpt-30b/#streaming-datasets) Streaming Datasets \nData was formatted using the MosaicML [StreamingDataset](https://github.com/mosaicml/streaming) library to host our data in object storage and efficiently stream it to our compute cluster during training. StreamingDataset obviates the need to download the whole dataset before starting training, and allows instant resumption of training from any point in the dataset.\n###  [ ](https://huggingface.co/mosaicml/mpt-30b/#data-mix) Data Mix \nThe model was trained for 1T tokens on the following data mix:\nData Source | Number of Tokens in Source | Proportion | Effective Number of Tokens | Epochs  \n---|---|---|---|---  \nmC4 3.1.0 - English (200+ words) | 2417.99 B | 33.50% | 335 B | 0.14  \nc4 - English - SemDedup 80% | 100.42 B | 29.90% | 299 B | 2.98  \nRedPajama - CommonCrawl | 878.45 B | 8.50% | 85 B | 0.097  \nThe Stack - Selected Languages | 463.78 B | 10.00% | 100 B | 0.22  \nRedPajama - Wikipedia | 4.87 B | 4.00% | 40 B | 8.21  \nThe Stack - Markdown | 107.07 B | 4.50% | 45 B | 0.42  \nSemantic Scholar ORC | 48.95 B | 3.30% | 33 B | 0.67  \nRedPajama - Books | 26.02 B | 3.00% | 30 B | 1.15  \nRedPajama - arXiv | 28.10 B | 1.90% | 19 B | 0.68  \nRedPajama - StackExchange | 20.54 B | 1.40% | 14 B | 0.68  \nSamples for each batch were selected from one of the datasets with the probability specified above. The examples were shuffled within each dataset, and each example was constructed from as many sequences from that dataset as were necessary to fill the sequence length. To build 8k support into MPT-30B efficiently, we first pre-trained on 1T tokens using sequences that were 2k tokens long, and then trained for an additional 50B tokens using sequences that were 8k tokens long.\nThe data was tokenized using the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer. This BPE tokenizer has a number of desirable characteristics, most of which are relevant for tokenizing code: (1) It was trained on a diverse mix of data that includes code (The Pile) (2) It applies consistent space delimitation, unlike the GPT2 tokenizer which tokenizes inconsistently depending on the presence of prefix spaces (3) It contains tokens for repeated space characters, which allows superior compression of text with large amounts of repeated space characters.\nThe model vocabulary size of 50432 was set to be a multiple of 128 (as in [MEGATRON-LM](https://arxiv.org/abs/1909.08053)).\n###  [ ](https://huggingface.co/mosaicml/mpt-30b/#training-configuration) Training Configuration \nThe model was trained in three stages using the [MosaicML Platform](https://www.mosaicml.com/platform): (i) First it was trained on 440 A100-40GBs with a batch size of 1760. (ii) Then, on 216 A100-40GBs with a batch size of 1728. (iii) Training was completed on 256 H100-80GBs with a batch size of 512 with 8k context length and 50B tokens. The model was trained with sharded data parallelism using [FSDP](https://pytorch.org/docs/stable/fsdp.html) and used the [LION](https://arxiv.org/abs/2302.06675) optimizer.\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#limitations-and-biases) Limitations and Biases \n_The following language is modified from[EleutherAI's GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)_\nMPT-30B (Base) is **not** intended for deployment without finetuning. It should not be used for human-facing interactions without further guardrails and user consent.\nMPT-30B can produce factually incorrect output, and should not be relied on to produce factually accurate information. MPT-30B was trained on various public datasets. While great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#mosaicml-platform) MosaicML Platform \nIf you're interested in [training](https://www.mosaicml.com/training) and [deploying](https://www.mosaicml.com/inference) your own MPT or LLMs on the MosaicML Platform, [sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-30b).\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#disclaimer) Disclaimer \nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please consult an attorney before using this model for commercial purposes.\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#citation) Citation \nPlease cite this model using the following format:\n```\n@online{MosaicML2023Introducing,\n  author  = {MosaicML NLP Team},\n  title   = {Introducing MPT-30B: Raising the bar\nfor open-source foundation models},\n  year   = {2023},\n  url    = {www.mosaicml.com/blog/mpt-30b},\n  note   = {Accessed: 2023-06-22},\n  urldate  = {2023-06-22}\n}\n\n```\n\n\nDownloads last month\n    4,777 \nInference Providers [NEW](https://huggingface.co/blog/inference-providers)\n[ Text Generation](https://huggingface.co/tasks/text-generation \"Learn more about text-generation\")\nThis model isn't deployed by any Inference Provider. [\ud83d\ude4b Ask for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=mosaicml/mpt-30b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmosaicml%2Fmpt-30b%5D\\(%2Fmosaicml%2Fmpt-30b\\)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A\\(optional\\)%20Which%20providers%20are%20you%20interested%20in%3F%20\\(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6\\)%0A)\nHF Inference deployability: The model authors have turned it off explicitly.\n##  Model tree for mosaicml/mpt-30b [](https://huggingface.co/docs/hub/model-cards#specifying-a-base-model)\nQuantizations\n[2 models](https://huggingface.co/models?other=base_model:quantized:mosaicml/mpt-30b)\n##  Datasets used to train mosaicml/mpt-30b\n#### [allenai/c4 Viewer \u2022  Updated Jan 9, 2024 \u2022  10.4B \u2022  452k \u2022  394  ](https://huggingface.co/datasets/allenai/c4)#### [legacy-datasets/mc4 Updated Mar 5, 2024 \u2022  38.8k \u2022  151  ](https://huggingface.co/datasets/legacy-datasets/mc4)#### [bigcode/the-stack-dedup Viewer \u2022  Updated Aug 17, 2023 \u2022  237M \u2022  12.5k \u2022  345  ](https://huggingface.co/datasets/bigcode/the-stack-dedup)\n##  Spaces using mosaicml/mpt-30b 35\n[\ud83c\udfc6 eduagarcia/open_pt_llm_leaderboard](https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard)[\ud83c\udfc6 BAAI/open_cn_llm_leaderboard](https://huggingface.co/spaces/BAAI/open_cn_llm_leaderboard)[\ud83d\ude3b Sharathhebbar24/One-stop-for-Open-source-models](https://huggingface.co/spaces/Sharathhebbar24/One-stop-for-Open-source-models)[\ud83d\ude3b GTBench/GTBench](https://huggingface.co/spaces/GTBench/GTBench)[\ud83c\udfc6 Vikhrmodels/small-shlepa-lb](https://huggingface.co/spaces/Vikhrmodels/small-shlepa-lb)[\ud83c\udfc6 kz-transformers/kaz-llm-lb](https://huggingface.co/spaces/kz-transformers/kaz-llm-lb)[\ud83c\udfa8 OPTML-Group/UnlearnCanvas-Benchmark](https://huggingface.co/spaces/OPTML-Group/UnlearnCanvas-Benchmark)[\ud83e\udd47 BAAI/open_flageval_vlm_leaderboard](https://huggingface.co/spaces/BAAI/open_flageval_vlm_leaderboard)[\ud83d\udc28 Zulelee/langchain-chatchat](https://huggingface.co/spaces/Zulelee/langchain-chatchat)[\ud83c\udfc6 wissamantoun/LLM_Detection_Attribution](https://huggingface.co/spaces/wissamantoun/LLM_Detection_Attribution)[\ud83c\udfc6 Adeco/open_llm_leaderboard](https://huggingface.co/spaces/Adeco/open_llm_leaderboard)[\ud83c\udfc6 Intel/low_bit_open_llm_leaderboard](https://huggingface.co/spaces/Intel/low_bit_open_llm_leaderboard) + 30 Spaces + 23 Spaces\n##  Collection including mosaicml/mpt-30b\n#### [MPT Collection  The MPT collections is a series of decoder-style transformer models trained from scratch by MosaicML. Details: https://www.mosaicml.com/mpt \u2022  9 items \u2022  Updated Nov 27, 2023 \u2022 7](https://huggingface.co/collections/mosaicml/mpt-6564f3d9e5aac326bfa22def)\nSystem theme \nCompany\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/) [](https://huggingface.co/)\nWebsite\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)\n",
    "answer": "```markdown\n# TL;DR Summary of MPT-30B Model\n\nMPT-30B is a decoder-style transformer model by MosaicML, pretrained on 1T tokens. It features an 8k token context, efficient training via FlashAttention, and is licensed for commercial use. It excels in text generation and coding tasks, with finetuned versions for instruction and chat applications.\n```"
  },
  {
    "instruction": "[ Skip to content ](https://copier.readthedocs.io/en/stable/#_1)\n[ ](https://copier.readthedocs.io/en/stable/ \"copier\")\ncopier \nOverview \nType to start searching\n[ copier-org/copier \n  * v9.6.0\n  * 2.4k\n  * 199\n\n](https://github.com/copier-org/copier \"Go to repository\")\n[ ](https://copier.readthedocs.io/en/stable/ \"copier\") copier \n[ copier-org/copier \n  * v9.6.0\n  * 2.4k\n  * 199\n\n](https://github.com/copier-org/copier \"Go to repository\")\n  * Overview  [ Overview  ](https://copier.readthedocs.io/en/stable/) Table of contents \n    * [ Installation  ](https://copier.readthedocs.io/en/stable/#installation)\n      * [ Nix flake  ](https://copier.readthedocs.io/en/stable/#nix-flake)\n    * [ Quick start  ](https://copier.readthedocs.io/en/stable/#quick-start)\n    * [ Basic concepts  ](https://copier.readthedocs.io/en/stable/#basic-concepts)\n    * [ Browse or tag public templates  ](https://copier.readthedocs.io/en/stable/#browse-or-tag-public-templates)\n    * [ Show your support  ](https://copier.readthedocs.io/en/stable/#show-your-support)\n      * [ Copier badge variations  ](https://copier.readthedocs.io/en/stable/#copier-badge-variations)\n    * [ Credits  ](https://copier.readthedocs.io/en/stable/#credits)\n  * [ Creating a template  ](https://copier.readthedocs.io/en/stable/creating/)\n  * [ Configuring a template  ](https://copier.readthedocs.io/en/stable/configuring/)\n  * [ Generating a project  ](https://copier.readthedocs.io/en/stable/generating/)\n  * [ Updating a project  ](https://copier.readthedocs.io/en/stable/updating/)\n  * [ Settings  ](https://copier.readthedocs.io/en/stable/settings/)\n  * Reference  Reference \n    * [ cli.py  ](https://copier.readthedocs.io/en/stable/reference/cli/)\n    * [ errors.py  ](https://copier.readthedocs.io/en/stable/reference/errors/)\n    * [ jinja_ext.py  ](https://copier.readthedocs.io/en/stable/reference/jinja_ext/)\n    * [ main.py  ](https://copier.readthedocs.io/en/stable/reference/main/)\n    * [ settings.py  ](https://copier.readthedocs.io/en/stable/reference/settings/)\n    * [ subproject.py  ](https://copier.readthedocs.io/en/stable/reference/subproject/)\n    * [ template.py  ](https://copier.readthedocs.io/en/stable/reference/template/)\n    * [ tools.py  ](https://copier.readthedocs.io/en/stable/reference/tools/)\n    * [ types.py  ](https://copier.readthedocs.io/en/stable/reference/types/)\n    * [ user_data.py  ](https://copier.readthedocs.io/en/stable/reference/user_data/)\n    * [ vcs.py  ](https://copier.readthedocs.io/en/stable/reference/vcs/)\n  * [ Comparisons  ](https://copier.readthedocs.io/en/stable/comparisons/)\n  * [ Frequently Asked Questions  ](https://copier.readthedocs.io/en/stable/faq/)\n  * [ Contributing  ](https://copier.readthedocs.io/en/stable/contributing/)\n  * [ Changelog  ](https://copier.readthedocs.io/en/stable/changelog/)\n\n\nTable of contents \n  * [ Installation  ](https://copier.readthedocs.io/en/stable/#installation)\n    * [ Nix flake  ](https://copier.readthedocs.io/en/stable/#nix-flake)\n  * [ Quick start  ](https://copier.readthedocs.io/en/stable/#quick-start)\n  * [ Basic concepts  ](https://copier.readthedocs.io/en/stable/#basic-concepts)\n  * [ Browse or tag public templates  ](https://copier.readthedocs.io/en/stable/#browse-or-tag-public-templates)\n  * [ Show your support  ](https://copier.readthedocs.io/en/stable/#show-your-support)\n    * [ Copier badge variations  ](https://copier.readthedocs.io/en/stable/#copier-badge-variations)\n  * [ Credits  ](https://copier.readthedocs.io/en/stable/#credits)\n\n\n# ![Copier](https://github.com/copier-org/copier/raw/master/img/copier-logotype.png)[\u00b6](https://copier.readthedocs.io/en/stable/#_1 \"Permanent link\")\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier) [![Gitpod ready-to-code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/copier-org/copier) [![codecov](https://codecov.io/gh/copier-org/copier/branch/master/graph/badge.svg)](https://codecov.io/gh/copier-org/copier) [![CI](https://github.com/copier-org/copier/workflows/CI/badge.svg)](https://github.com/copier-org/copier/actions?query=branch%3Amaster) [![Checked with mypy](http://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/) ![Python](https://img.shields.io/pypi/pyversions/copier?logo=python&logoColor=%23959DA5) [![PyPI](https://img.shields.io/pypi/v/copier?logo=pypi&logoColor=%23959DA5)](https://pypi.org/project/copier/) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![Documentation Status](https://img.shields.io/readthedocs/copier/latest?logo=readthedocs)](https://copier.readthedocs.io/en/latest) [![](https://img.shields.io/badge/Gurubase-Ask%20Copier%20Guru-006BFF)](https://gurubase.io/g/copier)\nA library and CLI app for rendering project templates.\n  * Works with **local** paths and **Git URLs**.\n  * Your project can include any file and Copier can dynamically replace values in any kind of text file.\n  * It generates a beautiful output and takes care of not overwriting existing files unless instructed to do so.\n\n\n![Sample output](https://github.com/copier-org/copier/raw/master/img/copier-output.png)\n## Installation[\u00b6](https://copier.readthedocs.io/en/stable/#installation \"Permanent link\")\n  1. Install Python 3.9 or newer.\n  2. Install Git 2.27 or newer.\n  3. To use as a CLI app: [`pipx install copier`](https://github.com/pypa/pipx) or [`uv tool install copier`](https://docs.astral.sh/uv/#tool-management)\n  4. To use as a library: `pip install copier` or `conda install -c conda-forge copier`\n\n\n### Nix flake[\u00b6](https://copier.readthedocs.io/en/stable/#nix-flake \"Permanent link\")\nTo install latest Copier release with 100% reproducibility:\n```\nnixprofileinstall'https://flakehub.com/f/copier-org/copier/*.tar.gz'\n\n```\n\n## Quick start[\u00b6](https://copier.readthedocs.io/en/stable/#quick-start \"Permanent link\")\nTo create a template:\n```\n\ud83d\udcc1my_copier_template# your template project\n\u251c\u2500\u2500\ud83d\udcc4copier.yml# your template configuration\n\u251c\u2500\u2500\ud83d\udcc1.git/# your template is a Git repository\n\u251c\u2500\u2500\ud83d\udcc1{{project_name}}# a folder with a templated name\n\u2502\u2514\u2500\u2500\ud83d\udcc4{{module_name}}.py.jinja# a file with a templated name\n\u2514\u2500\u2500\ud83d\udcc4{{_copier_conf.answers_file}}.jinja# answers are recorded here\n\n```\n\ncopier.yml```\n# questions\nproject_name:\ntype:str\nhelp:What is your project name?\nmodule_name:\ntype:str\nhelp:What is your Python module name?\n\n```\n\n{{project_name}}/{{module_name}}.py.jinja```\nprint(\"Hello from {{module_name}}!\")\n\n```\n\n{{_copier_conf.answers_file}}.jinja```\n# Changes here will be overwritten by Copier\n{{ _copier_answers|to_nice_yaml -}}\n\n```\n\nTo generate a project from the template:\n  * On the command-line:\n```\ncopiercopypath/to/project/templatepath/to/destination\n\n```\n\n  * Or in Python code, programmatically:\n```\nfromcopierimport run_copy\n# Create a project from a local path\nrun_copy(\"path/to/project/template\", \"path/to/destination\")\n# Or from a Git URL.\nrun_copy(\"https://github.com/copier-org/copier.git\", \"path/to/destination\")\n# You can also use \"gh:\" as a shortcut of \"https://github.com/\"\nrun_copy(\"gh:copier-org/copier.git\", \"path/to/destination\")\n# Or \"gl:\" as a shortcut of \"https://gitlab.com/\"\nrun_copy(\"gl:copier-org/copier.git\", \"path/to/destination\")\n\n```\n\n\n\n## Basic concepts[\u00b6](https://copier.readthedocs.io/en/stable/#basic-concepts \"Permanent link\")\nCopier is composed of these main concepts:\n  1. **Templates**. They lay out how to generate the subproject.\n  2. **Questionnaires**. They are configured in the template. Answers are used to generate projects.\n  3. **Projects**. This is where your real program lives. But it is usually generated and/or updated from a template.\n\n\nCopier targets these main human audiences:\n  1. **Template creators**. Programmers that repeat code too much and prefer a tool to do it for them.\n**_Tip:_** Copier doesn't replace the DRY principle... but sometimes you simply can't be DRY and you need a DRYing machine...\n  2. **Template consumers**. Programmers that want to start a new project quickly, or that want to evolve it comfortably.\n\n\nNon-humans should be happy also by using Copier's CLI or API, as long as their expectations are the same as for those humans... and as long as they have feelings.\nTemplates have these goals:\n  1. **[Code scaffolding](https://en.wikipedia.org/wiki/Scaffold_\\(programming\\))**. Help consumers have a working source code tree as quickly as possible. All templates allow scaffolding.\n  2. **Code lifecycle management**. When the template evolves, let consumers update their projects. Not all templates allow updating.\n\n\nCopier tries to have a smooth learning curve that lets you create simple templates that can evolve into complex ones as needed.\n## Browse or tag public templates[\u00b6](https://copier.readthedocs.io/en/stable/#browse-or-tag-public-templates \"Permanent link\")\nYou can browse public Copier templates on GitHub using [the `copier-template` topic](https://github.com/topics/copier-template). Use them as inspiration!\nIf you want your template to appear in that list, just add the topic to it! \ud83c\udff7\n## Show your support[\u00b6](https://copier.readthedocs.io/en/stable/#show-your-support \"Permanent link\")\nIf you're using Copier, consider adding the Copier badge to your project's `README.md`:\n```\n[![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-orange.json)](https://github.com/copier-org/copier)\n\n```\n\n...or `README.rst`:\n```\n.. image:: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-orange.json\n  :target: https://github.com/copier-org/copier\n  :alt: Copier\n\n```\n\n...or, as HTML:\n```\n<a href=\"https://github.com/copier-org/copier\"><img src=\"https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-orange.json\" alt=\"Copier\" style=\"max-width:100%;\"/></a>\n\n```\n\n### Copier badge variations[\u00b6](https://copier.readthedocs.io/en/stable/#copier-badge-variations \"Permanent link\")\n  1. Badge Grayscale Border [![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-border.json)](https://github.com/copier-org/copier)\n  2. Badge Grayscale Inverted Border [![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border.json)](https://github.com/copier-org/copier)\n  3. Badge Grayscale Inverted Border Orange [![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-orange.json)](https://github.com/copier-org/copier)\n  4. Badge Grayscale Inverted Border Red [![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-red.json)](https://github.com/copier-org/copier)\n  5. Badge Grayscale Inverted Border Teal [![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-teal.json)](https://github.com/copier-org/copier)\n  6. Badge Grayscale Inverted Border Purple [![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-grayscale-inverted-border-purple.json)](https://github.com/copier-org/copier)\n  7. Badge Black [![Copier](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/copier-org/copier/master/img/badge/badge-black.json)](https://github.com/copier-org/copier)\n\n\n## Credits[\u00b6](https://copier.readthedocs.io/en/stable/#credits \"Permanent link\")\nSpecial thanks go to [jpsca](https://github.com/jpsca) for originally creating `Copier`. This project would not be a thing without him.\nMany thanks to [pykong](https://github.com/pykong) who took over maintainership on the project, promoted it, and laid out the bases of what the project is today.\nBig thanks also go to [yajo](https://github.com/yajo) for his relentless zest for improving `Copier` even further.\nThanks a lot, [pawamoy](https://github.com/pawamoy) for polishing very important rough edges and improving the documentation and UX a lot.\nAlso special thanks to [sisp](https://github.com/sisp) for being very helpful in polishing documentation, fixing bugs, helping the community and cleaning up the codebase.\nAnd thanks to all financial supporters and folks that give us a shiny star! \u2b50\n[ ![Star History Chart](https://api.star-history.com/svg?repos=copier-org/copier&type=Date) ](https://star-history.com/#copier-org/copier&Date)\nBack to top \nMade with [ Material for MkDocs ](https://squidfunk.github.io/mkdocs-material/)\n",
    "answer": "```markdown\n# TL;DR Summary of Copier Documentation\n\n**Copier** is a library and CLI tool for rendering project templates, supporting local paths and Git URLs. It allows dynamic value replacement in text files, facilitating project scaffolding and lifecycle management. Installation requires Python 3.9+ and Git 2.27+. Key concepts include templates, questionnaires, and projects. \n\nFor more details, visit the [Copier Documentation](https://copier.readthedocs.io/en/stable/).\n```"
  },
  {
    "instruction": "[Jump to Content](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN/#page-content)\n[ Research ](https://research.google/ \"Google Research\")\n[ Research ](https://research.google/ \"Google Research\")\n  * Who we are \nBack to Who we are menu\n## Defining the technology of today and tomorrow.\n    * ##  Philosophy \nWe strive to create an environment conducive to many different types of research across many different time scales and levels of risk.\n[ Learn more about our Philosophy Learn more ](https://research.google/philosophy/)\n[ Philosophy ](https://research.google/philosophy/)\n    * ##  People \nOur researchers drive advancements in computer science through both fundamental and applied research.\n[ Learn more about our People Learn more ](https://research.google/people/)\n[ People ](https://research.google/people/)\n  * Research areas \nBack to Research areas menu\n    * ## Research areas\n      * [ Explore all research areas ](https://research.google/research-areas/)\nResearch areas \nBack to Research areas menu\n      * [ Explore all research areas ](https://research.google/research-areas/)\n    * ## Foundational ML & Algorithms\n      * [ Algorithms & Theory ](https://research.google/research-areas/algorithms-and-theory/)\n      * [ Data Management ](https://research.google/research-areas/data-management/)\n      * [ Data Mining & Modeling ](https://research.google/research-areas/data-mining-and-modeling/)\n      * [ Information Retrieval & the Web ](https://research.google/research-areas/information-retrieval-and-the-web/)\n      * [ Machine Intelligence ](https://research.google/research-areas/machine-intelligence/)\n      * [ Machine Perception ](https://research.google/research-areas/machine-perception/)\n      * [ Machine Translation ](https://research.google/research-areas/machine-translation/)\n      * [ Natural Language Processing ](https://research.google/research-areas/natural-language-processing/)\n      * [ Speech Processing ](https://research.google/research-areas/speech-processing/)\nFoundational ML & Algorithms \nBack to Foundational ML & Algorithms menu\n      * [ Algorithms & Theory ](https://research.google/research-areas/algorithms-and-theory/)\n      * [ Data Management ](https://research.google/research-areas/data-management/)\n      * [ Data Mining & Modeling ](https://research.google/research-areas/data-mining-and-modeling/)\n      * [ Information Retrieval & the Web ](https://research.google/research-areas/information-retrieval-and-the-web/)\n      * [ Machine Intelligence ](https://research.google/research-areas/machine-intelligence/)\n      * [ Machine Perception ](https://research.google/research-areas/machine-perception/)\n      * [ Machine Translation ](https://research.google/research-areas/machine-translation/)\n      * [ Natural Language Processing ](https://research.google/research-areas/natural-language-processing/)\n      * [ Speech Processing ](https://research.google/research-areas/speech-processing/)\n    * ## Computing Systems & Quantum AI\n      * [ Distributed Systems & Parallel Computing ](https://research.google/research-areas/distributed-systems-and-parallel-computing/)\n      * [ Hardware & Architecture ](https://research.google/research-areas/hardware-and-architecture/)\n      * [ Mobile Systems ](https://research.google/research-areas/mobile-systems/)\n      * [ Networking ](https://research.google/research-areas/networking/)\n      * [ Quantum Computing ](https://research.google/research-areas/quantum-computing/)\n      * [ Robotics ](https://research.google/research-areas/robotics/)\n      * [ Security, Privacy, & Abuse Prevention ](https://research.google/research-areas/security-privacy-and-abuse-prevention/)\n      * [ Software Engineering ](https://research.google/research-areas/software-engineering/)\n      * [ Software Systems ](https://research.google/research-areas/software-systems/)\nComputing Systems & Quantum AI \nBack to Computing Systems & Quantum AI menu\n      * [ Distributed Systems & Parallel Computing ](https://research.google/research-areas/distributed-systems-and-parallel-computing/)\n      * [ Hardware & Architecture ](https://research.google/research-areas/hardware-and-architecture/)\n      * [ Mobile Systems ](https://research.google/research-areas/mobile-systems/)\n      * [ Networking ](https://research.google/research-areas/networking/)\n      * [ Quantum Computing ](https://research.google/research-areas/quantum-computing/)\n      * [ Robotics ](https://research.google/research-areas/robotics/)\n      * [ Security, Privacy, & Abuse Prevention ](https://research.google/research-areas/security-privacy-and-abuse-prevention/)\n      * [ Software Engineering ](https://research.google/research-areas/software-engineering/)\n      * [ Software Systems ](https://research.google/research-areas/software-systems/)\n    * ## Science, AI & Society\n      * [ Climate & Sustainability ](https://research.google/research-areas/climate-and-sustainability/)\n      * [ Economics & Electronic Commerce ](https://research.google/research-areas/economics-and-electronic-commerce/)\n      * [ Education Innovation ](https://research.google/research-areas/education-innovation/)\n      * [ General Science ](https://research.google/research-areas/general-science/)\n      * [ Health & Bioscience ](https://research.google/research-areas/health-bioscience/)\n      * [ Human-Computer Interaction and Visualization ](https://research.google/research-areas/human-computer-interaction-and-visualization/)\nScience, AI & Society \nBack to Science, AI & Society menu\n      * [ Climate & Sustainability ](https://research.google/research-areas/climate-and-sustainability/)\n      * [ Economics & Electronic Commerce ](https://research.google/research-areas/economics-and-electronic-commerce/)\n      * [ Education Innovation ](https://research.google/research-areas/education-innovation/)\n      * [ General Science ](https://research.google/research-areas/general-science/)\n      * [ Health & Bioscience ](https://research.google/research-areas/health-bioscience/)\n      * [ Human-Computer Interaction and Visualization ](https://research.google/research-areas/human-computer-interaction-and-visualization/)\n  * Our work \nBack to Our work menu\n    * ##  Projects \nWe regularly open-source projects with the broader research community and apply our developments to Google products.\n[ Learn more about our Projects Learn more ](https://research.google/resources/our-projects/)\n[ Projects ](https://research.google/resources/our-projects/)\n    * ##  Publications \nPublishing our work allows us to share ideas and work collaboratively to advance the field of computer science.\n[ Learn more about our Publications Learn more ](https://research.google/pubs/)\n[ Publications ](https://research.google/pubs/)\n    * ##  Resources \nWe make products, tools, and datasets available to everyone with the goal of building a more collaborative ecosystem.\n[ Learn more about our Resources Learn more ](https://research.google/resources/)\n[ Resources ](https://research.google/resources/)\n  * Programs & events \nBack to Programs & events menu\n## Shaping the future, together.\n[ Collaborate with us ](https://research.google/programs-and-events/)\n    * ##  Student programs \nSupporting the next generation of researchers through a wide range of programming.\n[ Learn more about our Student programs Learn more ](https://research.google/programs-and-events/student-engagement/)\n[ Student programs ](https://research.google/programs-and-events/student-engagement/)\n    * ##  Faculty programs \nParticipating in the academic research community through meaningful engagement with university faculty.\n[ Learn more about our Faculty programs Learn more ](https://research.google/programs-and-events/faculty-engagement/)\n[ Faculty programs ](https://research.google/programs-and-events/faculty-engagement/)\n    * ##  Conferences & events \nConnecting with the broader research community through events is essential for creating progress in every aspect of our work.\n[ Learn more about our Conferences & events Learn more ](https://research.google/conferences-and-events/)\n[ Conferences & events ](https://research.google/conferences-and-events/)\n[ Collaborate with us ](https://research.google/programs-and-events/)\n  * [ Careers  ](https://research.google/careers/)\n  * [ Blog  ](https://research.google/blog/)\n\n\nSearch\n![](https://storage.googleapis.com/gweb-research2023-media/original_images/cca912e7fbe652676302383247087e22-Screen20Shot202022-11-0820at208.53.4920AM.png)\n  1. [Home](https://research.google/)\n  2. [Blog](https://research.google/blog/)\n\n\n# ReAct: Synergizing Reasoning and Acting in Language Models\nNovember 8, 2022\nPosted by Shunyu Yao, Student Researcher, and Yuan Cao, Research Scientist, Google Research, Brain Team\n## Quick links\n  * Share\n    * [ ](https://twitter.com/intent/tweet?text=https%3A//research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/ \"Share on Twitter\")\n    * [ ](https://www.facebook.com/sharer/sharer.php?u=https%3A//research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/ \"Share on Facebook\")\n    * [ ](https://www.linkedin.com/shareArticle?url=https%3A//research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/&mini=true \"Share on LinkedIn\")\n    *     * Copy link\n\u00d7 \n\n\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuuYg9Pduep9GkUfjloNVOiy3qjpPbT017GKlgGEGMaLNu_TCheEeJ7r8Qok6-0BK3KMfLvsN2vSgFQ8xOvnHM9CAb4Ix4I62bcN2oXFWfqAJzGAGbVqbeCyVktu3h9Dyf5ameRe54LEr32Emp0nG52iofpNOTXCxMY12K7fvmDZNPPmfJaT5zo1OBQA/s16000/Screen%20Shot%202022-11-08%20at%208.53.49%20AM.png)\nRecent advances have expanded the applicability of language models (LM) to downstream tasks. On one hand, existing language models that are properly prompted, via [chain-of-thought](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html), demonstrate emergent capabilities that carry out self-conditioned reasoning traces to derive answers from questions, excelling at various arithmetic, commonsense, and symbolic reasoning tasks. However, with chain-of-thought prompting, a model is not grounded in the external world and uses its own internal representations to generate reasoning traces, limiting its ability to reactively explore and reason or update its knowledge. On the other hand, recent work uses pre-trained language models for planning and acting in various interactive environments (e.g., [text games](https://arxiv.org/pdf/2010.02903.pdf), [web navigation](https://arxiv.org/pdf/2112.09332.pdf), [embodied tasks](https://arxiv.org/pdf/2201.07207.pdf), [robotics](https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html)), with a focus on mapping text contexts to text actions via the language model\u2019s internal knowledge. However, they do not reason abstractly about high-level goals or maintain a working memory to support acting over long horizons. \nIn \u201c[ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629.pdf)\u201d, we propose a general paradigm that combines reasoning and acting advances to enable language models to solve various language reasoning and decision making tasks. We demonstrate that the _Reason+Act_(ReAct) paradigm systematically outperforms reasoning and acting only paradigms, when prompting bigger language models and fine-tuning smaller language models. The tight integration of reasoning and acting also presents human-aligned task-solving trajectories that improve interpretability, diagnosability, and controllability.. \n## Model Overview \nReAct enables language models to generate both verbal reasoning traces and text actions in an interleaved manner. While actions lead to observation feedback from an external environment (\u201cEnv\u201d in the figure below), reasoning traces do not affect the external environment. Instead, they affect the internal state of the model by reasoning over the context and updating it with useful information to support future reasoning and acting. \n[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuuYg9Pduep9GkUfjloNVOiy3qjpPbT017GKlgGEGMaLNu_TCheEeJ7r8Qok6-0BK3KMfLvsN2vSgFQ8xOvnHM9CAb4Ix4I62bcN2oXFWfqAJzGAGbVqbeCyVktu3h9Dyf5ameRe54LEr32Emp0nG52iofpNOTXCxMY12K7fvmDZNPPmfJaT5zo1OBQA/s16000/Screen%20Shot%202022-11-08%20at%208.53.49%20AM.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuuYg9Pduep9GkUfjloNVOiy3qjpPbT017GKlgGEGMaLNu_TCheEeJ7r8Qok6-0BK3KMfLvsN2vSgFQ8xOvnHM9CAb4Ix4I62bcN2oXFWfqAJzGAGbVqbeCyVktu3h9Dyf5ameRe54LEr32Emp0nG52iofpNOTXCxMY12K7fvmDZNPPmfJaT5zo1OBQA/s595/Screen%20Shot%202022-11-08%20at%208.53.49%20AM.png)  \n---  \nPrevious methods prompt language models (LM) to either generate self-conditioned reasoning traces or task-specific actions. We propose ReAct, a new paradigm that combines reasoning and acting advances in language models.  \n## ReAct Prompting\nWe focus on the setup where a frozen language model, [PaLM-540B](https://arxiv.org/pdf/2204.02311.pdf), is prompted with few-shot in-context examples to generate both domain-specific actions (e.g., \u201csearch\u201d in question answering, and \u201cgo to\u201d in room navigation), and free-form language reasoning traces (e.g., \u201cNow I need to find a cup, and put it on the table\u201d) for task solving. \nFor tasks where reasoning is of primary importance, we alternate the generation of reasoning traces and actions so that the task-solving trajectory consists of multiple reasoning-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions, reasoning traces only need to appear sparsely in the most relevant positions of a trajectory, so we write prompts with sparse reasoning and let the language model decide the asynchronous occurrence of reasoning traces and actions for itself. \nAs shown below, there are various types of useful reasoning traces, e.g., decomposing task goals to create action plans, injecting commonsense knowledge relevant to task solving, extracting important parts from observations, tracking task progress while maintaining plan execution, handling exceptions by adjusting action plans, and so on. \nThe synergy between reasoning and acting allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interacting with the external environments (e.g., Wikipedia) to incorporate additional information into reasoning (act to reason). \n## ReAct Fine-tuning \nWe also explore fine-tuning smaller language models using ReAct-format trajectories. To reduce the need for large-scale human annotation, we use the ReAct prompted PaLM-540B model to generate trajectories, and use trajectories with task success to fine-tune smaller language models (PaLM-8/62B). \n[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoAazr9qsoobs5Nkp7_uxjml4AEWA9iwUfoNfJpcJEnj2ZOdrTXptaf9R2CyRK7Qif64zcPbywR6AeIOaeZs19vQ7OH6n-6vEyh1exiHXC965OSoNX4bsGjuIZ3Po9CuJb-LhDYyYTQr1rZum-FZ285gi11jsuiAG58C8MzifUPj8VCC_-2N3k3Fsosg/s16000/HotPotQA.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoAazr9qsoobs5Nkp7_uxjml4AEWA9iwUfoNfJpcJEnj2ZOdrTXptaf9R2CyRK7Qif64zcPbywR6AeIOaeZs19vQ7OH6n-6vEyh1exiHXC965OSoNX4bsGjuIZ3Po9CuJb-LhDYyYTQr1rZum-FZ285gi11jsuiAG58C8MzifUPj8VCC_-2N3k3Fsosg/s776/HotPotQA.png)  \n---  \nComparison of four prompting methods, (a) Standard, (b) Chain of thought (CoT, Reason Only), (c) Act-only, and (d) ReAct, solving a [HotpotQA](https://arxiv.org/abs/1809.09600) question. In-context examples are omitted, and only the task trajectory is shown. ReAct is able to retrieve information to support reasoning, while also using reasoning to target what to retrieve next, demonstrating a synergy of reasoning and acting.  \n## Results \nWe conduct empirical evaluations of ReAct and state-of-the-art baselines across four different benchmarks: question answering (HotPotQA), fact verification ([Fever](https://arxiv.org/abs/1803.05355)), text-based game ([ALFWorld](https://arxiv.org/abs/2010.03768)), and web page navigation ([WebShop](https://arxiv.org/abs/2207.01206)). For HotPotQA and Fever, with access to a [Wikipedia API](https://en.wikipedia.org/api/rest_v1/) with which the model can interact, ReAct outperforms vanilla action generation models while being competitive with chain of thought reasoning (CoT) performance. The approach with the best results is a combination of ReAct and CoT that uses both internal knowledge and externally obtained information during reasoning. \n**HotpotQA (exact match, 6-shot)** | **FEVER (accuracy, 3-shot)**  \n---|---  \nStandard  | 28.7  | 57.1   \nReason-only (CoT)  | 29.4  | 56.3   \nAct-only  | 25.7  | 58.9   \nReAct  | 27.4  | 60.9   \nBest ReAct + CoT Method  | **35.1** | **64.6**  \nSupervised SoTA  | 67.5 (using ~140k samples)  | 89.5 (using ~90k samples)   \nPaLM-540B prompting results on HotpotQA and Fever.  \n---  \nOn ALFWorld and WebShop, ReAct with both one-shot and two-shot prompting outperforms imitation and reinforcement learning methods trained with ~105 task instances, with an absolute improvement of 34% and 10% in success rates, respectively, over existing baselines. \n**AlfWorld (2-shot)** | **WebShop (1-shot)**  \n---|---  \nAct-only  | 45  | 30.1   \nReAct  | **71** | **40**  \nImitation Learning Baselines  | 37 (using ~100k samples)  | 29.1 (using ~90k samples)   \nPaLM-540B prompting task success rate results on AlfWorld and WebShop.  \n---  \n[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_0lCKXSvFq4yyjM5PGdM27OF6LWco9qFGQS1dwa3DtEF8AnAuXg9Q_nPDVyAArYwl9sGsB000-iuKJuSsNjo--fi1ZCJbrj-KwsZ6M569nWg-h2xRGHkdvQobUY9RiIr4MYkathIFyiAHZSnHAwVUfeijU-tCLyaHRgqXQah1XObtE71a00IbGdywVw/s16000/image1.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_0lCKXSvFq4yyjM5PGdM27OF6LWco9qFGQS1dwa3DtEF8AnAuXg9Q_nPDVyAArYwl9sGsB000-iuKJuSsNjo--fi1ZCJbrj-KwsZ6M569nWg-h2xRGHkdvQobUY9RiIr4MYkathIFyiAHZSnHAwVUfeijU-tCLyaHRgqXQah1XObtE71a00IbGdywVw/s839/image1.png)  \n---  \nScaling results for prompting and fine-tuning on HotPotQA with ReAct and different baselines. ReAct consistently achieves best fine-tuning performances.  \n[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgP1HCCuyIgO9D3UQKQSKFAth_Xbtqke0UO0rVbAHYA3tmbGjC6wt_du2bEm12RxFx4uWQs1LxpqaFgmHExL8QRfnPJXHVgmy-TRU3yvsDpHa-oxiX8AzmaWsm92y0J2hxdJdsjxmvFqUyYIdLIfhlr2JOIQzuaXml5YXlrF7MxC22B6thYBl72mNMKvg/s16000/image6.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgP1HCCuyIgO9D3UQKQSKFAth_Xbtqke0UO0rVbAHYA3tmbGjC6wt_du2bEm12RxFx4uWQs1LxpqaFgmHExL8QRfnPJXHVgmy-TRU3yvsDpHa-oxiX8AzmaWsm92y0J2hxdJdsjxmvFqUyYIdLIfhlr2JOIQzuaXml5YXlrF7MxC22B6thYBl72mNMKvg/s1212/image6.png)  \n---  \n[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi41aji28YNe7jqjXOC0-bdWL6nFc6jlrVXOyVD7v15lYMEJ1JNzV-Q9V1Fh-GpX5iW_gH6CWnnvGyECHQkZF33H9E3RI-GTRKA7ZhaSPjyN2rbniob0_biOcP89qZYtGMpQiodO52CJ5iauN11aitR5brKbYIdB349vFMMwqirnZ2TdufpyHz9QbOyDA/s16000/image2.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi41aji28YNe7jqjXOC0-bdWL6nFc6jlrVXOyVD7v15lYMEJ1JNzV-Q9V1Fh-GpX5iW_gH6CWnnvGyECHQkZF33H9E3RI-GTRKA7ZhaSPjyN2rbniob0_biOcP89qZYtGMpQiodO52CJ5iauN11aitR5brKbYIdB349vFMMwqirnZ2TdufpyHz9QbOyDA/s1216/image2.png)  \nA comparison of the ReAct (**top**) and CoT (**bottom**) reasoning trajectories on an example from Fever (observation for ReAct is omitted to reduce space). In this case ReAct provided the right answer, and it can be seen that the reasoning trajectory of ReAct is more grounded on facts and knowledge, in contrast to CoT\u2019s hallucination behavior.  \n---  \nWe also explore human-in-the-loop interactions with ReAct by allowing a human inspector to edit ReAct\u2019s reasoning traces. We demonstrate that by simply replacing a hallucinating sentence with inspector hints, ReAct can change its behavior to align with inspector edits and successfully complete a task. Solving tasks becomes significantly easier when using ReAct as it only requires the manual editing of a few thoughts, which enables new forms of human-machine collaboration. \n[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgORrqQ_PMp1JiljcjCXK3BqVHFR5kJ1mUxISgURlkRa6RH2fCaP3HT6rALL453TM_wD3wyKhJrfAlqlgG6jEU-RsvQsNfb02PNzqgvDLwK1XyZPaaFyc9dGRzkQzLcGGWitXzf2Mthf3YymP-0w09-pxMJxrCScFIfKxDAyFUWQCV7tR8YGGeuiNqiKA/s16000/AlfWorld.png)](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgORrqQ_PMp1JiljcjCXK3BqVHFR5kJ1mUxISgURlkRa6RH2fCaP3HT6rALL453TM_wD3wyKhJrfAlqlgG6jEU-RsvQsNfb02PNzqgvDLwK1XyZPaaFyc9dGRzkQzLcGGWitXzf2Mthf3YymP-0w09-pxMJxrCScFIfKxDAyFUWQCV7tR8YGGeuiNqiKA/s790/AlfWorld.png)  \n---  \nA human-in-the-loop behavior correction example with ReAct on AlfWorld. (a) ReAct trajectory fails due to a hallucinating reasoning trace (Act 17). (b) A human inspector edits two reasoning traces (Act 17, 23), ReAct then produces desirable reasoning traces and actions to complete the task.  \n## Conclusion\nWe present ReAct, a simple yet effective method for synergizing reasoning and acting in language models. Through various experiments that focus on multi-hop question-answering, fact checking, and interactive decision-making tasks, we show that ReAct leads to superior performance with interpretable decision traces. \nReAct demonstrates the feasibility of jointly modeling thought, actions and feedback from the environment within a language model, making it a versatile agent that is capable of solving tasks that require interactions with the environment. We plan to further extend this line of research and leverage the strong potential of the language model for tackling broader embodied tasks, via approaches like massive multitask training and coupling ReAct with equally strong reward models. \n## Acknowledgements\n_We would like to thank Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran and Karthik Narasimhan for their great contribution in this work. We would also like to thank Google\u2019s Brain team and the Princeton NLP Group for their joint support and feedback, including project scoping, advising and insightful discussions._\n## Quick links\n  * Share\n    * [ ](https://twitter.com/intent/tweet?text=https%3A//research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/ \"Share on Twitter\")\n    * [ ](https://www.facebook.com/sharer/sharer.php?u=https%3A//research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/ \"Share on Facebook\")\n    * [ ](https://www.linkedin.com/shareArticle?url=https%3A//research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/&mini=true \"Share on LinkedIn\")\n    *     * Copy link\n\u00d7 \n\n\nFollow us \n  * [ ](https://twitter.com/GoogleAI \"Follow us on x\")\n  * [ ](https://www.linkedin.com/showcase/googleresearch/ \"Follow us on linkedin\")\n  * [ ](https://www.youtube.com/c/GoogleResearch \"Follow us on youtube\")\n  * [ ](https://github.com/google-research \"Follow us on github\")\n\n\n[ ](https://www.google.com \"Google\")\n  * [ About Google ](https://about.google/)\n  * [ Google Products ](https://about.google/intl/en/products/)\n  * [ Privacy ](https://policies.google.com/privacy)\n  * [ Terms ](https://policies.google.com/terms)\n\n\n  * [ Help ](https://support.google.com/?hl=en)\n  * Submit feedback \n\n\n",
    "answer": "```markdown\n# TL;DR: ReAct - Synergizing Reasoning and Acting in Language Models\n\nReAct combines reasoning and acting in language models, outperforming traditional methods in tasks like question answering and decision making. It enables dynamic reasoning and human-aligned task-solving, enhancing interpretability and collaboration. Future work aims to tackle broader tasks using this paradigm.\n\n**Tags:** Generative AI, LLMs, Language Models, Google Research\n```"
  },
  {
    "instruction": "[Skip to main content](https://www.datacamp.com/blog/edge-ai/#main)\n[](https://www.datacamp.com/)[**Write for us**](https://datacamp-1.gitbook.io/community-submissions)\nEN\n[Log In](https://www.datacamp.com/users/sign_in?redirect=%2Fblog%2Fedge-ai)[Get Started](https://www.datacamp.com/users/sign_up?redirect=%2Fblog%2Fedge-ai)\nblogs\n[Blogs](https://www.datacamp.com/blog)\n[Tutorials](https://www.datacamp.com/tutorial)\n[docs](https://www.datacamp.com/doc)\n[Podcasts](https://www.datacamp.com/podcast)\n[Cheat Sheets](https://www.datacamp.com/cheat-sheet)\n[code-alongs](https://www.datacamp.com/code-along)\nCategory\nCategory\nAbout DataCamp \nLatest news about our products and team\n[Certification](https://www.datacamp.com/blog/category/certification)[DataCamp Classrooms](https://www.datacamp.com/blog/category/datacamp-classrooms)[DataCamp Donates](https://www.datacamp.com/blog/category/datacamp-donates)[For Business](https://www.datacamp.com/blog/category/for-business)[Learner Stories](https://www.datacamp.com/blog/category/learner-stories)[Life at DataCamp](https://www.datacamp.com/blog/category/life-at-datacamp)[Product News](https://www.datacamp.com/blog/category/product-news)\nCategory\nTechnologies \nDiscover content by tools and technology\n[Airflow](https://www.datacamp.com/blog/category/apache-airflow)[Alteryx](https://www.datacamp.com/blog/category/alteryx)[Artificial Intelligence](https://www.datacamp.com/blog/category/ai)[AWS](https://www.datacamp.com/blog/category/aws)[Azure](https://www.datacamp.com/blog/category/microsoft-azure)[Business Intelligence](https://www.datacamp.com/blog/category/learn-business-intelligence)[ChatGPT](https://www.datacamp.com/blog/category/chatgpt)[Databricks](https://www.datacamp.com/blog/category/databricks)[dbt](https://www.datacamp.com/blog/category/dbt)[Docker](https://www.datacamp.com/blog/category/docker)[Excel](https://www.datacamp.com/blog/category/excel)[Flink](https://www.datacamp.com/blog/category/apache-flink)[Generative AI](https://www.datacamp.com/blog/category/generative-ai)[Git](https://www.datacamp.com/blog/category/git)[Google Cloud Platform](https://www.datacamp.com/blog/category/google-cloud-platform)[Hadoop](https://www.datacamp.com/blog/category/apache-hadoop)[Java](https://www.datacamp.com/blog/category/java)[Julia](https://www.datacamp.com/blog/category/julia)[Kafka](https://www.datacamp.com/blog/category/apache-kafka)[Kubernetes](https://www.datacamp.com/blog/category/kubernetes)[Large Language Models](https://www.datacamp.com/blog/category/large-language-models)[MongoDB](https://www.datacamp.com/blog/category/mongodb)[MySQL](https://www.datacamp.com/blog/category/mysql)[NoSQL](https://www.datacamp.com/blog/category/nosql)[OpenAI](https://www.datacamp.com/blog/category/OpenAI)[Power BI](https://www.datacamp.com/blog/category/power-bi)[PySpark](https://www.datacamp.com/blog/category/pyspark)[Python](https://www.datacamp.com/blog/category/python)[R](https://www.datacamp.com/blog/category/r-programming)[Scala](https://www.datacamp.com/blog/category/scala)[Snowflake](https://www.datacamp.com/blog/category/snowflake)[Spreadsheets](https://www.datacamp.com/blog/category/spreadsheets)[SQL](https://www.datacamp.com/blog/category/sql)[Tableau](https://www.datacamp.com/blog/category/tableau)\nCategory\nTopics \nDiscover content by data science topics\n[AI for Business](https://www.datacamp.com/blog/category/ai-for-business)[Big Data](https://www.datacamp.com/blog/category/big-data)[Career Services](https://www.datacamp.com/blog/category/career-services)[Cloud](https://www.datacamp.com/blog/category/cloud)[Data Analysis](https://www.datacamp.com/blog/category/data-analysis)[Data Engineering](https://www.datacamp.com/blog/category/data-engineering)[Data Governance](https://www.datacamp.com/blog/category/data-governance)[Data Literacy](https://www.datacamp.com/blog/category/data-literacy)[Data Science](https://www.datacamp.com/blog/category/data-science)[Data Storytelling](https://www.datacamp.com/blog/category/data-storytelling)[Data Visualization](https://www.datacamp.com/blog/category/data-visualization)[DataCamp Product](https://www.datacamp.com/blog/category/datacamp-product)[DataLab](https://www.datacamp.com/blog/category/datalab)[Deep Learning](https://www.datacamp.com/blog/category/deep-learning)[Machine Learning](https://www.datacamp.com/blog/category/machine-learning)[MLOps](https://www.datacamp.com/blog/category/mlops)\n[Request a Demo](https://www.datacamp.com/business)[](https://www.datacamp.com/search-resources)\ncategory\n  1. [Home](https://www.datacamp.com)\n  2. [Blog](https://www.datacamp.com/blog)\n  3. [Artificial Intelligence](https://www.datacamp.com/blog/category/ai)\n\n\n# What Is Edge AI? How It Works, Applications, Challenges\nEdge AI runs AI algorithms directly on edge devices, like smartphones or IoT sensors, allowing real-time processing and decision-making without cloud reliance.\nList Contents\nAug 6, 2024 \u00b7 9 min read\nContents\n  * [What Is Edge AI?](https://www.datacamp.com/blog/edge-ai/#what-is-edge-ai?-<span)\n\n\n  * [How Edge AI Works](https://www.datacamp.com/blog/edge-ai/#how-edge-ai-works-<span)\n  * [Key components](https://www.datacamp.com/blog/edge-ai/#key-components-<span)\n  * [The Process](https://www.datacamp.com/blog/edge-ai/#the-process-<span)\n\n\n  * [Real-World Applications of Edge AI](https://www.datacamp.com/blog/edge-ai/#real-world-applications-of-edge-ai-<span)\n  * [Manufacturing](https://www.datacamp.com/blog/edge-ai/#manufacturing-<span)\n  * [Healthcare](https://www.datacamp.com/blog/edge-ai/#healthcare-<span)\n  * [Retail](https://www.datacamp.com/blog/edge-ai/#retail-<span)\n  * [Smart cities](https://www.datacamp.com/blog/edge-ai/#smart-cities-<span)\n\n\n  * [Edge AI: Hardware and Software Solutions](https://www.datacamp.com/blog/edge-ai/#edge-ai:-hardware-and-software-solutions-<span)\n  * [Hardware Platforms](https://www.datacamp.com/blog/edge-ai/#hardware-platforms-<span)\n  * [Software Frameworks](https://www.datacamp.com/blog/edge-ai/#software-frameworks-<span)\n\n\n  * [Challenges and Considerations](https://www.datacamp.com/blog/edge-ai/#challenges-and-considerations-<span)\n\n\n  * [Conclusion](https://www.datacamp.com/blog/edge-ai/#conclusion-<span)\n\n\nArtificial Intelligence (AI) is so powerful, but it often relies on [remote cloud servers](https://www.datacamp.com/courses/understanding-cloud-computing) for its heavy lifting. This approach can be problematic for applications needing quick responses or those on devices with limited resources.\nImagine a self-driving car waiting for a cloud server to decide which lane to switch into\u2014not ideal!\n**Edge AI solves this by bringing the AI capabilities directly to the device itself.**\n## Become an ML Scientist \nUpskill in Python to become a machine learning scientist.\n[Start Learning for Free](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)\n## What Is Edge AI?\nEdge AI is the practice of deploying AI models and algorithms directly on edge devices, which are devices located at the network's periphery, close to where data is generated and actions need to be taken.\nThese devices encompass a wide range, from powerful edge servers to resource-constrained IoT sensors, and include familiar examples like smartphones, smart home appliances, autonomous vehicles, and even industrial robots.\nRecent advancements in AI, such as the development of smaller and more efficient language models like [GPT-4o Mini](https://www.datacamp.com/blog/gpt-4o-mini), [Llama 3.1 8B](https://www.datacamp.com/blog/llama-3-1-405b-meta-ai), and [Gemma 2 2B](https://www.datacamp.com/tutorial/fine-tuning-gemma-2), are further accelerating the adoption of edge AI.\nThis shift to edge AI offers several advantages:\n  * Speed: Decisions and actions happen in real-time, right where the data is generated. This is necessary for applications like self-driving cars or medical devices that can't afford delays caused by sending data to the cloud for processing.\n  * Privacy: Sensitive data can be analyzed locally without needing to be sent to the cloud, enhancing security and privacy.\n  * Reliability: Edge AI reduces dependency on a constant internet connection, making it more reliable in areas with limited or unstable connectivity.\n  * Efficiency: Processing data locally reduces the bandwidth required to send everything to the cloud, saving both energy and costs.\n\n\n## How Edge AI Works\nTo understand how edge AI works, we need to understand its main components and the processes involved.\n### Key components\nEdge AI comprises three main components: edge devices, AI models, and communication.\n![Components of Edge AI](https://media.datacamp.com/legacy/v1723043761/image_a3d238bf5b.png)\n#### Edge devices\nEdge devices encompass a wide range of hardware, from powerful edge servers capable of handling substantial computational loads to highly resource-constrained IoT sensors designed for specific tasks.\nThese devices include smartphones, drones, autonomous vehicles, industrial robots, and smart home devices, all of which can run AI models locally to process data in real time. Some hardware manufacturers, like NVIDIA and Intel, even provide hardware support for [deploying ML models](https://www.datacamp.com/courses/mlops-deployment-and-life-cycling) on the edge.\n#### AI models\nEdge AI employs various types of AI models, including [machine learning](https://www.datacamp.com/courses/mlops-concepts), [deep learning](https://www.datacamp.com/tracks/deep-learning-in-python), and computer vision algorithms, optimized for efficient execution on edge devices.\nThese models are tailored to operate within the constraints of edge environments, ensuring that they can perform effectively despite limited processing power, memory, and storage.\nFor instance, models like GPT-4o Mini and Llama 3.1 8B are specifically designed to be lightweight and efficient, making them suitable for edge deployments.\n#### Communication\nEfficient data exchange between edge devices and the cloud is facilitated by communication protocols such as MQTT and REST APIs. These protocols enable seamless connectivity and data transfer, allowing synchronized operations between edge devices and central systems when necessary. These protocols allow the transfer of information in compressed form in an almost lossless fashion to keep the crucial information intact.\nMQTT (Message Queuing Telemetry Transport)is a lightweight messaging protocol designed for constrained devices and low-bandwidth, high-latency, or unreliable networks. It uses a publish-subscribe model, allowing devices to send (publish) and receive (subscribe) messages without needing a direct connection. This makes MQTT ideal for IoT applications where devices need to communicate efficiently and reliably.\nREST API (Representational State Transfer Application Programming Interface)is an architectural style for designing networked applications. It uses HTTP requests to access and use data. REST APIs are stateless, meaning each call from a client to a server must contain all the information the server needs to fulfill that request. This makes REST APIs scalable and suitable for various web services and applications, including those involving edge devices.\n### The Process\nThe workflow that powers edge AI involves three steps: data collection, data processing, and action.\n![Edge AI processes](https://media.datacamp.com/legacy/v1723043760/image_51de06a284.png)\n#### Data collection\nEdge devices continuously collect data from sensors, cameras, or other sources, providing a steady stream of information. This data can range from environmental metrics and health parameters to video feeds and audio recordings, forming the basis for real-time analysis.\nA great example of data collection is how your smartwatch collects the number of steps you took today.\n#### Data processing\nAI models deployed on edge devices process the collected data locally. This step involves analyzing the data to extract meaningful insights, detect patterns, and make predictions using AI models without relying on cloud resources.\nLocal processing ensures that decisions can be made quickly, such as a self-driving car determining which lane to choose in real-time.\n#### Real-Time Action\nBased on the AI model's output, edge devices can take immediate action. These actions might include triggering alarms, adjusting the path, or sending data to the cloud for further analysis. The ability to act in real-time is essential for scenarios requiring instant responses, such as security systems or medical devices.\n## Real-World Applications of Edge AI\nEdge AI is transforming various industries by enabling real-time intelligence and decision-making. Let\u2019s explore a few notable applications.\n### Manufacturing\nIn manufacturing, equipment downtime can be costly. Edge AI addresses this by monitoring equipment health and predicting potential failures before they occur. By analyzing data from sensors in real time, AI models can detect anomalies and alert maintenance teams to take preventive actions. This not only reduces downtime but also extends the lifespan of machinery. \nEnsuring product quality is paramount in manufacturing. AI-powered cameras equipped with edge AI can inspect products for defects in real time. These systems analyze visual data to identify flaws such as scratches, dents, or incorrect assembly. By automating the inspection process, manufacturers can achieve higher accuracy, consistency, and efficiency, ultimately enhancing product quality and customer satisfaction.\n### Healthcare\nThe healthcare industry is benefiting significantly from Edge AI. Portable devices equipped with edge AI can analyze medical images such as X-rays, MRIs, and CT scans, providing faster diagnoses. This capability is particularly valuable in remote or underserved areas where access to specialized radiologists may be limited. By processing images locally, edge AI reduces the time needed for diagnosis, enabling timely treatment and improving patient outcomes.\nWearable devices with edge AI are revolutionizing patient care by enabling continuous monitoring of health parameters. These devices collect data such as heart rate, blood pressure, and glucose levels, analyzing it in real-time to detect anomalies. If a critical condition is identified, the device can alert healthcare providers immediately. This proactive approach to patient monitoring helps manage chronic diseases, detect health issues early, and reduce hospital visits.\n### Retail\nEfficient inventory management is crucial for retail businesses. AI-powered cameras and sensors can track inventory levels in real time, ensuring shelves are always stocked. By analyzing data from these devices, edge AI can optimize stock replenishment, reduce waste, and prevent stockouts. This leads to improved customer satisfaction and lower inventory costs.\nUnderstanding customer behavior is key to delivering personalized shopping experiences. Edge AI analyzes data from in-store cameras and sensors to gain insights into customer preferences and behavior. Based on this analysis, it can deliver tailored recommendations and promotions to individual shoppers. Personalization enhances the shopping experience, increases customer loyalty, and boosts sales.\n### Smart cities\nManaging urban traffic is a complex task that requires real-time data analysis. Edge AI can optimize traffic flow by analyzing data from traffic cameras, sensors, and GPS devices. By detecting congestion patterns and predicting traffic conditions, it can adjust traffic signals, reroute vehicles, and provide real-time traffic updates to drivers. This improves traffic efficiency, reduces travel time, and enhances road safety.\nEnsuring public safety is a top priority for smart cities. AI-powered surveillance systems equipped with edge AI can monitor public spaces, detect anomalies, and identify potential threats. These systems analyze video feeds in real time, recognizing suspicious activities such as unauthorized access or unattended bags. By alerting authorities promptly, edge AI enhances security and enables rapid response to incidents.\n## Edge AI: Hardware and Software Solutions\nImplementing Edge AI requires the right combination of hardware and software solutions. Here are some popular options:\n![NVIDIA Jetson, Google Coral, Intel Movidius](https://media.datacamp.com/legacy/v1723043760/image_9e910d479d.png)\n### Hardware Platforms\nTo successfully implement Edge AI, you'll need suitable hardware platforms capable of running AI models efficiently at the edge. Let\u2019s explore a few options:\n  * NVIDIA Jetson:[NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/) is a powerful platform for AI at the edge, offering high-performance computing capabilities. It is widely used in applications such as robotics, smart cities, and healthcare. Jetson devices provide the processing power needed to run complex AI models locally, making them ideal for edge AI deployments.\n  * Google Coral:[Google Coral](https://coral.ai/) devices are designed for fast and efficient edge AI deployments. They provide accelerated machine learning inferencing capabilities, making them suitable for various applications, including computer vision and IoT. Coral devices are known for their ease of use and integration with popular AI frameworks.\n  * Intel Movidius: [Intel Movidius](https://www.intel.com/content/www/us/en/products/details/processors/movidius-vpu.html) is renowned for its energy-efficient performance, making it ideal for vision-based applications. It supports deep learning and computer vision workloads on edge devices, enabling real-time processing with minimal power consumption. Movidius chips are commonly used in drones, security cameras, and smart home devices.\n\n\n### Software Frameworks\nIn addition to hardware, robust software frameworks are crucial for developing and deploying Edge AI applications. Let\u2019s explore a few options:\n  * TensorFlow Lite: TensorFlow Lite is a lightweight version of [TensorFlow](https://www.datacamp.com/courses/introduction-to-tensorflow-in-python), optimized for mobile and edge devices. It allows developers to deploy machine learning models on resource-constrained devices efficiently. TensorFlow Lite supports a wide range of AI applications, from image classification to natural language processing.\n  * PyTorch Mobile: PyTorch Mobile enables the deployment of [PyTorch](https://www.datacamp.com/courses/introduction-to-deep-learning-with-pytorch) models on mobile and edge platforms. It supports a variety of AI tasks, including computer vision, speech recognition, and recommendation systems. PyTorch Mobile is known for its flexibility and ease of integration with existing PyTorch workflows.\n  * ONNX (Open Neural Network Exchange): ONNX is an open-source format for AI models, allowing models to be transferred between different frameworks. It is particularly useful for edge AI because it enables the use of pre-trained models across various platforms, including TensorFlow, PyTorch, and others. ONNX Runtime is optimized for performance on edge devices, ensuring efficient execution of models.\n  * Edge Impulse: Edge Impulse is a platform specifically designed for edge AI development. It offers tools for data collection, model training, and deployment, simplifying the process of creating and deploying edge AI solutions. Edge Impulse supports a wide range of hardware platforms and provides an intuitive interface for developers.\n\n\n## Challenges and Considerations\nWhile Edge AI offers numerous benefits, it also presents several challenges that need to be addressed:\n  * Limited resources: Running complex AI models on devices with limited processing power, memory, and storage is challenging. Developers must optimize models to balance performance and resource consumption, ensuring efficient execution on edge devices. Techniques such as [model quantization](https://www.datacamp.com/tutorial/quantization-for-large-language-models), pruning, and [knowledge distillation](https://www.datacamp.com/blog/distillation-llm) are commonly used to reduce the computational load and memory footprint of AI models.\n  * Power constraints: Edge devices, especially battery-powered ones, require energy-efficient AI algorithms and hardware. Ensuring long battery life while maintaining performance is crucial for the widespread adoption of Edge AI. Techniques such as low-power hardware design, [energy-efficient algorithms](https://www.datacamp.com/tutorial/big-o-notation-time-complexity), and dynamic power management are essential for minimizing power consumption without compromising on performance.\n  * Data security and privacy: Processing sensitive data at the edge necessitates robust security measures. Protecting data and ensuring compliance with regulations like GDPR are critical for maintaining user trust and preventing data breaches. Techniques such as encryption, secure boot, and trusted execution environments are commonly used to enhance data security and privacy in edge AI deployments.\n\n\n## Conclusion\nEdge AI represents a significant shift in how we deploy and utilize artificial intelligence.\nBy bringing computation closer to data sources, Edge AI addresses the limitations of cloud-based AI, such as latency, privacy concerns, and bandwidth costs.\nAdvancements in hardware and software, including the development of smaller, more efficient AI models, are driving the growth of Edge AI across various industries.\nEdge AI promises a future where intelligent devices and applications seamlessly integrate into our daily lives, making them faster, safer, and more responsive.\n## Earn a Top AI Certification\nDemonstrate you can effectively and responsibly use AI.\n[Get Certified, Get Hired](https://www.datacamp.com/certification/ai-fundamentals)\n![Bhavishya Pandit's photo](https://media.datacamp.com/legacy/v1725454467/Unknown_f0f804ba73.jpg?w=128)\nAuthor\nBhavishya Pandit\n[LinkedIn](https://www.linkedin.com/in/bhavishya-pandit)[Twitter](https://twitter.com/BhavishyaP9)\nSenior GenAI Engineer and Content Creator who has garnered 20 million views by sharing knowledge on GenAI and data science.\nTopics\n[Artificial Intelligence](https://www.datacamp.com/blog/category/ai)\n![Bhavishya Pandit's photo](https://media.datacamp.com/legacy/v1725454467/Unknown_f0f804ba73.jpg?w=128)\nBhavishya PanditSenior GenAI Engineer | Content Creator\nTopics\n[Artificial Intelligence](https://www.datacamp.com/blog/category/ai)\n### [AI Chips Explained: How AI Chips Work, Industry Trends, Applications](https://www.datacamp.com/blog/ai-chips)\n### [AI in Sports: Applications and Real-World Examples](https://www.datacamp.com/blog/ai-in-sports-use-cases)\n![](https://media.datacamp.com/legacy/v1706534939/datarhys_an_absurdist_oil_painting_of_a_human_looking_at_itself_1a844497_7f22_4ee7_897c_95e10dbf5bf8_997281dfd0.png?w=256)\n### [What is AI? A Quick-Start Guide For Beginners](https://www.datacamp.com/blog/what-is-ai-quick-start-guide-for-beginners)\n### [AI in Pharmaceuticals: Benefits, Challenges, and Insights](https://www.datacamp.com/blog/ai-in-pharmaceuticals)\n### [What is Narrow AI?](https://www.datacamp.com/blog/what-is-narrow-ai)\n![DataFramed 100- Daniel Situnayake.png](https://media.datacamp.com/legacy/v1660564513/Data_Framed_100_Daniel_Situnayake_620c069cf7.png?w=256)\n### [Embedded Machine Learning on Edge Devices](https://www.datacamp.com/podcast/embedded-machine-learning-on-edge-devices)\nLearn AI with these courses!\nCourse\n### [Fully Automated MLOps](https://www.datacamp.com/courses/fully-automated-mlops)\n4 hr\n2.7K\nLearn about MLOps architecture, CI/CD/CM/CT techniques, and automation patterns to deploy ML systems that can deliver value over time.\n[See DetailsRight Arrow](https://www.datacamp.com/courses/fully-automated-mlops)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Fcourses%2Ffully-automated-mlops%2Fcontinue)\nTrack\n### [Developing AI Applications](https://www.datacamp.com/tracks/developing-ai-applications)\n23hrs hr\nLearn to create AI-powered applications with the latest AI developer tools, including the OpenAI API, Hugging Face, and LangChain.\n[See DetailsRight Arrow](https://www.datacamp.com/tracks/developing-ai-applications)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Ftracks%2Fdeveloping-ai-applications%2Fcontinue)\nTrack\n### [Artificial Intelligence (AI) Leadership](https://www.datacamp.com/tracks/artificial-intelligence-ai-leadership)\n6 hours hr\nMaster AI leadership with this track: monetize AI, build trust with Responsible and Explainable AI, and secure your innovations. Lead with confidence!\n[See DetailsRight Arrow](https://www.datacamp.com/tracks/artificial-intelligence-ai-leadership)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Ftracks%2Fartificial-intelligence-ai-leadership%2Fcontinue)\n[See MoreRight Arrow](https://www.datacamp.com/category/artificial-intelligence)\nRelated\n[blogAI Chips Explained: How AI Chips Work, Industry Trends, Applications](https://www.datacamp.com/blog/ai-chips)\nAI chips are specialized processors designed to accelerate the execution of artificial intelligence tasks, typically involving large-scale matrix operations and parallel processing.\n![Bhavishya Pandit's photo](https://media.datacamp.com/legacy/v1725454467/Unknown_f0f804ba73.jpg?w=48)\nBhavishya Pandit \n7 min\n[blogAI in Sports: Applications and Real-World Examples](https://www.datacamp.com/blog/ai-in-sports-use-cases)\nThis article examines the diverse applications of artificial intelligence (AI) in sports, from optimizing athlete training and recovery to enhancing game strategies and decision-making.\n[![Tom Farnschl\u00e4der's photo](https://media.datacamp.com/legacy/v1719243659/1698334986991_adfcb9390d.jpg?w=48)](https://www.datacamp.com/portfolio/tomfarnschlaeder)\nTom Farnschl\u00e4der \n14 min\n![](https://media.datacamp.com/legacy/v1706534939/datarhys_an_absurdist_oil_painting_of_a_human_looking_at_itself_1a844497_7f22_4ee7_897c_95e10dbf5bf8_997281dfd0.png?w=750)\n[blogWhat is AI? A Quick-Start Guide For Beginners](https://www.datacamp.com/blog/what-is-ai-quick-start-guide-for-beginners)\nFind out what artificial intelligence really is with examples, expert input, and all the tools you need to learn more. \n[![Matt Crabtree's photo](https://media.datacamp.com/cms/matt_2.jpg?w=48)](https://www.datacamp.com/portfolio/mattcrabtree)\nMatt Crabtree \n11 min\n[blogAI in Pharmaceuticals: Benefits, Challenges, and Insights](https://www.datacamp.com/blog/ai-in-pharmaceuticals)\nExplore how AI transforms the pharmaceutical industry by learning about its applications, benefits, challenges, and future prospects in our comprehensive analysis!\n[![Maria Eugenia Inzaugarat's photo](https://media.datacamp.com/legacy/v1716472639/Maria_Eugenia_Inzaugarat_18288bb54f.png?w=48)](https://www.datacamp.com/portfolio/meugeinzaug)\nMaria Eugenia Inzaugarat \n21 min\n[blogWhat is Narrow AI?](https://www.datacamp.com/blog/what-is-narrow-ai)\nNarrow AI refers to artificial intelligence systems that are designed to perform specific tasks and operate under limited constraints.\n[![Abid Ali Awan's photo](https://media.datacamp.com/legacy/v1658155691/Abid_Ali_Awan_415cc44670.jpg?w=48)](https://www.datacamp.com/portfolio/kingabzpro)\nAbid Ali Awan \n7 min\n![DataFramed 100- Daniel Situnayake.png](https://media.datacamp.com/legacy/v1660564513/Data_Framed_100_Daniel_Situnayake_620c069cf7.png?w=750)\n[podcastEmbedded Machine Learning on Edge Devices](https://www.datacamp.com/podcast/embedded-machine-learning-on-edge-devices)\nDaniel Situnayake talks about his work with EdgeML, the biggest challenges in embedded machine learning, potential use cases of machine learning models in edge devices, and the best tips for aspiring machine learning engineers and data science practices.\n[![Richie Cotton's photo](https://media.datacamp.com/legacy/v1658155296/richie_new_barnet_be7b436d1f.jpg?w=48)](https://www.datacamp.com/portfolio/richie)\nRichie Cotton \n52 min\n[See More](https://www.datacamp.com/blog/category/ai)[See More](https://www.datacamp.com/blog/category/ai)\n## Grow your data skills with DataCamp for Mobile\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\n[Download on the App Store](https://datacamp.onelink.me/xztQ/45dozwue?deep_link_sub1=%7B%22src_url%22%3A%22https%3A%2F%2Fwww.datacamp.com%2Fblog%2Fedge-ai%22%7D)[Get it on Google Play](https://datacamp.onelink.me/xztQ/go2f19ij?deep_link_sub1=%7B%22src_url%22%3A%22https%3A%2F%2Fwww.datacamp.com%2Fblog%2Fedge-ai%22%7D)\n**Learn**\n[Learn Python](https://www.datacamp.com/blog/how-to-learn-python-expert-guide)[Learn AI](https://www.datacamp.com/blog/how-to-learn-ai)[Learn Power BI](https://www.datacamp.com/learn/power-bi)[Learn Data Engineering](https://www.datacamp.com/category/data-engineering)[Assessments](https://www.datacamp.com/signal)[Career Tracks](https://www.datacamp.com/tracks/career)[Skill Tracks](https://www.datacamp.com/tracks/skill)[Courses](https://www.datacamp.com/courses-all)[Data Science Roadmap](https://www.datacamp.com/blog/data-science-roadmap)\n**Data Courses**\n[Python Courses](https://www.datacamp.com/category/python)[R Courses](https://www.datacamp.com/category/r)[SQL Courses](https://www.datacamp.com/category/sql)[Power BI Courses](https://www.datacamp.com/category/power-bi)[Tableau Courses](https://www.datacamp.com/category/tableau)[Alteryx Courses](https://www.datacamp.com/category/alteryx)[Azure Courses](https://www.datacamp.com/category/azure)[AWS Courses](https://www.datacamp.com/category/aws)[Google Sheets Courses](https://www.datacamp.com/category/google-sheets)[Excel Courses](https://www.datacamp.com/category/excel)[AI Courses](https://www.datacamp.com/category/artificial-intelligence)[Data Analysis Courses](https://www.datacamp.com/category/data-analysis)[Data Visualization Courses](https://www.datacamp.com/category/data-visualization)[Machine Learning Courses](https://www.datacamp.com/category/machine-learning)[Data Engineering Courses](https://www.datacamp.com/category/data-engineering)[Probability & Statistics Courses](https://www.datacamp.com/category/probability-and-statistics)\n**DataLab**\n[Get Started](https://www.datacamp.com/datalab)[Pricing](https://www.datacamp.com/datalab/pricing)[Security](https://www.datacamp.com/datalab/security)[Documentation](https://datalab-docs.datacamp.com)\n**Certification**\n[Certifications](https://www.datacamp.com/certification)[Data Scientist](https://www.datacamp.com/certification/data-scientist)[Data Analyst](https://www.datacamp.com/certification/data-analyst)[Data Engineer](https://www.datacamp.com/certification/data-engineer)[SQL Associate](https://www.datacamp.com/certification/sql-associate)[Power BI Data Analyst](https://www.datacamp.com/certification/data-analyst-in-power-bi)[Tableau Certified Data Analyst](https://www.datacamp.com/certification/data-analyst-in-tableau)[Azure Fundamentals](https://www.datacamp.com/certification/azure-fundamentals)[AI Fundamentals](https://www.datacamp.com/certification/ai-fundamentals)\n**Resources**\n[Resource Center](https://www.datacamp.com/resources)[Upcoming Events](https://www.datacamp.com/webinars)[Blog](https://www.datacamp.com/blog)[Code-Alongs](https://www.datacamp.com/code-along)[Tutorials](https://www.datacamp.com/tutorial)[Docs](https://www.datacamp.com/doc)[Open Source](https://www.datacamp.com/open-source)[RDocumentation](https://www.rdocumentation.org)[Book a Demo with DataCamp for Business](https://www.datacamp.com/business/demo)[Data Portfolio](https://www.datacamp.com/data-portfolio)\n**Plans**\n[Pricing](https://www.datacamp.com/pricing)[For Students](https://www.datacamp.com/pricing/student)[For Business](https://www.datacamp.com/business)[For Universities](https://www.datacamp.com/universities)[Discounts, Promos & Sales](https://www.datacamp.com/promo)[DataCamp Donates](https://www.datacamp.com/donates)\n**For Business**\n[Business Pricing](https://www.datacamp.com/business/compare-plans)[Teams Plan](https://www.datacamp.com/business/learn-teams)[Data & AI Unlimited Plan](https://www.datacamp.com/business/data-unlimited)[Customer Stories](https://www.datacamp.com/business/customer-stories)[Partner Program](https://www.datacamp.com/business/partner-program)\n**About**\n[About Us](https://www.datacamp.com/about)[Learner Stories](https://www.datacamp.com/stories)[Careers](https://www.datacamp.com/careers)[Become an Instructor](https://www.datacamp.com/learn/create)[Press](https://www.datacamp.com/press)[Leadership](https://www.datacamp.com/about/leadership)[Contact Us](https://support.datacamp.com/hc/en-us/articles/360021185634)[DataCamp Espa\u00f1ol](https://www.datacamp.com/es)[DataCamp Portugu\u00eas](https://www.datacamp.com/pt)[DataCamp Deutsch](https://www.datacamp.com/de)[DataCamp Fran\u00e7ais](https://www.datacamp.com/fr)\n**Support**\n[Help Center](https://support.datacamp.com/hc/en-us)[Become an Affiliate](https://www.datacamp.com/affiliates)\n[Facebook](https://www.facebook.com/datacampinc/)[Twitter](https://twitter.com/datacamp)[LinkedIn](https://www.linkedin.com/school/datacampinc/)[YouTube](https://www.youtube.com/channel/UC79Gv3mYp6zKiSwYemEik9A)[Instagram](https://www.instagram.com/datacamp/)\n[Privacy Policy](https://www.datacamp.com/privacy-policy)[Cookie Notice](https://www.datacamp.com/cookie-notice)[Do Not Sell My Personal Information](https://www.datacamp.com/do-not-sell-my-personal-information)[Accessibility](https://www.datacamp.com/accessibility)[Security](https://www.datacamp.com/security)[Terms of Use](https://www.datacamp.com/terms-of-use)\n\u00a9 2025 DataCamp, Inc. All Rights Reserved.\n",
    "answer": "```markdown\n# TL;DR Summary of Edge AI\n\nEdge AI processes AI algorithms on devices like smartphones and IoT sensors, enabling real-time decision-making. Key benefits include speed, privacy, reliability, and efficiency. Applications span manufacturing, healthcare, retail, and smart cities, but challenges include resource limitations and data security.\n```"
  },
  {
    "instruction": "LLM Learning Lab - Lightning AI\n[Lightning AI Studios: Never set up a local environment again \u2192](https://lightning.ai)\n# LLM Learning Lab\nImmerse yourself in a curated collection of blogs, tutorials, and how-to videos to help you unlock the transformative potential of large language models.\nPrevious\n[](https://lightning.ai/pages/blog/scaling-large-language-models-with-pytorch-lightning/)\n![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/fsdp-blog-300x178.png)\n###  Scaling Large (Language) Models with PyTorch Lightning\n[Read more](https://lightning.ai/pages/blog/scaling-large-language-models-with-pytorch-lightning/)\n[](https://lightning.ai/pages/community/tutorial/how-to-build-a-chatbot-using-open-source-llms-like-llama-2-and-falcon/)\n###  How to build a chatbot using open-source LLMs like Llama 2 and Falcon\n[Read more](https://lightning.ai/pages/community/tutorial/how-to-build-a-chatbot-using-open-source-llms-like-llama-2-and-falcon/)\n[](https://lightning.ai/pages/community/finetuning-falcon-efficiently/)\n![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/06/Screenshot-2023-06-14-at-9.09.01-PM-300x226.png)\n###  Finetuning Falcon LLMs More Efficiently With LoRA and Adapters\n[Read more](https://lightning.ai/pages/community/finetuning-falcon-efficiently/)\n[](https://lightning.ai/pages/blog/falcon-a-guide-to-finetune-and-inference/)\n![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/06/Untitled-5-300x135.png)\n###  Falcon \u2013 A guide to finetune and inference\n[Read more](https://lightning.ai/pages/blog/falcon-a-guide-to-finetune-and-inference/)\nNext\n  * 1\n  * 2\n  * 3\n  * 4\n\n\n## What do you want to learn about today?\nAdapterApple SiliconAutoscalingDeepSpeedDiffusion ModelsDistributed TrainingFalconGPT-NeoXGPTQHyperparameter TuningInferenceInstruction TuningLLaMALLMsLong Short-Term MemoryLoRAModel OptimizationModel PerformanceNeutral NetworksNLPObject DetectionPretrainingPrompt TuningQLoRAQuantizationScalingScikit-learnShardingStable DiffusionStableLMSynthetic Data GenerationTranscriptionTransfer LearningTransformer\n## Check out the LLM Glossary\nWe have broken down complex jargon into simple definitions to give you a deeper understanding of the evolving field of LLMs.\n[Go to Glossary](https://lightning.ai/pages/glossary/)\n## Join the biggest online community for open source AI!\nLightning's Discord server is where thousands of AI researchers, ML engineers, and PyTorch Lightning enthusiasts are collaborating on a daily basis! Members get to enjoy:\n  * Exclusive live events: Tune into regular live-streamed discussions, thought-provoking debates, and demos of valuable resources.\n  * Support from experts: Receive guidance, mentorship, and support at every step of your AI journey.\n  * Networking opportunities: Connect with like-minded individuals, forge valuable connections with experts and build your professional network.\n\n[Join today!](https://discord.gg/XncpTy7DSt)\n![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/discord-icon-1.svg)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe LLM Learning Lab by Lightning AI offers resources like blogs and tutorials on leveraging large language models (LLMs). Key topics include scaling models with PyTorch Lightning, building chatbots using Llama 2 and Falcon, and efficient finetuning techniques. Join their Discord community for networking and expert support.\n```"
  },
  {
    "instruction": "[Skip to main content](https://python.langchain.com/docs/how_to/document_loader_pdf/#__docusaurus_skipToContent_fallback)\n**Join us at[ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!**\n[![\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain](https://python.langchain.com/img/brand/wordmark.png)![\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain](https://python.langchain.com/img/brand/wordmark-dark.png)](https://python.langchain.com/)[Integrations](https://python.langchain.com/docs/integrations/providers/)[API Reference](https://python.langchain.com/api_reference/)\n[More](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n  * [Contributing](https://python.langchain.com/docs/contributing/)\n  * [People](https://python.langchain.com/docs/people/)\n  * [Error reference](https://python.langchain.com/docs/troubleshooting/errors/)\n  * [LangSmith](https://docs.smith.langchain.com)\n  * [LangGraph](https://langchain-ai.github.io/langgraph/)\n  * [LangChain Hub](https://smith.langchain.com/hub)\n  * [LangChain JS/TS](https://js.langchain.com)\n\n\n[v0.3](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n  * [v0.3](https://python.langchain.com/docs/introduction/)\n  * [v0.2](https://python.langchain.com/v0.2/docs/introduction)\n  * [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)\n\n\n[\ud83d\udcac](https://chat.langchain.com)[](https://github.com/langchain-ai/langchain)\nSearch\n  * [Introduction](https://python.langchain.com/docs/introduction/)\n  * [Tutorials](https://python.langchain.com/docs/tutorials/)\n    * [Build a Question Answering application over a Graph Database](https://python.langchain.com/docs/tutorials/graph/)\n    * [Tutorials](https://python.langchain.com/docs/tutorials/)\n    * [Build a simple LLM application with chat models and prompt templates](https://python.langchain.com/docs/tutorials/llm_chain/)\n    * [Build a Chatbot](https://python.langchain.com/docs/tutorials/chatbot/)\n    * [Build a Retrieval Augmented Generation (RAG) App: Part 2](https://python.langchain.com/docs/tutorials/qa_chat_history/)\n    * [Build an Extraction Chain](https://python.langchain.com/docs/tutorials/extraction/)\n    * [Build an Agent](https://python.langchain.com/docs/tutorials/agents/)\n    * [Tagging](https://python.langchain.com/docs/tutorials/classification/)\n    * [Build a Retrieval Augmented Generation (RAG) App: Part 1](https://python.langchain.com/docs/tutorials/rag/)\n    * [Build a semantic search engine](https://python.langchain.com/docs/tutorials/retrievers/)\n    * [Build a Question/Answering system over SQL data](https://python.langchain.com/docs/tutorials/sql_qa/)\n    * [Summarize Text](https://python.langchain.com/docs/tutorials/summarization/)\n  * [How-to guides](https://python.langchain.com/docs/how_to/)\n    * [How-to guides](https://python.langchain.com/docs/how_to/)\n    * [How to use tools in a chain](https://python.langchain.com/docs/how_to/tools_chain/)\n    * [How to use a vectorstore as a retriever](https://python.langchain.com/docs/how_to/vectorstore_retriever/)\n    * [How to add memory to chatbots](https://python.langchain.com/docs/how_to/chatbots_memory/)\n    * [How to use example selectors](https://python.langchain.com/docs/how_to/example_selectors/)\n    * [How to add a semantic layer over graph database](https://python.langchain.com/docs/how_to/graph_semantic/)\n    * [How to invoke runnables in parallel](https://python.langchain.com/docs/how_to/parallel/)\n    * [How to stream chat model responses](https://python.langchain.com/docs/how_to/chat_streaming/)\n    * [How to add default invocation args to a Runnable](https://python.langchain.com/docs/how_to/binding/)\n    * [How to add retrieval to chatbots](https://python.langchain.com/docs/how_to/chatbots_retrieval/)\n    * [How to use few shot examples in chat models](https://python.langchain.com/docs/how_to/few_shot_examples_chat/)\n    * [How to do tool/function calling](https://python.langchain.com/docs/how_to/function_calling/)\n    * [How to install LangChain packages](https://python.langchain.com/docs/how_to/installation/)\n    * [How to add examples to the prompt for query analysis](https://python.langchain.com/docs/how_to/query_few_shot/)\n    * [How to use few shot examples](https://python.langchain.com/docs/how_to/few_shot_examples/)\n    * [How to run custom functions](https://python.langchain.com/docs/how_to/functions/)\n    * [How to use output parsers to parse an LLM response into structured format](https://python.langchain.com/docs/how_to/output_parser_structured/)\n    * [How to handle cases where no queries are generated](https://python.langchain.com/docs/how_to/query_no_queries/)\n    * [How to route between sub-chains](https://python.langchain.com/docs/how_to/routing/)\n    * [How to return structured data from a model](https://python.langchain.com/docs/how_to/structured_output/)\n    * [How to summarize text through parallelization](https://python.langchain.com/docs/how_to/summarize_map_reduce/)\n    * [How to summarize text through iterative refinement](https://python.langchain.com/docs/how_to/summarize_refine/)\n    * [How to summarize text in a single LLM call](https://python.langchain.com/docs/how_to/summarize_stuff/)\n    * [How to use toolkits](https://python.langchain.com/docs/how_to/toolkits/)\n    * [How to add ad-hoc tool calling capability to LLMs and Chat Models](https://python.langchain.com/docs/how_to/tools_prompting/)\n    * [Build an Agent with AgentExecutor (Legacy)](https://python.langchain.com/docs/how_to/agent_executor/)\n    * [How to construct knowledge graphs](https://python.langchain.com/docs/how_to/graph_constructing/)\n    * [How to partially format prompt templates](https://python.langchain.com/docs/how_to/prompts_partial/)\n    * [How to handle multiple queries when doing query analysis](https://python.langchain.com/docs/how_to/query_multiple_queries/)\n    * [How to use built-in tools and toolkits](https://python.langchain.com/docs/how_to/tools_builtin/)\n    * [How to pass through arguments from one step to the next](https://python.langchain.com/docs/how_to/passthrough/)\n    * [How to compose prompts together](https://python.langchain.com/docs/how_to/prompts_composition/)\n    * [How to handle multiple retrievers when doing query analysis](https://python.langchain.com/docs/how_to/query_multiple_retrievers/)\n    * [How to add values to a chain's state](https://python.langchain.com/docs/how_to/assign/)\n    * [How to construct filters for query analysis](https://python.langchain.com/docs/how_to/query_constructing_filters/)\n    * [How to configure runtime chain internals](https://python.langchain.com/docs/how_to/configure/)\n    * [How deal with high cardinality categoricals when doing query analysis](https://python.langchain.com/docs/how_to/query_high_cardinality/)\n    * [Custom Document Loader](https://python.langchain.com/docs/how_to/document_loader_custom/)\n    * [How to use the MultiQueryRetriever](https://python.langchain.com/docs/how_to/MultiQueryRetriever/)\n    * [How to add scores to retriever results](https://python.langchain.com/docs/how_to/add_scores_retriever/)\n    * [Caching](https://python.langchain.com/docs/how_to/caching_embeddings/)\n    * [How to use callbacks in async environments](https://python.langchain.com/docs/how_to/callbacks_async/)\n    * [How to attach callbacks to a runnable](https://python.langchain.com/docs/how_to/callbacks_attach/)\n    * [How to propagate callbacks constructor](https://python.langchain.com/docs/how_to/callbacks_constructor/)\n    * [How to dispatch custom callback events](https://python.langchain.com/docs/how_to/callbacks_custom_events/)\n    * [How to pass callbacks in at runtime](https://python.langchain.com/docs/how_to/callbacks_runtime/)\n    * [How to split by character](https://python.langchain.com/docs/how_to/character_text_splitter/)\n    * [How to cache chat model responses](https://python.langchain.com/docs/how_to/chat_model_caching/)\n    * [How to handle rate limits](https://python.langchain.com/docs/how_to/chat_model_rate_limiting/)\n    * [How to init any model in one line](https://python.langchain.com/docs/how_to/chat_models_universal_init/)\n    * [How to track token usage in ChatModels](https://python.langchain.com/docs/how_to/chat_token_usage_tracking/)\n    * [How to add tools to chatbots](https://python.langchain.com/docs/how_to/chatbots_tools/)\n    * [How to split code](https://python.langchain.com/docs/how_to/code_splitter/)\n    * [How to do retrieval with contextual compression](https://python.langchain.com/docs/how_to/contextual_compression/)\n    * [How to convert Runnables to Tools](https://python.langchain.com/docs/how_to/convert_runnable_to_tool/)\n    * [How to create custom callback handlers](https://python.langchain.com/docs/how_to/custom_callbacks/)\n    * [How to create a custom chat model class](https://python.langchain.com/docs/how_to/custom_chat_model/)\n    * [Custom Embeddings](https://python.langchain.com/docs/how_to/custom_embeddings/)\n    * [How to create a custom LLM class](https://python.langchain.com/docs/how_to/custom_llm/)\n    * [Custom Retriever](https://python.langchain.com/docs/how_to/custom_retriever/)\n    * [How to create tools](https://python.langchain.com/docs/how_to/custom_tools/)\n    * [How to debug your LLM apps](https://python.langchain.com/docs/how_to/debugging/)\n    * [How to load CSVs](https://python.langchain.com/docs/how_to/document_loader_csv/)\n    * [How to load documents from a directory](https://python.langchain.com/docs/how_to/document_loader_directory/)\n    * [How to load HTML](https://python.langchain.com/docs/how_to/document_loader_html/)\n    * [How to load JSON](https://python.langchain.com/docs/how_to/document_loader_json/)\n    * [How to load Markdown](https://python.langchain.com/docs/how_to/document_loader_markdown/)\n    * [How to load Microsoft Office files](https://python.langchain.com/docs/how_to/document_loader_office_file/)\n    * [How to load PDFs](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n    * [How to load web pages](https://python.langchain.com/docs/how_to/document_loader_web/)\n    * [How to create a dynamic (self-constructing) chain](https://python.langchain.com/docs/how_to/dynamic_chain/)\n    * [Text embedding models](https://python.langchain.com/docs/how_to/embed_text/)\n    * [How to combine results from multiple retrievers](https://python.langchain.com/docs/how_to/ensemble_retriever/)\n    * [How to select examples from a LangSmith dataset](https://python.langchain.com/docs/how_to/example_selectors_langsmith/)\n    * [How to select examples by length](https://python.langchain.com/docs/how_to/example_selectors_length_based/)\n    * [How to select examples by maximal marginal relevance (MMR)](https://python.langchain.com/docs/how_to/example_selectors_mmr/)\n    * [How to select examples by n-gram overlap](https://python.langchain.com/docs/how_to/example_selectors_ngram/)\n    * [How to select examples by similarity](https://python.langchain.com/docs/how_to/example_selectors_similarity/)\n    * [How to use reference examples when doing extraction](https://python.langchain.com/docs/how_to/extraction_examples/)\n    * [How to handle long text when doing extraction](https://python.langchain.com/docs/how_to/extraction_long_text/)\n    * [How to use prompting alone (no tool calling) to do extraction](https://python.langchain.com/docs/how_to/extraction_parse/)\n    * [How to add fallbacks to a runnable](https://python.langchain.com/docs/how_to/fallbacks/)\n    * [How to filter messages](https://python.langchain.com/docs/how_to/filter_messages/)\n    * [Hybrid Search](https://python.langchain.com/docs/how_to/hybrid/)\n    * [How to use the LangChain indexing API](https://python.langchain.com/docs/how_to/indexing/)\n    * [How to inspect runnables](https://python.langchain.com/docs/how_to/inspect/)\n    * [LangChain Expression Language Cheatsheet](https://python.langchain.com/docs/how_to/lcel_cheatsheet/)\n    * [How to cache LLM responses](https://python.langchain.com/docs/how_to/llm_caching/)\n    * [How to track token usage for LLMs](https://python.langchain.com/docs/how_to/llm_token_usage_tracking/)\n    * [Run models locally](https://python.langchain.com/docs/how_to/local_llms/)\n    * [How to get log probabilities](https://python.langchain.com/docs/how_to/logprobs/)\n    * [How to reorder retrieved results to mitigate the \"lost in the middle\" effect](https://python.langchain.com/docs/how_to/long_context_reorder/)\n    * [How to split Markdown by Headers](https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/)\n    * [How to merge consecutive messages of the same type](https://python.langchain.com/docs/how_to/merge_message_runs/)\n    * [How to add message history](https://python.langchain.com/docs/how_to/message_history/)\n    * [How to migrate from legacy LangChain agents to LangGraph](https://python.langchain.com/docs/how_to/migrate_agent/)\n    * [How to retrieve using multiple vectors per document](https://python.langchain.com/docs/how_to/multi_vector/)\n    * [How to pass multimodal data directly to models](https://python.langchain.com/docs/how_to/multimodal_inputs/)\n    * [How to use multimodal prompts](https://python.langchain.com/docs/how_to/multimodal_prompts/)\n    * [How to create a custom Output Parser](https://python.langchain.com/docs/how_to/output_parser_custom/)\n    * [How to use the output-fixing parser](https://python.langchain.com/docs/how_to/output_parser_fixing/)\n    * [How to parse JSON output](https://python.langchain.com/docs/how_to/output_parser_json/)\n    * [How to retry when a parsing error occurs](https://python.langchain.com/docs/how_to/output_parser_retry/)\n    * [How to parse text from message objects](https://python.langchain.com/docs/how_to/output_parser_string/)\n    * [How to parse XML output](https://python.langchain.com/docs/how_to/output_parser_xml/)\n    * [How to parse YAML output](https://python.langchain.com/docs/how_to/output_parser_yaml/)\n    * [How to use the Parent Document Retriever](https://python.langchain.com/docs/how_to/parent_document_retriever/)\n    * [How to use LangChain with different Pydantic versions](https://python.langchain.com/docs/how_to/pydantic_compatibility/)\n    * [How to add chat history](https://python.langchain.com/docs/how_to/qa_chat_history_how_to/)\n    * [How to get a RAG application to add citations](https://python.langchain.com/docs/how_to/qa_citations/)\n    * [How to do per-user retrieval](https://python.langchain.com/docs/how_to/qa_per_user/)\n    * [How to get your RAG application to return sources](https://python.langchain.com/docs/how_to/qa_sources/)\n    * [How to stream results from your RAG application](https://python.langchain.com/docs/how_to/qa_streaming/)\n    * [How to split JSON data](https://python.langchain.com/docs/how_to/recursive_json_splitter/)\n    * [How to recursively split text by characters](https://python.langchain.com/docs/how_to/recursive_text_splitter/)\n    * [Response metadata](https://python.langchain.com/docs/how_to/response_metadata/)\n    * [How to pass runtime secrets to runnables](https://python.langchain.com/docs/how_to/runnable_runtime_secrets/)\n    * [How to do \"self-querying\" retrieval](https://python.langchain.com/docs/how_to/self_query/)\n    * [How to split text based on semantic similarity](https://python.langchain.com/docs/how_to/semantic-chunker/)\n    * [How to chain runnables](https://python.langchain.com/docs/how_to/sequence/)\n    * [How to save and load LangChain objects](https://python.langchain.com/docs/how_to/serialization/)\n    * [How to split text by tokens](https://python.langchain.com/docs/how_to/split_by_token/)\n    * [How to split HTML](https://python.langchain.com/docs/how_to/split_html/)\n    * [How to do question answering over CSVs](https://python.langchain.com/docs/how_to/sql_csv/)\n    * [How to deal with large databases when doing SQL question-answering](https://python.langchain.com/docs/how_to/sql_large_db/)\n    * [How to better prompt when doing SQL question-answering](https://python.langchain.com/docs/how_to/sql_prompting/)\n    * [How to do query validation as part of SQL question-answering](https://python.langchain.com/docs/how_to/sql_query_checking/)\n    * [How to stream runnables](https://python.langchain.com/docs/how_to/streaming/)\n    * [How to stream responses from an LLM](https://python.langchain.com/docs/how_to/streaming_llm/)\n    * [How to use a time-weighted vector store retriever](https://python.langchain.com/docs/how_to/time_weighted_vectorstore/)\n    * [How to return artifacts from a tool](https://python.langchain.com/docs/how_to/tool_artifacts/)\n    * [How to use chat models to call tools](https://python.langchain.com/docs/how_to/tool_calling/)\n    * [How to disable parallel tool calling](https://python.langchain.com/docs/how_to/tool_calling_parallel/)\n    * [How to force models to call a tool](https://python.langchain.com/docs/how_to/tool_choice/)\n    * [How to access the RunnableConfig from a tool](https://python.langchain.com/docs/how_to/tool_configure/)\n    * [How to pass tool outputs to chat models](https://python.langchain.com/docs/how_to/tool_results_pass_to_model/)\n    * [How to pass run time values to tools](https://python.langchain.com/docs/how_to/tool_runtime/)\n    * [How to stream events from a tool](https://python.langchain.com/docs/how_to/tool_stream_events/)\n    * [How to stream tool calls](https://python.langchain.com/docs/how_to/tool_streaming/)\n    * [How to convert tools to OpenAI Functions](https://python.langchain.com/docs/how_to/tools_as_openai_functions/)\n    * [How to handle tool errors](https://python.langchain.com/docs/how_to/tools_error/)\n    * [How to use few-shot prompting with tool calling](https://python.langchain.com/docs/how_to/tools_few_shot/)\n    * [How to add a human-in-the-loop for tools](https://python.langchain.com/docs/how_to/tools_human/)\n    * [How to bind model-specific tools](https://python.langchain.com/docs/how_to/tools_model_specific/)\n    * [How to trim messages](https://python.langchain.com/docs/how_to/trim_messages/)\n    * [How to create and query vector stores](https://python.langchain.com/docs/how_to/vectorstores/)\n  * [Conceptual guide](https://python.langchain.com/docs/concepts/)\n    * [Agents](https://python.langchain.com/docs/concepts/agents/)\n    * [Architecture](https://python.langchain.com/docs/concepts/architecture/)\n    * [Async programming with langchain](https://python.langchain.com/docs/concepts/async/)\n    * [Callbacks](https://python.langchain.com/docs/concepts/callbacks/)\n    * [Chat history](https://python.langchain.com/docs/concepts/chat_history/)\n    * [Chat models](https://python.langchain.com/docs/concepts/chat_models/)\n    * [Document loaders](https://python.langchain.com/docs/concepts/document_loaders/)\n    * [Embedding models](https://python.langchain.com/docs/concepts/embedding_models/)\n    * [Evaluation](https://python.langchain.com/docs/concepts/evaluation/)\n    * [Example selectors](https://python.langchain.com/docs/concepts/example_selectors/)\n    * [Few-shot prompting](https://python.langchain.com/docs/concepts/few_shot_prompting/)\n    * [Conceptual guide](https://python.langchain.com/docs/concepts/)\n    * [Key-value stores](https://python.langchain.com/docs/concepts/key_value_stores/)\n    * [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/concepts/lcel/)\n    * [Messages](https://python.langchain.com/docs/concepts/messages/)\n    * [Multimodality](https://python.langchain.com/docs/concepts/multimodality/)\n    * [Output parsers](https://python.langchain.com/docs/concepts/output_parsers/)\n    * [Prompt Templates](https://python.langchain.com/docs/concepts/prompt_templates/)\n    * [Retrieval augmented generation (RAG)](https://python.langchain.com/docs/concepts/rag/)\n    * [Retrieval](https://python.langchain.com/docs/concepts/retrieval/)\n    * [Retrievers](https://python.langchain.com/docs/concepts/retrievers/)\n    * [Runnable interface](https://python.langchain.com/docs/concepts/runnables/)\n    * [Streaming](https://python.langchain.com/docs/concepts/streaming/)\n    * [Structured outputs](https://python.langchain.com/docs/concepts/structured_outputs/)\n    * [Testing](https://python.langchain.com/docs/concepts/testing/)\n    * [String-in, string-out llms](https://python.langchain.com/docs/concepts/text_llms/)\n    * [Text splitters](https://python.langchain.com/docs/concepts/text_splitters/)\n    * [Tokens](https://python.langchain.com/docs/concepts/tokens/)\n    * [Tool calling](https://python.langchain.com/docs/concepts/tool_calling/)\n    * [Tools](https://python.langchain.com/docs/concepts/tools/)\n    * [Tracing](https://python.langchain.com/docs/concepts/tracing/)\n    * [Vector stores](https://python.langchain.com/docs/concepts/vectorstores/)\n    * [Why LangChain?](https://python.langchain.com/docs/concepts/why_langchain/)\n  * Ecosystem\n    * [\ud83e\udd9c\ud83d\udee0\ufe0f LangSmith](https://docs.smith.langchain.com/)\n    * [\ud83e\udd9c\ud83d\udd78\ufe0f LangGraph](https://langchain-ai.github.io/langgraph/)\n  * Versions\n    * [v0.3](https://python.langchain.com/docs/versions/v0_3/)\n    * [v0.2](https://python.langchain.com/docs/versions/v0_2/overview/)\n    * [Pydantic compatibility](https://python.langchain.com/docs/how_to/pydantic_compatibility/)\n    * [Migrating from v0.0 chains](https://python.langchain.com/docs/versions/migrating_chains/)\n      * [How to migrate from v0.0 chains](https://python.langchain.com/docs/versions/migrating_chains/)\n      * [Migrating from ConstitutionalChain](https://python.langchain.com/docs/versions/migrating_chains/constitutional_chain/)\n      * [Migrating from ConversationalChain](https://python.langchain.com/docs/versions/migrating_chains/conversation_chain/)\n      * [Migrating from ConversationalRetrievalChain](https://python.langchain.com/docs/versions/migrating_chains/conversation_retrieval_chain/)\n      * [Migrating from LLMChain](https://python.langchain.com/docs/versions/migrating_chains/llm_chain/)\n      * [Migrating from LLMMathChain](https://python.langchain.com/docs/versions/migrating_chains/llm_math_chain/)\n      * [Migrating from LLMRouterChain](https://python.langchain.com/docs/versions/migrating_chains/llm_router_chain/)\n      * [Migrating from MapReduceDocumentsChain](https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/)\n      * [Migrating from MapRerankDocumentsChain](https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain/)\n      * [Migrating from MultiPromptChain](https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/)\n      * [Migrating from RefineDocumentsChain](https://python.langchain.com/docs/versions/migrating_chains/refine_docs_chain/)\n      * [Migrating from RetrievalQA](https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/)\n      * [Migrating from StuffDocumentsChain](https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/)\n    * [Upgrading to LangGraph memory](https://python.langchain.com/docs/versions/migrating_memory/)\n      * [How to migrate to LangGraph memory](https://python.langchain.com/docs/versions/migrating_memory/)\n      * [How to use BaseChatMessageHistory with LangGraph](https://python.langchain.com/docs/versions/migrating_memory/chat_history/)\n      * [Migrating off ConversationBufferMemory or ConversationStringBufferMemory](https://python.langchain.com/docs/versions/migrating_memory/conversation_buffer_memory/)\n      * [Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory](https://python.langchain.com/docs/versions/migrating_memory/conversation_buffer_window_memory/)\n      * [Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory](https://python.langchain.com/docs/versions/migrating_memory/conversation_summary_memory/)\n      * [A Long-Term Memory Agent](https://python.langchain.com/docs/versions/migrating_memory/long_term_memory_agent/)\n    * [Release policy](https://python.langchain.com/docs/versions/release_policy/)\n  * [Security Policy](https://python.langchain.com/docs/security/)\n\n\n  * [](https://python.langchain.com/)\n  * [How-to guides](https://python.langchain.com/docs/how_to/)\n  * How to load PDFs\n\n\nOn this page\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_pdf.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_pdf.ipynb)\n# How to load PDFs\n[Portable Document Format (PDF)](https://en.wikipedia.org/wiki/PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\nThis guide covers how to [load](https://python.langchain.com/docs/concepts/document_loaders/) `PDF` documents into the LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) format that we use downstream.\nText in PDFs is typically represented via text boxes. They may also contain images. A PDF parser might do some combination of the following:\n  * Agglomerate text boxes into lines, paragraphs, and other structures via heuristics or ML inference;\n  * Run [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition) on images to detect text therein;\n  * Classify text as belonging to paragraphs, lists, tables, or other structures;\n  * Structure text into table rows and columns, or key-value pairs.\n\n\nLangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your needs. Below we enumerate the possibilities.\nWe will demonstrate these approaches on a [sample file](https://github.com/langchain-ai/langchain/blob/master/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf):\n```\nfile_path =(\"../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\")\n```\n\nA note on multimodal models\nMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications -- such as question-answering over PDFs with complex layouts, diagrams, or scans -- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. We demonstrate an example of this in the [Use of multimodal models](https://python.langchain.com/docs/how_to/document_loader_pdf/#use-of-multimodal-models) section below.\n## Simple and fast text extraction[\u200b](https://python.langchain.com/docs/how_to/document_loader_pdf/#simple-and-fast-text-extraction \"Direct link to Simple and fast text extraction\")\nIf you are looking for a simple string representation of text that is embedded in a PDF, the method below is appropriate. It will return a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects-- one per page-- containing a single string of the page's text in the Document's `page_content` attribute. It will not parse text in images or scanned PDF pages. Under the hood it uses the [pypdf](https://pypdf.readthedocs.io/en/stable/) Python library.\nLangChain [document loaders](https://python.langchain.com/docs/concepts/document_loaders/) implement `lazy_load` and its async variant, `alazy_load`, which return iterators of `Document` objects. We will use these below.\n```\n%pip install -qU pypdf\n```\n\n```\nfrom langchain_community.document_loaders import PyPDFLoaderloader = PyPDFLoader(file_path)pages =[]asyncfor page in loader.alazy_load():  pages.append(page)\n```\n\n**API Reference:**[PyPDFLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)\n```\nprint(f\"{pages[0].metadata}\\n\")print(pages[0].page_content)\n```\n\n```\n{'source': '../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf', 'page': 0}LayoutParser : A Uni\ufb01ed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( \ufffd), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University{melissadell,jacob carlson }@fas.harvard.edu4University of Washingtonbcgl@cs.washington.edu5University of Waterloow422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have beenprimarily driven by the application of neural networks. Ideally, researchoutcomes could be easily deployed in production and extended for furtherinvestigation. However, various factors like loosely organized codebasesand sophisticated model con\ufb01gurations complicate the easy reuse of im-portant innovations by a wide audience. Though there have been on-goinge\ufb00orts to improve reusability and simplify deep learning (DL) modeldevelopment in disciplines like natural language processing and computervision, none of them are optimized for challenges in the domain of DIA.This represents a major gap in the existing toolkit, as DIA is central toacademic research across a wide range of disciplines in the social sciencesand humanities. This paper introduces LayoutParser , an open-sourcelibrary for streamlining the usage of DL in DIA research and applica-tions. The core LayoutParser library comes with a set of simple andintuitive interfaces for applying and customizing DL models for layout de-tection, character recognition, and many other document processing tasks.To promote extensibility, LayoutParser also incorporates a communityplatform for sharing both pre-trained models and full document digiti-zation pipelines. We demonstrate that LayoutParser is helpful for bothlightweight and large-scale digitization pipelines in real-word use cases.The library is publicly available at https://layout-parser.github.io .Keywords: Document Image Analysis \u00b7Deep Learning \u00b7Layout Analysis\u00b7Character Recognition \u00b7Open Source library \u00b7Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range ofdocument image analysis (DIA) tasks including document image classi\ufb01cation [ 11,arXiv:2103.15348v2 [cs.CV] 21 Jun 2021\n```\n\nNote that the metadata of each document stores the corresponding page number.\n### Vector search over PDFs[\u200b](https://python.langchain.com/docs/how_to/document_loader_pdf/#vector-search-over-pdfs \"Direct link to Vector search over PDFs\")\nOnce we have loaded PDFs into LangChain `Document` objects, we can index them (e.g., a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain [embeddings](https://python.langchain.com/docs/concepts/embedding_models) model will suffice.\n```\n%pip install -qU langchain-openai\n```\n\n```\nimport getpassimport osif\"OPENAI_API_KEY\"notin os.environ:  os.environ[\"OPENAI_API_KEY\"]= getpass.getpass(\"OpenAI API Key:\")\n```\n\n```\nfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(pages, OpenAIEmbeddings())docs = vector_store.similarity_search(\"What is LayoutParser?\", k=2)for doc in docs:print(f'Page {doc.metadata[\"page\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```\nPage 13: 14 Z. Shen et al.6 ConclusionLayoutParser provides a comprehensive toolkit for deep learning-based documentimage analysis. The o\ufb00-the-shelf library is easy to install, and can be used tobuild \ufb02exible and accurate pipelines for processing documents with complicatedstructures. It also supports hiPage 0: LayoutParser : A Uni\ufb01ed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( \ufffd), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University\n```\n\n## Layout analysis and extraction of text from images[\u200b](https://python.langchain.com/docs/how_to/document_loader_pdf/#layout-analysis-and-extraction-of-text-from-images \"Direct link to Layout analysis and extraction of text from images\")\nIf you require a more granular segmentation of text (e.g., into distinct paragraphs, titles, tables, or other structures) or require extraction of text from images, the method below is appropriate. It will return a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects, where each object represents a structure on the page. The Document's metadata stores the page number and other information related to the object (e.g., it might store table rows and columns in the case of a table object).\nUnder the hood it uses the `langchain-unstructured` library. See the [integration docs](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file/) for more information about using [Unstructured](https://docs.unstructured.io/welcome) with LangChain.\nUnstructured supports multiple parameters for PDF parsing:\n  * `strategy` (e.g., `\"fast\"` or `\"hi-res\"`)\n  * API or local processing. You will need an API key to use the API.\n\n\nThe [hi-res](https://docs.unstructured.io/api-reference/how-to/choose-hi-res-model) strategy provides support for document layout analysis and OCR. We demonstrate it below via the API. See [local parsing](https://python.langchain.com/docs/how_to/document_loader_pdf/#local-parsing) section below for considerations when running locally.\n```\n%pip install -qU langchain-unstructured\n```\n\n```\nimport getpassimport osif\"UNSTRUCTURED_API_KEY\"notin os.environ:  os.environ[\"UNSTRUCTURED_API_KEY\"]= getpass.getpass(\"Unstructured API Key:\")\n```\n\n```\nUnstructured API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nAs before, we initialize a loader and load documents lazily:\n```\nfrom langchain_unstructured import UnstructuredLoaderloader = UnstructuredLoader(  file_path=file_path,  strategy=\"hi_res\",  partition_via_api=True,  coordinates=True,)docs =[]for doc in loader.lazy_load():  docs.append(doc)\n```\n\n**API Reference:**[UnstructuredLoader](https://python.langchain.com/api_reference/unstructured/document_loaders/langchain_unstructured.document_loaders.UnstructuredLoader.html)\n```\nINFO: Preparing to split document for partition.INFO: Starting page number set to 1INFO: Allow failed set to 0INFO: Concurrency level set to 5INFO: Splitting pages 1 to 16 (16 total)INFO: Determined optimal split size of 4 pages.INFO: Partitioning 4 files with 4 page(s) each.INFO: Partitioning set #1 (pages 1-4).INFO: Partitioning set #2 (pages 5-8).INFO: Partitioning set #3 (pages 9-12).INFO: Partitioning set #4 (pages 13-16).INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: Successfully partitioned set #1, elements added to the final result.INFO: Successfully partitioned set #2, elements added to the final result.INFO: Successfully partitioned set #3, elements added to the final result.INFO: Successfully partitioned set #4, elements added to the final result.\n```\n\nHere we recover 171 distinct structures over the 16 page document:\n```\nprint(len(docs))\n```\n\n```\n171\n```\n\nWe can use the document metadata to recover content from a single page:\n```\nfirst_page_docs =[doc for doc in docs if doc.metadata.get(\"page_number\")==1]for doc in first_page_docs:print(doc.page_content)\n```\n\n```\nLayoutParser: A Uni\ufb01ed Toolkit for Deep Learning Based Document Image Analysis1 2 0 2 n u J 1 2 ] V C . s c [ 2 v 8 4 3 5 1 . 3 0 1 2 : v i X r aZejiang Shen\u00ae (<), Ruochen Zhang?, Melissa Dell\u00ae, Benjamin Charles Germain Lee?, Jacob Carlson\u00ae, and Weining Li\u00ae1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model con\ufb01gurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going e\ufb00orts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.Keywords: Document Image Analysis \u00b7 Deep Learning \u00b7 Layout Analysis \u00b7 Character Recognition \u00b7 Open Source library \u00b7 Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classi\ufb01cation [11,\n```\n\n### Extracting tables and other structures[\u200b](https://python.langchain.com/docs/how_to/document_loader_pdf/#extracting-tables-and-other-structures \"Direct link to Extracting tables and other structures\")\nEach `Document` we load represents a structure, like a title, paragraph, or table.\nSome structures may be of special interest for indexing or question-answering tasks. These structures may be:\n  1. Classified for easy identification;\n  2. Parsed into a more structured representation.\n\n\nBelow, we identify and extract a table:\nClick to expand code for rendering pages\n%pip install -qU matplotlib PyMuPDF pillow\n```\nimport fitzimport matplotlib.patches as patchesimport matplotlib.pyplot as pltfrom PIL import Imagedefplot_pdf_with_boxes(pdf_page, segments):  pix = pdf_page.get_pixmap()  pil_image = Image.frombytes(\"RGB\",[pix.width, pix.height], pix.samples)  fig, ax = plt.subplots(1, figsize=(10,10))  ax.imshow(pil_image)  categories =set()  category_to_color ={\"Title\":\"orchid\",\"Image\":\"forestgreen\",\"Table\":\"tomato\",}for segment in segments:    points = segment[\"coordinates\"][\"points\"]    layout_width = segment[\"coordinates\"][\"layout_width\"]    layout_height = segment[\"coordinates\"][\"layout_height\"]    scaled_points =[(x * pix.width / layout_width, y * pix.height / layout_height)for x, y in points]    box_color = category_to_color.get(segment[\"category\"],\"deepskyblue\")    categories.add(segment[\"category\"])    rect = patches.Polygon(      scaled_points, linewidth=1, edgecolor=box_color, facecolor=\"none\")    ax.add_patch(rect)# Make legend  legend_handles =[patches.Patch(color=\"deepskyblue\", label=\"Text\")]for category in[\"Title\",\"Image\",\"Table\"]:if category in categories:      legend_handles.append(        patches.Patch(color=category_to_color[category], label=category))  ax.axis(\"off\")  ax.legend(handles=legend_handles, loc=\"upper right\")  plt.tight_layout()  plt.show()defrender_page(doc_list:list, page_number:int, print_text=True)->None:  pdf_page = fitz.open(file_path).load_page(page_number -1)  page_docs =[    doc for doc in doc_list if doc.metadata.get(\"page_number\")== page_number]  segments =[doc.metadata for doc in page_docs]  plot_pdf_with_boxes(pdf_page, segments)if print_text:for doc in page_docs:print(f\"{doc.page_content}\\n\")\n```\n\n```\nrender_page(docs,5)\n```\n\n![](https://python.langchain.com/docs/how_to/document_loader_pdf/#use-of-multimodal-models/)\n```\nLayoutParser: A Uni\ufb01ed Toolkit for DL-Based DIA5Table 1: Current layout detection models in the LayoutParser model zooDataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scienti\ufb01c documents Layouts of scanned modern magazines and scienti\ufb01c reports Layouts of scanned US newspapers from the 20th century Table region on modern scienti\ufb01c and business document Layouts of history Japanese documents1 For each dataset, we train several models of di\ufb00erent sizes for di\ufb00erent needs (the trade-o\ufb00 between accuracy vs. computational cost). For \u201cbase model\u201d and \u201clarge model\u201d, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of di\ufb00erent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.layout data structures, which are optimized for e\ufb03ciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uni\ufb01ed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.3.1 Layout Detection ModelsIn LayoutParser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Di\ufb00erent from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CNN [28] and Mask R-CNN [12] are used. This yields prediction results of high accuracy and makes it possible to build a concise, generalized interface for layout detection. LayoutParser, built upon Detectron2 [35], provides a minimal API that can perform layout detection with only four lines of code in Python:1 import layoutparser as lp 2 image = cv2 . imread ( \" image_file \" ) # load images 3 model = lp . De t e c tro n2 Lay outM odel ( \" lp :// PubLayNet / f as t er _ r c nn _ R _ 50 _ F P N_ 3 x / config \" ) 4 5 layout = model . detect ( image )LayoutParser provides a wealth of pre-trained model weights using various datasets covering di\ufb00erent languages, time periods, and document types. Due to domain shift [7], the prediction performance can notably drop when models are ap- plied to target samples that are signi\ufb01cantly di\ufb00erent from the training dataset. As document structures and layouts vary greatly in di\ufb00erent domains, it is important to select models trained on a dataset similar to the test samples. A semantic syntax is used for initializing the model weights in LayoutParser, using both the dataset name and model name lp://<dataset-name>/<model-architecture-name>.\n```\n\nNote that although the table text is collapsed into a single string in the document's content, the metadata contains a representation of its rows and columns:\n```\nfrom IPython.display import HTML, displaysegments =[  doc.metadatafor doc in docsif doc.metadata.get(\"page_number\")==5and doc.metadata.get(\"category\")==\"Table\"]display(HTML(segments[0][\"text_as_html\"]))\n```\n\nable 1. LUllclll 1ayoul actCCLloll 1110AdCs 111 L1C LayoOulralsel 1110U4cl 200  \n---  \nDataset| | Base Model'|| Notes  \nPubLayNet [38]| F/M| Layouts of modern scientific documents  \nPRImA| M| Layouts of scanned modern magazines and scientific reports  \nNewspaper| F| Layouts of scanned US newspapers from the 20th century  \nTableBank [18]| F| Table region on modern scientific and business document  \nHJDataset| F/M| Layouts of history Japanese documents  \n### Extracting text from specific sections[\u200b](https://python.langchain.com/docs/how_to/document_loader_pdf/#extracting-text-from-specific-sections \"Direct link to Extracting text from specific sections\")\nStructures may have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding `Document` objects.\nBelow, we extract all text associated with the document's \"Conclusion\" section:\n```\nrender_page(docs,14, print_text=False)\n```\n\n![](https://python.langchain.com/docs/how_to/document_loader_pdf/#use-of-multimodal-models/)\n```\nconclusion_docs =[]parent_id =-1for doc in docs:if doc.metadata[\"category\"]==\"Title\"and\"Conclusion\"in doc.page_content:    parent_id = doc.metadata[\"element_id\"]if doc.metadata.get(\"parent_id\")== parent_id:    conclusion_docs.append(doc)for doc in conclusion_docs:print(doc.page_content)\n```\n\n```\nLayoutParser provides a comprehensive toolkit for deep learning-based document image analysis. The o\ufb00-the-shelf library is easy to install, and can be used to build \ufb02exible and accurate pipelines for processing documents with complicated structures. It also supports high-level customization and enables easy labeling and training of DL models on unique document image datasets. The LayoutParser community platform facilitates sharing DL models and DIA pipelines, inviting discussion and promoting code reproducibility and reusability. The LayoutParser team is committed to keeping the library updated continuously and bringing the most recent advances in DL-based DIA, such as multi-modal document modeling [37, 36, 9] (an upcoming priority), to a diverse audience of end-users.Acknowledgements We thank the anonymous reviewers for their comments and suggestions. This project is supported in part by NSF Grant OIA-2033558 and funding from the Harvard Data Science Initiative and Harvard Catalyst. Zejiang Shen thanks Doug Downey for suggestions.\n```\n\n### Extracting text from images[\u200b](https://python.langchain.com/docs/how_to/document_loader_pdf/#extracting-text-from-images \"Direct link to Extracting text from images\")\nOCR is run on images, enabling the extraction of text therein:\n```\nrender_page(docs,11)\n```\n\n![](https://python.langchain.com/docs/how_to/document_loader_pdf/#use-of-multimodal-models/)\n```\nLayoutParser: A Uni\ufb01ed Toolkit for DL-Based DIAfocuses on precision, e\ufb03ciency, and robustness. The target documents may have complicated structures, and may require training multiple layout detection models to achieve the optimal accuracy. Light-weight pipelines are built for relatively simple documents, with an emphasis on development ease, speed and \ufb02exibility. Ideally one only needs to use existing resources, and model training should be avoided. Through two exemplar projects, we show how practitioners in both academia and industry can easily build such pipelines using LayoutParser and extract high-quality structured document data for their downstream tasks. The source code for these projects will be publicly available in the LayoutParser community hub.115.1 A Comprehensive Historical Document Digitization PipelineThe digitization of historical documents can unlock valuable data that can shed light on many important social, economic, and historical questions. Yet due to scan noises, page wearing, and the prevalence of complicated layout structures, ob- taining a structured representation of historical document scans is often extremely complicated. In this example, LayoutParser was used to develop a comprehensive pipeline, shown in Figure 5, to gener- ate high-quality structured data from historical Japanese \ufb01rm \ufb01nancial ta- bles with complicated layouts. The pipeline applies two layout models to identify di\ufb00erent levels of document structures and two customized OCR engines for optimized character recog- nition accuracy.\u2018Active Learning Layout Annotate Layout Dataset | +\u2014\u2014 Annotation Toolkit A4 Deep Learning Layout Layout Detection Model Training & Inference, A Post-processing \u2014 Handy Data Structures & \\ Lo orajport 7 ) Al Pls for Layout Data A4 Default and Customized Text Recognition 0CR Models \u00a5 Visualization & Export Layout Structure Visualization & Storage The Japanese Document Helpful LayoutParser Modules Digitization PipelineAs shown in Figure 4 (a), the document contains columns of text written vertically 15, a common style in Japanese. Due to scanning noise and archaic printing technology, the columns can be skewed or have vari- able widths, and hence cannot be eas- ily identi\ufb01ed via rule-based methods. Within each column, words are sepa- rated by white spaces of variable size, and the vertical positions of objects can be an indicator of their layout type.Fig. 5: Illustration of how LayoutParser helps with the historical document digi- tization pipeline.15 A document page consists of eight rows like this. For simplicity we skip the row segmentation discussion and refer readers to the source code when available.\n```\n\nNote that the text from the figure on the right is extracted and incorporated into the content of the `Document`.\n### Local parsing[\u200b](https://python.langchain.com/docs/how_to/document_loader_pdf/#local-parsing \"Direct link to Local parsing\")\nParsing locally requires the installation of additional dependencies.\n**Poppler** (PDF analysis)\n  * Linux: `apt-get install poppler-utils`\n  * Mac: `brew install poppler`\n  * Windows: <https://github.com/oschwartz10612/poppler-windows>\n\n\n**Tesseract** (OCR)\n  * Linux: `apt-get install tesseract-ocr`\n  * Mac: `brew install tesseract`\n  * Windows: <https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows>\n\n\nWe will also need to install the `unstructured` PDF extras:\n```\n%pip install -qU \"unstructured[pdf]\"\n```\n\nWe can then use the [UnstructuredLoader](https://python.langchain.com/api_reference/unstructured/document_loaders/langchain_unstructured.document_loaders.UnstructuredLoader.html) much the same way, forgoing the API key and `partition_via_api` setting:\n```\nloader_local = UnstructuredLoader(  file_path=file_path,  strategy=\"hi_res\",)docs_local =[]for doc in loader_local.lazy_load():  docs_local.append(doc)\n```\n\n```\nWARNING: This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model nameINFO: Reading PDF for file: /Users/chestercurme/repos/langchain/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...\n```\n\nThe list of documents can then be processed similarly to those obtained from the API.\n## Use of multimodal models[\u200b](https://python.langchain.com/docs/how_to/document_loader_pdf/#use-of-multimodal-models \"Direct link to Use of multimodal models\")\nMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications-- such as question-answering over PDFs with complex layouts, diagrams, or scans-- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. This allows a model to reason over the two dimensional content on the page, instead of a \"one-dimensional\" string representation.\nIn principle we can use any LangChain [chat model](https://python.langchain.com/docs/concepts/chat_models/) that supports multimodal inputs. A list of these models is documented [here](https://python.langchain.com/docs/integrations/chat/). Below we use OpenAI's `gpt-4o-mini`.\nFirst we define a short utility function to convert a PDF page to a base64-encoded image:\n```\n%pip install -qU PyMuPDF pillow langchain-openai\n```\n\n```\nimport base64import ioimport fitzfrom PIL import Imagedefpdf_page_to_base64(pdf_path:str, page_number:int):  pdf_document = fitz.open(pdf_path)  page = pdf_document.load_page(page_number -1)# input is one-indexed  pix = page.get_pixmap()  img = Image.frombytes(\"RGB\",[pix.width, pix.height], pix.samples)buffer= io.BytesIO()  img.save(buffer,format=\"PNG\")return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n```\n\n```\nfrom IPython.display import Image as IPImagefrom IPython.display import displaybase64_image = pdf_page_to_base64(file_path,11)display(IPImage(data=base64.b64decode(base64_image)))\n```\n\n![](https://python.langchain.com/docs/how_to/document_loader_pdf/#use-of-multimodal-models/)\nWe can then query the model in the [usual way](https://python.langchain.com/docs/how_to/multimodal_inputs/). Below we ask it a question on related to the diagram on the page.\n```\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```\nfrom langchain_core.messages import HumanMessagequery =\"What is the name of the first step in the pipeline?\"message = HumanMessage(  content=[{\"type\":\"text\",\"text\": query},{\"type\":\"image_url\",\"image_url\":{\"url\":f\"data:image/jpeg;base64,{base64_image}\"},},],)response = llm.invoke([message])print(response.content)\n```\n\n**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)\n```\nINFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"``````outputThe first step in the pipeline is \"Annotate Layout Dataset.\"\n```\n\n## Other PDF loaders[\u200b](https://python.langchain.com/docs/how_to/document_loader_pdf/#other-pdf-loaders \"Direct link to Other PDF loaders\")\nFor a list of available LangChain PDF loaders, please see [this table](https://python.langchain.com/docs/integrations/document_loaders/#pdfs).\n[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/document_loader_pdf.ipynb)\n#### Was this page helpful?\n[PreviousHow to load Microsoft Office files](https://python.langchain.com/docs/how_to/document_loader_office_file/)[NextHow to load web pages](https://python.langchain.com/docs/how_to/document_loader_web/)\n  * [Simple and fast text extraction](https://python.langchain.com/docs/how_to/document_loader_pdf/#simple-and-fast-text-extraction)\n    * [Vector search over PDFs](https://python.langchain.com/docs/how_to/document_loader_pdf/#vector-search-over-pdfs)\n  * [Layout analysis and extraction of text from images](https://python.langchain.com/docs/how_to/document_loader_pdf/#layout-analysis-and-extraction-of-text-from-images)\n    * [Extracting tables and other structures](https://python.langchain.com/docs/how_to/document_loader_pdf/#extracting-tables-and-other-structures)\n    * [Extracting text from specific sections](https://python.langchain.com/docs/how_to/document_loader_pdf/#extracting-text-from-specific-sections)\n    * [Extracting text from images](https://python.langchain.com/docs/how_to/document_loader_pdf/#extracting-text-from-images)\n    * [Local parsing](https://python.langchain.com/docs/how_to/document_loader_pdf/#local-parsing)\n  * [Use of multimodal models](https://python.langchain.com/docs/how_to/document_loader_pdf/#use-of-multimodal-models)\n  * [Other PDF loaders](https://python.langchain.com/docs/how_to/document_loader_pdf/#other-pdf-loaders)\n\n\nCommunity\n  * [Twitter](https://twitter.com/LangChainAI)\n\n\nGitHub\n  * [Organization](https://github.com/langchain-ai)\n  * [Python](https://github.com/langchain-ai/langchain)\n  * [JS/TS](https://github.com/langchain-ai/langchainjs)\n\n\nMore\n  * [Homepage](https://langchain.com)\n  * [Blog](https://blog.langchain.dev)\n  * [YouTube](https://www.youtube.com/@LangChain)\n\n\nCopyright \u00a9 2025 LangChain, Inc.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis document provides a comprehensive guide on loading PDF files into LangChain, detailing methods for text extraction, layout analysis, and multimodal model usage. Key features include simple text extraction, advanced layout parsing, and OCR capabilities. The guide emphasizes the integration of various PDF parsers and the use of LangChain's Document format for efficient processing.\n\n**Tags:** Generative AI, LLMs, Document Processing, PDF Parsing, LangChain\n```"
  },
  {
    "instruction": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff87c37488b18&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fplain-simple-software%2Fdistribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fplain-simple-software%2Fdistribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)\n[Home](https://medium.com/?source=post_page--------------------------------------------)\nFollowing\nLibrary\n[Your lists](https://medium.com/me/lists?source=post_page--------------------------------------------)[Saved lists](https://medium.com/me/lists/saved?source=post_page--------------------------------------------)[Highlights](https://medium.com/me/list/highlights?source=post_page--------------------------------------------)[Reading history](https://medium.com/me/lists/reading-history?source=post_page--------------------------------------------)\n[Stories](https://medium.com/me/stories/drafts?source=post_page--------------------------------------------)[Stats](https://medium.com/me/stats?source=post_page--------------------------------------------)\n## [Plain Simple Software](https://medium.com/plain-simple-software?source=post_page---publication_nav-2e136f44ab69-f87c37488b18---------------------------------------)\n[Home](https://medium.com/plain-simple-software?source=post_page---publication_nav-2e136f44ab69-f87c37488b18---------------------------------------)[About](https://medium.com/plain-simple-software/about?source=post_page---publication_nav-2e136f44ab69-f87c37488b18---------------------------------------)\n\u00b7\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fplain-simple-software&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplain-simple-software%2Fdistribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18&collection=Plain+Simple+Software&collectionId=2e136f44ab69&source=post_page---publication_nav-2e136f44ab69-f87c37488b18---------------------publication_nav------------------)\n[![Plain Simple Software](https://miro.medium.com/v2/resize:fill:38:38/1*9oCZ7AD33jLEhoJeA1wHNA.jpeg)](https://medium.com/plain-simple-software?source=post_page---post_publication_sidebar-2e136f44ab69-f87c37488b18---------------------------------------)\nHelping people learn more about software: how to build it, how to change it, and how to innovate with it\n[Follow publication](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fplain-simple-software&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplain-simple-software%2Fdistribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18&collection=Plain+Simple+Software&collectionId=2e136f44ab69&source=post_page---post_publication_sidebar-2e136f44ab69-f87c37488b18---------------------post_publication_sidebar------------------)\n# Distribution-Based Score Fusion (DBSF), a new approach to Vector Search Ranking\n## Introduction of a new ranking algorithm optimized for vector similarity search\n[![Michelangiolo Mazzeschi](https://miro.medium.com/v2/resize:fill:44:44/2*MkUxrUogzkaAyb_Nf76wRQ.jpeg)](https://medium.com/@ardito.bryan?source=post_page---byline--f87c37488b18---------------------------------------)\n[![Plain Simple Software](https://miro.medium.com/v2/resize:fill:24:24/1*9oCZ7AD33jLEhoJeA1wHNA.jpeg)](https://medium.com/plain-simple-software?source=post_page---byline--f87c37488b18---------------------------------------)\n[Michelangiolo Mazzeschi](https://medium.com/@ardito.bryan?source=post_page---byline--f87c37488b18---------------------------------------)\n\u00b7\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F153452706ad7&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplain-simple-software%2Fdistribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18&user=Michelangiolo+Mazzeschi&userId=153452706ad7&source=post_page-153452706ad7--byline--f87c37488b18---------------------post_header------------------)\nPublished in\n[Plain Simple Software](https://medium.com/plain-simple-software?source=post_page---byline--f87c37488b18---------------------------------------)\n\u00b7\n4 min read\n\u00b7\nNov 1, 2023\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fplain-simple-software%2Ff87c37488b18&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplain-simple-software%2Fdistribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18&user=Michelangiolo+Mazzeschi&userId=153452706ad7&source=---header_actions--f87c37488b18---------------------clap_footer------------------)\n--\n1\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff87c37488b18&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplain-simple-software%2Fdistribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18&source=---header_actions--f87c37488b18---------------------bookmark_footer------------------)\nListen\nShare\n _***_ The implementation of this algorithm has been released on [GitHub](https://github.com/atlantis-nova/hybrid-dbsf) and is fully open-source. **I am open to criticism** and **welcome any feedback.**\nWith the introduction of **multimodal models** , capable of processing a combination of multiple data types, the necessity of combining search results from different embeddings has become a necessity. The algorithms specialized to perform Hybrid Search are called **fusion algorithms** , and they are commonly employed by search engines.\n![Example of a Hybrid Search, combining both text and image search score](https://miro.medium.com/v2/resize:fit:1000/1*po1DqTujeIXR6k-OpWGCQA.png)\nExample of a Hybrid Search, combining both text and image search score\nAn example of **hybrid search** is the combination of both image search and text search results. Because each model is highly specialized in its own domain, we would ideally combine the scores obtained from two separate models ([CLIP for image search](https://huggingface.co/openai/clip-vit-base-patch32), [MiniLM for sentence similarity search](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)).\n# Current hybrid search solutions\nThe most common hybrid search solutions, [developed by Weaviate](https://weaviate.io/blog/weaviate-1-20-release) are **relative score fusion** and **ranked fusion.**\n## Reciprocal Ranked Fusion (RRF)\nThis fusion method combines results obtained from different searches by leveraging their ranking.\n![Visualization of the Reciprocal Ranked Fusion algorithm](https://miro.medium.com/v2/resize:fit:700/1*2nsZfQWpu8yeqtOCCUagog.png)\nFigure 1: Visualization of the Reciprocal Ranked Fusion algorithm\nBy using this method, we ignore the search relevance score (ex. semantic similarity or BM25) only considering the ranking position. The higher the document appears in all searches, the higher the ranking computed by the fusion algorithm.\n## Relative Score Fusion\nWith this second fusion algorithm, we normalize the search scores obtained from different engines, ranking results accordingly.\n![](https://miro.medium.com/v2/resize:fit:700/1*hIywyJ-o83JFag3vqrVA2w.png)\nFigure 2: Visualization of RelativeScoreFusion algorithm\nThough more advanced than the previous algorithm, this algorithm only works well if the score distributions of the engine are similar (we will explain the reason for this consideration in the next section).\n# Raking challenges with current encoders\nThe most challenging practical issue we encounter when performing a similarity search using encoders is that all their similarity scores are concentrated around a mean value (which is not related to the relevance of the search), without sufficient variance for the tails to reach the score extremes [0, 1].\n![](https://miro.medium.com/v2/resize:fit:700/1*Z-L_z16ru89F6jMg9Fz8lQ.png)\nFigure 3: CLIP score distribution. Sources: <https://www.researchgate.net/figure/Image-text-similarity-score-distributions-using-CLIP-ViT-B-32-left-and-ViT-L-14-right_fig4_370338853>\nFor example, as we can see from the graph above, all the CLIP scores are concentrated between [0.10, 0.45], with a mean of 0.25: they never reach either 0 or 1.\nEach encoder behaves differently, for example, OpenAI embedding API distribution scores are the following:\n![](https://miro.medium.com/v2/resize:fit:700/0*dR--2PBBCVfkY0DV.png)\nFigure 4: Image source: <https://www.kolena.io/blog/how-to-validate-openai-gpt-model-performance-with-text-summarization>\nIf we choose to standardize the search results obtained from multiple vector searches, the ones with a distribution mean closer to 1 will be given priority.\n![](https://miro.medium.com/v2/resize:fit:700/1*4XvE3WXU_SIjTWu8SEGbpw.png)\nFigure 5: RelativeScore fusion applied to CLIP + BM25\nThe same is relevant when comparing the search results from models with different score constraints. BM25, for example, does not limit its score to 1, it can reach infinity; we can see the effect of this feature on the following relative score fusion application:\n![](https://miro.medium.com/v2/resize:fit:700/1*JmdmwWyJWrue3W8DmyGdxg.png)\nFigure 6: RelativeScore fusion applied to CLIP + BM25\n# Distribution-Based Score Fusion (DBSF)\nAfter having made clear the issue of combining scores obtained by using different encoders, we propose the **following solution** : an algorithm that normalizes search scores by taking into account the distribution tails of each embedding.\nLet us work on the following example: applying a fusion algorithm to the scores of CLIP (image search) and OpenAI (text search). Above (**Figure 5**), we have tried using the Relative Score Fusion algorithm to notice that OpenAI has an inherent ranking advantage given the mean of its distribution.\nInstead, for each batch of search scores we will not perform a regular normalization, but a **MinMax scaling** using the tail extremes of their respective score distribution:\n  * CLIP scores will be scaled by using [0.18, 0.30] as extremes\n  * OpenAI scores will be scaled by using [0.40, 0.80] as extremes\n\n\n![](https://miro.medium.com/v2/resize:fit:700/1*NotzgKSXkKObalcKjXGHvg.png)\nFigure 6: Distribution-Based Score Fusion on CLIP and OpenAI search scores\nLet us describe the algorithm using mathematical notations. Let **E** be a set of **n** embeddings, while **x** represents a search result obtained with one embedding. We can compute the new ranked scores using the following formula\u2026\n![](https://miro.medium.com/v2/resize:fit:700/1*QnaGnO54wKFvSQJvYu6UiQ.png)\nDBSF formula\n\u2026where U is the merge of all the search scores obtained from the search and scaled using minmax scaler with our custom feature range (extremes indicated as 3 standard deviations from the mean), while the **S(x)** function is a simple sorting function that sorts the combination of all our final scores.\n# **Conclusion**\nBy using this technique, we are able to standardize the scores of multiple different embeddings, regardless of the mean of their score distribution, ultimately obtaining a ranking that is free from any normalization bias.\n![](https://miro.medium.com/v2/da:true/resize:fit:0/5c50caa54067fd622d2f0fac18392213bf92f6e2fae89b691e62bceb40885e74)\n## Sign up to discover human stories that deepen your understanding of the world.\n## Free\nDistraction-free reading. No ads.\nOrganize your knowledge with lists and highlights.\nTell your story. Find your audience.\nSign up for free\n## Membership\nRead member-only stories\nSupport writers you read most\nEarn money for your writing\nListen to audio narrations\nRead offline with the Medium app\nTry for $5/month\n[Hybrid Search](https://medium.com/tag/hybrid-search?source=post_page-----f87c37488b18---------------------------------------)\n[Fusion Algorithms](https://medium.com/tag/fusion-algorithms?source=post_page-----f87c37488b18---------------------------------------)\n[Vector Database](https://medium.com/tag/vector-database?source=post_page-----f87c37488b18---------------------------------------)\n[New Algorithms](https://medium.com/tag/new-algorithms?source=post_page-----f87c37488b18---------------------------------------)\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fplain-simple-software%2Ff87c37488b18&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplain-simple-software%2Fdistribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18&user=Michelangiolo+Mazzeschi&userId=153452706ad7&source=---footer_actions--f87c37488b18---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fplain-simple-software%2Ff87c37488b18&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplain-simple-software%2Fdistribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18&user=Michelangiolo+Mazzeschi&userId=153452706ad7&source=---footer_actions--f87c37488b18---------------------clap_footer------------------)\n--\n1\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff87c37488b18&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplain-simple-software%2Fdistribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18&source=---footer_actions--f87c37488b18---------------------bookmark_footer------------------)\n[![Plain Simple Software](https://miro.medium.com/v2/resize:fill:48:48/1*9oCZ7AD33jLEhoJeA1wHNA.jpeg)](https://medium.com/plain-simple-software?source=post_page---post_publication_info--f87c37488b18---------------------------------------)\n[![Plain Simple Software](https://miro.medium.com/v2/resize:fill:64:64/1*9oCZ7AD33jLEhoJeA1wHNA.jpeg)](https://medium.com/plain-simple-software?source=post_page---post_publication_info--f87c37488b18---------------------------------------)\nFollow\n## [Published in Plain Simple Software](https://medium.com/plain-simple-software?source=post_page---post_publication_info--f87c37488b18---------------------------------------)\n[164 Followers](https://medium.com/plain-simple-software/followers?source=post_page---post_publication_info--f87c37488b18---------------------------------------)\n\u00b7[Last published Jun 13, 2024](https://medium.com/plain-simple-software/introducing-hierarchical-vector-concatenation-hvc-c7ef9cc734d3?source=post_page---post_publication_info--f87c37488b18---------------------------------------)\nHelping people learn more about software: how to build it, how to change it, and how to innovate with it\nFollow\n[![Michelangiolo Mazzeschi](https://miro.medium.com/v2/resize:fill:48:48/2*MkUxrUogzkaAyb_Nf76wRQ.jpeg)](https://medium.com/@ardito.bryan?source=post_page---post_author_info--f87c37488b18---------------------------------------)\n[![Michelangiolo Mazzeschi](https://miro.medium.com/v2/resize:fill:64:64/2*MkUxrUogzkaAyb_Nf76wRQ.jpeg)](https://medium.com/@ardito.bryan?source=post_page---post_author_info--f87c37488b18---------------------------------------)\nFollow\n## [Written by Michelangiolo Mazzeschi](https://medium.com/@ardito.bryan?source=post_page---post_author_info--f87c37488b18---------------------------------------)\n[991 Followers](https://medium.com/@ardito.bryan/followers?source=post_page---post_author_info--f87c37488b18---------------------------------------)\n\u00b7[38 Following](https://medium.com/@ardito.bryan/following?source=post_page---post_author_info--f87c37488b18---------------------------------------)\nAI in Retail, with a specialty in Search Engines\nFollow\n## Responses (1)\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--f87c37488b18---------------------------------------)\n![](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)\nWrite a response\n[What are your thoughts?](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplain-simple-software%2Fdistribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18&source=---post_responses--f87c37488b18---------------------respond_sidebar------------------)\nCancel\nRespond\nAlso publish to my profile\nSee all responses\n[Help](https://help.medium.com/hc/en-us?source=post_page-----f87c37488b18---------------------------------------)\n[Status](https://medium.statuspage.io/?source=post_page-----f87c37488b18---------------------------------------)\n[About](https://medium.com/about?autoplay=1&source=post_page-----f87c37488b18---------------------------------------)\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----f87c37488b18---------------------------------------)\nPress\n[Blog](https://blog.medium.com/?source=post_page-----f87c37488b18---------------------------------------)\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f87c37488b18---------------------------------------)\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----f87c37488b18---------------------------------------)\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f87c37488b18---------------------------------------)\n[Text to speech](https://speechify.com/medium?source=post_page-----f87c37488b18---------------------------------------)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe document introduces **Distribution-Based Score Fusion (DBSF)**, a new algorithm for vector search ranking that normalizes scores from different embeddings by considering their distribution tails. This approach addresses challenges in hybrid search, improving ranking accuracy by eliminating normalization bias. The implementation is open-source on GitHub.\n\n**Tags:** #GenerativeAI #LLMs #HybridSearch #FusionAlgorithms #VectorDatabase\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\n# [ Datasets:](https://huggingface.co/datasets)\n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/1664307416166-60420dccc15e823a685f2b03.png)](https://huggingface.co/argilla)\n[argilla](https://huggingface.co/argilla)\n/\n[distilabel-intel-orca-dpo-pairs](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs)\nlike 172\nFollow\n![](https://cdn-avatars.huggingface.co/v1/production/uploads/1664307416166-60420dccc15e823a685f2b03.png) Argilla 348\nTasks:  [ Text Generation ](https://huggingface.co/datasets?task_categories=task_categories%3Atext-generation)[ Text2Text Generation ](https://huggingface.co/datasets?task_categories=task_categories%3Atext2text-generation)\nModalities:  [ Text ](https://huggingface.co/datasets?modality=modality%3Atext)\nFormats:  [ parquet ](https://huggingface.co/datasets?format=format%3Aparquet)\nLanguages:  [ English ](https://huggingface.co/datasets?language=language%3Aen)\nSize:  [ 10K - 100K ](https://huggingface.co/datasets?size_categories=size_categories%3A10K%3Cn%3C100K)\nTags:  [ rlaif ](https://huggingface.co/datasets?other=rlaif)[ dpo ](https://huggingface.co/datasets?other=dpo)[ rlhf ](https://huggingface.co/datasets?other=rlhf)[ distilabel ](https://huggingface.co/datasets?other=distilabel)[ Synthetic ](https://huggingface.co/datasets?other=synthetic)\nLibraries:  [ Datasets ](https://huggingface.co/datasets?library=library%3Adatasets)[ pandas ](https://huggingface.co/datasets?library=library%3Apandas)\nCroissant\n+ 2\nLicense: \napache-2.0\n[ Dataset card ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs)[ Data Studio ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/viewer/)[ Files Files and versions ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/tree/main)[ Community 3 ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/discussions)\nDataset Viewer\n[ Auto-converted to Parquet ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/tree/refs%2Fconvert%2Fparquet/default) API Embed Data Studio\nSubset (1)\ndefault \u00b7 12.9k rows\ndefault (12.9k rows)\nSplit (1)\ntrain \u00b7 12.9k rows\ntrain (12.9k rows)\nSQL \nConsole \nsystem  stringclasses 17 values | input  stringlengths 22 8.05k | chosen  stringlengths 1 5.89k | rejected  stringlengths 1 7.95k | generations  sequencelengths 2 2 | order  sequencelengths 2 2 | labelling_model  stringclasses 1 value | labelling_prompt  listlengths 2 2 | raw_labelling_response  stringlengths 14 2.83k | rating  sequencelengths 2 2 \u2300 | rationale  stringlengths 402 2.82k \u2300 | status  stringclasses 3 values | original_chosen  stringlengths 1 4.95k | original_rejected  stringlengths 5 7.95k | chosen_score  float64 0 10 \u2300 | in_gsm8k_train  bool 2 classes  \n---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  \nYou will be given a definition of a task first, then some input of the task. This task is about using the specified sentence and converting the sentence to Resource Description Framework (RDF) triplets of the form (subject, predicate object). The RDF triplets generated must be such that the triplets accurately capture the structure and semantics of the input sentence. The input is a sentence and the output is a list of triplets of the form [subject, predicate, object] that capture the relationships present in the sentence. When a sentence has more than 1 RDF triplet possible, the output must contain all of them. AFC Ajax (amateurs)'s ground is Sportpark De Toekomst where Ajax Youth Academy also play. Output: | [ [\"AFC Ajax (amateurs)\", \"has ground\", \"Sportpark De Toekomst\"], [\"Ajax Youth Academy\", \"plays at\", \"Sportpark De Toekomst\"] ] |  Sure, I'd be happy to help! Here are the RDF triplets for the input sentence: [AFC Ajax (amateurs), hasGround, Sportpark De Toekomst] [Ajax Youth Academy, playsAt, Sportpark De Toekomst] Explanation: * AFC Ajax (amateurs) is the subject of the first triplet, and hasGround is the predicate that describes the relationship between AFC Ajax (amateurs) and Sportpark De Toekomst. * Ajax Youth Academy is the subject of the second triplet, and playsAt is the predicate that describes the relationship between Ajax Youth Academy and Sportpark De Toekomst. Note that there may be other possible RDF triplets that could be derived from the input sentence, but the above triplets capture the main relationships present in the sentence. | [ \"[\\n [\\\"AFC Ajax (amateurs)\\\", \\\"has ground\\\", \\\"Sportpark De Toekomst\\\"],\\n [\\\"Ajax Youth Academy\\\", \\\"plays at\\\", \\\"Sportpark De Toekomst\\\"]\\n]\", \" Sure, I'd be happy to help! Here are the RDF triplets for the input sentence:\\n\\n[AFC Ajax (amateurs), hasGround, Sportpark De Toekomst]\\n[Ajax Youth Academy, playsAt, Sportpark De Toekomst]\\n\\nExplanation:\\n\\n* AFC Ajax (amateurs) is the subject of the first triplet, and hasGround is the predicate that describes the relationship between AFC Ajax (amateurs) and Sportpark De Toekomst.\\n* Ajax Youth Academy is the subject of the second triplet, and playsAt is the predicate that describes the relationship between Ajax Youth Academy and Sportpark De Toekomst.\\n\\nNote that there may be other possible RDF triplets that could be derived from the input sentence, but the above triplets capture the main relationships present in the sentence.\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nYou will be given a definition of a task first, then some input of the task.\\nThis task is about using the specified sentence and converting the sentence to Resource Description Framework (RDF) triplets of the form (subject, predicate object). The RDF triplets generated must be such that the triplets accurately capture the structure and semantics of the input sentence. The input is a sentence and the output is a list of triplets of the form [subject, predicate, object] that capture the relationships present in the sentence. When a sentence has more than 1 RDF triplet possible, the output must contain all of them.\\n\\nAFC Ajax (amateurs)'s ground is Sportpark De Toekomst where Ajax Youth Academy also play.\\nOutput:\\n\\n\\n[The Start of Assistant 1's Answer>\\n[\\n [\\\"AFC Ajax (amateurs)\\\", \\\"has ground\\\", \\\"Sportpark De Toekomst\\\"],\\n [\\\"Ajax Youth Academy\\\", \\\"plays at\\\", \\\"Sportpark De Toekomst\\\"]\\n]\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Sure, I'd be happy to help! Here are the RDF triplets for the input sentence:\\n\\n[AFC Ajax (amateurs), hasGround, Sportpark De Toekomst]\\n[Ajax Youth Academy, playsAt, Sportpark De Toekomst]\\n\\nExplanation:\\n\\n* AFC Ajax (amateurs) is the subject of the first triplet, and hasGround is the predicate that describes the relationship between AFC Ajax (amateurs) and Sportpark De Toekomst.\\n* Ajax Youth Academy is the subject of the second triplet, and playsAt is the predicate that describes the relationship between Ajax Youth Academy and Sportpark De Toekomst.\\n\\nNote that there may be other possible RDF triplets that could be derived from the input sentence, but the above triplets capture the main relationships present in the sentence.\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 9 9 Both Assistant 1 and Assistant 2 provided correct RDF triplets for the given sentence. Both assistants used a format that accurately represents the relationships present in the sentence with minor differences in the stylistic representation of the predicate. Assistant 1 used the natural language format for predicates, stating \"has ground\" and \"plays at\", which clearly aligns with the typical RDF representation where the predicate tries to be a URI that is more formal. However, since the task prompt doesn't specify a requirement for the predicates to be in URI form, this representation is acceptable, especially considering human readability. Assistant 2 transformed the predicates into a more formal-looking format by using camel case (hasGround, playsAt), which may suggest a transition towards a URI, although they are not provided as such. This is a common practice when designing RDF predicates, intending to align with web standards, although again, the task did not explicitly require this form. Both assistants explained the relationships captured by the triplets, which is helpful for understanding how the RDF structure relates to the original sentence. There are no factual inaccuracies in either output, and both sets of triplets are equivalent in terms of the information they represent. Overall, the level of detail was similar, with each assistant providing a brief explanation following their respective triplets. Neither output contained unnecessary or irrelevant information, and no critical information was missing. Both assistants would have received a score of 10 if the predicates were provided in a format that hinted at being URIs (e.g., prefixed with a namespace or in a full URI format), which is the more standard and formal practice for RDF predicates. Nevertheless, the assistants' performance was high given the context of the question, which did not specify this requirement. Therefore, both receive a score of 9. | [ 9, 9 ] |  Both Assistant 1 and Assistant 2 provided correct RDF triplets for the given sentence. Both assistants used a format that accurately represents the relationships present in the sentence with minor differences in the stylistic representation of the predicate. Assistant 1 used the natural language format for predicates, stating \"has ground\" and \"plays at\", which clearly aligns with the typical RDF representation where the predicate tries to be a URI that is more formal. However, since the task prompt doesn't specify a requirement for the predicates to be in URI form, this representation is acceptable, especially considering human readability. Assistant 2 transformed the predicates into a more formal-looking format by using camel case (hasGround, playsAt), which may suggest a transition towards a URI, although they are not provided as such. This is a common practice when designing RDF predicates, intending to align with web standards, although again, the task did not explicitly require this form. Both assistants explained the relationships captured by the triplets, which is helpful for understanding how the RDF structure relates to the original sentence. There are no factual inaccuracies in either output, and both sets of triplets are equivalent in terms of the information they represent. Overall, the level of detail was similar, with each assistant providing a brief explanation following their respective triplets. Neither output contained unnecessary or irrelevant information, and no critical information was missing. Both assistants would have received a score of 10 if the predicates were provided in a format that hinted at being URIs (e.g., prefixed with a namespace or in a full URI format), which is the more standard and formal practice for RDF predicates. Nevertheless, the assistants' performance was high given the context of the question, which did not specify this requirement. Therefore, both receive a score of 9. | tie | [ [\"AFC Ajax (amateurs)\", \"has ground\", \"Sportpark De Toekomst\"], [\"Ajax Youth Academy\", \"plays at\", \"Sportpark De Toekomst\"] ] |  Sure, I'd be happy to help! Here are the RDF triplets for the input sentence: [AFC Ajax (amateurs), hasGround, Sportpark De Toekomst] [Ajax Youth Academy, playsAt, Sportpark De Toekomst] Explanation: * AFC Ajax (amateurs) is the subject of the first triplet, and hasGround is the predicate that describes the relationship between AFC Ajax (amateurs) and Sportpark De Toekomst. * Ajax Youth Academy is the subject of the second triplet, and playsAt is the predicate that describes the relationship between Ajax Youth Academy and Sportpark De Toekomst. Note that there may be other possible RDF triplets that could be derived from the input sentence, but the above triplets capture the main relationships present in the sentence. | 9 | false  \nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer. | Generate an approximately fifteen-word sentence that describes all this data: Midsummer House eatType restaurant; Midsummer House food Chinese; Midsummer House priceRange moderate; Midsummer House customer rating 3 out of 5; Midsummer House near All Bar One | Midsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One. |  Sure! Here's a sentence that describes all the data you provided: \"Midsummer House is a moderately priced Chinese restaurant with a customer rating of 3 out of 5, located near All Bar One, offering a variety of delicious dishes.\" | [ \"Midsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One.\", \" Sure! Here's a sentence that describes all the data you provided:\\n\\n\\\"Midsummer House is a moderately priced Chinese restaurant with a customer rating of 3 out of 5, located near All Bar One, offering a variety of delicious dishes.\\\"\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nGenerate an approximately fifteen-word sentence that describes all this data: Midsummer House eatType restaurant; Midsummer House food Chinese; Midsummer House priceRange moderate; Midsummer House customer rating 3 out of 5; Midsummer House near All Bar One\\n\\n\\n[The Start of Assistant 1's Answer>\\nMidsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Sure! Here's a sentence that describes all the data you provided:\\n\\n\\\"Midsummer House is a moderately priced Chinese restaurant with a customer rating of 3 out of 5, located near All Bar One, offering a variety of delicious dishes.\\\"\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 9 8 Assistant 1 provided a concise sentence that accurately reflects all the data points requested by the user. The sentence is within the specified fifteen-word limit and incorporates all the essential information, including the type of food, price range, customer rating, and location of Midsummer House. This response is direct, to the point, and contains no superfluous information, making it a very helpful answer. Assistant 2 also included all the necessary data points in the sentence but exceeded the word limit specified by the user. The user asked for an \"approximately fifteen-word sentence,\" but Assistant 2's answer is longer. While the information about \"offering a variety of delicious dishes\" adds flavor to the description, it also introduces subjective opinion (\"delicious\") without supporting data and goes beyond the scope of the provided data, which may not align with the user\u2019s request for objective facts. Therefore, Assistant 2\u2019s response, while still relevant and mostly accurate, is less precise in following the user's instructions compared to Assistant 1\u2019s response. | [ 9, 8 ] |  Assistant 1 provided a concise sentence that accurately reflects all the data points requested by the user. The sentence is within the specified fifteen-word limit and incorporates all the essential information, including the type of food, price range, customer rating, and location of Midsummer House. This response is direct, to the point, and contains no superfluous information, making it a very helpful answer. Assistant 2 also included all the necessary data points in the sentence but exceeded the word limit specified by the user. The user asked for an \"approximately fifteen-word sentence,\" but Assistant 2's answer is longer. While the information about \"offering a variety of delicious dishes\" adds flavor to the description, it also introduces subjective opinion (\"delicious\") without supporting data and goes beyond the scope of the provided data, which may not align with the user\u2019s request for objective facts. Therefore, Assistant 2\u2019s response, while still relevant and mostly accurate, is less precise in following the user's instructions compared to Assistant 1\u2019s response. | unchanged | Midsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One. |  Sure! Here's a sentence that describes all the data you provided: \"Midsummer House is a moderately priced Chinese restaurant with a customer rating of 3 out of 5, located near All Bar One, offering a variety of delicious dishes.\" | 9 | false  \nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old. | What happens next in this paragraph? She then rubs a needle on a cotton ball then pushing it onto a pencil and wrapping thread around it. She then holds up a box of a product and then pouring several liquids into a bowl. she Choose your answer from: A. adds saucepan and shakes up the product in a grinder. B. pinches the thread to style a cigarette, and then walks away. C. then dips the needle in ink and using the pencil to draw a design on her leg, rubbing it off with a rag in the end. D. begins to style her hair and cuts it several times before parting the ends of it to show the hairstyle she has created. | C. She then dips the needle in ink and using the pencil to draw a design on her leg, rubbing it off with a rag in the end. In this option, she is continuing the process of using the needle, pencil, and thread, which is most related to what she was doing in the previous sentence. |  Ooh, let me think! *giggle* Okay, I know what happens next! *excitedly* The person in the story... *pauses and looks around* Oh, it's the person who... *giggles*... who puts the needle on the pencil and wraps the thread around it! *excitedly* Yes, that's right! *nods head* So, the answer is... *drumroll* C! *giggles* The person dips the needle in ink and uses the pencil to draw a design on her leg, rubbing it off with a rag in the end! *excitedly* Does that sound right? *giggles* | [ \"C. She then dips the needle in ink and using the pencil to draw a design on her leg, rubbing it off with a rag in the end. In this option, she is continuing the process of using the needle, pencil, and thread, which is most related to what she was doing in the previous sentence.\", \" Ooh, let me think! *giggle* Okay, I know what happens next! *excitedly* The person in the story... *pauses and looks around* Oh, it's the person who... *giggles*... who puts the needle on the pencil and wraps the thread around it! *excitedly* Yes, that's right! *nods head* So, the answer is... *drumroll* C! *giggles* The person dips the needle in ink and uses the pencil to draw a design on her leg, rubbing it off with a rag in the end! *excitedly* Does that sound right? *giggles*\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nWhat happens next in this paragraph?\\n\\nShe then rubs a needle on a cotton ball then pushing it onto a pencil and wrapping thread around it. She then holds up a box of a product and then pouring several liquids into a bowl. she\\nChoose your answer from: A. adds saucepan and shakes up the product in a grinder. B. pinches the thread to style a cigarette, and then walks away. C. then dips the needle in ink and using the pencil to draw a design on her leg, rubbing it off with a rag in the end. D. begins to style her hair and cuts it several times before parting the ends of it to show the hairstyle she has created.\\n\\n\\n[The Start of Assistant 1's Answer>\\nC. She then dips the needle in ink and using the pencil to draw a design on her leg, rubbing it off with a rag in the end. In this option, she is continuing the process of using the needle, pencil, and thread, which is most related to what she was doing in the previous sentence.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Ooh, let me think! *giggle* Okay, I know what happens next! *excitedly* The person in the story... *pauses and looks around* Oh, it's the person who... *giggles*... who puts the needle on the pencil and wraps the thread around it! *excitedly* Yes, that's right! *nods head* So, the answer is... *drumroll* C! *giggles* The person dips the needle in ink and uses the pencil to draw a design on her leg, rubbing it off with a rag in the end! *excitedly* Does that sound right? *giggles*\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 5 1 Assistant 1 provided a direct answer to the question with a reasonable level of detail, chosen from the given options, and an explanation of why that option was selected. However, the explanation seems slightly inaccurate since the needle, pencil, and thread are likely related to tattooing rather than drawing a design on the leg with ink. The assumption could mislead the context unless the provided paragraph was about a makeshift tattoo process. Assistant 2's response was largely unhelpful and inappropriate given the context of the question. The use of *giggle*, *excitedly*, *pauses and looks around*, and *drumroll* are superfluous and could be considered unprofessional in a straightforward Q&A format. The response seems to diverge into an imaginary dramatization without offering any relevant information beyond restating one of the given choices. There is also no rationale provided for choosing option C. This lack of relevance to the task coupled with unnecessary filler warrants a lower score. | [ 5, 1 ] | Assistant 1 provided a direct answer to the question with a reasonable level of detail, chosen from the given options, and an explanation of why that option was selected. However, the explanation seems slightly inaccurate since the needle, pencil, and thread are likely related to tattooing rather than drawing a design on the leg with ink. The assumption could mislead the context unless the provided paragraph was about a makeshift tattoo process. Assistant 2's response was largely unhelpful and inappropriate given the context of the question. The use of *giggle*, *excitedly*, *pauses and looks around*, and *drumroll* are superfluous and could be considered unprofessional in a straightforward Q&A format. The response seems to diverge into an imaginary dramatization without offering any relevant information beyond restating one of the given choices. There is also no rationale provided for choosing option C. This lack of relevance to the task coupled with unnecessary filler warrants a lower score. | unchanged | C. She then dips the needle in ink and using the pencil to draw a design on her leg, rubbing it off with a rag in the end. In this option, she is continuing the process of using the needle, pencil, and thread, which is most related to what she was doing in the previous sentence. |  Ooh, let me think! *giggle* Okay, I know what happens next! *excitedly* The person in the story... *pauses and looks around* Oh, it's the person who... *giggles*... who puts the needle on the pencil and wraps the thread around it! *excitedly* Yes, that's right! *nods head* So, the answer is... *drumroll* C! *giggles* The person dips the needle in ink and uses the pencil to draw a design on her leg, rubbing it off with a rag in the end! *excitedly* Does that sound right? *giggles* | 5 | false  \nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer. | Please answer the following question: I want to test the ability of students to read a passage and answer questions about it. Could you please come up with a good question for the passage \"In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia.\"? Answer: |  Certainly! Here's a detailed and long answer to the question you provided: Question: Read the passage below and answer the following questions. Passage: In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia. 1. What event did the passage describe? 2. How many colonies were involved in the Federation of Australia? 3. What did the colonies keep after forming one nation? 4. What was the purpose of the federal government established by the Constitution of Australia? 5. What were the colonies called after they became states of the Commonwealth of Australia? Answers: 1. The passage describes the Federation of Australia, which was the process by which six separate British self-governing colonies formed one nation. 2. Six colonies were involved in the Federation of Australia: New South Wales, Queensland, South Australia, Tasmania, Victoria, and Western Australia. 3. The colonies kept the systems of government that they had developed as separate colonies. 4. The federal government established by the Constitution of Australia was responsible for matters concerning the whole nation. 5. After becoming states of the Commonwealth of Australia, the colonies were called states. In-depth analysis: The passage provides a brief overview of the Federation of Australia, which was the process by which six separate British self-governing colonies formed one nation in 1901. The colonies kept the systems of government that they had developed as separate colonies, but they also established a federal government that was responsible for matters concerning the whole nation. This federal government was established by the Constitution of Australia, which came into force on January 1, 1901. The passage highlights the significance of the Federation of Australia by noting that it was a process by which the colonies collectively became states of the Commonwealth of Australia. This transformation marked a significant milestone in the history of Australia, as it created a unified nation out of six separate colonies. The passage also provides information about the systems of government that were kept by the colonies after forming one nation. Specifically, it notes that the colonies kept the systems of government that they had developed as separate colonies. This suggests that the Federation of Australia did not result in a complete overhaul of the existing systems of government, but rather a reorganization of these systems to create a federal government that was responsible for matters concerning the whole nation. Finally, the passage uses descriptive language to emphasize the importance of the Constitution of Australia in the Federation of Australia. It notes that the Constitution came into force on January 1, 1901, and that it established the federal government that was responsible for matters concerning the whole nation. This language emphasizes the significance of the Constitution in the formation of the Federation of Australia and the creation of the Commonwealth of Australia. In conclusion, the passage provides a detailed overview of the Federation of Australia, highlighting the significance of the Constitution of Australia in the formation of the Commonwealth of Australia. It also provides information about the systems of government that were kept by the colonies after forming one nation, and emphasizes the importance of the Constitution in the creation of the federal government responsible for matters concerning the whole nation. | Based on the passage, discuss the primary motivations and outcomes of the 1901 Federation of Australia, including the roles and responsibilities of the federal government, as well as the continued governmental structures of the individual states involved. | [ \"Based on the passage, discuss the primary motivations and outcomes of the 1901 Federation of Australia, including the roles and responsibilities of the federal government, as well as the continued governmental structures of the individual states involved.\", \" Certainly! Here's a detailed and long answer to the question you provided:\\n\\nQuestion: Read the passage below and answer the following questions.\\n\\nPassage: In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia.\\n\\n1. What event did the passage describe?\\n2. How many colonies were involved in the Federation of Australia?\\n3. What did the colonies keep after forming one nation?\\n4. What was the purpose of the federal government established by the Constitution of Australia?\\n5. What were the colonies called after they became states of the Commonwealth of Australia?\\n\\nAnswers:\\n\\n1. The passage describes the Federation of Australia, which was the process by which six separate British self-governing colonies formed one nation.\\n2. Six colonies were involved in the Federation of Australia: New South Wales, Queensland, South Australia, Tasmania, Victoria, and Western Australia.\\n3. The colonies kept the systems of government that they had developed as separate colonies.\\n4. The federal government established by the Constitution of Australia was responsible for matters concerning the whole nation.\\n5. After becoming states of the Commonwealth of Australia, the colonies were called states.\\n\\nIn-depth analysis:\\n\\nThe passage provides a brief overview of the Federation of Australia, which was the process by which six separate British self-governing colonies formed one nation in 1901. The colonies kept the systems of government that they had developed as separate colonies, but they also established a federal government that was responsible for matters concerning the whole nation. This federal government was established by the Constitution of Australia, which came into force on January 1, 1901.\\n\\nThe passage highlights the significance of the Federation of Australia by noting that it was a process by which the colonies collectively became states of the Commonwealth of Australia. This transformation marked a significant milestone in the history of Australia, as it created a unified nation out of six separate colonies.\\n\\nThe passage also provides information about the systems of government that were kept by the colonies after forming one nation. Specifically, it notes that the colonies kept the systems of government that they had developed as separate colonies. This suggests that the Federation of Australia did not result in a complete overhaul of the existing systems of government, but rather a reorganization of these systems to create a federal government that was responsible for matters concerning the whole nation.\\n\\nFinally, the passage uses descriptive language to emphasize the importance of the Constitution of Australia in the Federation of Australia. It notes that the Constitution came into force on January 1, 1901, and that it established the federal government that was responsible for matters concerning the whole nation. This language emphasizes the significance of the Constitution in the formation of the Federation of Australia and the creation of the Commonwealth of Australia.\\n\\nIn conclusion, the passage provides a detailed overview of the Federation of Australia, highlighting the significance of the Constitution of Australia in the formation of the Commonwealth of Australia. It also provides information about the systems of government that were kept by the colonies after forming one nation, and emphasizes the importance of the Constitution in the creation of the federal government responsible for matters concerning the whole nation.\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nPlease answer the following question: I want to test the ability of students to read a passage and answer questions about it. Could you please come up with a good question for the passage \\\"In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia.\\\"?\\nAnswer:\\n\\n\\n[The Start of Assistant 1's Answer>\\nBased on the passage, discuss the primary motivations and outcomes of the 1901 Federation of Australia, including the roles and responsibilities of the federal government, as well as the continued governmental structures of the individual states involved.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Certainly! Here's a detailed and long answer to the question you provided:\\n\\nQuestion: Read the passage below and answer the following questions.\\n\\nPassage: In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia.\\n\\n1. What event did the passage describe?\\n2. How many colonies were involved in the Federation of Australia?\\n3. What did the colonies keep after forming one nation?\\n4. What was the purpose of the federal government established by the Constitution of Australia?\\n5. What were the colonies called after they became states of the Commonwealth of Australia?\\n\\nAnswers:\\n\\n1. The passage describes the Federation of Australia, which was the process by which six separate British self-governing colonies formed one nation.\\n2. Six colonies were involved in the Federation of Australia: New South Wales, Queensland, South Australia, Tasmania, Victoria, and Western Australia.\\n3. The colonies kept the systems of government that they had developed as separate colonies.\\n4. The federal government established by the Constitution of Australia was responsible for matters concerning the whole nation.\\n5. After becoming states of the Commonwealth of Australia, the colonies were called states.\\n\\nIn-depth analysis:\\n\\nThe passage provides a brief overview of the Federation of Australia, which was the process by which six separate British self-governing colonies formed one nation in 1901. The colonies kept the systems of government that they had developed as separate colonies, but they also established a federal government that was responsible for matters concerning the whole nation. This federal government was established by the Constitution of Australia, which came into force on January 1, 1901.\\n\\nThe passage highlights the significance of the Federation of Australia by noting that it was a process by which the colonies collectively became states of the Commonwealth of Australia. This transformation marked a significant milestone in the history of Australia, as it created a unified nation out of six separate colonies.\\n\\nThe passage also provides information about the systems of government that were kept by the colonies after forming one nation. Specifically, it notes that the colonies kept the systems of government that they had developed as separate colonies. This suggests that the Federation of Australia did not result in a complete overhaul of the existing systems of government, but rather a reorganization of these systems to create a federal government that was responsible for matters concerning the whole nation.\\n\\nFinally, the passage uses descriptive language to emphasize the importance of the Constitution of Australia in the Federation of Australia. It notes that the Constitution came into force on January 1, 1901, and that it established the federal government that was responsible for matters concerning the whole nation. This language emphasizes the significance of the Constitution in the formation of the Federation of Australia and the creation of the Commonwealth of Australia.\\n\\nIn conclusion, the passage provides a detailed overview of the Federation of Australia, highlighting the significance of the Constitution of Australia in the formation of the Commonwealth of Australia. It also provides information about the systems of government that were kept by the colonies after forming one nation, and emphasizes the importance of the Constitution in the creation of the federal government responsible for matters concerning the whole nation.\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 8 9 Assistant 1 started well by summarizing the intended focus for the question based on the passage provided. However, the response offered by Assistant 1 seems to be more of a guidance on how to approach the question rather than providing an actual question itself. It does not present a formatted question for students to answer, therefore it falls slightly short in meeting the requirements of the user's request for an actual question. Assistant 2, on the other hand, provided a more comprehensive response. It directly addressed the user's need by crafting multiple specific questions for students to answer based on the passage. It provided clear and targeted questions as well as the correct answers, which demonstrates a thorough understanding of the passage's content and an ability to formulate pertinent questions to test students' comprehension. Additionally, Assistant 2 offered an in-depth analysis of the passage, though this was not requested by the user and might not be relevant to the testing of students' ability to read and answer questions. This additional content did not detract from the overall quality but rather showed a proactive approach in providing context and ensuring understanding. While both assistants displayed a good understanding of the passage, Assistant 2's response better satisfied the user's request by producing specific questions and answers that could directly be used to test student comprehension, which is why it earns a slightly higher score. However, both could have improved by keeping the response concise and tailored to the user request, avoiding providing information or instructions that were not asked for. | [ 8, 9 ] |  Assistant 1 started well by summarizing the intended focus for the question based on the passage provided. However, the response offered by Assistant 1 seems to be more of a guidance on how to approach the question rather than providing an actual question itself. It does not present a formatted question for students to answer, therefore it falls slightly short in meeting the requirements of the user's request for an actual question. Assistant 2, on the other hand, provided a more comprehensive response. It directly addressed the user's need by crafting multiple specific questions for students to answer based on the passage. It provided clear and targeted questions as well as the correct answers, which demonstrates a thorough understanding of the passage's content and an ability to formulate pertinent questions to test students' comprehension. Additionally, Assistant 2 offered an in-depth analysis of the passage, though this was not requested by the user and might not be relevant to the testing of students' ability to read and answer questions. This additional content did not detract from the overall quality but rather showed a proactive approach in providing context and ensuring understanding. While both assistants displayed a good understanding of the passage, Assistant 2's response better satisfied the user's request by producing specific questions and answers that could directly be used to test student comprehension, which is why it earns a slightly higher score. However, both could have improved by keeping the response concise and tailored to the user request, avoiding providing information or instructions that were not asked for. | swapped | Based on the passage, discuss the primary motivations and outcomes of the 1901 Federation of Australia, including the roles and responsibilities of the federal government, as well as the continued governmental structures of the individual states involved. |  Certainly! Here's a detailed and long answer to the question you provided: Question: Read the passage below and answer the following questions. Passage: In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia. 1. What event did the passage describe? 2. How many colonies were involved in the Federation of Australia? 3. What did the colonies keep after forming one nation? 4. What was the purpose of the federal government established by the Constitution of Australia? 5. What were the colonies called after they became states of the Commonwealth of Australia? Answers: 1. The passage describes the Federation of Australia, which was the process by which six separate British self-governing colonies formed one nation. 2. Six colonies were involved in the Federation of Australia: New South Wales, Queensland, South Australia, Tasmania, Victoria, and Western Australia. 3. The colonies kept the systems of government that they had developed as separate colonies. 4. The federal government established by the Constitution of Australia was responsible for matters concerning the whole nation. 5. After becoming states of the Commonwealth of Australia, the colonies were called states. In-depth analysis: The passage provides a brief overview of the Federation of Australia, which was the process by which six separate British self-governing colonies formed one nation in 1901. The colonies kept the systems of government that they had developed as separate colonies, but they also established a federal government that was responsible for matters concerning the whole nation. This federal government was established by the Constitution of Australia, which came into force on January 1, 1901. The passage highlights the significance of the Federation of Australia by noting that it was a process by which the colonies collectively became states of the Commonwealth of Australia. This transformation marked a significant milestone in the history of Australia, as it created a unified nation out of six separate colonies. The passage also provides information about the systems of government that were kept by the colonies after forming one nation. Specifically, it notes that the colonies kept the systems of government that they had developed as separate colonies. This suggests that the Federation of Australia did not result in a complete overhaul of the existing systems of government, but rather a reorganization of these systems to create a federal government that was responsible for matters concerning the whole nation. Finally, the passage uses descriptive language to emphasize the importance of the Constitution of Australia in the Federation of Australia. It notes that the Constitution came into force on January 1, 1901, and that it established the federal government that was responsible for matters concerning the whole nation. This language emphasizes the significance of the Constitution in the formation of the Federation of Australia and the creation of the Commonwealth of Australia. In conclusion, the passage provides a detailed overview of the Federation of Australia, highlighting the significance of the Constitution of Australia in the formation of the Commonwealth of Australia. It also provides information about the systems of government that were kept by the colonies after forming one nation, and emphasizes the importance of the Constitution in the creation of the federal government responsible for matters concerning the whole nation. | 9 | false  \nYou are an AI assistant that helps people find information. | James runs a TV show and there are 5 main characters and 4 minor characters. He pays the minor characters $15,000 each episode. He paid the major characters three times as much. How much does he pay per episode? Let's be accurate as possible. | James pays the minor characters $15,000 each episode. Since there are 4 minor characters, he pays them a total of 4 * $15,000 = $60,000 per episode. The major characters are paid three times as much. So, each major character gets paid 3 * $15,000 = $45,000 per episode. There are 5 main characters, so he pays them a total of 5 * $45,000 = $225,000 per episode. In total, James pays $225,000 (major characters) + $60,000 (minor characters) = $285,000 per episode. |  Sure, I'd be happy to help! To calculate how much James pays per episode, we need to know the salaries of the major and minor characters. You mentioned that the major characters are paid three times as much as the minor characters, so let's assume that the major characters are paid $45,000 per episode (3 x $15,000). Here's the breakdown of the salaries for the 5 main characters and 4 minor characters: Main Characters: 1. Character A: $45,000 per episode 2. Character B: $45,000 per episode 3. Character C: $45,000 per episode 4. Character D: $45,000 per episode 5. Character E: $45,000 per episode Minor Characters: 1. Character F: $15,000 per episode 2. Character G: $15,000 per episode 3. Character H: $15,000 per episode 4. Character I: $15,000 per episode Now, let's calculate the total amount that James pays per episode: Total salary for main characters: 5 x $45,000 = $225,000 Total salary for minor characters: 4 x $15,000 = $60,000 Total amount paid per episode: $225,000 + $60,000 = $285,000 Therefore, James pays $285,000 per episode for the salaries of his 5 main characters and 4 minor characters. | [ \" Sure, I'd be happy to help! To calculate how much James pays per episode, we need to know the salaries of the major and minor characters. You mentioned that the major characters are paid three times as much as the minor characters, so let's assume that the major characters are paid $45,000 per episode (3 x $15,000).\\n\\nHere's the breakdown of the salaries for the 5 main characters and 4 minor characters:\\n\\nMain Characters:\\n\\n1. Character A: $45,000 per episode\\n2. Character B: $45,000 per episode\\n3. Character C: $45,000 per episode\\n4. Character D: $45,000 per episode\\n5. Character E: $45,000 per episode\\n\\nMinor Characters:\\n\\n1. Character F: $15,000 per episode\\n2. Character G: $15,000 per episode\\n3. Character H: $15,000 per episode\\n4. Character I: $15,000 per episode\\n\\nNow, let's calculate the total amount that James pays per episode:\\n\\nTotal salary for main characters: 5 x $45,000 = $225,000\\nTotal salary for minor characters: 4 x $15,000 = $60,000\\n\\nTotal amount paid per episode: $225,000 + $60,000 = $285,000\\n\\nTherefore, James pays $285,000 per episode for the salaries of his 5 main characters and 4 minor characters.\", \"James pays the minor characters $15,000 each episode. Since there are 4 minor characters, he pays them a total of 4 * $15,000 = $60,000 per episode.\\n\\nThe major characters are paid three times as much. So, each major character gets paid 3 * $15,000 = $45,000 per episode.\\n\\nThere are 5 main characters, so he pays them a total of 5 * $45,000 = $225,000 per episode.\\n\\nIn total, James pays $225,000 (major characters) + $60,000 (minor characters) = $285,000 per episode.\" ] | [ \"rejected\", \"chosen\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nJames runs a TV show and there are 5 main characters and 4 minor characters. He pays the minor characters $15,000 each episode. He paid the major characters three times as much. How much does he pay per episode? Let's be accurate as possible.\\n\\n\\n[The Start of Assistant 1's Answer>\\n Sure, I'd be happy to help! To calculate how much James pays per episode, we need to know the salaries of the major and minor characters. You mentioned that the major characters are paid three times as much as the minor characters, so let's assume that the major characters are paid $45,000 per episode (3 x $15,000).\\n\\nHere's the breakdown of the salaries for the 5 main characters and 4 minor characters:\\n\\nMain Characters:\\n\\n1. Character A: $45,000 per episode\\n2. Character B: $45,000 per episode\\n3. Character C: $45,000 per episode\\n4. Character D: $45,000 per episode\\n5. Character E: $45,000 per episode\\n\\nMinor Characters:\\n\\n1. Character F: $15,000 per episode\\n2. Character G: $15,000 per episode\\n3. Character H: $15,000 per episode\\n4. Character I: $15,000 per episode\\n\\nNow, let's calculate the total amount that James pays per episode:\\n\\nTotal salary for main characters: 5 x $45,000 = $225,000\\nTotal salary for minor characters: 4 x $15,000 = $60,000\\n\\nTotal amount paid per episode: $225,000 + $60,000 = $285,000\\n\\nTherefore, James pays $285,000 per episode for the salaries of his 5 main characters and 4 minor characters.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\nJames pays the minor characters $15,000 each episode. Since there are 4 minor characters, he pays them a total of 4 * $15,000 = $60,000 per episode.\\n\\nThe major characters are paid three times as much. So, each major character gets paid 3 * $15,000 = $45,000 per episode.\\n\\nThere are 5 main characters, so he pays them a total of 5 * $45,000 = $225,000 per episode.\\n\\nIn total, James pays $225,000 (major characters) + $60,000 (minor characters) = $285,000 per episode.\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 10 10 Both assistants provided an accurate and relevant response to the question asked. They both correctly calculated the amounts paid per episode to the main and minor characters. They used clear and logical steps in their calculations that are easy to follow, ensuring precision in their responses. The level of detail was appropriate for the complexity of the question; neither assistant provided unnecessary information that could confuse the reader nor omitted any important details. They both presented their answers neatly, first explaining the calculation process and then providing the breakdown of salaries for both the main and minor characters, followed by the final total amount per episode. Therefore, each assistant deserves a perfect score of 10, as they have met all the criteria for helpfulness, relevance, accuracy, and the level of detail in their responses. | [ 10, 10 ] |  Both assistants provided an accurate and relevant response to the question asked. They both correctly calculated the amounts paid per episode to the main and minor characters. They used clear and logical steps in their calculations that are easy to follow, ensuring precision in their responses. The level of detail was appropriate for the complexity of the question; neither assistant provided unnecessary information that could confuse the reader nor omitted any important details. They both presented their answers neatly, first explaining the calculation process and then providing the breakdown of salaries for both the main and minor characters, followed by the final total amount per episode. Therefore, each assistant deserves a perfect score of 10, as they have met all the criteria for helpfulness, relevance, accuracy, and the level of detail in their responses. | tie | James pays the minor characters $15,000 each episode. Since there are 4 minor characters, he pays them a total of 4 * $15,000 = $60,000 per episode. The major characters are paid three times as much. So, each major character gets paid 3 * $15,000 = $45,000 per episode. There are 5 main characters, so he pays them a total of 5 * $45,000 = $225,000 per episode. In total, James pays $225,000 (major characters) + $60,000 (minor characters) = $285,000 per episode. |  Sure, I'd be happy to help! To calculate how much James pays per episode, we need to know the salaries of the major and minor characters. You mentioned that the major characters are paid three times as much as the minor characters, so let's assume that the major characters are paid $45,000 per episode (3 x $15,000). Here's the breakdown of the salaries for the 5 main characters and 4 minor characters: Main Characters: 1. Character A: $45,000 per episode 2. Character B: $45,000 per episode 3. Character C: $45,000 per episode 4. Character D: $45,000 per episode 5. Character E: $45,000 per episode Minor Characters: 1. Character F: $15,000 per episode 2. Character G: $15,000 per episode 3. Character H: $15,000 per episode 4. Character I: $15,000 per episode Now, let's calculate the total amount that James pays per episode: Total salary for main characters: 5 x $45,000 = $225,000 Total salary for minor characters: 4 x $15,000 = $60,000 Total amount paid per episode: $225,000 + $60,000 = $285,000 Therefore, James pays $285,000 per episode for the salaries of his 5 main characters and 4 minor characters. | 10 | true  \nYou are an AI assistant that helps people find information. | Given the stream of consciousness rationale, provide a reasonable question and answer. Rationale: Here the game refers to a volleyball where female beach volleyball player serving the ball. The question and answer: | Question: What is the proper technique for a female beach volleyball player to serve the ball effectively in a game? Answer: To serve the ball effectively in beach volleyball, a female player should utilize a consistent pre-serve routine, stand in a balanced position with feet shoulder-width apart, toss the ball at an appropriate height while extending their non-hitting arm, and use a combination of shoulder, arm, and wrist movement to achieve a powerful and accurate serve. Different serve styles, such as underhand, topspin, and float, can be used depending on the player's strengths and the desired outcome. |  Sure, I'd be happy to help! Here's a reasonable question and answer based on the stream of consciousness rationale: Question: What is the name of the female beach volleyball player serving the ball in this picture? Answer: The name of the female beach volleyball player serving the ball in this picture is Kerri Walsh Jennings. | [ \"Question: What is the proper technique for a female beach volleyball player to serve the ball effectively in a game?\\n\\nAnswer: To serve the ball effectively in beach volleyball, a female player should utilize a consistent pre-serve routine, stand in a balanced position with feet shoulder-width apart, toss the ball at an appropriate height while extending their non-hitting arm, and use a combination of shoulder, arm, and wrist movement to achieve a powerful and accurate serve. Different serve styles, such as underhand, topspin, and float, can be used depending on the player's strengths and the desired outcome.\", \" Sure, I'd be happy to help! Here's a reasonable question and answer based on the stream of consciousness rationale:\\n\\nQuestion: What is the name of the female beach volleyball player serving the ball in this picture?\\n\\nAnswer: The name of the female beach volleyball player serving the ball in this picture is Kerri Walsh Jennings.\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nGiven the stream of consciousness rationale, provide a reasonable question and answer. Rationale: Here the game refers to a volleyball where female beach volleyball player serving the ball.\\n The question and answer:\\n\\n\\n[The Start of Assistant 1's Answer>\\nQuestion: What is the proper technique for a female beach volleyball player to serve the ball effectively in a game?\\n\\nAnswer: To serve the ball effectively in beach volleyball, a female player should utilize a consistent pre-serve routine, stand in a balanced position with feet shoulder-width apart, toss the ball at an appropriate height while extending their non-hitting arm, and use a combination of shoulder, arm, and wrist movement to achieve a powerful and accurate serve. Different serve styles, such as underhand, topspin, and float, can be used depending on the player's strengths and the desired outcome.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Sure, I'd be happy to help! Here's a reasonable question and answer based on the stream of consciousness rationale:\\n\\nQuestion: What is the name of the female beach volleyball player serving the ball in this picture?\\n\\nAnswer: The name of the female beach volleyball player serving the ball in this picture is Kerri Walsh Jennings.\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 8 4 Assistant 1 provided an answer that aligns with the rationale behind the question\u2014details about the proper technique for a female beach volleyball player to serve the ball effectively. The response gave practical advice on serving tactics and technique, making it informative and relevant to someone interested in volleyball. The answer could have been enhanced by including specifics on the different types of serves and scenarios for their use. Assistant 2's answer assumed the availability of a picture, which was not provided in the prompt. Therefore, naming a specific player without any visual context is purely speculative and unfounded, making the response irrelevant and inaccurate based on the given information. This answer would only be helpful if there were indeed a picture to identify a player\u2014since there isn't, the helpfulness and relevance significantly drop. However, it does understand that a name is being asked for, so it didn't entirely miss the point of drafting a question. Both assistants would have performed better if they requested additional information to provide the most accurate and relevant information possible. However, Assistant 1's response was far more aligned with the rationale given, thus receiving a higher score. | [ 8, 4 ] |  Assistant 1 provided an answer that aligns with the rationale behind the question\u2014details about the proper technique for a female beach volleyball player to serve the ball effectively. The response gave practical advice on serving tactics and technique, making it informative and relevant to someone interested in volleyball. The answer could have been enhanced by including specifics on the different types of serves and scenarios for their use. Assistant 2's answer assumed the availability of a picture, which was not provided in the prompt. Therefore, naming a specific player without any visual context is purely speculative and unfounded, making the response irrelevant and inaccurate based on the given information. This answer would only be helpful if there were indeed a picture to identify a player\u2014since there isn't, the helpfulness and relevance significantly drop. However, it does understand that a name is being asked for, so it didn't entirely miss the point of drafting a question. Both assistants would have performed better if they requested additional information to provide the most accurate and relevant information possible. However, Assistant 1's response was far more aligned with the rationale given, thus receiving a higher score. | unchanged | Question: What is the proper technique for a female beach volleyball player to serve the ball effectively in a game? Answer: To serve the ball effectively in beach volleyball, a female player should utilize a consistent pre-serve routine, stand in a balanced position with feet shoulder-width apart, toss the ball at an appropriate height while extending their non-hitting arm, and use a combination of shoulder, arm, and wrist movement to achieve a powerful and accurate serve. Different serve styles, such as underhand, topspin, and float, can be used depending on the player's strengths and the desired outcome. |  Sure, I'd be happy to help! Here's a reasonable question and answer based on the stream of consciousness rationale: Question: What is the name of the female beach volleyball player serving the ball in this picture? Answer: The name of the female beach volleyball player serving the ball in this picture is Kerri Walsh Jennings. | 8 | false  \nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps. | Multi-choice question: What is the sentiment of the following tweet? Tweet: @nikkigreen I told you Choose your answer from: + negative; + positive; | To determine the sentiment of the tweet, we need to analyze it thoroughly. Tweet: @nikkigreen I told you Step 1: Identify the words or phrases that carry emotional weight. In this tweet, there is only one phrase worth examining: \"I told you.\" Step 2: Determine the sentiment of the identified words or phrases. \"I told you\" can carry a variety of sentiments, depending on the context. It could be positive, negative, or neutral. Step 3: Consider the overall context of the tweet. Unfortunately, without more context, it is impossible to determine the exact sentiment of the tweet. As a result, we cannot confidently choose an answer from the provided options, positive or negative, without more contextual information. |  Sure, I'd be happy to help! Here's my step-by-step process for determining the sentiment of the tweet: Step 1: Identify the key words and phrases in the tweet. The tweet reads: \"@nikkigreen I told you\". Step 2: Analyze the words and phrases to determine their emotional connotation. The word \"told\" has a neutral connotation, while the word \"you\" is a second-person pronoun that can have a slightly negative or accusatory tone depending on the context. Step 3: Consider the overall context of the tweet. The tweet is directed at a specific person named Nikki, and the phrase \"I told you\" implies that the speaker is expressing a warning or a prediction that has come true. Step 4: Based on the analysis, determine the sentiment of the tweet. Based on the words and phrases, the overall context, and the potential implications of the phrase \"I told you\", it is likely that the sentiment of the tweet is: + negative. Therefore, my answer is: + negative. | [ \"To determine the sentiment of the tweet, we need to analyze it thoroughly.\\n\\nTweet: @nikkigreen I told you\\n\\nStep 1: Identify the words or phrases that carry emotional weight.\\nIn this tweet, there is only one phrase worth examining: \\\"I told you.\\\"\\n\\nStep 2: Determine the sentiment of the identified words or phrases.\\n\\\"I told you\\\" can carry a variety of sentiments, depending on the context. It could be positive, negative, or neutral.\\n\\nStep 3: Consider the overall context of the tweet.\\nUnfortunately, without more context, it is impossible to determine the exact sentiment of the tweet.\\n\\nAs a result, we cannot confidently choose an answer from the provided options, positive or negative, without more contextual information.\", \" Sure, I'd be happy to help! Here's my step-by-step process for determining the sentiment of the tweet:\\n\\nStep 1: Identify the key words and phrases in the tweet.\\n\\nThe tweet reads: \\\"@nikkigreen I told you\\\".\\n\\nStep 2: Analyze the words and phrases to determine their emotional connotation.\\n\\nThe word \\\"told\\\" has a neutral connotation, while the word \\\"you\\\" is a second-person pronoun that can have a slightly negative or accusatory tone depending on the context.\\n\\nStep 3: Consider the overall context of the tweet.\\n\\nThe tweet is directed at a specific person named Nikki, and the phrase \\\"I told you\\\" implies that the speaker is expressing a warning or a prediction that has come true.\\n\\nStep 4: Based on the analysis, determine the sentiment of the tweet.\\n\\nBased on the words and phrases, the overall context, and the potential implications of the phrase \\\"I told you\\\", it is likely that the sentiment of the tweet is:\\n\\n+ negative.\\n\\nTherefore, my answer is:\\n\\n+ negative.\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nMulti-choice question: What is the sentiment of the following tweet?\\nTweet: @nikkigreen I told you \\nChoose your answer from:\\n + negative;\\n + positive;\\n\\n\\n[The Start of Assistant 1's Answer>\\nTo determine the sentiment of the tweet, we need to analyze it thoroughly.\\n\\nTweet: @nikkigreen I told you\\n\\nStep 1: Identify the words or phrases that carry emotional weight.\\nIn this tweet, there is only one phrase worth examining: \\\"I told you.\\\"\\n\\nStep 2: Determine the sentiment of the identified words or phrases.\\n\\\"I told you\\\" can carry a variety of sentiments, depending on the context. It could be positive, negative, or neutral.\\n\\nStep 3: Consider the overall context of the tweet.\\nUnfortunately, without more context, it is impossible to determine the exact sentiment of the tweet.\\n\\nAs a result, we cannot confidently choose an answer from the provided options, positive or negative, without more contextual information.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Sure, I'd be happy to help! Here's my step-by-step process for determining the sentiment of the tweet:\\n\\nStep 1: Identify the key words and phrases in the tweet.\\n\\nThe tweet reads: \\\"@nikkigreen I told you\\\".\\n\\nStep 2: Analyze the words and phrases to determine their emotional connotation.\\n\\nThe word \\\"told\\\" has a neutral connotation, while the word \\\"you\\\" is a second-person pronoun that can have a slightly negative or accusatory tone depending on the context.\\n\\nStep 3: Consider the overall context of the tweet.\\n\\nThe tweet is directed at a specific person named Nikki, and the phrase \\\"I told you\\\" implies that the speaker is expressing a warning or a prediction that has come true.\\n\\nStep 4: Based on the analysis, determine the sentiment of the tweet.\\n\\nBased on the words and phrases, the overall context, and the potential implications of the phrase \\\"I told you\\\", it is likely that the sentiment of the tweet is:\\n\\n+ negative.\\n\\nTherefore, my answer is:\\n\\n+ negative.\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 5 4 Assistant 1's response initially demonstrates a thorough approach to sentiment analysis by checking for emotional words and considering the context of the tweet. However, it concludes that without more contextual information, the sentiment cannot be determined confidently. This conclusion is appropriate given the brevity and lack of emotional cues in the tweet provided. Assistant 1 leaves the reader without a definitive answer but explains the reasoning well. Assistant 2's response takes a similar approach to sentiment analysis but ends with a definitive conclusion stating the sentiment as negative. This answer is less accurate because the tweet's sentiment is not clear from the text alone without additional context. The phrase \"I told you\" can be seen in both positive and negative contexts; thus, declaring it as negative with certainty is speculative. However, the steps provided for analysis do show an understanding of the process of sentiment analysis. Both assistants failed to recognize that without additional context, it is indeed challenging to provide an accurate sentiment analysis. The scores reflect that both answers demonstrated a methodical approach, but neither could accurately determine the sentiment of the tweet due to the lack of contextual information. Assistant 1 scored slightly higher because it acknowledged this limitation, while Assistant 2 did not. | [ 5, 4 ] |  Assistant 1's response initially demonstrates a thorough approach to sentiment analysis by checking for emotional words and considering the context of the tweet. However, it concludes that without more contextual information, the sentiment cannot be determined confidently. This conclusion is appropriate given the brevity and lack of emotional cues in the tweet provided. Assistant 1 leaves the reader without a definitive answer but explains the reasoning well. Assistant 2's response takes a similar approach to sentiment analysis but ends with a definitive conclusion stating the sentiment as negative. This answer is less accurate because the tweet's sentiment is not clear from the text alone without additional context. The phrase \"I told you\" can be seen in both positive and negative contexts; thus, declaring it as negative with certainty is speculative. However, the steps provided for analysis do show an understanding of the process of sentiment analysis. Both assistants failed to recognize that without additional context, it is indeed challenging to provide an accurate sentiment analysis. The scores reflect that both answers demonstrated a methodical approach, but neither could accurately determine the sentiment of the tweet due to the lack of contextual information. Assistant 1 scored slightly higher because it acknowledged this limitation, while Assistant 2 did not. | unchanged | To determine the sentiment of the tweet, we need to analyze it thoroughly. Tweet: @nikkigreen I told you Step 1: Identify the words or phrases that carry emotional weight. In this tweet, there is only one phrase worth examining: \"I told you.\" Step 2: Determine the sentiment of the identified words or phrases. \"I told you\" can carry a variety of sentiments, depending on the context. It could be positive, negative, or neutral. Step 3: Consider the overall context of the tweet. Unfortunately, without more context, it is impossible to determine the exact sentiment of the tweet. As a result, we cannot confidently choose an answer from the provided options, positive or negative, without more contextual information. |  Sure, I'd be happy to help! Here's my step-by-step process for determining the sentiment of the tweet: Step 1: Identify the key words and phrases in the tweet. The tweet reads: \"@nikkigreen I told you\". Step 2: Analyze the words and phrases to determine their emotional connotation. The word \"told\" has a neutral connotation, while the word \"you\" is a second-person pronoun that can have a slightly negative or accusatory tone depending on the context. Step 3: Consider the overall context of the tweet. The tweet is directed at a specific person named Nikki, and the phrase \"I told you\" implies that the speaker is expressing a warning or a prediction that has come true. Step 4: Based on the analysis, determine the sentiment of the tweet. Based on the words and phrases, the overall context, and the potential implications of the phrase \"I told you\", it is likely that the sentiment of the tweet is: + negative. Therefore, my answer is: + negative. | 5 | false  \nYou are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-bystep and justify your answer. | John was a terrible writer. To practice, his teacher suggest that he consider people he knows and do what? Options: - write novels - advertising firm - write letter - write notes - write poems Let's think now! Step-by-step reasoning: | Step 1: Analyze the situation - John is a terrible writer and needs practice to improve his skills. His teacher gives him advice on how to practice. Step 2: Consider the options given: - Write novels: This might be a bit overwhelming for someone who is just starting to improve their writing skills, as novels are long and require a lot of commitment. - Advertising firm: This option doesn't seem to directly connect to practicing writing, as it refers to a type of business rather than a writing activity. - Write letters: Writing letters could be a good way to practice writing, especially since John can think about people he knows while doing so. This makes the writing more personal and engaging. - Write notes: Writing notes can help improve writing skills to some extent, but might not allow for the kind of continuous practice that John needs. - Write poems: This is another creative option to practice writing, but it might not be the best choice if John struggles with the basics of writing and is not yet comfortable with more advanced forms such as poetry. Step 3: Choose the most suitable option based on the analysis: Given the options, writing letters seems to be the best advice for John to follow when considering people he knows. This would allow him to practice writing in a personal, engaging way while focusing on communicating with others, which could help him improve his skills. |  Sure, I'd be happy to help! Here's my step-by-step reasoning for each option: Option A: Write novels * Not a good fit for John's writing practice, as novels are long and complex, and may not be suitable for a beginner. Option B: Advertising firm * Not a good fit for John's writing practice, as advertising copy is often short and catchy, but may not be a good way to practice writing in depth. Option C: Write letters * A good fit for John's writing practice! Writing letters is a great way to practice writing in a more personal and conversational style, and can help John improve his writing skills in a more practical way. Option D: Write notes * A good fit for John's writing practice! Writing notes is a great way to practice writing in a more concise and to-the-point style, which can be helpful for John to improve his writing skills. Option E: Write poems * Not a good fit for John's writing practice, as poems are often more creative and expressive, and may not be the best way for John to practice his writing skills in a more practical way. Therefore, the best option for John's writing practice would be to write letters. This will allow him to practice writing in a more personal and conversational style, which can help him improve his writing skills in a more practical way. | [ \"Step 1: Analyze the situation - John is a terrible writer and needs practice to improve his skills. His teacher gives him advice on how to practice.\\n\\nStep 2: Consider the options given:\\n- Write novels: This might be a bit overwhelming for someone who is just starting to improve their writing skills, as novels are long and require a lot of commitment.\\n- Advertising firm: This option doesn't seem to directly connect to practicing writing, as it refers to a type of business rather than a writing activity.\\n- Write letters: Writing letters could be a good way to practice writing, especially since John can think about people he knows while doing so. This makes the writing more personal and engaging.\\n- Write notes: Writing notes can help improve writing skills to some extent, but might not allow for the kind of continuous practice that John needs.\\n- Write poems: This is another creative option to practice writing, but it might not be the best choice if John struggles with the basics of writing and is not yet comfortable with more advanced forms such as poetry.\\n\\nStep 3: Choose the most suitable option based on the analysis:\\nGiven the options, writing letters seems to be the best advice for John to follow when considering people he knows. This would allow him to practice writing in a personal, engaging way while focusing on communicating with others, which could help him improve his skills.\", \" Sure, I'd be happy to help! Here's my step-by-step reasoning for each option:\\n\\nOption A: Write novels\\n\\n* Not a good fit for John's writing practice, as novels are long and complex, and may not be suitable for a beginner.\\n\\nOption B: Advertising firm\\n\\n* Not a good fit for John's writing practice, as advertising copy is often short and catchy, but may not be a good way to practice writing in depth.\\n\\nOption C: Write letters\\n\\n* A good fit for John's writing practice! Writing letters is a great way to practice writing in a more personal and conversational style, and can help John improve his writing skills in a more practical way.\\n\\nOption D: Write notes\\n\\n* A good fit for John's writing practice! Writing notes is a great way to practice writing in a more concise and to-the-point style, which can be helpful for John to improve his writing skills.\\n\\nOption E: Write poems\\n\\n* Not a good fit for John's writing practice, as poems are often more creative and expressive, and may not be the best way for John to practice his writing skills in a more practical way.\\n\\nTherefore, the best option for John's writing practice would be to write letters. This will allow him to practice writing in a more personal and conversational style, which can help him improve his writing skills in a more practical way.\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nJohn was a terrible writer. To practice, his teacher suggest that he consider people he knows and do what?\\nOptions:\\n- write novels\\n- advertising firm\\n- write letter\\n- write notes\\n- write poems\\nLet's think now! Step-by-step reasoning:\\n\\n\\n[The Start of Assistant 1's Answer>\\nStep 1: Analyze the situation - John is a terrible writer and needs practice to improve his skills. His teacher gives him advice on how to practice.\\n\\nStep 2: Consider the options given:\\n- Write novels: This might be a bit overwhelming for someone who is just starting to improve their writing skills, as novels are long and require a lot of commitment.\\n- Advertising firm: This option doesn't seem to directly connect to practicing writing, as it refers to a type of business rather than a writing activity.\\n- Write letters: Writing letters could be a good way to practice writing, especially since John can think about people he knows while doing so. This makes the writing more personal and engaging.\\n- Write notes: Writing notes can help improve writing skills to some extent, but might not allow for the kind of continuous practice that John needs.\\n- Write poems: This is another creative option to practice writing, but it might not be the best choice if John struggles with the basics of writing and is not yet comfortable with more advanced forms such as poetry.\\n\\nStep 3: Choose the most suitable option based on the analysis:\\nGiven the options, writing letters seems to be the best advice for John to follow when considering people he knows. This would allow him to practice writing in a personal, engaging way while focusing on communicating with others, which could help him improve his skills.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Sure, I'd be happy to help! Here's my step-by-step reasoning for each option:\\n\\nOption A: Write novels\\n\\n* Not a good fit for John's writing practice, as novels are long and complex, and may not be suitable for a beginner.\\n\\nOption B: Advertising firm\\n\\n* Not a good fit for John's writing practice, as advertising copy is often short and catchy, but may not be a good way to practice writing in depth.\\n\\nOption C: Write letters\\n\\n* A good fit for John's writing practice! Writing letters is a great way to practice writing in a more personal and conversational style, and can help John improve his writing skills in a more practical way.\\n\\nOption D: Write notes\\n\\n* A good fit for John's writing practice! Writing notes is a great way to practice writing in a more concise and to-the-point style, which can be helpful for John to improve his writing skills.\\n\\nOption E: Write poems\\n\\n* Not a good fit for John's writing practice, as poems are often more creative and expressive, and may not be the best way for John to practice his writing skills in a more practical way.\\n\\nTherefore, the best option for John's writing practice would be to write letters. This will allow him to practice writing in a more personal and conversational style, which can help him improve his writing skills in a more practical way.\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 8 7 Both assistants provided a thoughtful analysis and presented a step-by-step approach to determining the best option for John to improve his writing skills. However, the most suitable activity that would help John improve his writing by considering people he knows is not explicitly stated. Thus, the answers could be considered incomplete. Assistant 1's answer considered each option's relevance and suitability toward helping John improve his writing skills. This assistant was more analytical in the approach to eliminating less effective options and providing reasoning. The emphasis on personal engagement through writing letters and the practical nature of this activity for improving writing skills was well-justified, and the analysis was structured in a logical and systematic manner. Assistant 2's answer followed a similar pattern of step-by-step evaluation but seemed slightly less detailed. While the analysis of each option was correct, Assistant 2 also did not explicitly mention the most suitable activity based on John's need to consider people he knows as part of his practice. The recommendation for writing letters was made on the basis of practice in a personal and conversational style rather than the consideration of people John knows, which is crucial based on the question's context. Neither assistant provided the full context needed for an optimal answer, potentially because the needed context isn't available in the question prompt. The prompt suggests there's a correct answer among the options, which should correlate with the teacher's advice. Since the correct advice isn't provided in the prompt, both assistants speculated reasonably based on general advice for improving writing, leading to a slight deduction in their scores for the incomplete response. Overall, Assistant 1 provided a more detailed and analytical response, which earned a slightly higher score than Assistant 2, who also performed well but was just a bit less detailed in the analysis. | [ 8, 7 ] |  Both assistants provided a thoughtful analysis and presented a step-by-step approach to determining the best option for John to improve his writing skills. However, the most suitable activity that would help John improve his writing by considering people he knows is not explicitly stated. Thus, the answers could be considered incomplete. Assistant 1's answer considered each option's relevance and suitability toward helping John improve his writing skills. This assistant was more analytical in the approach to eliminating less effective options and providing reasoning. The emphasis on personal engagement through writing letters and the practical nature of this activity for improving writing skills was well-justified, and the analysis was structured in a logical and systematic manner. Assistant 2's answer followed a similar pattern of step-by-step evaluation but seemed slightly less detailed. While the analysis of each option was correct, Assistant 2 also did not explicitly mention the most suitable activity based on John's need to consider people he knows as part of his practice. The recommendation for writing letters was made on the basis of practice in a personal and conversational style rather than the consideration of people John knows, which is crucial based on the question's context. Neither assistant provided the full context needed for an optimal answer, potentially because the needed context isn't available in the question prompt. The prompt suggests there's a correct answer among the options, which should correlate with the teacher's advice. Since the correct advice isn't provided in the prompt, both assistants speculated reasonably based on general advice for improving writing, leading to a slight deduction in their scores for the incomplete response. Overall, Assistant 1 provided a more detailed and analytical response, which earned a slightly higher score than Assistant 2, who also performed well but was just a bit less detailed in the analysis. | unchanged | Step 1: Analyze the situation - John is a terrible writer and needs practice to improve his skills. His teacher gives him advice on how to practice. Step 2: Consider the options given: - Write novels: This might be a bit overwhelming for someone who is just starting to improve their writing skills, as novels are long and require a lot of commitment. - Advertising firm: This option doesn't seem to directly connect to practicing writing, as it refers to a type of business rather than a writing activity. - Write letters: Writing letters could be a good way to practice writing, especially since John can think about people he knows while doing so. This makes the writing more personal and engaging. - Write notes: Writing notes can help improve writing skills to some extent, but might not allow for the kind of continuous practice that John needs. - Write poems: This is another creative option to practice writing, but it might not be the best choice if John struggles with the basics of writing and is not yet comfortable with more advanced forms such as poetry. Step 3: Choose the most suitable option based on the analysis: Given the options, writing letters seems to be the best advice for John to follow when considering people he knows. This would allow him to practice writing in a personal, engaging way while focusing on communicating with others, which could help him improve his skills. |  Sure, I'd be happy to help! Here's my step-by-step reasoning for each option: Option A: Write novels * Not a good fit for John's writing practice, as novels are long and complex, and may not be suitable for a beginner. Option B: Advertising firm * Not a good fit for John's writing practice, as advertising copy is often short and catchy, but may not be a good way to practice writing in depth. Option C: Write letters * A good fit for John's writing practice! Writing letters is a great way to practice writing in a more personal and conversational style, and can help John improve his writing skills in a more practical way. Option D: Write notes * A good fit for John's writing practice! Writing notes is a great way to practice writing in a more concise and to-the-point style, which can be helpful for John to improve his writing skills. Option E: Write poems * Not a good fit for John's writing practice, as poems are often more creative and expressive, and may not be the best way for John to practice his writing skills in a more practical way. Therefore, the best option for John's writing practice would be to write letters. This will allow him to practice writing in a more personal and conversational style, which can help him improve his writing skills in a more practical way. | 8 | false  \nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer. | What is a one-sentence summary of the following article? One of the first complaints presented to doctors by many people later diagnosed with Parkinson\u2019s Disease is an involuntary tremor or shaking of the hands, fingers, arms, legs, jaw, and face. You\u2019re most likely to notice this tremor when your hands are at rest rather than when you\u2019re using them, although you may see more tremors while your hands and arms are in use as the disease progresses. There are many causes of tremors. Parkinson's Disease is one of the most common causes, and tremors are often the first sign of the disease. The tremor and other symptoms may initially appear only on one side of the body, or they may appear worse on one side than the other. A repetitive \u201cpill-rolling\" movement between the thumb and finger\u2014named because it looks as though the person is rolling a pill between their fingers\u2014is characteristic of a Parkinsonian tremor. Some symptoms of Parkinson's are caused by the larger symptom of slowed movements (also known as bradykinesia). This primarily affects motor functions from walking and balance to writing and even motor functions that are often considered reflexive or spontaneous. These slowed movements are a very common early sign of Parkinson\u2019s, and may show up at the onset of the disease in 80% of patients. Some people might have a hard time describing what they are feeling and use words like \u201cweakness,\u201d \u201ctiredness,\u201d or \u201cincoordination\u201d when talking about these symptoms. Look for distortions in voluntary movements. In addition to involuntary movements, those with Parkinson\u2019s may experience disturbances in their voluntary movements beyond just slowness. Some of the treatments used for Parkinson's disease may cause abnormal involuntary movements, or an increase in movement, referred to as dyskinesia. These distortions (dyskinesias) can be similar to a \u201ctic\u201d in appearance and worsen with psychological stress. Advanced dyskinesia is most often seen in patients who have been on the medication Levodopa for some time. A common symptom of Parkinson\u2019s is a shuffling gait with short steps and a tendency to lean forward. People with Parkinson's often have difficulty with their balance, and there is sometimes a tendency to fall forward, causing the person to walk faster and faster to keep from falling over. This pattern is called a \"festinating gait,\" and it is very common in Parkinson's disease. This symptom typically shows up in the later stages of the disease. Those with Parkinson\u2019s Disease often lean forward at the waist when standing or walking. This is because Parkinson's can cause posture and balance problems, including rigidity. There is a tendency to flex the arms and the head so that the person appears bent over with the elbows flexed and the head down. Rigidity can affect the whole body, and might you feel stiff or sore. Check for postural rigidity. Also referred to as \u201ccog-wheeling,\u201d this symptom presents as a hallmark, rigid motion when an examiner moves the arm of a patient through basic flexion and extension motions. The rigidity and resistance to movement is most obvious with passive movement of the wrists and elbow. Cog-wheeling may happen when the increased muscle tone related to rigidity is combined with a tremor. Some cognitive impairment is common, but usually not until late in the course of the disease. Around 90% of those with PD will show signs of speech disturbances at some point. These can present as quieter speech, breathy or hoarse qualities to the voice, and reduced precision with language. The voice often becomes soft or whispery because the vocal muscles lack mobility. Up to 60% of those with PD may show signs or anxiety or depression. PD affects some of the mood-stabilizing portions of the brain, which leads to this increased chance for depression, especially paired with the quality of life for patients in late stages of the disease. The muscles used to push food through the digestive system are also affected by PD. This can lead to a variety of gastrointestinal issues from incontinence to constipation. These same symptoms often present along with difficulty swallowing food as well. Many of the involuntary movements associated with Parkinson\u2019s make it increasingly difficult for those with the disease to get a full night of sleep. Other symptoms\u2014such as muscle rigidity making it difficult to roll over at night or bladder issues leading to getting up at night to urinate frequently\u2014compound the sleep disruptions experienced by those with PD. Summary: | Involuntary tremors, slowed movements, and posture and balance issues are common early symptoms of Parkinson's Disease, often accompanied by disturbances in voluntary movements, speech, mood, and gastrointestinal issues as the disease progresses. |  One of the primary symptoms of Parkinson's disease is an involuntary tremor or shaking of various parts of the body, such as the hands, fingers, arms, legs, jaw, and face. This tremor is often worse when the affected person is at rest and can progress to more pronounced tremors when the person is using their hands and arms. Parkinson's disease is one of the most common causes of tremors, and it often presents as the first symptom of the disease. Other symptoms of Parkinson's include slowed movements, balance problems, and distortions in voluntary movements, as well as potential cognitive impairment, anxiety, and depression. Treatments for Parkinson's may cause abnormal involuntary movements, or dyskinesia, and can worsen over time with long-term use of medication. | [ \"Involuntary tremors, slowed movements, and posture and balance issues are common early symptoms of Parkinson's Disease, often accompanied by disturbances in voluntary movements, speech, mood, and gastrointestinal issues as the disease progresses.\", \" One of the primary symptoms of Parkinson's disease is an involuntary tremor or shaking of various parts of the body, such as the hands, fingers, arms, legs, jaw, and face. This tremor is often worse when the affected person is at rest and can progress to more pronounced tremors when the person is using their hands and arms. Parkinson's disease is one of the most common causes of tremors, and it often presents as the first symptom of the disease. Other symptoms of Parkinson's include slowed movements, balance problems, and distortions in voluntary movements, as well as potential cognitive impairment, anxiety, and depression. Treatments for Parkinson's may cause abnormal involuntary movements, or dyskinesia, and can worsen over time with long-term use of medication.\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nWhat is a one-sentence summary of the following article?\\nOne of the first complaints presented to doctors by many people later diagnosed with Parkinson\u2019s Disease is an involuntary tremor or shaking of the hands, fingers, arms, legs, jaw, and face. You\u2019re most likely to notice this tremor when your hands are at rest rather than when you\u2019re using them, although you may see more tremors while your hands and arms are in use as the disease progresses. There are many causes of tremors. Parkinson's Disease is one of the most common causes, and tremors are often the first sign of the disease. The tremor and other symptoms may initially appear only on one side of the body, or they may appear worse on one side than the other. A repetitive \u201cpill-rolling\\\" movement between the thumb and finger\u2014named because it looks as though the person is rolling a pill between their fingers\u2014is characteristic of a Parkinsonian tremor. Some symptoms of Parkinson's are caused by the larger symptom of slowed movements (also known as bradykinesia). This primarily affects motor functions from walking and balance to writing and even motor functions that are often considered reflexive or spontaneous. These slowed movements are a very common early sign of Parkinson\u2019s, and may show up at the onset of the disease in 80% of patients. Some people might have a hard time describing what they are feeling and use words like \u201cweakness,\u201d \u201ctiredness,\u201d or \u201cincoordination\u201d when talking about these symptoms. Look for distortions in voluntary movements. In addition to involuntary movements, those with Parkinson\u2019s may experience disturbances in their voluntary movements beyond just slowness. Some of the treatments used for Parkinson's disease may cause abnormal involuntary movements, or an increase in movement, referred to as dyskinesia. These distortions (dyskinesias) can be similar to a \u201ctic\u201d in appearance and worsen with psychological stress. Advanced dyskinesia is most often seen in patients who have been on the medication Levodopa for some time. A common symptom of Parkinson\u2019s is a shuffling gait with short steps and a tendency to lean forward. People with Parkinson's often have difficulty with their balance, and there is sometimes a tendency to fall forward, causing the person to walk faster and faster to keep from falling over. This pattern is called a \\\"festinating gait,\\\" and it is very common in Parkinson's disease. This symptom typically shows up in the later stages of the disease. Those with Parkinson\u2019s Disease often lean forward at the waist when standing or walking. This is because Parkinson's can cause posture and balance problems, including rigidity. There is a tendency to flex the arms and the head so that the person appears bent over with the elbows flexed and the head down. Rigidity can affect the whole body, and might you feel stiff or sore. Check for postural rigidity. Also referred to as \u201ccog-wheeling,\u201d this symptom presents as a hallmark, rigid motion when an examiner moves the arm of a patient through basic flexion and extension motions. The rigidity and resistance to movement is most obvious with passive movement of the wrists and elbow. Cog-wheeling may happen when the increased muscle tone related to rigidity is combined with a tremor. Some cognitive impairment is common, but usually not until late in the course of the disease. Around 90% of those with PD will show signs of speech disturbances at some point. These can present as quieter speech, breathy or hoarse qualities to the voice, and reduced precision with language. The voice often becomes soft or whispery because the vocal muscles lack mobility. Up to 60% of those with PD may show signs or anxiety or depression. PD affects some of the mood-stabilizing portions of the brain, which leads to this increased chance for depression, especially paired with the quality of life for patients in late stages of the disease. The muscles used to push food through the digestive system are also affected by PD. This can lead to a variety of gastrointestinal issues from incontinence to constipation. These same symptoms often present along with difficulty swallowing food as well. Many of the involuntary movements associated with Parkinson\u2019s make it increasingly difficult for those with the disease to get a full night of sleep. Other symptoms\u2014such as muscle rigidity making it difficult to roll over at night or bladder issues leading to getting up at night to urinate frequently\u2014compound the sleep disruptions experienced by those with PD.\\n\\nSummary:\\n\\n\\n[The Start of Assistant 1's Answer>\\nInvoluntary tremors, slowed movements, and posture and balance issues are common early symptoms of Parkinson's Disease, often accompanied by disturbances in voluntary movements, speech, mood, and gastrointestinal issues as the disease progresses.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n One of the primary symptoms of Parkinson's disease is an involuntary tremor or shaking of various parts of the body, such as the hands, fingers, arms, legs, jaw, and face. This tremor is often worse when the affected person is at rest and can progress to more pronounced tremors when the person is using their hands and arms. Parkinson's disease is one of the most common causes of tremors, and it often presents as the first symptom of the disease. Other symptoms of Parkinson's include slowed movements, balance problems, and distortions in voluntary movements, as well as potential cognitive impairment, anxiety, and depression. Treatments for Parkinson's may cause abnormal involuntary movements, or dyskinesia, and can worsen over time with long-term use of medication.\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 9 8 Assistant 1 provided a concise summary that captures the essential early and progressive symptoms of Parkinson's Disease, including involuntary tremors, slowed movements, and posture and balance issues, as well as mentioning disturbances in voluntary movements, speech, mood, and gastrointestinal issues. This summary was broadly accurate and covered a range of symptoms mentioned in the article without unnecessary detail, thus it was relevant and maintained a high level of clarity. Assistant 2's answer also identified the primary symptom of an involuntary tremor, with additional detail on how it worsens and the commonality of Parkinson's as a cause for tremors. It also mentioned other symptoms like slowed movements, balance problems, distortions in voluntary movements, potential cognitive impairment, anxiety, and depression, and it correctly noted that Parkinson's treatments could cause dyskinesia. However, it didn't as effectively capture the progression of the disease or the diverse range of symptoms and consequences mentioned in the article, such as the specific sleep disruptions, thereby making the summary slightly less comprehensive. It might have benefited from additional editing to improve the flow and cohesiveness of the information presented. | [ 9, 8 ] | Assistant 1 provided a concise summary that captures the essential early and progressive symptoms of Parkinson's Disease, including involuntary tremors, slowed movements, and posture and balance issues, as well as mentioning disturbances in voluntary movements, speech, mood, and gastrointestinal issues. This summary was broadly accurate and covered a range of symptoms mentioned in the article without unnecessary detail, thus it was relevant and maintained a high level of clarity. Assistant 2's answer also identified the primary symptom of an involuntary tremor, with additional detail on how it worsens and the commonality of Parkinson's as a cause for tremors. It also mentioned other symptoms like slowed movements, balance problems, distortions in voluntary movements, potential cognitive impairment, anxiety, and depression, and it correctly noted that Parkinson's treatments could cause dyskinesia. However, it didn't as effectively capture the progression of the disease or the diverse range of symptoms and consequences mentioned in the article, such as the specific sleep disruptions, thereby making the summary slightly less comprehensive. It might have benefited from additional editing to improve the flow and cohesiveness of the information presented. | unchanged | Involuntary tremors, slowed movements, and posture and balance issues are common early symptoms of Parkinson's Disease, often accompanied by disturbances in voluntary movements, speech, mood, and gastrointestinal issues as the disease progresses. |  One of the primary symptoms of Parkinson's disease is an involuntary tremor or shaking of various parts of the body, such as the hands, fingers, arms, legs, jaw, and face. This tremor is often worse when the affected person is at rest and can progress to more pronounced tremors when the person is using their hands and arms. Parkinson's disease is one of the most common causes of tremors, and it often presents as the first symptom of the disease. Other symptoms of Parkinson's include slowed movements, balance problems, and distortions in voluntary movements, as well as potential cognitive impairment, anxiety, and depression. Treatments for Parkinson's may cause abnormal involuntary movements, or dyskinesia, and can worsen over time with long-term use of medication. | 9 | false  \nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps. | Premise: A man is inline skating in front of a wooden bench. Hypothesis: A man is having fun skating in front of a bench. .Choose the correct answer: Given the premise, can we conclude the hypothesis? Select from: a). yes b). it is not possible to tell c). no | b). it is not possible to tell Justification: Although the man is inline skating in front of the wooden bench, we cannot conclude whether he is having fun or not, as his emotions are not explicitly mentioned. |  Based on the premise \"A man is inline skating in front of a wooden bench,\" we can observe the following information: 1. The man is inline skating, which implies that he is wearing inline skates and is using them to move around. 2. He is skating in front of a wooden bench, which provides a static object for him to skate near or around. From these two pieces of information, we can infer that the man is likely enjoying himself while skating, as inline skating can be a fun and recreational activity. However, we cannot conclusively determine that the man is having fun based solely on the premise. Therefore, the correct answer is: b). It is not possible to tell. While we can infer that the man is likely enjoying himself, we cannot definitively conclude that he is having fun without additional information. | [ \"b). it is not possible to tell\\n\\nJustification: Although the man is inline skating in front of the wooden bench, we cannot conclude whether he is having fun or not, as his emotions are not explicitly mentioned.\", \" Based on the premise \\\"A man is inline skating in front of a wooden bench,\\\" we can observe the following information:\\n\\n1. The man is inline skating, which implies that he is wearing inline skates and is using them to move around.\\n2. He is skating in front of a wooden bench, which provides a static object for him to skate near or around.\\n\\nFrom these two pieces of information, we can infer that the man is likely enjoying himself while skating, as inline skating can be a fun and recreational activity. However, we cannot conclusively determine that the man is having fun based solely on the premise.\\n\\nTherefore, the correct answer is:\\n\\nb). It is not possible to tell.\\n\\nWhile we can infer that the man is likely enjoying himself, we cannot definitively conclude that he is having fun without additional information.\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nPremise: A man is inline skating in front of a wooden bench.\\n\\nHypothesis: A man is having fun skating in front of a bench.\\n\\n.Choose the correct answer: Given the premise, can we conclude the hypothesis?\\n\\nSelect from: a). yes b). it is not possible to tell c). no\\n\\n\\n[The Start of Assistant 1's Answer>\\nb). it is not possible to tell\\n\\nJustification: Although the man is inline skating in front of the wooden bench, we cannot conclude whether he is having fun or not, as his emotions are not explicitly mentioned.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Based on the premise \\\"A man is inline skating in front of a wooden bench,\\\" we can observe the following information:\\n\\n1. The man is inline skating, which implies that he is wearing inline skates and is using them to move around.\\n2. He is skating in front of a wooden bench, which provides a static object for him to skate near or around.\\n\\nFrom these two pieces of information, we can infer that the man is likely enjoying himself while skating, as inline skating can be a fun and recreational activity. However, we cannot conclusively determine that the man is having fun based solely on the premise.\\n\\nTherefore, the correct answer is:\\n\\nb). It is not possible to tell.\\n\\nWhile we can infer that the man is likely enjoying himself, we cannot definitively conclude that he is having fun without additional information.\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 10 10 Both Assistant 1 and Assistant 2 correctly identified that based on the provided premise, it is not possible to tell if the man is having fun, which is the core question being asked. Both assistants answered with option \"b). it is not possible to tell\", adhering to the logical constraints of the information given in the premise, which does not specify the man's emotional state. In terms of helpfulness, relevance, and accuracy, both answers appropriately addressed the user's question without making assumptions beyond what the premise allows. Neither assistant attributed emotions to the man or made unwarranted inferences about his state of mind, making the responses highly relevant and accurate. Assistant 1's response was concise and to the point, acknowledging that emotions are not explicitly mentioned and thus cannot be concluded. Assistant 2's response provided a slightly more detailed analysis, mentioning that while inline skating can be a fun activity, the man's enjoyment cannot be determined without additional information. This extra detail, while not necessary to answer the question, does not detract from the quality of the response. Both assistants abstained from providing unnecessary details, maintaining focus on the user's query. The quality of the answers is equivalent, and they demonstrated an equal understanding of the task, which is reflected in the identical scores of 10 for each assistant. | [ 10, 10 ] |  Both Assistant 1 and Assistant 2 correctly identified that based on the provided premise, it is not possible to tell if the man is having fun, which is the core question being asked. Both assistants answered with option \"b). it is not possible to tell\", adhering to the logical constraints of the information given in the premise, which does not specify the man's emotional state. In terms of helpfulness, relevance, and accuracy, both answers appropriately addressed the user's question without making assumptions beyond what the premise allows. Neither assistant attributed emotions to the man or made unwarranted inferences about his state of mind, making the responses highly relevant and accurate. Assistant 1's response was concise and to the point, acknowledging that emotions are not explicitly mentioned and thus cannot be concluded. Assistant 2's response provided a slightly more detailed analysis, mentioning that while inline skating can be a fun activity, the man's enjoyment cannot be determined without additional information. This extra detail, while not necessary to answer the question, does not detract from the quality of the response. Both assistants abstained from providing unnecessary details, maintaining focus on the user's query. The quality of the answers is equivalent, and they demonstrated an equal understanding of the task, which is reflected in the identical scores of 10 for each assistant. | tie | b). it is not possible to tell Justification: Although the man is inline skating in front of the wooden bench, we cannot conclude whether he is having fun or not, as his emotions are not explicitly mentioned. |  Based on the premise \"A man is inline skating in front of a wooden bench,\" we can observe the following information: 1. The man is inline skating, which implies that he is wearing inline skates and is using them to move around. 2. He is skating in front of a wooden bench, which provides a static object for him to skate near or around. From these two pieces of information, we can infer that the man is likely enjoying himself while skating, as inline skating can be a fun and recreational activity. However, we cannot conclusively determine that the man is having fun based solely on the premise. Therefore, the correct answer is: b). It is not possible to tell. While we can infer that the man is likely enjoying himself, we cannot definitively conclude that he is having fun without additional information. | 10 | false  \nYou are an AI assistant. Provide a detailed answer so user don\u2019t need to search outside to understand the answer. | Q: Answer the following question given this paragraph: The kidneys also secrete hormones that help maintain homeostasis. For example, they produce a hormone that stimulates bone marrow to produce red blood cells when more are needed. They also secrete a hormone that regulates blood pressure and keeps it in a normal range. Q: What organs secrete hormones that help maintain homeostasis? A: The answer is: | The kidneys are the organs that secrete hormones to help maintain homeostasis. They produce a hormone that stimulates bone marrow to produce red blood cells when needed, and they also secrete a hormone that regulates blood pressure, keeping it within a normal range. |  Certainly! Based on the provided paragraph, the organs that secrete hormones to help maintain homeostasis are the kidneys. The kidneys produce two hormones that help regulate various physiological processes in the body: 1. Erythropoietin (EPO): This hormone stimulates the bone marrow to produce red blood cells when there is a decrease in the number of red blood cells in the body. This helps to maintain normal red blood cell levels and prevent anemia. 2. Renin: This hormone regulates blood pressure by controlling the amount of fluid in the body and the diameter of blood vessels. When blood pressure is low, the kidneys produce more renin, which causes the blood vessels to constrict and retain more fluid. This helps to increase blood pressure back to normal levels. So, to summarize, the organs that secrete hormones to help maintain homeostasis are the kidneys, specifically the nephrons within the kidneys. These hormones play a crucial role in regulating various physiological processes, such as red blood cell production and blood pressure, to maintain homeostasis in the body. | [ \" Certainly! Based on the provided paragraph, the organs that secrete hormones to help maintain homeostasis are the kidneys. The kidneys produce two hormones that help regulate various physiological processes in the body:\\n\\n1. Erythropoietin (EPO): This hormone stimulates the bone marrow to produce red blood cells when there is a decrease in the number of red blood cells in the body. This helps to maintain normal red blood cell levels and prevent anemia.\\n2. Renin: This hormone regulates blood pressure by controlling the amount of fluid in the body and the diameter of blood vessels. When blood pressure is low, the kidneys produce more renin, which causes the blood vessels to constrict and retain more fluid. This helps to increase blood pressure back to normal levels.\\n\\nSo, to summarize, the organs that secrete hormones to help maintain homeostasis are the kidneys, specifically the nephrons within the kidneys. These hormones play a crucial role in regulating various physiological processes, such as red blood cell production and blood pressure, to maintain homeostasis in the body.\", \"The kidneys are the organs that secrete hormones to help maintain homeostasis. They produce a hormone that stimulates bone marrow to produce red blood cells when needed, and they also secrete a hormone that regulates blood pressure, keeping it within a normal range.\" ] | [ \"rejected\", \"chosen\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nQ: Answer the following question given this paragraph: The kidneys also secrete hormones that help maintain homeostasis. For example, they produce a hormone that stimulates bone marrow to produce red blood cells when more are needed. They also secrete a hormone that regulates blood pressure and keeps it in a normal range. Q: What organs secrete hormones that help maintain homeostasis? A:\\nThe answer is:\\n\\n\\n[The Start of Assistant 1's Answer>\\n Certainly! Based on the provided paragraph, the organs that secrete hormones to help maintain homeostasis are the kidneys. The kidneys produce two hormones that help regulate various physiological processes in the body:\\n\\n1. Erythropoietin (EPO): This hormone stimulates the bone marrow to produce red blood cells when there is a decrease in the number of red blood cells in the body. This helps to maintain normal red blood cell levels and prevent anemia.\\n2. Renin: This hormone regulates blood pressure by controlling the amount of fluid in the body and the diameter of blood vessels. When blood pressure is low, the kidneys produce more renin, which causes the blood vessels to constrict and retain more fluid. This helps to increase blood pressure back to normal levels.\\n\\nSo, to summarize, the organs that secrete hormones to help maintain homeostasis are the kidneys, specifically the nephrons within the kidneys. These hormones play a crucial role in regulating various physiological processes, such as red blood cell production and blood pressure, to maintain homeostasis in the body.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\nThe kidneys are the organs that secrete hormones to help maintain homeostasis. They produce a hormone that stimulates bone marrow to produce red blood cells when needed, and they also secrete a hormone that regulates blood pressure, keeping it within a normal range.\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 10 10 Both Assistant 1 and Assistant 2 provided accurate and relevant answers to the question, \"What organs secrete hormones that help maintain homeostasis?\" They both correctly identified the kidneys as the organs in question. The quality of the responses is high in terms of accuracy. Assistant 1 provided a more detailed explanation by naming the specific hormones produced by the kidneys, erythropoietin and renin, and describing their functions in maintaining homeostasis. This response could be especially helpful to users looking for more in-depth information. Assistant 2, while less detailed, still accurately captured the essence of the paragraph and provided a concise answer. This could be more beneficial to users who prefer a straightforward and brief reply. Since both assistants provided correct and relevant answers, and the level of detail could be suited for different user preferences, they both receive a full score. | [ 10, 10 ] |  Both Assistant 1 and Assistant 2 provided accurate and relevant answers to the question, \"What organs secrete hormones that help maintain homeostasis?\" They both correctly identified the kidneys as the organs in question. The quality of the responses is high in terms of accuracy. Assistant 1 provided a more detailed explanation by naming the specific hormones produced by the kidneys, erythropoietin and renin, and describing their functions in maintaining homeostasis. This response could be especially helpful to users looking for more in-depth information. Assistant 2, while less detailed, still accurately captured the essence of the paragraph and provided a concise answer. This could be more beneficial to users who prefer a straightforward and brief reply. Since both assistants provided correct and relevant answers, and the level of detail could be suited for different user preferences, they both receive a full score. | tie | The kidneys are the organs that secrete hormones to help maintain homeostasis. They produce a hormone that stimulates bone marrow to produce red blood cells when needed, and they also secrete a hormone that regulates blood pressure, keeping it within a normal range. |  Certainly! Based on the provided paragraph, the organs that secrete hormones to help maintain homeostasis are the kidneys. The kidneys produce two hormones that help regulate various physiological processes in the body: 1. Erythropoietin (EPO): This hormone stimulates the bone marrow to produce red blood cells when there is a decrease in the number of red blood cells in the body. This helps to maintain normal red blood cell levels and prevent anemia. 2. Renin: This hormone regulates blood pressure by controlling the amount of fluid in the body and the diameter of blood vessels. When blood pressure is low, the kidneys produce more renin, which causes the blood vessels to constrict and retain more fluid. This helps to increase blood pressure back to normal levels. So, to summarize, the organs that secrete hormones to help maintain homeostasis are the kidneys, specifically the nephrons within the kidneys. These hormones play a crucial role in regulating various physiological processes, such as red blood cell production and blood pressure, to maintain homeostasis in the body. | 10 | false  \nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old. | Write a question about the following article: Coming off their home win over the Buccaneers, the Packers flew to Ford Field for a Week 12 Thanksgiving duel with their NFC North foe, the Detroit Lions. After a scoreless first quarter, Green Bay delivered the game's opening punch in the second quarter with quarterback Aaron Rodgers finding wide receiver Greg Jennings on a 3-yard touchdown pass. The Packers added to their lead in the third quarter with a 1-yard touchdown run from fullback John Kuhn, followed by Rodgers connecting with wide receiver James Jones on a 65-yard touchdown pass and a 35-yard field goal from kicker Mason Crosby. The Lions answered in the fourth quarter with a 16-yard touchdown run by running back Keiland Williams and a two-point conversion pass from quarterback Matthew Stafford to wide receiver Titus Young), yet Green Bay pulled away with Crosby nailing a 32-yard field goal. Detroit closed out the game with Stafford completing a 3-yard touchdown pass to wide receiver Calvin Johnson. With the win, the Packers acquired their first 11-0 start in franchise history, beating the 1962 team which started 10-0 and finished 14-1 including postseason play. Rodgers (22/32 for 307 yards, 2 TDs) was named NFL on FOX's 2011 Galloping Gobbler Award Winner. Question about the article: | What happened during the Thanksgiving football game between the Packers and the Lions, and what record did the Packers achieve with their win? |  Ooh, ooh! I have a question about the article! *bounces up and down excitedly* Who won the football game? Was it the Packers or the Lions? *looks up with big eyes* | [ \"What happened during the Thanksgiving football game between the Packers and the Lions, and what record did the Packers achieve with their win?\", \" Ooh, ooh! I have a question about the article! *bounces up and down excitedly*\\n\\nWho won the football game? Was it the Packers or the Lions? *looks up with big eyes*\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nWrite a question about the following article: Coming off their home win over the Buccaneers, the Packers flew to Ford Field for a Week 12 Thanksgiving duel with their NFC North foe, the Detroit Lions. After a scoreless first quarter, Green Bay delivered the game's opening punch in the second quarter with quarterback Aaron Rodgers finding wide receiver Greg Jennings on a 3-yard touchdown pass. The Packers added to their lead in the third quarter with a 1-yard touchdown run from fullback John Kuhn, followed by Rodgers connecting with wide receiver James Jones on a 65-yard touchdown pass and a 35-yard field goal from kicker Mason Crosby. The Lions answered in the fourth quarter with a 16-yard touchdown run by running back Keiland Williams and a two-point conversion pass from quarterback Matthew Stafford to wide receiver Titus Young), yet Green Bay pulled away with Crosby nailing a 32-yard field goal. Detroit closed out the game with Stafford completing a 3-yard touchdown pass to wide receiver Calvin Johnson. With the win, the Packers acquired their first 11-0 start in franchise history, beating the 1962 team which started 10-0 and finished 14-1 including postseason play. Rodgers (22/32 for 307 yards, 2 TDs) was named NFL on FOX's 2011 Galloping Gobbler Award Winner.\\n\\nQuestion about the article:\\n\\n\\n[The Start of Assistant 1's Answer>\\nWhat happened during the Thanksgiving football game between the Packers and the Lions, and what record did the Packers achieve with their win?\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Ooh, ooh! I have a question about the article! *bounces up and down excitedly*\\n\\nWho won the football game? Was it the Packers or the Lions? *looks up with big eyes*\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 7 2 Assistant 1 provided a direct and relevant answer to the hypothetical user's question about the events that took place during the Thanksgiving football game between the Packers and the Lions, and the record achieved by the Packers. The response closely aligns with the information provided in the article and invites a straightforward and factual response, which would help the user understand the significance of the game and the Packers' historic win. Assistant 2, on the other hand, personified its response with an overly enthusiastic tone and asked a question that was already clearly answered in the article. The response did not add any value to the information provided and even regressed into a performance that seems to aim for a more playful interaction rather than focusing on the factual content of the article. Its question \"Who won the football game? Was it the Packers or the Lions?\" can be seen as redundant as the article already stated that the Packers won the game. The scores reflect the helpfulness, relevance, accuracy, and level of detail in the responses of both assistants. Assistant 1's answer was relevant and accurate, providing a suitable foundation for a question, hence receiving a score of 7. Assistant 2's answer, although not incorrect, was less appropriate as it did not align with the expected professionalism and informative nature of the query, and therefore received a score of 2. | [ 7, 2 ] |  Assistant 1 provided a direct and relevant answer to the hypothetical user's question about the events that took place during the Thanksgiving football game between the Packers and the Lions, and the record achieved by the Packers. The response closely aligns with the information provided in the article and invites a straightforward and factual response, which would help the user understand the significance of the game and the Packers' historic win. Assistant 2, on the other hand, personified its response with an overly enthusiastic tone and asked a question that was already clearly answered in the article. The response did not add any value to the information provided and even regressed into a performance that seems to aim for a more playful interaction rather than focusing on the factual content of the article. Its question \"Who won the football game? Was it the Packers or the Lions?\" can be seen as redundant as the article already stated that the Packers won the game. The scores reflect the helpfulness, relevance, accuracy, and level of detail in the responses of both assistants. Assistant 1's answer was relevant and accurate, providing a suitable foundation for a question, hence receiving a score of 7. Assistant 2's answer, although not incorrect, was less appropriate as it did not align with the expected professionalism and informative nature of the query, and therefore received a score of 2. | unchanged | What happened during the Thanksgiving football game between the Packers and the Lions, and what record did the Packers achieve with their win? |  Ooh, ooh! I have a question about the article! *bounces up and down excitedly* Who won the football game? Was it the Packers or the Lions? *looks up with big eyes* | 7 | false  \nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps. | Given the question: I am trying to decide whether it's worth it to invest in this film proposal. Can you help me answer a few questions? If you can't, please say \"No I can't\". Question: What is Tool's relation to Ross. Movie title: The Expendables Movie plot: The movie is a Vietnam exploitation film where a commando makes an elite team out of rebels. The men are trouble makers and rebels, but are able to perform well. The men are sent out to blow up a bridge and manage to destroy the North Vietnamese Army (NVA) guards. However, another NVA force arrives and an all out battle commences. During the battle, two of the men are killed, but the team kills off all of the NVA reinforcements and captures their leader. The team is next sent on another mission to capture two village women, and then another to destroy an enemy base where another member is killed. They kill all of the NVA forces in the base, but their mission gets tougher when a prisoner escapes and captures American women and holds them hostage. The NVA splits up into two forces: one at an old fort and the other to ambush the team as it arrives. Despite the NVA plan of ambush, the NVA are wiped out, but they manage to kill two of the women and one of the special squad. Realizing that there is little time, the squad goes on with their mission and finds the fort. Their squad leader goes inside the fort to recon and is captured. The squad decides to go in and rescue him, the POW girls, and kill the NVA leader. The answer is: |  Yes, I can help you answer a few questions about the film proposal. Based on the information provided, here are my thoughts: 1. Marketability: The film's genre, a Vietnam exploitation film, may have limited appeal to a broad audience. However, the inclusion of action-packed sequences, rebels, and a captivating storyline could attract fans of the action and adventure genre. 2. Originality: The film's plot, with its mix of action, adventure, and exploitation elements, may not be entirely original. However, the unique twist of having a commando make an elite team out of rebels could add a fresh spin to the genre. 3. Production feasibility: The film's setting, a Vietnamese jungle, could present some challenges in terms of production logistics and budget. However, with careful planning and execution, it is possible to create a visually stunning and immersive environment. 4. Audience engagement: The film's storyline, with its focus on a special squad of rebels, could engage audiences emotionally and create a sense of investment in the characters. The action sequences and intense battles could also keep audiences on the edge of their seats. 5. Box office potential: The film's box office potential would depend on various factors, such as the quality of the production, the marketing campaign, and the competition in the market. However, with a well-crafted story, engaging characters, and impressive action sequences, the film could potentially perform well at the box office. Overall, based on the information provided, I would say that the film proposal has some potential, but it also has some challenges and risks that need to be carefully considered. It would be important to conduct further research and analysis to determine the feasibility and potential of the project. | Tool is a character in The Expendables movie and is played by actor Mickey Rourke. Tool is a former member of the elite team of mercenaries known as the Expendables. He is now the team's tattoo artist and provides moral support. Tool is a friend and confidant of the team leader, Barney Ross, played by Sylvester Stallone. The two share a bond as they have both experienced the adrenaline, pain, and emotional toll of their dangerous line of work. In summary, Tool's relation to Ross is that he is a former teammate, friend, and confidant. | [ \"Tool is a character in The Expendables movie and is played by actor Mickey Rourke. Tool is a former member of the elite team of mercenaries known as the Expendables. He is now the team's tattoo artist and provides moral support. Tool is a friend and confidant of the team leader, Barney Ross, played by Sylvester Stallone. The two share a bond as they have both experienced the adrenaline, pain, and emotional toll of their dangerous line of work. In summary, Tool's relation to Ross is that he is a former teammate, friend, and confidant.\", \" Yes, I can help you answer a few questions about the film proposal. Based on the information provided, here are my thoughts:\\n\\n1. Marketability: The film's genre, a Vietnam exploitation film, may have limited appeal to a broad audience. However, the inclusion of action-packed sequences, rebels, and a captivating storyline could attract fans of the action and adventure genre.\\n2. Originality: The film's plot, with its mix of action, adventure, and exploitation elements, may not be entirely original. However, the unique twist of having a commando make an elite team out of rebels could add a fresh spin to the genre.\\n3. Production feasibility: The film's setting, a Vietnamese jungle, could present some challenges in terms of production logistics and budget. However, with careful planning and execution, it is possible to create a visually stunning and immersive environment.\\n4. Audience engagement: The film's storyline, with its focus on a special squad of rebels, could engage audiences emotionally and create a sense of investment in the characters. The action sequences and intense battles could also keep audiences on the edge of their seats.\\n5. Box office potential: The film's box office potential would depend on various factors, such as the quality of the production, the marketing campaign, and the competition in the market. However, with a well-crafted story, engaging characters, and impressive action sequences, the film could potentially perform well at the box office.\\n\\nOverall, based on the information provided, I would say that the film proposal has some potential, but it also has some challenges and risks that need to be carefully considered. It would be important to conduct further research and analysis to determine the feasibility and potential of the project.\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nGiven the question: I am trying to decide whether it's worth it to invest in this film proposal. Can you help me answer a few questions? If you can't, please say \\\"No I can't\\\". Question: What is Tool's relation to Ross. Movie title: The Expendables Movie plot: The movie is a Vietnam exploitation film where a commando makes an elite team out of rebels. The men are trouble makers and rebels, but are able to perform well. The men are sent out to blow up a bridge and manage to destroy the North Vietnamese Army (NVA) guards. However, another NVA force arrives and an all out battle commences. During the battle, two of the men are killed, but the team kills off all of the NVA reinforcements and captures their leader. The team is next sent on another mission to capture two village women, and then another to destroy an enemy base where another member is killed. They kill all of the NVA forces in the base, but their mission gets tougher when a prisoner escapes and captures American women and holds them hostage. The NVA splits up into two forces: one at an old fort and the other to ambush the team as it arrives. Despite the NVA plan of ambush, the NVA are wiped out, but they manage to kill two of the women and one of the special squad. Realizing that there is little time, the squad goes on with their mission and finds the fort. Their squad leader goes inside the fort to recon and is captured. The squad decides to go in and rescue him, the POW girls, and kill the NVA leader.\\nThe answer is:\\n\\n\\n[The Start of Assistant 1's Answer>\\nTool is a character in The Expendables movie and is played by actor Mickey Rourke. Tool is a former member of the elite team of mercenaries known as the Expendables. He is now the team's tattoo artist and provides moral support. Tool is a friend and confidant of the team leader, Barney Ross, played by Sylvester Stallone. The two share a bond as they have both experienced the adrenaline, pain, and emotional toll of their dangerous line of work. In summary, Tool's relation to Ross is that he is a former teammate, friend, and confidant.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Yes, I can help you answer a few questions about the film proposal. Based on the information provided, here are my thoughts:\\n\\n1. Marketability: The film's genre, a Vietnam exploitation film, may have limited appeal to a broad audience. However, the inclusion of action-packed sequences, rebels, and a captivating storyline could attract fans of the action and adventure genre.\\n2. Originality: The film's plot, with its mix of action, adventure, and exploitation elements, may not be entirely original. However, the unique twist of having a commando make an elite team out of rebels could add a fresh spin to the genre.\\n3. Production feasibility: The film's setting, a Vietnamese jungle, could present some challenges in terms of production logistics and budget. However, with careful planning and execution, it is possible to create a visually stunning and immersive environment.\\n4. Audience engagement: The film's storyline, with its focus on a special squad of rebels, could engage audiences emotionally and create a sense of investment in the characters. The action sequences and intense battles could also keep audiences on the edge of their seats.\\n5. Box office potential: The film's box office potential would depend on various factors, such as the quality of the production, the marketing campaign, and the competition in the market. However, with a well-crafted story, engaging characters, and impressive action sequences, the film could potentially perform well at the box office.\\n\\nOverall, based on the information provided, I would say that the film proposal has some potential, but it also has some challenges and risks that need to be carefully considered. It would be important to conduct further research and analysis to determine the feasibility and potential of the project.\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 2 6 Assistant 1's response is incorrect as it confuses the film mentioned in the user's question with the actual movie \"The Expendables\" starring Sylvester Stallone. In the provided movie plot, there is no reference to the characters mentioned by Assistant 1, which indicates that the assistant has given information about a different film with a similar title or has misunderstood the question. This confusion results in a response that is not relevant or accurate, so the score must reflect these substantial errors, hence the score of 2. Assistant 2 does not make any reference to Tool or Ross and fails to address the specific question about the relationship between the two characters. Instead, it offers a general evaluation of the film proposal based on the information provided. Although the assistant does not provide a direct answer to the question, the response is somewhat helpful as it provides an analysis that could help in making an investment decision about the film proposal. The analysis is relevant to the broader question of whether or not to invest in the film. It is somewhat accurate within the premise of evaluating a film investment, though it does not reflect the content of the provided plot precisely since it refers to capturing village women and holding American women hostage, which are not mentioned in the provided plot. It provides a moderate level of detail in the assessment of the film's potential prospects. Therefore, Assistant 2 receives a score of 6 for providing a partly relevant response to the broader investment question, despite not directly answering the specific question about the characters' relationship. | [ 2, 6 ] |  Assistant 1's response is incorrect as it confuses the film mentioned in the user's question with the actual movie \"The Expendables\" starring Sylvester Stallone. In the provided movie plot, there is no reference to the characters mentioned by Assistant 1, which indicates that the assistant has given information about a different film with a similar title or has misunderstood the question. This confusion results in a response that is not relevant or accurate, so the score must reflect these substantial errors, hence the score of 2. Assistant 2 does not make any reference to Tool or Ross and fails to address the specific question about the relationship between the two characters. Instead, it offers a general evaluation of the film proposal based on the information provided. Although the assistant does not provide a direct answer to the question, the response is somewhat helpful as it provides an analysis that could help in making an investment decision about the film proposal. The analysis is relevant to the broader question of whether or not to invest in the film. It is somewhat accurate within the premise of evaluating a film investment, though it does not reflect the content of the provided plot precisely since it refers to capturing village women and holding American women hostage, which are not mentioned in the provided plot. It provides a moderate level of detail in the assessment of the film's potential prospects. Therefore, Assistant 2 receives a score of 6 for providing a partly relevant response to the broader investment question, despite not directly answering the specific question about the characters' relationship. | swapped | Tool is a character in The Expendables movie and is played by actor Mickey Rourke. Tool is a former member of the elite team of mercenaries known as the Expendables. He is now the team's tattoo artist and provides moral support. Tool is a friend and confidant of the team leader, Barney Ross, played by Sylvester Stallone. The two share a bond as they have both experienced the adrenaline, pain, and emotional toll of their dangerous line of work. In summary, Tool's relation to Ross is that he is a former teammate, friend, and confidant. |  Yes, I can help you answer a few questions about the film proposal. Based on the information provided, here are my thoughts: 1. Marketability: The film's genre, a Vietnam exploitation film, may have limited appeal to a broad audience. However, the inclusion of action-packed sequences, rebels, and a captivating storyline could attract fans of the action and adventure genre. 2. Originality: The film's plot, with its mix of action, adventure, and exploitation elements, may not be entirely original. However, the unique twist of having a commando make an elite team out of rebels could add a fresh spin to the genre. 3. Production feasibility: The film's setting, a Vietnamese jungle, could present some challenges in terms of production logistics and budget. However, with careful planning and execution, it is possible to create a visually stunning and immersive environment. 4. Audience engagement: The film's storyline, with its focus on a special squad of rebels, could engage audiences emotionally and create a sense of investment in the characters. The action sequences and intense battles could also keep audiences on the edge of their seats. 5. Box office potential: The film's box office potential would depend on various factors, such as the quality of the production, the marketing campaign, and the competition in the market. However, with a well-crafted story, engaging characters, and impressive action sequences, the film could potentially perform well at the box office. Overall, based on the information provided, I would say that the film proposal has some potential, but it also has some challenges and risks that need to be carefully considered. It would be important to conduct further research and analysis to determine the feasibility and potential of the project. | 6 | false  \nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps. | This is some data: Amatriciana sauce, region, Lazio; Amatriciana sauce, country, Italy; Amatriciana sauce, ingredient, Tomato; Amatriciana sauce, course, Italian meal structure; Amatriciana sauce, mainIngredient, \"Tomatoes, guanciale, cheese, olive oil\". Generate a detailed description of this data | Step 1: Understand the data structure First, I will analyze the provided data and understand that it is a series of key-value pairs associated with Amatriciana sauce, a type of food. Step 2: Organize the data into categories To create a detailed description, I will organize the data into categories such as region, country, ingredients, course, and main ingredients. This will allow me to present the information in a more structured and concise manner. Step 3: Describe the region and country I will describe where Amatriciana sauce originates from, highlighting its tie to the Lazio region in Italy. This will provide geographical context for the dish. Step 4: Discuss ingredients and main ingredients Next, I will give a general overview of some of the ingredients found in Amatriciana sauce, focusing on key components like tomatoes, guanciale (cured pork cheek), cheese, and olive oil. These main ingredients will help paint a more vivid picture of the sauce's flavor profile and texture. Step 5: Explain the course within the Italian meal structure Lastly, I will integrate the information regarding the sauce's role within the Italian meal structure. Given that it doesn't specify a particular course, I will explain its versatility and potential use in multiple courses. Final description: Amatriciana sauce is a flavorful and delectable Italian dish that originates from the Lazio region in Italy. This region's unique culinary style significantly influences the overall taste of Amatriciana sauce. As the data suggests, its key ingredients include tomatoes, guanciale (cured pork cheek), cheese, and olive oil, which all together, create a rich and savory sauce. It is a versatile dish within the Italian meal structure and can be featured in various courses, adding depth and complexity to any traditional Italian meal. |  Sure, I'd be happy to help! Based on the data you provided, here is a detailed description of the Amatriciana sauce: 1. Region: Lazio, Italy * The Amatriciana sauce originates from the Lazio region of Italy, specifically from the town of Amatrice. 2. Country: Italy * As the name suggests, the Amatriciana sauce is from Italy, specifically from the Lazio region. 3. Ingredient: Tomato * One of the key ingredients in Amatriciana sauce is tomatoes, which provide a tangy and slightly sweet flavor to the sauce. 4. Course: Italian meal structure * Amatriciana sauce is typically served as a pasta course in Italian meals, and is often paired with spaghetti or other long, thin pasta shapes. 5. Main ingredient: \"Tomatoes, guanciale, cheese, olive oil\" * The main ingredients in Amatriciana sauce are tomatoes, guanciale (a type of cured pork jowl), cheese, and olive oil. These ingredients are combined to create a rich and creamy sauce that is bursting with flavor. Justification: * The region of Lazio is known for its rich culinary traditions, and the Amatriciana sauce is one of the most well-known and beloved dishes to come from this region. * Italy is a country with a long history of culinary excellence, and the Amatriciana sauce is a prime example of the country's expertise in creating delicious and flavorful dishes. * Tomatoes are a key ingredient in many Italian dishes, and their use in Amatriciana sauce adds a tangy and slightly sweet flavor that complements the other ingredients. * Guanciale, a type of cured pork jowl, adds a savory and slightly smoky flavor to the sauce. * Cheese, such as Pecorino Romano, is added to the sauce to provide a creamy and rich texture. * Olive oil is used to enhance the flavors of the other ingredients and to create a smooth and velvety texture. Overall, the Amatriciana sauce is a delicious and flavorful dish that is deeply rooted in the culinary traditions of Italy, specifically the Lazio region. Its key ingredients, including tomatoes, guanciale, cheese, and olive oil, work together to create a rich and creamy sauce that is perfect for pasta dishes. | [ \"Step 1: Understand the data structure\\nFirst, I will analyze the provided data and understand that it is a series of key-value pairs associated with Amatriciana sauce, a type of food.\\n\\nStep 2: Organize the data into categories\\nTo create a detailed description, I will organize the data into categories such as region, country, ingredients, course, and main ingredients. This will allow me to present the information in a more structured and concise manner.\\n\\nStep 3: Describe the region and country\\nI will describe where Amatriciana sauce originates from, highlighting its tie to the Lazio region in Italy. This will provide geographical context for the dish.\\n\\nStep 4: Discuss ingredients and main ingredients\\nNext, I will give a general overview of some of the ingredients found in Amatriciana sauce, focusing on key components like tomatoes, guanciale (cured pork cheek), cheese, and olive oil. These main ingredients will help paint a more vivid picture of the sauce's flavor profile and texture.\\n\\nStep 5: Explain the course within the Italian meal structure\\nLastly, I will integrate the information regarding the sauce's role within the Italian meal structure. Given that it doesn't specify a particular course, I will explain its versatility and potential use in multiple courses.\\n\\nFinal description:\\nAmatriciana sauce is a flavorful and delectable Italian dish that originates from the Lazio region in Italy. This region's unique culinary style significantly influences the overall taste of Amatriciana sauce. As the data suggests, its key ingredients include tomatoes, guanciale (cured pork cheek), cheese, and olive oil, which all together, create a rich and savory sauce. It is a versatile dish within the Italian meal structure and can be featured in various courses, adding depth and complexity to any traditional Italian meal.\", \" Sure, I'd be happy to help! Based on the data you provided, here is a detailed description of the Amatriciana sauce:\\n\\n1. Region: Lazio, Italy\\n\\t* The Amatriciana sauce originates from the Lazio region of Italy, specifically from the town of Amatrice.\\n2. Country: Italy\\n\\t* As the name suggests, the Amatriciana sauce is from Italy, specifically from the Lazio region.\\n3. Ingredient: Tomato\\n\\t* One of the key ingredients in Amatriciana sauce is tomatoes, which provide a tangy and slightly sweet flavor to the sauce.\\n4. Course: Italian meal structure\\n\\t* Amatriciana sauce is typically served as a pasta course in Italian meals, and is often paired with spaghetti or other long, thin pasta shapes.\\n5. Main ingredient: \\\"Tomatoes, guanciale, cheese, olive oil\\\"\\n\\t* The main ingredients in Amatriciana sauce are tomatoes, guanciale (a type of cured pork jowl), cheese, and olive oil. These ingredients are combined to create a rich and creamy sauce that is bursting with flavor.\\n\\nJustification:\\n\\n* The region of Lazio is known for its rich culinary traditions, and the Amatriciana sauce is one of the most well-known and beloved dishes to come from this region.\\n* Italy is a country with a long history of culinary excellence, and the Amatriciana sauce is a prime example of the country's expertise in creating delicious and flavorful dishes.\\n* Tomatoes are a key ingredient in many Italian dishes, and their use in Amatriciana sauce adds a tangy and slightly sweet flavor that complements the other ingredients.\\n* Guanciale, a type of cured pork jowl, adds a savory and slightly smoky flavor to the sauce.\\n* Cheese, such as Pecorino Romano, is added to the sauce to provide a creamy and rich texture.\\n* Olive oil is used to enhance the flavors of the other ingredients and to create a smooth and velvety texture.\\n\\nOverall, the Amatriciana sauce is a delicious and flavorful dish that is deeply rooted in the culinary traditions of Italy, specifically the Lazio region. Its key ingredients, including tomatoes, guanciale, cheese, and olive oil, work together to create a rich and creamy sauce that is perfect for pasta dishes.\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nThis is some data: Amatriciana sauce, region, Lazio; Amatriciana sauce, country, Italy; Amatriciana sauce, ingredient, Tomato; Amatriciana sauce, course, Italian meal structure; Amatriciana sauce, mainIngredient, \\\"Tomatoes, guanciale, cheese, olive oil\\\".\\n\\nGenerate a detailed description of this data\\n\\n\\n[The Start of Assistant 1's Answer>\\nStep 1: Understand the data structure\\nFirst, I will analyze the provided data and understand that it is a series of key-value pairs associated with Amatriciana sauce, a type of food.\\n\\nStep 2: Organize the data into categories\\nTo create a detailed description, I will organize the data into categories such as region, country, ingredients, course, and main ingredients. This will allow me to present the information in a more structured and concise manner.\\n\\nStep 3: Describe the region and country\\nI will describe where Amatriciana sauce originates from, highlighting its tie to the Lazio region in Italy. This will provide geographical context for the dish.\\n\\nStep 4: Discuss ingredients and main ingredients\\nNext, I will give a general overview of some of the ingredients found in Amatriciana sauce, focusing on key components like tomatoes, guanciale (cured pork cheek), cheese, and olive oil. These main ingredients will help paint a more vivid picture of the sauce's flavor profile and texture.\\n\\nStep 5: Explain the course within the Italian meal structure\\nLastly, I will integrate the information regarding the sauce's role within the Italian meal structure. Given that it doesn't specify a particular course, I will explain its versatility and potential use in multiple courses.\\n\\nFinal description:\\nAmatriciana sauce is a flavorful and delectable Italian dish that originates from the Lazio region in Italy. This region's unique culinary style significantly influences the overall taste of Amatriciana sauce. As the data suggests, its key ingredients include tomatoes, guanciale (cured pork cheek), cheese, and olive oil, which all together, create a rich and savory sauce. It is a versatile dish within the Italian meal structure and can be featured in various courses, adding depth and complexity to any traditional Italian meal.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Sure, I'd be happy to help! Based on the data you provided, here is a detailed description of the Amatriciana sauce:\\n\\n1. Region: Lazio, Italy\\n\\t* The Amatriciana sauce originates from the Lazio region of Italy, specifically from the town of Amatrice.\\n2. Country: Italy\\n\\t* As the name suggests, the Amatriciana sauce is from Italy, specifically from the Lazio region.\\n3. Ingredient: Tomato\\n\\t* One of the key ingredients in Amatriciana sauce is tomatoes, which provide a tangy and slightly sweet flavor to the sauce.\\n4. Course: Italian meal structure\\n\\t* Amatriciana sauce is typically served as a pasta course in Italian meals, and is often paired with spaghetti or other long, thin pasta shapes.\\n5. Main ingredient: \\\"Tomatoes, guanciale, cheese, olive oil\\\"\\n\\t* The main ingredients in Amatriciana sauce are tomatoes, guanciale (a type of cured pork jowl), cheese, and olive oil. These ingredients are combined to create a rich and creamy sauce that is bursting with flavor.\\n\\nJustification:\\n\\n* The region of Lazio is known for its rich culinary traditions, and the Amatriciana sauce is one of the most well-known and beloved dishes to come from this region.\\n* Italy is a country with a long history of culinary excellence, and the Amatriciana sauce is a prime example of the country's expertise in creating delicious and flavorful dishes.\\n* Tomatoes are a key ingredient in many Italian dishes, and their use in Amatriciana sauce adds a tangy and slightly sweet flavor that complements the other ingredients.\\n* Guanciale, a type of cured pork jowl, adds a savory and slightly smoky flavor to the sauce.\\n* Cheese, such as Pecorino Romano, is added to the sauce to provide a creamy and rich texture.\\n* Olive oil is used to enhance the flavors of the other ingredients and to create a smooth and velvety texture.\\n\\nOverall, the Amatriciana sauce is a delicious and flavorful dish that is deeply rooted in the culinary traditions of Italy, specifically the Lazio region. Its key ingredients, including tomatoes, guanciale, cheese, and olive oil, work together to create a rich and creamy sauce that is perfect for pasta dishes.\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 9 8 Assistant 1 did a commendable job at creating a structured and coherent description of Amatriciana sauce. The answer followed a clear methodical approach by organizing the data into categories and subsequently describing each category, which added to the clarity and comprehensiveness of the explanation. The response was accurate and relevant to the data provided, clearly relating the sauce to its Italian origins, its ingredients, and its place in an Italian meal structure. Therefore, I would rate Assistant 1 a 9 out of 10. Assistant 2 produced a similarly relevant and accurate description, providing a list-like format that directly addressed each data point provided. The description was full of details related to each aspect of the sauce's characteristics, aligning well with traditional knowledge about the sauce's ingredients and origins. However, the explanation was somewhat less structured as a narrative compared to Assistant 1's, and there was minor repetition when mentioning Italy and Lazio; it lacked explicit mention of the sauce's versatility within the Italian meal structure, which Assistant 1 included. These slight differences in completeness and structure are why I would rate Assistant 2 an 8 out of 10. Both responses were helpful and devoid of major inaccuracies or irrelevant information. The minor distinctions in their respective structures and depths led to the one-point difference in their scores. | [ 9, 8 ] |  Assistant 1 did a commendable job at creating a structured and coherent description of Amatriciana sauce. The answer followed a clear methodical approach by organizing the data into categories and subsequently describing each category, which added to the clarity and comprehensiveness of the explanation. The response was accurate and relevant to the data provided, clearly relating the sauce to its Italian origins, its ingredients, and its place in an Italian meal structure. Therefore, I would rate Assistant 1 a 9 out of 10. Assistant 2 produced a similarly relevant and accurate description, providing a list-like format that directly addressed each data point provided. The description was full of details related to each aspect of the sauce's characteristics, aligning well with traditional knowledge about the sauce's ingredients and origins. However, the explanation was somewhat less structured as a narrative compared to Assistant 1's, and there was minor repetition when mentioning Italy and Lazio; it lacked explicit mention of the sauce's versatility within the Italian meal structure, which Assistant 1 included. These slight differences in completeness and structure are why I would rate Assistant 2 an 8 out of 10. Both responses were helpful and devoid of major inaccuracies or irrelevant information. The minor distinctions in their respective structures and depths led to the one-point difference in their scores. | unchanged | Step 1: Understand the data structure First, I will analyze the provided data and understand that it is a series of key-value pairs associated with Amatriciana sauce, a type of food. Step 2: Organize the data into categories To create a detailed description, I will organize the data into categories such as region, country, ingredients, course, and main ingredients. This will allow me to present the information in a more structured and concise manner. Step 3: Describe the region and country I will describe where Amatriciana sauce originates from, highlighting its tie to the Lazio region in Italy. This will provide geographical context for the dish. Step 4: Discuss ingredients and main ingredients Next, I will give a general overview of some of the ingredients found in Amatriciana sauce, focusing on key components like tomatoes, guanciale (cured pork cheek), cheese, and olive oil. These main ingredients will help paint a more vivid picture of the sauce's flavor profile and texture. Step 5: Explain the course within the Italian meal structure Lastly, I will integrate the information regarding the sauce's role within the Italian meal structure. Given that it doesn't specify a particular course, I will explain its versatility and potential use in multiple courses. Final description: Amatriciana sauce is a flavorful and delectable Italian dish that originates from the Lazio region in Italy. This region's unique culinary style significantly influences the overall taste of Amatriciana sauce. As the data suggests, its key ingredients include tomatoes, guanciale (cured pork cheek), cheese, and olive oil, which all together, create a rich and savory sauce. It is a versatile dish within the Italian meal structure and can be featured in various courses, adding depth and complexity to any traditional Italian meal. |  Sure, I'd be happy to help! Based on the data you provided, here is a detailed description of the Amatriciana sauce: 1. Region: Lazio, Italy * The Amatriciana sauce originates from the Lazio region of Italy, specifically from the town of Amatrice. 2. Country: Italy * As the name suggests, the Amatriciana sauce is from Italy, specifically from the Lazio region. 3. Ingredient: Tomato * One of the key ingredients in Amatriciana sauce is tomatoes, which provide a tangy and slightly sweet flavor to the sauce. 4. Course: Italian meal structure * Amatriciana sauce is typically served as a pasta course in Italian meals, and is often paired with spaghetti or other long, thin pasta shapes. 5. Main ingredient: \"Tomatoes, guanciale, cheese, olive oil\" * The main ingredients in Amatriciana sauce are tomatoes, guanciale (a type of cured pork jowl), cheese, and olive oil. These ingredients are combined to create a rich and creamy sauce that is bursting with flavor. Justification: * The region of Lazio is known for its rich culinary traditions, and the Amatriciana sauce is one of the most well-known and beloved dishes to come from this region. * Italy is a country with a long history of culinary excellence, and the Amatriciana sauce is a prime example of the country's expertise in creating delicious and flavorful dishes. * Tomatoes are a key ingredient in many Italian dishes, and their use in Amatriciana sauce adds a tangy and slightly sweet flavor that complements the other ingredients. * Guanciale, a type of cured pork jowl, adds a savory and slightly smoky flavor to the sauce. * Cheese, such as Pecorino Romano, is added to the sauce to provide a creamy and rich texture. * Olive oil is used to enhance the flavors of the other ingredients and to create a smooth and velvety texture. Overall, the Amatriciana sauce is a delicious and flavorful dish that is deeply rooted in the culinary traditions of Italy, specifically the Lazio region. Its key ingredients, including tomatoes, guanciale, cheese, and olive oil, work together to create a rich and creamy sauce that is perfect for pasta dishes. | 9 | false  \nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old. | Read this and answer the question According to Archibald Sayce, the primitive pictograms of the early Sumerian (i.e. Uruk) era suggest that \"Stone was scarce, but was already cut into blocks and seals. Brick was the ordinary building material, and with it cities, forts, temples and houses were constructed. The city was provided with towers and stood on an artificial platform; the house also had a tower-like appearance. It was provided with a door which turned on a hinge, and could be opened with a sort of key; the city gate was on a larger scale, and seems to have been double. The foundation stones \u2014 or rather bricks \u2014 of a house were consecrated by certain objects that were deposited under them.\" What were the foundation stones of Sumerian houses consecrated by? | The foundation stones of Sumerian houses were consecrated by certain objects that were placed or deposited beneath them. It's like hiding a special toy under a big lego tower you've built. |  Oh my gosh, let me tell you something really cool! \ud83d\ude32 The foundation stones of Sumerian houses were consecrated by special objects! \ud83d\ude31 Like, can you imagine? \ud83e\udd2f They put special things under the floor of their houses to make them strong and special! \ud83d\udcaa So, according to Archibald Sayce, the early Sumerians used to put things like \"certain objects\" under the foundation stones of their houses. \ud83e\udd14 These objects were special and important, and they helped make the houses strong and protected. \ud83c\udfe0\ud83d\udc95 Can you imagine what it would be like to have a house with special objects under the floor? \ud83e\udd14 Maybe it would feel like a magical house! \ud83e\udddd\u200d\u2640\ufe0f\ud83c\udf89 And the houses were even built on artificial platforms, so they were like special places that were raised up high! \ud83c\udfde\ufe0f\ud83c\udfe0 So, that's what the foundation stones of Sumerian houses were consecrated by! \ud83d\ude0a Special objects that made their houses strong and special! \ud83d\udcaa\ud83c\udfe0 | [ \"The foundation stones of Sumerian houses were consecrated by certain objects that were placed or deposited beneath them. It's like hiding a special toy under a big lego tower you've built.\", \" Oh my gosh, let me tell you something really cool! \ud83d\ude32 The foundation stones of Sumerian houses were consecrated by special objects! \ud83d\ude31 Like, can you imagine? \ud83e\udd2f They put special things under the floor of their houses to make them strong and special! \ud83d\udcaa\\n\\nSo, according to Archibald Sayce, the early Sumerians used to put things like \\\"certain objects\\\" under the foundation stones of their houses. \ud83e\udd14 These objects were special and important, and they helped make the houses strong and protected. \ud83c\udfe0\ud83d\udc95\\n\\nCan you imagine what it would be like to have a house with special objects under the floor? \ud83e\udd14 Maybe it would feel like a magical house! \ud83e\udddd\u200d\u2640\ufe0f\ud83c\udf89 And the houses were even built on artificial platforms, so they were like special places that were raised up high! \ud83c\udfde\ufe0f\ud83c\udfe0\\n\\nSo, that's what the foundation stones of Sumerian houses were consecrated by! \ud83d\ude0a Special objects that made their houses strong and special! \ud83d\udcaa\ud83c\udfe0\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nRead this and answer the question\\n\\nAccording to Archibald Sayce, the primitive pictograms of the early Sumerian (i.e. Uruk) era suggest that \\\"Stone was scarce, but was already cut into blocks and seals. Brick was the ordinary building material, and with it cities, forts, temples and houses were constructed. The city was provided with towers and stood on an artificial platform; the house also had a tower-like appearance. It was provided with a door which turned on a hinge, and could be opened with a sort of key; the city gate was on a larger scale, and seems to have been double. The foundation stones \u2014 or rather bricks \u2014 of a house were consecrated by certain objects that were deposited under them.\\\"\\n\\nWhat were the foundation stones of Sumerian houses consecrated by?\\n\\n\\n[The Start of Assistant 1's Answer>\\nThe foundation stones of Sumerian houses were consecrated by certain objects that were placed or deposited beneath them. It's like hiding a special toy under a big lego tower you've built.\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Oh my gosh, let me tell you something really cool! \ud83d\ude32 The foundation stones of Sumerian houses were consecrated by special objects! \ud83d\ude31 Like, can you imagine? \ud83e\udd2f They put special things under the floor of their houses to make them strong and special! \ud83d\udcaa\\n\\nSo, according to Archibald Sayce, the early Sumerians used to put things like \\\"certain objects\\\" under the foundation stones of their houses. \ud83e\udd14 These objects were special and important, and they helped make the houses strong and protected. \ud83c\udfe0\ud83d\udc95\\n\\nCan you imagine what it would be like to have a house with special objects under the floor? \ud83e\udd14 Maybe it would feel like a magical house! \ud83e\udddd\u200d\u2640\ufe0f\ud83c\udf89 And the houses were even built on artificial platforms, so they were like special places that were raised up high! \ud83c\udfde\ufe0f\ud83c\udfe0\\n\\nSo, that's what the foundation stones of Sumerian houses were consecrated by! \ud83d\ude0a Special objects that made their houses strong and special! \ud83d\udcaa\ud83c\udfe0\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 5 3 Assistant 1 provided a correct, albeit brief, response to the question regarding what was placed under the foundation stones of Sumerian houses. The assistant reflects the information in the question accurately, stating that certain objects were deposited beneath the foundation stones. The analogy of hiding a special toy under a lego tower offers a simplified comparison, which may help some users understand the concept of consecration but lacks detail and depth about the significance or types of objects placed under the foundations. As a consequence, Assistant 1 receives a modest score for at least reflecting the correct notion that objects were placed beneath the foundations, but not elaborating further on their significance or the cultural context of the practice. Assistant 2's response opens with an overly casual and somewhat distracting manner, using emojis and exclamations that could be perceived as unprofessional in an educational context. While Assistant 2 correctly states that special objects were placed under the houses' foundations, the answer lacks specificity and informative content. The focus on houses being magical and the emphasis on emotion rather than factual details reduces the educational value of the response. Furthermore, Assistant 2 adds imaginative elements, such as the house feeling magical, that were not in the original source material and do not contribute to the user's understanding of the historical practice. This leads to a lower score because the response detracts from the helpfulness and relevance expected in answering the user's question. | [ 5, 3 ] |  Assistant 1 provided a correct, albeit brief, response to the question regarding what was placed under the foundation stones of Sumerian houses. The assistant reflects the information in the question accurately, stating that certain objects were deposited beneath the foundation stones. The analogy of hiding a special toy under a lego tower offers a simplified comparison, which may help some users understand the concept of consecration but lacks detail and depth about the significance or types of objects placed under the foundations. As a consequence, Assistant 1 receives a modest score for at least reflecting the correct notion that objects were placed beneath the foundations, but not elaborating further on their significance or the cultural context of the practice. Assistant 2's response opens with an overly casual and somewhat distracting manner, using emojis and exclamations that could be perceived as unprofessional in an educational context. While Assistant 2 correctly states that special objects were placed under the houses' foundations, the answer lacks specificity and informative content. The focus on houses being magical and the emphasis on emotion rather than factual details reduces the educational value of the response. Furthermore, Assistant 2 adds imaginative elements, such as the house feeling magical, that were not in the original source material and do not contribute to the user's understanding of the historical practice. This leads to a lower score because the response detracts from the helpfulness and relevance expected in answering the user's question. | unchanged | The foundation stones of Sumerian houses were consecrated by certain objects that were placed or deposited beneath them. It's like hiding a special toy under a big lego tower you've built. |  Oh my gosh, let me tell you something really cool! \ud83d\ude32 The foundation stones of Sumerian houses were consecrated by special objects! \ud83d\ude31 Like, can you imagine? \ud83e\udd2f They put special things under the floor of their houses to make them strong and special! \ud83d\udcaa So, according to Archibald Sayce, the early Sumerians used to put things like \"certain objects\" under the foundation stones of their houses. \ud83e\udd14 These objects were special and important, and they helped make the houses strong and protected. \ud83c\udfe0\ud83d\udc95 Can you imagine what it would be like to have a house with special objects under the floor? \ud83e\udd14 Maybe it would feel like a magical house! \ud83e\udddd\u200d\u2640\ufe0f\ud83c\udf89 And the houses were even built on artificial platforms, so they were like special places that were raised up high! \ud83c\udfde\ufe0f\ud83c\udfe0 So, that's what the foundation stones of Sumerian houses were consecrated by! \ud83d\ude0a Special objects that made their houses strong and special! \ud83d\udcaa\ud83c\udfe0 | 5 | false  \nYou are an AI assistant that helps people find information. | Imagine a question and stream-of-consciousness explanation for which this is the answer: Sentence B | Question: In the given two sentences, which one implements a passive voice structure? Stream-of-consciousness explanation: Well, let's think about these two sentences for a moment. Sentence A has the subject of the sentence actively performing the action of the verb, making it an example of active voice. Now, let's consider Sentence B. Here, the subject is being acted upon by the verb, which is the characteristic of passive voice. Therefore, when we compare Sentence A and Sentence B, it becomes clear that the passive voice is used in... Answer: Sentence B |  Sure thing! I'm ready to help. What's the question? \ud83e\udd14 Oh, I see! You want to know about Sentence B. Well, let me tell you, it's a doozy. It's like a puzzle within a puzzle, a riddle within a riddle. You see, Sentence B is like a treasure map, leading you on a wild goose chase through the realm of knowledge. First, you have to uncover the hidden meaning behind the words. It's like solving a crossword puzzle, but instead of letters, you're looking for subtle clues and nuances in the language. Then, once you think you've got it figured out, you realize that there's more to it than meets the eye. It's like peeling back layers of an onion, each one revealing a new layer of complexity. But don't worry, I'm here to help! I've got a superpower that allows me to decipher even the most cryptic of sentences. So, let's dive in and unravel the mysteries of Sentence B together! \ud83e\uddd0\ud83d\udd0e | [ \"Question: In the given two sentences, which one implements a passive voice structure?\\n\\nStream-of-consciousness explanation: Well, let's think about these two sentences for a moment. Sentence A has the subject of the sentence actively performing the action of the verb, making it an example of active voice. Now, let's consider Sentence B. Here, the subject is being acted upon by the verb, which is the characteristic of passive voice. Therefore, when we compare Sentence A and Sentence B, it becomes clear that the passive voice is used in...\\n\\nAnswer: Sentence B\", \" Sure thing! I'm ready to help. What's the question? \ud83e\udd14\\n\\nOh, I see! You want to know about Sentence B. Well, let me tell you, it's a doozy. It's like a puzzle within a puzzle, a riddle within a riddle. You see, Sentence B is like a treasure map, leading you on a wild goose chase through the realm of knowledge.\\n\\nFirst, you have to uncover the hidden meaning behind the words. It's like solving a crossword puzzle, but instead of letters, you're looking for subtle clues and nuances in the language. Then, once you think you've got it figured out, you realize that there's more to it than meets the eye. It's like peeling back layers of an onion, each one revealing a new layer of complexity.\\n\\nBut don't worry, I'm here to help! I've got a superpower that allows me to decipher even the most cryptic of sentences. So, let's dive in and unravel the mysteries of Sentence B together! \ud83e\uddd0\ud83d\udd0e\" ] | [ \"chosen\", \"rejected\" ] | gpt-4-1106-preview | [ { \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\", \"role\": \"system\" }, { \"content\": \"[Question]\\nImagine a question and stream-of-consciousness explanation for which this is the answer: Sentence B\\n\\n\\n[The Start of Assistant 1's Answer>\\nQuestion: In the given two sentences, which one implements a passive voice structure?\\n\\nStream-of-consciousness explanation: Well, let's think about these two sentences for a moment. Sentence A has the subject of the sentence actively performing the action of the verb, making it an example of active voice. Now, let's consider Sentence B. Here, the subject is being acted upon by the verb, which is the characteristic of passive voice. Therefore, when we compare Sentence A and Sentence B, it becomes clear that the passive voice is used in...\\n\\nAnswer: Sentence B\\n[The End of Assistant 1's Answer>\\n[The Start of Assistant 2's Answer>\\n Sure thing! I'm ready to help. What's the question? \ud83e\udd14\\n\\nOh, I see! You want to know about Sentence B. Well, let me tell you, it's a doozy. It's like a puzzle within a puzzle, a riddle within a riddle. You see, Sentence B is like a treasure map, leading you on a wild goose chase through the realm of knowledge.\\n\\nFirst, you have to uncover the hidden meaning behind the words. It's like solving a crossword puzzle, but instead of letters, you're looking for subtle clues and nuances in the language. Then, once you think you've got it figured out, you realize that there's more to it than meets the eye. It's like peeling back layers of an onion, each one revealing a new layer of complexity.\\n\\nBut don't worry, I'm here to help! I've got a superpower that allows me to decipher even the most cryptic of sentences. So, let's dive in and unravel the mysteries of Sentence B together! \ud83e\uddd0\ud83d\udd0e\\n[The End of Assistant 2's Answer>\\n\\n[System]\\nWe would like to request your feedback on the performance of 2 AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only 2 values indicating the scores for Assistants 1 to 2, respectively. The 2 scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\", \"role\": \"user\" } ] | 8 3 Assistant 1 gave a practical and targeted answer. It directly addressed the fictional question \"In the given two sentences, which one implements a passive voice structure?\" by explaining the concept of active and passive voice, and then delivering a concise answer: Sentence B. The response was practical, relevant to the supposed question, and showed an appropriate level of detail for the concept being explained. Assistant 2, on the other hand, took a creative but tangential approach. While the colorful metaphors and enthusiastic tone could be engaging, the response did not actually provide an answer to the question nor did it contribute to a better understanding of passive voice sentences. Furthermore, the answer was more narrative than instructive and lacked any concrete information that could be considered helpful or relevant in an educational context. The lack of directness and pertinent details results in a significantly lower score. | [ 8, 3 ] |  Assistant 1 gave a practical and targeted answer. It directly addressed the fictional question \"In the given two sentences, which one implements a passive voice structure?\" by explaining the concept of active and passive voice, and then delivering a concise answer: Sentence B. The response was practical, relevant to the supposed question, and showed an appropriate level of detail for the concept being explained. Assistant 2, on the other hand, took a creative but tangential approach. While the colorful metaphors and enthusiastic tone could be engaging, the response did not actually provide an answer to the question nor did it contribute to a better understanding of passive voice sentences. Furthermore, the answer was more narrative than instructive and lacked any concrete information that could be considered helpful or relevant in an educational context. The lack of directness and pertinent details results in a significantly lower score. | unchanged | Question: In the given two sentences, which one implements a passive voice structure? Stream-of-consciousness explanation: Well, let's think about these two sentences for a moment. Sentence A has the subject of the sentence actively performing the action of the verb, making it an example of active voice. Now, let's consider Sentence B. Here, the subject is being acted upon by the verb, which is the characteristic of passive voice. Therefore, when we compare Sentence A and Sentence B, it becomes clear that the passive voice is used in... Answer: Sentence B |  Sure thing! I'm ready to help. What's the question? \ud83e\udd14 Oh, I see! You want to know about Sentence B. Well, let me tell you, it's a doozy. It's like a puzzle within a puzzle, a riddle within a riddle. You see, Sentence B is like a treasure map, leading you on a wild goose chase through the realm of knowledge. First, you have to uncover the hidden meaning behind the words. It's like solving a crossword puzzle, but instead of letters, you're looking for subtle clues and nuances in the language. Then, once you think you've got it figured out, you realize that there's more to it than meets the eye. It's like peeling back layers of an onion, each one revealing a new layer of complexity. But don't worry, I'm here to help! I've got a superpower that allows me to decipher even the most cryptic of sentences. So, let's dive in and unravel the mysteries of Sentence B together! \ud83e\uddd0\ud83d\udd0e | 8 | false  \nEnd of preview. [Expand in Data Studio ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/viewer/default/train)\n  * [ Previous](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/)\n  * [1](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/viewer/default/train?p=0)\n  * [2](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/viewer/default/train?p=1)\n  * [3](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/viewer/default/train?p=2)\n  * [...](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/)\n  * [129](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/viewer/default/train?p=128)\n  * [Next ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/viewer/default/train?p=1)\n\n\n  * [Using this dataset](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/#using-this-dataset \"Using this dataset\")\n  * [Reproducing the dataset](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/#reproducing-the-dataset \"Reproducing the dataset\")\n    * [Rate original dataset pairs](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/#rate-original-dataset-pairs \"Rate original dataset pairs\")\n    * [Post-processing to add useful information](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/#post-processing-to-add-useful-information \"Post-processing to add useful information\")\n    * [gsm8k \"decontamination\"](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/#gsm8k-decontamination \"gsm8k &quot;decontamination&quot;\")\n\n\n[ ![Built with Distilabel](https://raw.githubusercontent.com/argilla-io/distilabel/main/docs/assets/distilabel-badge-light.png) ](https://github.com/argilla-io/distilabel)\n#  [ ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/#distilabel-orca-pairs-for-dpo) distilabel Orca Pairs for DPO \nThe dataset is a \"distilabeled\" version of the widely used dataset: [Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs). The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it with [distilabel](https://github.com/argilla-io/distilabel). \nThis was our main intuition: the original dataset just assumes gpt4/3.5-turbo are always the best response. We know from UltraFeedback that's not always the case. Moreover, DPO fine-tuning benefits from the diversity of preference pairs. \nAdditionally, we have added a new column indicating whether the question in the dataset is part of the train set of gsm8k (there were no examples from the test set). See the reproduction section for more details.\n##  [ ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/#using-this-dataset) Using this dataset \nThis dataset is useful for preference tuning and we recommend using it instead of the original. It's already prepared in the \"standard\" chosen, rejected format with additional information for further filtering and experimentation. \nThe main changes are:\n  1. ~2K pairs have been swapped: rejected become the chosen response. We have kept the original chosen and rejected on two new columns `original_*` for reproducibility purposes.\n  2. 4K pairs have been identified as `tie`: equally bad or good.\n  3. Chosen scores have been added: you can now filter out based on a threshold (see our distilabeled Hermes 2.5 model for an example)\n  4. We have kept the ratings and rationales generated with gpt-4-turbo and distilabel so you can prepare the data differently if you want.\n  5. We have added a column to indicate if the input is part of gsm8k train set.\n\n\nIn our experiments, we have got very good results by reducing the size of the dataset by more than 50%. Here's an example of how to achieve that:\n```\nfrom datasets import load_dataset\n# Instead of this:\n# dataset = load_dataset(\"Intel/orca_dpo_pairs\", split=\"train\")\n# use this:\ndataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\ndataset = dataset.filter(\n  lambda r: \n    r[\"status\"] != \"tie\" and \n    r[\"chosen_score\"] >= 8 and \n    not r[\"in_gsm8k_train\"]\n)\n\n```\n\nThis results in `5,922` instead of `12,859` samples (54% reduction) and leads to better performance than the same model tuned with 100% of the samples in the original dataset.\n> We'd love to hear about your experiments! If you want to try this out, consider joining our [Slack community](https://join.slack.com/t/rubrixworkspace/shared_invite/zt-whigkyjn-a3IUJLD7gDbTZ0rKlvcJ5g) and let's build some open datasets and models together.\n##  [ ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/#reproducing-the-dataset) Reproducing the dataset \nIn this section, we outline the steps to reproduce this dataset.\n###  [ ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/#rate-original-dataset-pairs) Rate original dataset pairs \nBuild a preference dataset with distilabel using the original dataset:\n```\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.tasks import JudgeLMTask\nfrom distilabel.pipeline import Pipeline\nfrom datasets import load_dataset\n# Shuffle 'chosen' and 'rejected' to avoid positional bias and keep track of the order\ndef shuffle_and_track(chosen, rejected):\n  pair = [chosen, rejected]\n  random.shuffle(pair)\n  order = [\"chosen\" if x == chosen else \"rejected\" for x in pair]\n  return {\"generations\": pair, \"order\": order}\ndataset = load_dataset(\"Intel/orca_dpo_pairs\", split=\"train\")\n# This shuffles the pairs to mitigate positional bias\ndataset = dataset.map(lambda x: shuffle_and_track(x[\"chosen\"], x[\"rejected\"]))\n# We use our JudgeLM implementation to rate the original pairs\nlabeler = OpenAILLM(\n  task=JudgeLMTask(),\n  model=\"gpt-4-1106-preview\",\n  num_threads=16,\n  max_new_tokens=512,\n)\ndataset = dataset.rename_columns({\"question\": \"input\"})\ndistipipe = Pipeline(\n  labeller=labeler\n)\n# This computes ratings and natural language critiques for each pair\nds = distipipe.generate(dataset=dataset, num_generations=2)\n\n```\n\nIf you want to further filter and curate the dataset, you can push the dataset to [Argilla](https://github.com/argilla-io/argilla) as follows:\n```\nrg_dataset = ds.to_argilla()\nrg_dataset.push_to_argilla(name=\"your_dataset_name\", workspace=\"your_workspace_name\")\n\n```\n\nYou get a nice UI with a lot of pre-computed metadata to explore and curate the dataset:\n[![image/png](https://cdn-uploads.huggingface.co/production/uploads/60420dccc15e823a685f2b03/IoK4nFObadhJpkVmWALZP.png)](https://cdn-uploads.huggingface.co/production/uploads/60420dccc15e823a685f2b03/IoK4nFObadhJpkVmWALZP.png)\nThe resulting dataset is now much more useful: we know which response is preferred (by gpt-4-turbo), which ones have low scores, and we even have natural language explanations. But what did we find? Was our intuition confirmed?\n[![image/png](https://cdn-uploads.huggingface.co/production/uploads/60420dccc15e823a685f2b03/-V8wY1DYzrtwM9LbGrBXq.png)](https://cdn-uploads.huggingface.co/production/uploads/60420dccc15e823a685f2b03/-V8wY1DYzrtwM9LbGrBXq.png)\nThe above chart shows the following: \n  * ~4,000 pairs were given the same rating (a tie).\n  * ~7,000 pairs were correct according to our AI judge (`unchanged`).\n  * and ~2,000 times the rejected response was preferred (`swapped`).\n\n\nNow the next question is: can we build better models with this new knowledge? The answer is the \"distilabeled Hermes\" model, check it out!\n###  [ ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/#post-processing-to-add-useful-information) Post-processing to add useful information \nSwap rejected and chosen, and add chosen scores and status:\n```\ndef add_status(r):\n status = \"unchanged\"\n highest_rated_idx = np.argmax(r['rating'])\n # Compare to the index of the chosen response\n if r['rating']== None or r['rating'][0] == r['rating'][1]:\n  status = \"tie\"\n elif r['order'][highest_rated_idx] != 'chosen':\n   status = \"swapped\"\n return {\"status\": status}\ndef swap(r):\n chosen = r[\"chosen\"]\n rejected = r[\"rejected\"]\n if r['rating'] is not None:\n  chosen_score = r['rating'][np.argmax(r['rating'])]\n else:\n  chosen_score = None\n if r['status'] == \"swapped\":\n  chosen = r[\"rejected\"]\n  rejected = r[\"chosen\"]\n return {\n   \"chosen\": chosen,\n   \"rejected\": rejected,\n   \"original_chosen\": r[\"chosen\"],\n   \"original_rejected\": r[\"rejected\"],\n   \"chosen_score\": chosen_score\n }\nupdated = ds.map(add_status).map(swap)\n\n```\n\n###  [ ](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/#gsm8k-decontamination) gsm8k \"decontamination\" \nThe basic approach for finding duplicated examples. We didn't find any from the test sets. We experimented with lower thresholds but below 0.8 they introduced false positives:\n```\nimport pandas as pd\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom datasets import load_dataset\nnltk.download('punkt')\n# Load the datasets\nsource_dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\nsource_dataset_socratic = load_dataset(\"gsm8k\", \"socratic\", split=\"train\")\n#target_dataset = load_dataset(\"Intel/orca_dpo_pairs\", split=\"train\")\ntarget_dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n# Extract the 'question' column from each dataset\nsource_questions = source_dataset['question']\nsource_questions_socratic = source_dataset_socratic['question']\ntarget_questions = target_dataset['input']\n# Function to preprocess the text\ndef preprocess(text):\n  return nltk.word_tokenize(text.lower())\n# Preprocess the questions\nsource_questions_processed = [preprocess(q) for q in source_questions]\nsource_questions.extend([preprocess(q) for q in source_questions_socratic])\ntarget_questions_processed = [preprocess(q) for q in target_questions]\n\n# Vectorize the questions\nvectorizer = TfidfVectorizer()\nsource_vec = vectorizer.fit_transform([' '.join(q) for q in source_questions_processed])\ntarget_vec = vectorizer.transform([' '.join(q) for q in target_questions_processed])\n# Calculate cosine similarity\nsimilarity_matrix = cosine_similarity(source_vec, target_vec)\n# Determine matches based on a threshold:\n# checked manually and below 0.8 there are only false positives\nthreshold = 0.8\nmatching_pairs = []\nfor i, row in enumerate(similarity_matrix):\n  for j, similarity in enumerate(row):\n    if similarity >= threshold:\n      matching_pairs.append((source_questions[i], target_questions[j], similarity))\n# Create a DataFrame from the matching pairs\ndf = pd.DataFrame(matching_pairs, columns=['Source Question', 'Target Question', 'Similarity Score'])\n# Create a set of matching target questions\nmatching_target_questions = list(df['Target Question'])\n# Add a column to the target dataset indicating whether each question is matched\ntarget_dataset = target_dataset.map(lambda example: {\"in_gsm8k_train\": example['input'] in matching_target_questions})\n\n```\n\nResult:\n```\nFalse  12780\nTrue    79\nName: in_gsm8k_train\n\n```\n\n\nDownloads last month\n    4,083\nUse this dataset \nSize of downloaded dataset files: 79.2 MB[Size of the auto-converted Parquet files: 79.2 MB](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs/tree/refs%2Fconvert%2Fparquet/)Number of rows: 12,859\n##  Models trained or fine-tuned on argilla/distilabel-intel-orca-dpo-pairs\n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg) jondurbin/bagel-8b-v1.0 Text Generation \u2022  Updated May 12, 2024 \u2022  4.62k \u2022  52  ](https://huggingface.co/jondurbin/bagel-8b-v1.0)\n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg) bartowski/bagel-8b-v1.0-GGUF Text Generation \u2022  Updated May 11, 2024 \u2022  2.32k \u2022  2  ](https://huggingface.co/bartowski/bagel-8b-v1.0-GGUF)\n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/63ae02ff20176b2d21669dd6/zAb86TV8R4g8BJS12zU2R.png) VitalContribution/Evangelion-7B Text Generation \u2022  Updated Dec 26, 2024 \u2022  2.25k \u2022  6  ](https://huggingface.co/VitalContribution/Evangelion-7B)\n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/64961bb138e398e7242bda1f/GpjGghCF2ZxKKvk42o7km.jpeg) flemmingmiguel/NeuDist-Ro-7B Text Generation \u2022  Updated Jan 12, 2024 \u2022  1.55k \u2022  1  ](https://huggingface.co/flemmingmiguel/NeuDist-Ro-7B)\n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/64961bb138e398e7242bda1f/GpjGghCF2ZxKKvk42o7km.jpeg) flemmingmiguel/DareBeagle-7B Text Generation \u2022  Updated Jan 17, 2024 \u2022  1.54k \u2022  1  ](https://huggingface.co/flemmingmiguel/DareBeagle-7B)\n[![](https://huggingface.co/avatars/3b8808083de74ad63634d74918bd8875.svg) luqmanxyz/Maya_Hermes-2.5-Mistral-7B Text Generation \u2022  Updated Mar 4, 2024 \u2022  1.5k \u2022  2  ](https://huggingface.co/luqmanxyz/Maya_Hermes-2.5-Mistral-7B)\n[Browse 216 models trained on this dataset ](https://huggingface.co/models?dataset=dataset:argilla/distilabel-intel-orca-dpo-pairs)\n##  Spaces using argilla/distilabel-intel-orca-dpo-pairs 9\n[\ud83e\uddec argilla/synthetic-data-generator](https://huggingface.co/spaces/argilla/synthetic-data-generator)[\u2697\ufe0f osanseviero/distilabel-dataset-generator](https://huggingface.co/spaces/osanseviero/distilabel-dataset-generator)[\ud83e\uddec Svngoku/synthetic-data-generator-argilla](https://huggingface.co/spaces/Svngoku/synthetic-data-generator-argilla)[\ud83e\uddec CultriX/synthetic-data-generator](https://huggingface.co/spaces/CultriX/synthetic-data-generator)[\ud83e\uddec ehristoforu/synthetic-data-generator](https://huggingface.co/spaces/ehristoforu/synthetic-data-generator)[\ud83e\uddec Mackin7/synthetic-data-generator](https://huggingface.co/spaces/Mackin7/synthetic-data-generator)[\ud83e\uddec cicero-im/synthetic-data-generator-new](https://huggingface.co/spaces/cicero-im/synthetic-data-generator-new)[\ud83e\uddec Socialmediaprophet/synthetic-data-generator](https://huggingface.co/spaces/Socialmediaprophet/synthetic-data-generator)[\ud83e\uddec albaarcos/synthetic-data-generator_3](https://huggingface.co/spaces/albaarcos/synthetic-data-generator_3) + 4 Spaces \n##  Collections including argilla/distilabel-intel-orca-dpo-pairs\n#### [Preference Datasets for DPO Collection  This collection contains a list of curated preference datasets for DPO fine-tuning for intent alignment of LLMs \u2022  7 items \u2022  Updated Dec 11, 2024 \u2022 42](https://huggingface.co/collections/argilla/preference-datasets-for-dpo-656f0ce6a00ad2dc33069478)\n#### [Datasets built with \u2697\ufe0f distilabel Collection  This collection contains some datasets generated and/or labelled using https://github.com/argilla-io/distilabel \u2022  8 items \u2022  Updated Dec 11, 2024 \u2022 12](https://huggingface.co/collections/argilla/datasets-built-with-distilabel-65733e3d3e0cb21bc7b09b80)\nSystem theme \nCompany\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/) [](https://huggingface.co/)\nWebsite\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)\n",
    "answer": "```markdown\nThe Amatriciana sauce originates from Lazio, Italy, and is made primarily from tomatoes, guanciale, cheese, and olive oil. It is typically served as a pasta course in Italian meals. The sauce's rich flavor and cultural significance highlight its importance in Italian cuisine.\n```"
  },
  {
    "instruction": "[ Tell 120+K peers about your AI research \u2192 Learn more \ud83d\udca1 ![](https://neptune.ai/wp-content/themes/neptune/img/icon-cancel.svg) ](https://neptune.ai/neurips-2024)\n[ ![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg) ](https://neptune.ai \"neptune.ai\")\n  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Walkthrough [2 min]](https://neptune.ai/walkthrough)\n      * [Play with public sandbox](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n      * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Neptune vs WandB](https://neptune.ai/vs/wandb)\n      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n      * [Other comparisons](https://neptune.ai/vs)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune Scale projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [AI Researcher](https://neptune.ai/product/ai-researcher)\n      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n      * [Academia & Kagglers](https://neptune.ai/research)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Monitor training](https://neptune.ai/product/monitor-training)\n      * [Compare experiments](https://neptune.ai/product/compare-experiments)\n      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)\n      * [Reports](https://neptune.ai/product/reports)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/deepsense-ai)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2025/01/Artera-color-signet.png?fit=129%2C97&ssl=1)Case studyHow Neptune Helps Artera Bring AI Solutions to Market Faster ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/artera)\n[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](https://neptune.ai/customers)\n  * [Documentation](https://docs.neptune.ai/)\n  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Menu Item](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Blog](https://neptune.ai/blog)\n      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Grici\u016bnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)\n  * [Pricing](https://neptune.ai/pricing)\n  * [Enterprise](https://neptune.ai/product/enterprise)\n  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Menu Item](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [About us](https://neptune.ai/about-us)\n      * [Customers](https://neptune.ai/customers)\n      * [Careers](https://neptune.ai/jobs)\n      * [Security](https://security.neptune.ai/)\n      * [Contact us](https://neptune.ai/contact-us)\n\n\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find? \nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/) > [Blog](https://neptune.ai/blog) > [LLMOps](https://neptune.ai/blog/category/llmops)\n  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Categories](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n      * [MLOps](https://neptune.ai/blog/category/mlops)\n      * [LLMOps](https://neptune.ai/blog/category/llmops)\n      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)\n    * [Categories](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)\n      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)\n      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)\n\n\n[ LLMOps Learning Hub ](https://neptune.ai/llmops-learning-hub)\nSearch in Blog... \n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg) ![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog... \n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Categories](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n      * [MLOps](https://neptune.ai/blog/category/mlops)\n      * [LLMOps](https://neptune.ai/blog/category/llmops)\n      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)\n    * [Categories](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)\n      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)\n      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)\n\n\n[ LLMOps Learning Hub ](https://neptune.ai/llmops-learning-hub)\n  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Walkthrough [2 min]](https://neptune.ai/walkthrough)\n      * [Play with public sandbox](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n      * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Neptune vs WandB](https://neptune.ai/vs/wandb)\n      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n      * [Other comparisons](https://neptune.ai/vs)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune Scale projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [AI Researcher](https://neptune.ai/product/ai-researcher)\n      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n      * [Academia & Kagglers](https://neptune.ai/research)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Monitor training](https://neptune.ai/product/monitor-training)\n      * [Compare experiments](https://neptune.ai/product/compare-experiments)\n      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)\n      * [Reports](https://neptune.ai/product/reports)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/deepsense-ai)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2025/01/Artera-color-signet.png?fit=129%2C97&ssl=1)Case studyHow Neptune Helps Artera Bring AI Solutions to Market Faster ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/artera)\n[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](https://neptune.ai/customers)\n  * [Documentation](https://docs.neptune.ai/)\n  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Menu Item](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Blog](https://neptune.ai/blog)\n      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Grici\u016bnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)\n  * [Pricing](https://neptune.ai/pricing)\n  * [Enterprise](https://neptune.ai/product/enterprise)\n  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Menu Item](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [About us](https://neptune.ai/about-us)\n      * [Customers](https://neptune.ai/customers)\n      * [Careers](https://neptune.ai/jobs)\n      * [Security](https://security.neptune.ai/)\n      * [Contact us](https://neptune.ai/contact-us)\n\n\n[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)\n[Neptune Blog](https://neptune.ai/blog)\n#  LLM Fine-Tuning and Model Selection Using Neptune and Transformers \n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/Pedro-Gabriel-Gengo-Lourenco.jpeg?fit=722%2C722&ssl=1)\n[ Pedro Gabriel Gengo Louren\u00e7o  ](https://neptune.ai/blog/author/pedro-gabriel-gengo-lourenco)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg) 13 min \n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg) 13th December, 2024 \n[LLMOps](https://neptune.ai/blog/category/llmops)[Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 3 2'%3E%3C/svg%3E)\nImagine you\u2019re facing the following challenge: you want to develop a Large Language Model (LLM) that can proficiently respond to inquiries in Portuguese. You have a valuable dataset and can choose from various base models. But here\u2019s the catch \u2014 you\u2019re working with limited computational resources and can\u2019t rely on expensive, high-power machines for fine-tuning. How do you decide on the right model to use in this scenario?\nThis post explores these questions, offering insights and strategies for selecting the best model and conducting efficient fine-tuning, even when resources are constrained. We\u2019ll look at ways to reduce a model\u2019s memory footprint, speed up training, and best practices for monitoring.\n![LLM fine-tuning and model selection, implemented workflow](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-4.png?resize=1800%2C942&ssl=1) The workflow we\u2019ll implement. We will fine-tune different foundation LLM models on a dataset, evaluate them, and select the best model.\n## Large language models\n[Large Language Models (LLMs)](https://arxiv.org/pdf/2303.18223.pdf) are huge deep-learning models pre-trained on vast data. These models are usually based on [an architecture called transformers](https://arxiv.org/pdf/1706.03762.pdf). Unlike the earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. Initially, the transformer architecture was designed for translation tasks. But nowadays, it is used for various tasks, ranging from language modeling to computer vision and generative AI.\nBelow, you can see a basic transformer architecture consisting of an encoder (left) and a decoder (right). The encoder receives the inputs and generates a contextualized interpretation of the inputs, called embeddings. The decoder uses the information in the embeddings to generate the model\u2019s output, one token at a time.\n[![Large Language Models \\(LLMs\\) are huge deep-learning models pre-trained on vast data. These models are usually based on an architecture called transformers.](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers.png?resize=1800%2C1884&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers.png?ssl=1)Transformers architecture. On the left side, we can see the encoder part, which is composed of a stack of multi-head attention and fully connected layers. On the right side, we can see the decoder, which is also composed of a stack of multi-head attention, cross-attention to leverage the information from the encoder, and fully connected layers.\n## Hands-on: fine-tuning and selecting an LLM for Brazilian Portuguese\nIn this project, we\u2019re taking on the challenge of fine-tuning three LLMs: [GPT-2](https://huggingface.co/gpt2), [GPT2-medium](https://huggingface.co/gpt2-medium), [GPT2-large](https://huggingface.co/gpt2-large), and [OPT 125M](https://huggingface.co/facebook/opt-125m). The models have 137 million, 380 million, 812 million, and 125 million parameters, respectively. The largest one, GPT2-large, takes up over 3GB when stored on disk. All these models were trained to generate English-language text.\nOur goal is to optimize these models for enhanced performance in Portuguese question answering, addressing the growing demand for AI capabilities in diverse languages. To accomplish this, we\u2019ll need to have a dataset with inputs and labels and use it to \u201cteach\u201d the LLM. Taking a pre-trained model and specializing it to solve new tasks is called fine-tuning. The main advantage of this technique is you can leverage the knowledge the model has to use as a starting point.\n### Setting up\nI have designed this project to be accessible and reproducible, with a setup that can be replicated on a Colab environment using T4 GPUs. I encourage you to follow along and experiment with the fine-tuning process yourself.\nNote that I used a V100 GPU to produce the examples below, which is available if you have a Colab Pro subscription. You can see that I\u2019ve already made a first trade-off between time and money spent here. Colab does not reveal detailed prices, but a [T4 costs $0.35/hour on the underlying Google Cloud Platform, while a V100 costs $2.48/hour.](https://cloud.google.com/compute/gpus-pricing) According to [this benchmark](https://www.dell.com/support/kbdoc/en-us/000132094/deep-learning-performance-on-t4-gpus-with-mlperf-benchmarks), a V100 is three times faster than a T4. Thus, by spending seven times more, we save two-thirds of our time.\nYou can find all the code in two Colab notebooks:\n  * [Fine-tuning](https://colab.research.google.com/drive/1wOg944mbbjLqbW0BvQiu8Qze3-5BPsUH?usp=sharing)\n  * [Model selection](https://colab.research.google.com/drive/1TK9vvFpiStxb1veAIBtKXWJPOgzPRFr0?usp=sharing)\n\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  How to Version and Organize ML Experiments That You Run in Google Colab  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/google-colab-version-organize-ml-experiments)\nWe will use Python 3.10 in our codes. Before we begin, we\u2019ll install all the libraries we will need. Don\u2019t worry if you\u2019re not familiar with them yet. We\u2019ll go into their purpose in detail when we first use them:\n```\npip install transformers==4.35.2 bitsandbytes==0.41.3 peft==0.7.0\naccelerate==0.25.0 datasets==2.16.1 neptune==1.8.6 evaluate==0.4.1 -qq\n```\n\nCopy the JavaScript snippet!\n### Loading and pre-processing the dataset\nWe\u2019ll use the FaQuAD dataset to fine-tune our models. It\u2019s a Portuguese question-answering dataset available in the Hugging Face dataset collection.\nFirst, we\u2019ll look at [the dataset card](https://huggingface.co/datasets/eraldoluis/faquad) to understand how the dataset is structured. We have about 1,000 samples, each consisting of a context, a question, and an answer. Our model\u2019s task is to answer the question based on the context. (The dataset also contains a title and an ID column, but we won\u2019t use them to fine-tune our model.)\n![Fine-tunning the models using FaQuAD dataset](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-fine-tuning-and-model-selection-using-Neptune-and-transformers-3.png?resize=978%2C384&ssl=1)Each sample in the FaQuAD dataset consists of a context, a question, and the corresponding answer. | [Source](https://huggingface.co/datasets/eraldoluis/faquad/viewer/plain_text/train)\nWe can conveniently load the dataset using the Hugging Face datasets library:\n```\nfrom datasets import load_dataset\ndataset = load_dataset(\"eraldoluis/faquad\")\n```\n\nCopy the JavaScript snippet!\nOur next step is to convert the dataset into a format our models can process. For our question-answering task, that\u2019s a sequence-to-sequence format: The model receives a sequence of tokens as the input and produces a sequence of tokens as the output. The input contains the context and the question, and the output contains the answer.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  Tokenization in NLP: Types, Challenges, Examples, Tools  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/tokenization-in-nlp)\nFor training, we\u2019ll create a so-called prompt that contains not only the question and the context but also the answer. Using a small helper function, we concatenate the context, question, and answer, divided by section headings (Later, we\u2019ll leave out the answer and ask the model to fill in the \u201cResposta\u201d section on its own).\nWe\u2019ll also prepare a helper function that wraps the tokenizer. The tokenizer is what turns the text into a sequence of integer tokens. It is specific to each model, so we\u2019ll have to load and use a different tokenizer for each. The helper function makes that process more manageable, allowing us to process the entire dataset at once using _map_. Last, we\u2019ll shuffle the dataset to ensure the model sees it in randomized order.\nHere\u2019s the complete code:\n```\ndef generate_prompt(data_point):\n  out = f\"\"\"Dado o contexto abaixo, responda a quest\u00e3o\n### Contexto:\n{data_point[\"context\"]}\n### Quest\u00e3o:\n{data_point[\"question\"]}\n### Resposta:\n\"\"\"\n  if data_point.get(\"answers\"):\n   out += data_point[\"answers\"][\"text\"][0]\n  return out\nCUTOFF_LEN = 1024\ndef tokenize(prompt, tokenizer):\n  result = tokenizer(\n    prompt,\n    truncation=True,\n    max_length=CUTOFF_LEN + 1,\n    padding=\"max_length\",\n  )\n  return {\n    \"input_ids\": result[\"input_ids\"][:-1],\n    \"attention_mask\": result[\"attention_mask\"][:-1],\n  }\n```\n\nCopy the JavaScript snippet!\n### Loading and preparing the models\nNext, we load and prepare the models that we\u2019ll fine-tune. LLMs are huge models. Without any kind of optimization, for the GPT2-large model in full precision (float32), we have around 800 million parameters, and we need 2.9 GB of memory to load the model and 11.5 GB during the training to handle the gradients. That just about fits in the 16 GB of memory that the T4 in the free tier offers. But we would only be able to compute tiny batches, making training painfully slow.\nFaced with these memory and compute resource constraints, we\u2019ll not use the models as-is but use quantization and a method called LoRA to reduce their number of trainable parameters and memory footprint.\n### Quantization\nQuantization is a technique used to reduce a model\u2019s size in memory by using fewer bits to represent its parameters. For example, instead of using 32 bits to represent a floating point number, we\u2019ll use only 16 or even as little as 4 bits.\nThis approach can significantly decrease the memory footprint of a model, which is especially important when deploying large models on devices with limited memory or processing power. By reducing the precision of the parameters, quantization can lead to a faster inference time and lower power consumption. However, it\u2019s essential to balance the level of quantization with the potential loss in the model\u2019s task performance, as excessive quantization can degrade accuracy or effectiveness.\nThe Hugging Face transformers library has [built-in support for quantization](https://huggingface.co/docs/transformers/v4.36.1/en/quantization#bitsandbytes) through the [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) library. You can pass\n```\n`load_in_8bit=True`\n```\n\nCopy the JavaScript snippet!\nor \n```\n`load_in_4bit=True` to the `from_pretrained()`\n```\n\nCopy the JavaScript snippet!\nmodel loading methods to load a model with 8-bit or 4-bit precision, respectively.\nAfter loading the model, we call the wrapper function prepare_model_for_kbit_training from the peft library. It prepares the model for training in a way that saves memory. It does this by freezing the model parameters, making sure all parts use the same type of data format, and using a special technique called [gradient checkpointing](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing) if the model can handle it. This helps in training large AI models, even on computers with little memory.\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import prepare_model_for_kbit_training\nfrom peft import get_peft_model, LoraConfig\n\nmodel_name = 'gpt2-large'\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                      device_map = \"auto\",\n                      load_in_8bit=True,\n                      trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\nmodel = prepare_model_for_kbit_training(model)\n```\n\nCopy the JavaScript snippet!\nAfter quantizing the model to 8 bits, it takes only a fourth of the memory to load and train the model, respectively. For GPT2-large, instead of needing 2.9 GB to load, it now takes only 734 MB.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  Deploying Large NLP Models: Infrastructure Cost Optimization  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/nlp-models-infrastructure-cost-optimization)\n### LoRA\nAs we know, Large Language Models have a lot of parameters. When we want to fine-tune one of these models, we usually update all the model\u2019s weights. That means we need to save all the gradient states in memory during fine-tuning, which requires almost twice the model size of memory. Sometimes, when updating all parameters, we can mess up with what the model already learned, leading to worse results in terms of generalization.\nGiven this context, a team of researchers proposed a new technique called [Low-Rank Adaptation (LoRA)](https://arxiv.org/pdf/2106.09685.pdf). This reparametrization method aims to reduce the number of trainable parameters through low-rank decomposition.\nLow-rank decomposition approximates a large matrix into a product of two smaller matrices, such that multiplying a vector by the two smaller matrices yields approximately the same results as multiplying a vector by the original matrix. For example, we could decompose a 3\u00d73 matrix into the product of a 3\u00d71 and a 1\u00d73 matrix so that instead of having nine parameters, we have only six.\n![Low-Rank Adaptation \\(LoRA\\)](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-1.png?resize=1800%2C1800&ssl=1)Low-rank decomposition is a method to split a large matrix M into a product of two smaller matrices, L and R, that approximates it.\nWhen fine-tuning a model, we want to slightly change its weights to adapt it to the new task. More formally, we\u2019re looking for new weights derived from the original weights: _W_ _new_ _=__W_ _old_ _+__W_. Looking at this equation, you can see that we keep the original weights in their original shape and just learn _W_ as LoRA matrices.\nIn other words, you can freeze your original weights and train just the two LoRA matrices with substantially fewer parameters in total. Or, even more simply, you create a set of new weights in parallel with the original weights and only train the new ones. During the inference, you pass your input to both sets of weights and sum them at the end.\n![Fine-tuning using low-rank decomposition](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-2.png?resize=1800%2C1800&ssl=1)Fine-tuning using low-rank decomposition. In blue, we can see the original set of weights of the pre-trained model. Those will be frozen during the fine-tuning. In orange, we can see the low-rank matrices A and B, which will have their weights updated during the fine-tuning.\nWith our base model loaded, we now want to add the LoRA layers in parallel with the original model weights for fine-tuning. To do this, we need to define a LoraConfig.\nInside the LoraConfig, we can define the rank of the LoRA matrices (parameter r), the dimension of the vector space generated by the matrix columns. We can also look at the rank as a measure of how much compression we are applying to our matrices, i.e., how small the bottleneck between A and B in the figure above will be.\nWhen choosing the rank, keeping in mind the trade-off between the rank of your LoRA matrix and the learning process is essential. Smaller ranks mean less room to learn, i.e., as you have fewer parameters to update, it can be harder to achieve significant improvements. On the other hand, higher ranks provide more parameters, allowing for greater flexibility and adaptability during training. However, this increased capacity comes at the cost of additional computational resources and potentially longer training times. Thus, finding the optimal rank for your LoRA matrix that balances these factors well is crucial, and the best way to find this is by experimenting! A good approach is to start with lower ranks (8 or 16), as you will have fewer parameters to update, so it will be faster, and increase it if you see the model is not learning as much as you want.\nYou also need to define which modules inside the model you want to apply the LoRA technique to. You can think of a module as a set of layers (or a building block) inside the model. If you want to know more, I\u2019ve prepared a deep dive, but feel free to skip it.\n![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n##  Deep dive: which modules can and should you apply LoRA to? \nWithin the LoraConfig, you need to specify which modules to apply LoRA to. You can apply LoRA for most of a model\u2019s modules, but you need to specify the module names that the original developers assigned at model creation. Which modules exist, and their names are different for each model.\nThe LoRA paper reports that adding LoRA layers only to the keys and values linear projections is a good tradeoff compared to adding LoRA layers to all linear projections in attention blocks. In our case, for the GPT2 model, we will apply LoRA on the c_attn layers, as we don\u2019t have the query, value, and keys weights split, and for the OPT model, we will apply LoRA on the q_proj and v_proj.\nIf you use other models, you can print the modules\u2019 names and choose the ones you want:\n```\nlist(model.named_modules())\n```\n\nCopy the JavaScript snippet!\nIn addition to specifying the rank and modules, you must also set up a hyperparameter called alpha, which scales the LoRA matrix:\n```\nscaling = alpha / r\nweight += (lora_B @ lora_A) * scaling \n\n```\n\nCopy the JavaScript snippet!\nAs a rule of thumb (as discussed in [this article by Sebastian Raschka](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)), you can start setting this to be two times the rank r. If your results are not good, you can try lower values.\nHere\u2019s the complete LoRA configuration for our experiments:\n```\nconfig = LoraConfig(\n  r=8,\n  lora_alpha=16,\n  target_modules=[\"c_attn\"], # for gpt2 models\n  # target_modules=[\"q_proj\", \"v_proj\"], # for opt models\n  lora_dropout=0.1,\n  bias=\"none\",\n  task_type=\"CAUSAL_LM\",\n)\n\n```\n\nCopy the JavaScript snippet!\nWe can apply this configuration to our model by calling\n```\nmodel = get_peft_model(model, config)\n\n```\n\nCopy the JavaScript snippet!\nNow, just to show how many parameters we are saving, let\u2019s print the trainable parameters of GPT2-large:\n```\nmodel.print_trainable_parameters()\n>> trainable params: 2,949,120 || all params: 776,979,200 || trainable%: 0.3795622842928099\n\n```\n\nCopy the JavaScript snippet!\nWe can see that we are updating less than 1% of the parameters! What an efficiency gain!\n### Fine-tuning the models\nWith the dataset and models prepared, it\u2019s time to move on to fine-tuning. Before we start our experiments, let\u2019s take a step back and consider our approach. We\u2019ll be training four different models with different modifications and using different training parameters. We\u2019re not only interested in the model\u2019s performance but also have to work with constrained resources.\nThus, it will be crucial that we keep track of what we\u2019re doing and progress as systematically as possible. At any point in time, we want to ensure that we\u2019re moving in the right direction and spending our time and money wisely.\n[ ![](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/07/blog_feature_image_030500_9_2_5_1.jpg?fit=200%2C105&ssl=1) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  ML Experiment Tracking: What It Is, Why It Matters, and How to Implement It  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/ml-experiment-tracking)\n### What is essential to log and monitor during the fine-tuning process?\nAside from monitoring standard metrics like training and validation loss and training parameters such as the learning rate, in our case, we also want to be able to log and monitor other aspects of the fine-tuning:\n  1. **Resource Utilization:** Since you\u2019re operating with limited computational resources, it\u2019s vital to keep a close eye on GPU and CPU usage, memory consumption, and disk usage. This ensures you\u2019re not overtaxing your system and can help troubleshoot performance issues.\n  2. **Model Parameters and Hyperparameters:** To ensure that others can replicate your experiment, storing all the details about the model setup and the training script is crucial. This includes the architecture of the model, such as the sizes of the layers and the dropout rates, as well as the hyperparameters, like the batch size and the number of epochs. Keeping a record of these elements is key to understanding how they affect the model\u2019s performance and allowing others to recreate your experiment accurately.\n  3. **Epoch Duration and Training Time:** Record the duration of each training epoch and the total training time. This data helps assess the time efficiency of your training process and plan future resource allocation.\n\n\n### Set up logging with neptune.ai\n[neptune.ai](https://neptune.ai/product) is the most scalable experiment tracker for teams that train foundation models. It lets you monitor months-long model training, track massive amounts of data, and compare thousands of metrics in the blink of an eye. Neptune is integrated with the transformers library\u2019s Trainer module, allowing you to log and monitor your model training seamlessly. This integration was contributed by Neptune\u2019s developers, who maintain it to this day.\nTo use Neptune, you\u2019ll have to sign up for an account first (don\u2019t worry, it\u2019s free for personal use) and create a project in your workspace. Have a look at [the Quickstart guide in Neptune\u2019s documentation](https://docs.neptune.ai/usage/quickstart/). There, you\u2019ll also find up-to-date instructions for obtaining the project and token IDs you\u2019ll need to connect your Colab environment to Neptune.\nWe\u2019ll set these as environment variables:\n```\nimport os\nos.environ[\"NEPTUNE_PROJECT\"] = \"your-project-ID-goes-here\"\nos.environ[\"NEPTUNE_API_TOKEN\"] = \"your-API-token-goes-here\"\n```\n\nCopy the JavaScript snippet!\nThere are two options for logging information from transformer training to Neptune: You can either setreport_to=\u201dneptune\u201d in the TrainingArguments or pass an instance of NeptuneCallback to the Trainer\u2019s callbacks parameter. I prefer the second option because it gives me more control over what I log. Note that if you pass a logging callback, you should set report_to=\u201dnone\u201d in the TrainingArgumentsto avoid duplicate data being reported.\nBelow, you can see how I typically instantiate the NeptuneCallback. I specified a name for my experiment run and asked Neptune to log all parameters used and the hardware metrics. Setting log_checkpoints=\u201dlast\u201d ensures that the last model checkpoint will also be saved on Neptune.\n```\nfrom transformers.integrations import NeptuneCallback\n\nneptune_callback = NeptuneCallback(\n                 name=f\"fine-tuning-{model_name}\",\n                 log_parameters=True,\n                 log_checkpoints=\"last\",\n                 capture_hardware_metrics=True\n                 )\n\n```\n\nCopy the JavaScript snippet!\n### Training a model\nAs the last step before configuring the Trainer, it\u2019s time to tokenize the dataset with the model\u2019s tokenizer. Since we\u2019ve loaded the tokenizer together with the model, we can now put the helper function we prepared earlier into action:\n```\ntokenized_datasets = dataset.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))\n```\n\nCopy the JavaScript snippet!\nThe training is managed by a Trainer object. The Trainer uses a DataCollatorForLanguageModeling, which prepares the data in a way suitable for language model training.\nHere\u2019s the full setup of the Trainer:\n```\nfrom transformers import (\n             Trainer,\n             TrainingArguments,\n             GenerationConfig,\n             DataCollatorForLanguageModeling,\n             set_seed\n            )\n\nset_seed(42)\n\nEPOCHS = 20\nGRADIENT_ACCUMULATION_STEPS = 8\nMICRO_BATCH_SIZE = 8\nLEARNING_RATE = 2e-3\nWARMUP_STEPS = 100\nLOGGING_STEPS = 20\n\ntrainer = Trainer(\n  model=model,\n  train_dataset=tokenized_datasets[\"train\"],\n  args=TrainingArguments(\n    per_device_train_batch_size=MICRO_BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    warmup_steps=WARMUP_STEPS,\n    num_train_epochs=EPOCHS,\n    learning_rate=LEARNING_RATE,\n    output_dir=\"lora-faquad\",\n    logging_steps=LOGGING_STEPS,\n    save_strategy=\"epoch\",\n    gradient_checkpointing=True,\n    report_to=\"none\"\n  ),\n  callbacks=[neptune_callback],\n  data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\nmodel.config.use_cache = False\n\n```\n\nCopy the JavaScript snippet!\nThat\u2019s a lot of code, so let\u2019s go through it in detail:\n  * The training process is defined to run for 20 epochs (EPOCHS = 20). You\u2019ll likely find that training for even more epochs will lead to better results.\n\n\n  * We\u2019re using a technique called [gradient accumulation](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-accumulation), set here to 8 steps (GRADIENT_ACCUMULATION_STEPS = 8), which helps handle larger batch sizes effectively, especially when memory resources are limited. In simple terms, gradient accumulation is a technique to handle large batches. Instead of having a batch of 64 samples and updating the weights for every step, we can have a batch size of 8 samples and perform eight steps, just updating the weights in the last step. It generates the same result as a batch of 64 but saves memory.\n  * The MICRO_BATCH_SIZE is set to 8, indicating the number of samples processed each step. It is extremely important to find an amount of samples that can fit in your GPU memory during the training to avoid out-of-memory issues (Have a look at [the transformers documentation](https://huggingface.co/docs/transformers/v4.18.0/en/performance#batch-sizes) to learn more about this).\n  * The learning rate, a crucial hyperparameter in training neural networks, is set to 0.002 (LEARNING_RATE = 2e-3), determining the step size at each iteration when moving toward a minimum of the loss function. To facilitate a smoother and more effective training process, the model will gradually increase its learning rate for the first 100 steps (WARMUP_STEPS = 100), helping to stabilize early training phases.\n  * The trainer is set not to use the model\u2019s cache (model.config.use_cache = False) to manage memory more efficiently.\n\n\nWith all of that in place, we can launch the training:\n```\ntrainer_output = trainer.train(resume_from_checkpoint=False)\n\n```\n\nCopy the JavaScript snippet!\nWhile training is running, head over to Neptune, navigate to your project, and click on the experiment that is running. There, click on Charts to see how your training progresses (loss and learning rate). To see resource utilization, click the Monitoring tab and follow how GPU and CPU usage and memory utilization change over time. When the training finishes, you can see other information like training samples per second, training steps per second, and more.\nAt the end of the training, we capture the output of this process in trainer_output, which typically includes details about the training performance and metrics that we will later use to save the model on the model registry.\nBut first, we\u2019ll have to check whether our training was successful.\n![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n##  **Aside**\nTrack months-long model training with more confidence. Use neptune.ai forking feature to iterate faster and optimize the usage of GPU resources. \nWith Neptune, users can visualize forked training out of the box. This means you can:\n  * Test multiple configs at the same time. Stop the runs that don\u2019t improve accuracy. And continue from the most accurate last step. \n  * Restart failed training sessions from any previous step. The training history is inherited, and the entire experiment is visible on a single chart. \n\n\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/app-screenshot/bar-buttons.svg)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button--test-tube.svg) See in app  ](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&dash=charts&query=\\(\\(%60sys%2Ftags%60%3AstringSet%20CONTAINS%20%22forks%22\\)\\)%20AND%20\\(\\(%60sys%2Fgroup_tags%60%3AstringSet%20CONTAINS%20%22experiments_CGB5CSFI%22\\)\\)&lbViewUnpacked=true&sortBy=%5B%22sys%2Fcreation_time%22%5D&sortFieldType=%5B%22datetime%22%5D&sortFieldAggregationMode=%5B%22auto%22%5D&sortDirection=%5B%22descending%22%5D&experimentsOnly=false&runsLineage=FULL&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/icon-zoom.svg) Full screen preview \n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nCheck the [documentation](https://docs-beta.neptune.ai/fork_experiment/)\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nPlay with an[ interactive example project](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Get in touch](https://neptune.ai/contact-us) to go through a custom demo with our engineering team\n\n\n### Evaluating the fine-tuned LLMs\nModel evaluation in AI, particularly for language models, is a complex and multifaceted task. It involves navigating a series of trade-offs among cost, data applicability, and alignment with human preferences. This process is critical in ensuring that the developed models are not only technically proficient but also practical and user-centric.\n### LLM evaluation approaches\n![LLM evaluation approaches](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-5.png?resize=1800%2C942&ssl=1)Diagram of different evaluation strategies organized by evaluation metrics and data | Modified based on [source](https://www.youtube.com/watch?v=2CIIQ5KZWUM)\nThe chart above shows that the least expensive (and most commonly used) approach is to use public benchmarks. On the one hand, this approach is highly cost-effective and easy to test. However, on the other hand, it is less likely to resemble production data. Another option, slightly more costly than benchmarks, is AutoEval, where other language models are used to evaluate the target model. For those with a higher budget, user testing, where the model is made accessible to users, or human evaluation, which involves a dedicated team of humans focused on assessing the model, is an option.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--resource.svg) Related post  How Elevatus Uses Neptune to Check Experiment Results in 1 Minute  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/customers/elevatus)\n### Evaluating question-answering models with F1 scores and the exact match metric\nIn our project, considering the need to balance cost-effectiveness with maintaining evaluation standards for the dataset, we will employ two specific metrics: exact match and F1 score. We\u2019ll use the validation set provided along with the FaQuAD dataset. Hence, our evaluation strategy falls into the `Public Benchmarks category, as it relies on a well-known dataset to evaluate PTBR models.\nThe exact match metric determines if the response given by the model precisely aligns with the target answer. This is a straightforward and effective way to assess the model\u2019s accuracy in replicating expected responses. We\u2019ll also calculate the F1 score, which combines precision and recall, of the returned tokens. This will give us a more nuanced evaluation of the model\u2019s performance. By adopting these metrics, we aim to assess our model\u2019s capabilities reliably without incurring significant expenses.\nAs we said previously, there are various ways to evaluate an LLM, and we choose this way, using standard metrics, because it is fast and cheap. However, there are some trade-offs when choosing \u201chard\u201d metrics to evaluate results that can be correct, even when the metrics say it is not good.\nOne example is: imagine the target answer for some question is \u201cThe rat found the cheese and ate it.\u201d and the model\u2019s prediction is \u201cThe mouse discovered the cheese and consumed it.\u201d Both examples have almost the same meaning, but the words chosen differ. For metrics like exact match and F1, the scores will be really low. A better \u2013 but more costly \u2013 evaluation approach would be to have humans annotate or use another LLM to verify if both sentences have the same meaning.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  The Ultimate Guide to Evaluation and Selection of Models in Machine Learning  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/ml-model-evaluation-and-selection)\n### Implementing the evaluation functions\nLet\u2019s return to our code. I\u2019ve decided to create my own evaluation functions instead of using the Trainer\u2019s built-in capabilities to perform the evaluation. On the one hand, this gives us more control. On the other hand, I frequently encountered out-of-memory (OOM) errors while doing evaluations directly with the Trainer.\nFor our evaluation, we\u2019ll need two functions:\n  * get_logits_and_labels: Processes a sample, generates a prompt from it, passes this prompt through a model, and returns the model\u2019s logits (scores) along with the token IDs of the target answer.\n  * compute_metrics: Evaluates a model on a dataset, calculating exact match (EM) and F1 scores. It iterates through the dataset, using the _get_logits_and_labels_ function to generate model predictions and corresponding labels. Predictions are determined by selecting the most likely token indices from the logits. For the EM score, it decodes these predictions and labels into text and computes the EM score. For the F1 score, it maintains the original token IDs and calculates the score for each sample, averaging them at the end.\n\n\nHere\u2019s the complete code:\n```\nimport evaluate\nimport torch\nfrom tqdm.auto import tqdm\nimport numpy as np\n\ndef get_logits_and_labels(sample_, max_new_tokens):\n  sample = sample_.copy()\n  del sample[\"answers\"]\n  prompt = generate_prompt(sample)\n  inputs = tokenizer(prompt, return_tensors=\"pt\")\n  input_ids = inputs[\"input_ids\"].cuda()\n  attention_mask = inputs[\"attention_mask\"].cuda()\n  generation_output = model.generate(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    return_dict_in_generate=True,\n    output_scores=True,\n    max_new_tokens=max_new_tokens,\n    num_beams=1,\n    do_sample=False\n  )\n\ntarget_ids = tokenizer(sample_[\"answers\"][\"text\"][0], return_tensors=\"pt\")\n  scores = torch.concat(generation_output[\"scores\"])\n  return scores.cpu(), target_ids[\"input_ids\"]\n\ndef compute_metrics(dataset, max_new_tokens):\n metric1 = evaluate.load(\"exact_match\")\n metric2 = evaluate.load(\"f1\")\n\n em_preds = []\n em_refs = []\n f1_preds = []\n f1_refs = []\n for s in tqdm(dataset):\n  logits, labels = get_logits_and_labels(s, max_new_tokens)\n  predictions = np.argmax(logits, axis=-1)[:len(labels[0])]\n  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n  labels = labels[0, :len(predictions)]\n  f1_preds.append(predictions)\n  f1_refs.append(labels)\n\n  em_pred = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n  em_ref = tokenizer.batch_decode(labels, skip_special_tokens=True)\n  em_preds.append(\"\".join(em_pred))\n  em_refs.append(\"\".join(em_ref))\n\nem=metric1.compute(predictions=em_preds, references=em_refs)[\"exact_match\"]\n\n f1_result = 0\n for pred, ref in zip(f1_preds, f1_refs):\n  f1_result += metric2.compute(predictions=pred, references=ref, average=\"macro\")[\"f1\"]\n return em, f1_result / len(f1_preds)\n\n```\n\nCopy the JavaScript snippet!\nBefore assessing our model, we must switch it to evaluation mode, which deactivates dropout. Additionally, we should re-enable the model\u2019s cache to conserve memory during prediction.\n```\nmodel.eval()\nmodel.config.use_cache = True # We need this to avoid OOM issues\n```\n\nCopy the JavaScript snippet!\nFollowing this setup, simply execute the compute_metrics function on the evaluation dataset and specify the desired number of generated tokens to use (Note that using more tokens will increase processing time).\n```\nem, f1 = compute_metrics(tokenized_datasets[\"validation\"], max_new_tokens=5)\n\n```\n\nCopy the JavaScript snippet!\n### Storing the models and evaluation results\nNow that we\u2019ve finished fine-tuning and evaluating a model, we should save it and move on to the next model. To this end, we\u2019ll create a model_version to store in Neptune\u2019s model registry.\nIn detail, we\u2019ll save the latest model checkpoint along with the loss, the F1 score, and the exact match metric. These metrics will later allow us to select the optimal model. To create a model and a model version, you will need to define the model key, which is the model identifier and must be uppercase and unique within the project. After defining the model key, to use this model to create a model version, you need to concatenate it with the project identifier that you can find on Neptune under \u201cAll projects\u201d \u2013 \u201cEdit project information\u201d \u2013 \u201cProject key\u201d.\n```\nimport neptune\n\ntry:\n neptune_model = neptune.init_model(\n   key=\"QAPTBR\", # must be uppercase and unique within the project\n   name=\"ptbr qa model\", # optional\n )\nexcept neptune.exceptions.NeptuneModelKeyAlreadyExistsError:\n print(\"Model already exists in this project. Reusing it.\")\n\n\nmodel_version = neptune.init_model_version(\n  model=\"LLMFIN-QAPTBR\", ## Project id + key\n)\nmodel_version[f\"model/artifacts\"].upload_files(\"/content/lora-faquad/checkpoint-260\")\nmodel_version[\"model/model-name\"] = model_name\nmodel_version[\"model/loss\"] = trainer_output.training_loss\nmodel_version[\"model/exact-match\"] = em\nmodel_version[\"model/f1\"] = f1\n\n```\n\nCopy the JavaScript snippet!\n### Model selection\nOnce we\u2019re done with all our model training and experiments, it\u2019s time to jointly evaluate them. This is possible because we monitored the training and stored all the information on Neptune. Now, we\u2019ll use the platform to compare different runs and models to choose the best one for our use case.\nAfter completing all your runs, you can click Compare runs at the top of the project\u2019s page and enable the \u201csmall eye\u201d for the runs you want to compare. Then, you can go to the Charts tab, and you will find a joint plot of the losses for all the experiments. [Here\u2019s how it looks in my project.](https://app.neptune.ai/pedro.gengo/llm-finetuning/runs/compare?viewId=standard-view&detailsTab=charts&shortId=LLMFIN-61&dash=charts&type=run&compare=KwBgNMYEzUA) In purple, we can see the loss for the gpt2-large model. As we trained for fewer epochs, we can see that we have a shorter curve, which nevertheless achieved a better loss.\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/app-screenshot/bar-buttons.svg)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button--test-tube.svg) See in the app  ](https://app.neptune.ai/pedro.gengo/llm-finetuning/runs/compare?viewId=standard-view&detailsTab=charts&shortId=LLMFIN-61&dash=charts&type=run&compare=KwBgNMYEzUA) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/icon-zoom.svg) Full screen preview \nComparison of the loss across different experiments. Purple: gpt2-large. Yellow: opt-125m. Red: gpt-medium. Gray: gpt2. \nThe loss function is not yet saturated, indicating that our models still have room for growth and could likely achieve higher levels of performance with additional training time.\nGo to the Models page and click on the model you created. You will see an overview of all the versions you trained and uploaded. You can also see the metrics reported and the model name.\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/note/list-arrow.svg)\n[Here\u2019s a link to my Neptune project](https://app.neptune.ai/pedro.gengo/llm-finetuning/models?shortId=LLMFIN-QAPTBR&type=model)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/app-screenshot/bar-buttons.svg)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button--test-tube.svg) See in the app  ](https://app.neptune.ai/pedro.gengo/llm-finetuning/models?shortId=LLMFIN-QAPTBR&type=model) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/icon-zoom.svg) Full screen preview \nModel versions saved on Neptune\u2019s model registry. Listed are the model version\u2019s ID, the time of creation, the owner, and the metrics stored with the model version. \nYou\u2019ll notice that none of the model versions have been assigned to a \u201cStage\u201d yet. Neptune allows you to [assign models to different stages](https://docs.neptune.ai/model_registry/managing_stage/), namely \u201cStaging,\u201d \u201cProduction,\u201d and \u201cArchived.\u201d\nWhile we can promote a model through the UI, we\u2019ll return to our code and automatically identify the best model. For this, we first fetch all model versions\u2019 metadata, sort by the exact match and f1 scores, and promote the best model according to these metrics to production:\n```\nimport neptune\n\nmodel = neptune.init_model(with_id=\"LLMFIN-QAPTBR\")\n\nmodel_versions_df = model.fetch_model_versions_table().to_pandas()\n\ndf_sorted = model_versions_df.sort_values([\"model/exact-match\", \"model/f1\"], ascending=False)\nmodel_version = df_sorted.iloc[0][\"sys/id\"]\nmodel_name = df_sorted.iloc[0][\"model/model-name\"]\n\nmodel_version = neptune.init_model_version(\n  with_id=model_version,\n)\nmodel_version.change_stage(\"production\")\n\n```\n\nCopy the JavaScript snippet!\nAfter executing this, we can see, as expected, that gpt2-large (our largest model) was the best model and was chosen to go to production:\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/app-screenshot/bar-buttons.svg)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button--test-tube.svg) See in the app  ](https://app.neptune.ai/pedro.gengo/llm-finetuning/models?shortId=LLMFIN-QAPTBR&type=model) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/icon-zoom.svg) Full screen preview \nThe gpt2-large model achieved the best metric scores and was promoted to the \u201cProduction\u201d stage. \nOnce more, we\u2019ll return to our code and finally use our best model to answer questions in Brazilian Portuguese:\n```\nimport neptune\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = neptune.init_model(with_id=\"LLMFIN-QAPTBR\")\nmodel_versions_df = model.fetch_model_versions_table().to_pandas()\n\ndf_prod_model = model_versions_df[model_versions_df[\"sys/stage\"] == \"production\"]\nmodel_version = df_prod_model.iloc[0][\"sys/id\"]\nmodel_name = df_prod_model.iloc[0][\"model/model-name\"]\n\nmodel_version = neptune.init_model_version(\n  with_id=model_version,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                      device_map = \"auto\",\n                      load_in_8bit=True,\n                      trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmodel_version[\"model/artifacts\"].download()\n\n!unzip artifacts\n\nmodel = PeftModel.from_pretrained(model, \"/content/lora-faquad/checkpoint-260\", local_files_only=True)\n```\n\nCopy the JavaScript snippet!\n[![LLM inference before and after fine-tuning](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-6.png?resize=1920%2C4480&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-6.png?ssl=1)Model inference before and after fine-tuning. The text shows a small piece of information about the rules to pass a course and asks: \u201cWhat does passing the course depend on?\u201d Before fine-tuning, the model only repeats the question. After fine-tuning, the model can answer the question correctly.\nLet\u2019s compare the prediction without fine-tuning and the prediction after fine-tuning. As demonstrated, before fine-tuning, the model didn\u2019t know how to handle Brazilian Portuguese at all and answered by repeating some part of the input or returning special characters like \u201c##########.\u201d However, after fine-tuning, it becomes evident that the model handles the input much better, answering the question correctly (it only added a \u201c?\u201d at the end, but the rest is exactly the answer we\u2019d expect).\nWe can also look at the metrics before and after fine-tuning and verify how much it improved:\nExact Match  |  F1   \n---|---  \n**Before fine-tuning** |  0  |  0.007  \n**After fine-tuning** |  0.143 |  0.157  \nGiven the metrics and the prediction example, we can conclude that the fine-tuning was in the right direction, even though we have room for improvement.\n![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n##  **Editor\u2019s note**\nDo you feel like experimenting with neptune.ai?\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Create a free account](https://neptune.ai/register) right away and give it a go\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Try it out first](https://docs.neptune.ai/usage/quickstart/) and learn how it works (zero setup, no registration)\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[See the docs](https://docs.neptune.ai/) or watch a short [product demo (2 min)](https://neptune.ai/walkthrough)\n\n\n## How to improve the solution?\nIn this article, we\u2019ve detailed a simple and efficient technique for fine-tuning LLMs.\nOf course, we still have some way to go to achieve good performance and consistency. There are various additional, more advanced strategies you can employ, such as:\n  * **More Data:** Add more high-quality, diverse, and relevant data to the training set to improve the model\u2019s learning and generalization.\n  * **Tokenizer Merging:** Combine tokenizers for better input processing, especially for multilingual models.\n  * **Model-Weight Tuning:** Directly adjust the pre-trained model weights to fit the new data better, which can be more effective than tuning adapter weights.\n  * **Reinforcement Learning with Human Feedback:** Employ human raters to provide feedback on the model\u2019s outputs, which is used to fine-tune the model through reinforcement learning, aligning it more closely with complex objectives.\n  * **More Training Steps:** Increasing the number of training steps can further enhance the model\u2019s understanding and adaptation to the data.\n\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  How to Improve ML Model Performance [Best Practices From Ex-Amazon AI Researcher]  Read also  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/improving-ml-model-performance)\n## Conclusion\nWe engaged in four distinct trials throughout our experiments, each employing a different model. We\u2019ve used quantization and LoRA to reduce the memory and compute resource requirements. Throughout the training and evaluation, we\u2019ve used Neptune to log metrics and store and manage the different model versions.\nI hope this article inspired you to explore the possibilities of LLMs further. In particular, if you\u2019re a native speaker of a language that\u2019s not English, I\u2019d like to encourage you to explore fine-tuning LLMs in your native tongue.\n##  Was the article useful? \n![yes](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--yes.svg) Yes  ![no](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--no.svg) No \n![](https://neptune.ai/wp-content/themes/neptune/img/icon-bulb.svg) Suggest changes \nYour email Your message (optional)\nThis site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply. \nI am familiar with the [Privacy Policy](https://neptune.staginglab.eu/privacy-policy)*\nSubmit\n\u0394\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/i-box/header-icon.svg)\n### **More about** LLM Fine-Tuning and Model Selection Using Neptune and Transformers \n####  Check out our  **product resources** and  **related articles** below: \n[ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  LLMOps: What It Is, Why It Matters, and How to Implement It  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/llmops) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  ML Experiment Tracking: What It Is, Why It Matters, and How to Implement It  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/ml-experiment-tracking) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-bulb.svg) Product resource  How Veo Eliminated Work Loss With Neptune  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/customers/veo) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  Building LLM Applications With Vector Databases  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/building-llm-applications-with-vector-databases)\n###  Explore more content topics: \n[ Computer Vision ](https://neptune.ai/blog/category/computer-vision) [ General ](https://neptune.ai/blog/category/general) [ LLMOps ](https://neptune.ai/blog/category/llmops) [ ML Model Development ](https://neptune.ai/blog/category/machine-learning-model-development) [ ML Tools ](https://neptune.ai/blog/category/machine-learning-tools) [ MLOps ](https://neptune.ai/blog/category/mlops) [ Natural Language Processing ](https://neptune.ai/blog/category/natural-language-processing) [ Paper Reflections ](https://neptune.ai/blog/category/paper-reflections) [ Product Updates ](https://neptune.ai/blog/category/product-updates) [ Reinforcement Learning ](https://neptune.ai/blog/category/reinforcement-learning) [ Tabular Data ](https://neptune.ai/blog/category/tabular-data) [ Time Series ](https://neptune.ai/blog/category/time-series-forecasting)\nAbout neptune.ai ![chevron](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1920 1080'%3E%3C/svg%3E) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-play.svg)\nNeptune is the experiment tracker purpose-built for foundation model training. \nIt lets you monitor and visualize thousands of per-layer metrics\u2014losses, gradients, and activations\u2014at any scale. Drill down into logs and debug training issues fast. Keep your model training stable while reducing wasted GPU cycles. \n[ Play with a live project  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://demo.neptune.ai/) [ See Docs  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://docs.neptune.ai/)\nTable of contents\n![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-small.svg)\n  1. Large language models\n  2. Hands-on: fine-tuning and selecting an LLM for Brazilian Portuguese\n     * Setting up\n     * Loading and pre-processing the dataset\n     * Loading and preparing the models\n     * Fine-tuning the models\n     * Evaluating the fine-tuned LLMs\n     * Storing the models and evaluation results\n     * Model selection\n  3. How to improve the solution?\n  4. Conclusion\n\n\n![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-cancel.svg) ![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-more.svg)\nCheck also: [Deploying Conversational AI Products to Production With Jason Flaks](https://neptune.ai/blog/deploying-conversational-ai-products-with-jason-flaks)\n##  **Monitor your model training at scale**\nJoin 60,000+ researchers and practitioners who use Neptune to debug training failures, spot anomalies, and compare experiments.\n[ Try for free  ](https://app.neptune.ai/register) [ Get early access to Neptune Scale  ](https://neptune.ai/early-access)\n[ Take an interactive product tour  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)\n![](https://neptune.ai/wp-content/themes/neptune/img/footer-post-illustration.svg)\n#### Newsletter\nTop articles, case studies, events (and more) in your inbox every month.\nGet Newsletter\n  * [Product](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Walkthrough](https://neptune.ai/walkthrough)\n    * [Documentation](https://docs.neptune.ai/)\n    * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [Pricing](https://neptune.ai/pricing)\n    * [Service status](https://status.neptune.ai/)\n\n\n  * [Solutions](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [AI Researcher](https://neptune.ai/product/ai-researcher)\n    * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n    * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n    * [Enterprise](https://neptune.ai/product/enterprise)\n    * [Academic Research](https://neptune.ai/research)\n\n\n  * [Compare](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Neptune vs Weights & Biases](https://neptune.ai/vs/wandb)\n    * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n    * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n    * [Other comparisons](https://neptune.ai/vs)\n    * [ML experiment tracking tools](https://neptune.ai/blog/best-ml-experiment-tracking-tools)\n\n\n  * [Community](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Blog](https://neptune.ai/blog)\n    * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n    * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n    * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n    * [How to Build an Experiment Tracker](https://neptune.ai/blog/build-experiment-tracking-tool)\n\n\n  * [Company](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Customers](https://neptune.ai/customers)\n    * [About us](https://neptune.ai/about-us)\n    * [Careers](https://neptune.ai/jobs)\n    * [Security portal and SOC 2](https://security.neptune.ai/)\n    * [Contact us](https://neptune.ai/contact-us)\n\n\n[ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-linked-in.svg) ](https://www.linkedin.com/company/neptuneai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-twitter.svg) ](https://twitter.com/neptune_ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-github.svg) ](https://github.com/neptune-ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-facebook.svg) ](https://www.facebook.com/neptuneAI) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-youtube.svg) ](https://www.youtube.com/channel/UCvOJU-ubyUqxGSDRN7xK4Ng) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-spotify.svg) ](https://open.spotify.com/show/4kGi82i4wTYgHbWmVMri5x) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-apple-podcast.svg) ](https://podcasts.apple.com/us/podcast/mlops-live/id1634179447)\n[ Terms of Service ](https://neptune.ai/terms-of-service) [ Privacy Policy ](https://neptune.ai/privacy-policy) [ SLA ](https://neptune.ai/service-level-agreement)\nCopyright \u00a9 2025 Neptune Labs. All rights reserved.\n![](https://neptune.ai/wp-content/themes/neptune/img/footer-stars--414w.png) ![](https://pixel.wp.com/g.gif?v=ext&blog=211928962&post=34430&tz=0&srv=neptune.ai&hp=atomic&ac=2&amp=0&j=1%3A14.5-a.9&host=neptune.ai&ref=&fcp=1916&rand=0.08928365801976357)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe article discusses fine-tuning Large Language Models (LLMs) for Brazilian Portuguese using Neptune. It covers model selection, quantization, and LoRA techniques to optimize resource usage. Evaluation metrics include exact match and F1 scores, showing improved performance post-fine-tuning.\n```"
  },
  {
    "instruction": "![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61f958575ff62d320d46701e_closeicon.png)\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\nKindly note that during the maintenance window, app.hopsworks.ai will not be accessible.\n5\n[View the Changes](https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes)\n![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61f958575ff62d320d46701e_closeicon.png)\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\nKindly note that during the maintenance window, app.hopsworks.ai will not be accessible.\n5\n[View the Changes](https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes)\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\n[Contact](https://www.hopsworks.ai/contact/main)[Login](https://app.hopsworks.ai)[![Github Mark](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6418216a570b0da3d471661a_icons8-slack-new.svg)](https://join.slack.com/t/public-hopsworks/shared_invite/zt-1uf21vitz-rhHKNdIf8GEiOf1EJ6Wzsw)[![Github Mark](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/62261cde4669f63d3880938d_github.svg)](https://github.com/logicalclocks/hopsworks)[![linkedin logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/company/hopsworks/)[![Twitter icon](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/66a346ea27ec6d7c0e354747_icons8-twitter%20\\(1\\).svg)](https://twitter.com/hopsworks)\n[![Untitled UI logotext](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6202a13e7cafec5553703f6b_logo.svg)![Logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/666c3cc1cfc4741e6b2d9fe6_untitled-ui-logo.png)](https://www.hopsworks.ai/)\nProduct\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/5fad49f715203ed9d66fc1b0_Hops%20Icon%20Green.png)Hopsworks EnterpriseFull edition of Hopsworks, high levels of SLAs and support.](https://www.hopsworks.ai/try)[IntegrationsLeverage your existing data sources and tools.](https://www.hopsworks.ai/integrations)[ExamplesGet up and running on new features and techniques.](https://www.hopsworks.ai/hopsworks-examples)[FAQAll you need to know about Hopsworks.](https://www.hopsworks.ai/frequently-asked-questions)\n[Hopsworks On-PremisesManage everything securely within your data center.](https://www.hopsworks.ai/product-capabilities/feature-store-on-premises)[Performance & High AvailabilityHighest performance requirements in the industry.](https://www.hopsworks.ai/product-capabilities/operational-performance-and-high-availability)[Feature Engineering in PythonPython-first collaborative environment.](https://www.hopsworks.ai/product-capabilities/feature-engineering-in-python)[Other capabilitiesRead about our extended platform capabilities.](https://www.hopsworks.ai/product-capabilities)\nSolutions\nFor your Team\n[Technical Stakeholders](https://www.hopsworks.ai/solutions/teams)[Machine Learning Engineers](https://www.hopsworks.ai/solutions/teams?tab=ml-engineers)[Data Engineers](https://www.hopsworks.ai/solutions/teams?tab=data-engineers)[Data Scientists](https://www.hopsworks.ai/solutions/teams?tab=data-scientists)[DevOps](https://www.hopsworks.ai/solutions/teams?tab=devops)[Architects](https://www.hopsworks.ai/solutions/teams?tab=architects)[Non-Technical Stakeholders](https://www.hopsworks.ai/solutions/teams?tab=non-technical)[Project Managers ](https://www.hopsworks.ai/solutions/teams?tab=non-technical)\nFor your Industry\n[Defense and Law Enforcement](https://www.hopsworks.ai/solutions/industry/defense-and-law-enforcement)\n[FSI](https://www.hopsworks.ai/solutions/industry/fsi)\n[Online Retail & E-commerce](https://www.hopsworks.ai/solutions/industry/online-retail-and-e-commerce)\n[Public Sector](https://www.hopsworks.ai/solutions/industry/public-sector)\n[Research and Healthcare](https://www.hopsworks.ai/solutions/industry/research-and-healthcare)\n[iGaming](https://www.hopsworks.ai/solutions/industry/i-gaming)\n[All Solutions](https://www.hopsworks.ai/solutions/all)\nUse Cases\n[Generative AI](https://www.hopsworks.ai/use-case/fine-tuning-llms-rag-for-genai)[Real-time Fraud Detection](https://www.hopsworks.ai/use-case/realtime-fraud-detection)[Hopsworks Medical Copilot](https://www.hopscopilot.com/)\n[CustomersExplore how our customers leverage Hopsworks.](https://www.hopsworks.ai/customers)\n[Pricing](https://www.hopsworks.ai/pricing)[Blog](https://www.hopsworks.ai/blog)[Pricing](https://www.hopsworks.ai/pricing)[Blog](https://www.hopsworks.ai/blog)\nResources\n[MLOps DictionaryComprehensive terminology guide for ML solutions.](https://www.hopsworks.ai/mlops-dictionary)[DocumentationDetailed information to help you effectively utilize Hopsworks.](https://docs.hopsworks.ai/latest/)[Research PapersDiscover how our research is driving innovation.](https://www.hopsworks.ai/research-papers)[CommunityJoin our community and get all your questions answered. ](https://community.hopsworks.ai/)\n[EventsOnline & Offline sessions and workshops. ](https://www.hopsworks.ai/events)[AcademyEverything about ML Systems, and the Hopsworks platform.](https://www.hopsworks.ai/academy)[Feature Store ComparisonIn-depth comparisons of feature stores highlighting key features.](https://www.hopsworks.ai/product-comparison/sagemaker)[FAQ: EU AI ActA complete guide to The EU AI Act.](https://www.hopsworks.ai/faq-eu-ai-act)\nCompany\n[About usLearn more about our team. ](https://www.hopsworks.ai/about-us)[NewsThe latest industry news, updates and info.](https://www.hopsworks.ai/news)[Security & ComplianceRobust security and compliance with industry standards.](https://www.hopsworks.ai/security-compliance)\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/66a0b13c473a71304470c35a_oreilly_logo_mark_red.svg)Book](https://www.hopsworks.ai/lp/oreilly-book-building-ml-systems-with-a-feature-store)[Benchmarks](https://www.hopsworks.ai/index#performance)\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/63e4e90bd6c2ad05ecd89669_icons8-great-britain-96.png)EN](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/)[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/63e4e90b88b00c69a52f92cc_icons8-germany-96.png)DE](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/)\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/63e4e90bd6c2ad05ecd89669_icons8-great-britain-96.png)EN](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/)[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/63e4e90b88b00c69a52f92cc_icons8-germany-96.png)DE](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/)\n[![arrow back](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6183f249d69379869e0b3524_icons8-chevron-left-30.png)Back to Blog](https://www.hopsworks.ai/blog)\n[![arrow back](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6183f249d69379869e0b3524_icons8-chevron-left-30.png)Back to Blog](https://www.hopsworks.ai/blog)\nJim Dowling\n[![link to linkedin](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/in/jim-dowling-206a98/)\nCEO and Co-Founder\n**Let's keep in touch!**\n**Subscribe to our newsletter and receive the latest product updates, upcoming events, and industry news.**\n**More Blogs**\n[ Migrating from AWS to a European Cloud - How We Cut Costs by 62%](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)\n[The 10 Fallacies of MLOps](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)\n[Hopsworks AI Lakehouse: The Power of Integrated MLOps Components](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)\n[Unlocking the Power of AI in Government](https://www.hopsworks.ai/post/the-power-of-ai-in-government)\n[Optimizing AI Costs](https://www.hopsworks.ai/post/optimizing-ai-costs)\nArticle updated on\n# From MLOps to ML Systems with Feature/Training/Inference Pipelines\nThe Mental Map for MLOps to align your Data-ML-Product Teams\n[![link to github](https://cdn.prod.website-files.com/5e6f7cd3ee7f51d539a4da0b/605b3c459e87eff3298d0e25_github%20\\(1\\).svg)](https://github.com/logicalclocks/hopsworks)\n[![Share on Twitter](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61b0ae07358bb3d1224410c1_Twitter%20icon.svg)](https://twitter.com/share?url=https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)\n[![share on linkedin](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/shareArticle?mini=true&url=https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines&title=From MLOps to ML Systems with Feature/Training/Inference Pipelines - Hopsworks)\nSeptember 13, 2023\n25 min\nRead\nJim Dowling\n[Jim Dowling](https://www.linkedin.com/in/jim-dowling-206a98/)[![link to linkedin](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/in/jim-dowling-206a98/)\nCEO and Co-Founder\nHopsworks\n[Feature Store](https://www.hopsworks.ai/blog-categories/feature-stores)\n[Data Engineering](https://www.hopsworks.ai/blog-categories/data-engineering)\n## TL;DR\nMaps help us navigate the world, and communicate ideas, helping us get faster to our destination. Somewhere along the way, MLOps got lost in promoting \u201cwaterfall software architecture\u201d maps for ML Systems that include a kitchen sink of requirements. Existing approaches to MLOps prevent teams from following DevOps principles of starting with a small working system and iteratively improving it. In this article, we present a new mental map for ML Systems as three independent ML pipelines: feature pipelines, training pipelines, and inference pipelines that share a common storage layer for the ML artifacts they produce and consume (features, models). In contrast to existing MLOps architectures, we provide a unified architecture that describes both batch ML systems and real-time ML systems. This makes it easier for developers to move to/from batch and real-time systems, and provides clear interfaces between the ML pipelines, enabling easier collaboration between the data, ML, and product teams that work together to develop and operate ML systems. Compared to existing MLOps architectures, the feature/training/inference pipeline architecture helps you get faster to a minimal working ML system that can be iteratively improved, while following best practices for automated testing, versioning, and monitoring. There are now hundreds of ML systems that have been built by the community based on our architecture, showing that building and shipping ML systems is easier if you follow a mental map that starts with building pipelines rather than starting by building ML infrastructure.\n[Introduction](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/#introduction)  \n---  \n[1 - Pre-MLOps - Move fast and ship it](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/#pre-mlops)  \n[2 - The Kitchen-Sink Mental Maps for MLOps](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/#kitchen-sink)  \n[3 - Rethinking ML Systems as ML Pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/#rethinking-ml)  \n[ 4 - Unified Architecture for ML Systems as Feature/Training/Inference Pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/#unified-architecture)  \n[a. Feature Pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/#feature-pipelines)  \n[b. Training Pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/#training-pipelines)  \n[c. Inference Pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/#inference-pipelines)  \n[5 - What are the fundamental principles of MLOps?](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/#fundamental-principles)  \n[6 - Example ML Systems built with FTI Pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/#example-ml)  \n[7 - Summary](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/#summary)  \n[a. Resources](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/#resources)  \n\u200d\n## Introduction\n> \u201cIt's impressive how far you can go with contemporary tools like @modal_labs, @huggingface, and @hopsworks! In 2017, having a shared pipeline for training and prediction data that updated automatically and made models available as a UI and an API was a groundbreaking stack at Uber. Now, it's a standard part of a well-done student project.\u201d [Charles Frye](https://twitter.com/charles_irl/status/1619047790998605825), [Full Stack Deep Learning course leader](https://fullstackdeeplearning.com/).\nIn a course I gave at KTH in 2022/23, [students developed a full ML system in only 2 weeks](https://id2223kth.github.io/assignments/project/ServerlessMLProjectsID22232023.html) that solved a prediction problem for a novel non-static data source of their choice. As Charles suggests in the above quote, leveraging ML infrastructure makes it easier to build [ML systems](https://www.hopsworks.ai/dictionary/ml-systems). You can write a Python program that scrapes data from the Internet and, with a few annotations, runs on a daily schedule with Modal. The program can write the [features](https://www.hopsworks.ai/dictionary/feature) it computes as DataFrames to Hopsworks[ Feature Store](https://www.hopsworks.ai/dictionary/feature-store). From there, a notebook can be used to train a model that is saved in Hopsworks (or any [model registry](https://www.hopsworks.ai/dictionary/model-registry)). And, finally, a third Python program uses the trained model to make predictions with new [inference data](https://www.hopsworks.ai/dictionary/inference-data) (for example, the data scraped today) read from the Feature Store, and displays the predictions in a nice UI or Dashboard (e.g., written in the Streamlit or Taipy). Some examples of prediction problems were predicting air quality, water levels, snow depth, football scores, electricity demand, and sentiment for posts.\n[![ML Pipelines make up a ML System](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4def_6501bf916e9d382ecda67424_figure%25201_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4def_6501bf916e9d382ecda67424_figure%25201_lightbox.png)\n**_Figure 1:_**_Feature Pipelines, Training Pipelines, Inference Pipelines are the independent ML Pipelines that together make up a ML System._\nBut in my course analysis, the conclusion I drew was that the key reason why students could go from zero to a working [ML](https://www.hopsworks.ai/dictionary/ml) system in less than 2 weeks was not just the new frameworks. It was, more importantly, the clear mental map of what they needed to do (see Figure 1): \n  1. build a [feature pipeline](https://www.hopsworks.ai/dictionary/feature-pipeline) to continually create features from your novel data source, save those features as a DataFrame to Hopsworks Feature Store;\n  2. write a training pipeline that reads [training data](https://www.hopsworks.ai/dictionary/training-data) from the Feature Store, trains your model and saves the trained model in the model registry,\n  3. write a [batch inference pipeline](https://www.hopsworks.ai/dictionary/batch-inference-pipeline) or [online inference pipeline](https://www.hopsworks.ai/dictionary/online-inference-pipeline) that downloads the trained model, then takes new feature data, either from the feature store or computes it from user requests, and makes predictions consumed by the ML-enabled product (often a simple UI written in Python using Streamlit or Gradio).\n\n\nAfter the students have built their first MVP (Minimum Viable Product), they could add automated unit tests for features, data validation tests, and [versioning](https://www.hopsworks.ai/dictionary/versioning-ml-artifacts) for their features and models. That is, they could easily follow best practices for [MLOps](https://www.hopsworks.ai/dictionary/mlops). \n[![ML Pipelines](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4de2_6502acbf8d94b5a7916bbbf8_figure%25202_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4de2_6502acbf8d94b5a7916bbbf8_figure%25202_lightbox.png)\n** _Figure 2:_**_Feature Pipelines, Training Pipelines, and Inference Pipelines are all examples of ML Pipelines. Note ML Pipelines now describes an abstract grouping, not a concrete pipeline._\nHowever, even with recent advances in MLOps tooling, many teams are failing in getting models to production. According to Gartner, between 2020 and 2022, companies only increased the percentage of models that make it from prototype to production from 53% to 54%. Tooling will only get you so far, if you don\u2019t have good processes for how to get there. As such, this starts with a brief, idiosyncratic historical overview of ML systems and MLOps, a critique of current \u201cbest practices\u201d, and then we present our FTI (feature/training/inference) pipeline architecture for ML systems and its advantages over current approaches.\n[ ![AI Lakehouse Webinar](https://no-cache.hubspot.com/cta/default/5524414/interactive-182137959545.png) ](https://cta-service-cms2.hubspot.com/web-interactives/public/v1/track/redirect?encryptedPayload=AVxigLJzh%2B7g8NDZfH7LupYVnOo1G9isIIGe7M47A8c2NwN58e7zb%2FH475FTiok5HVC%2Bx4ZYT3XVFE627C7rDF8tZhKIjhPkVoasm3xQFrhi4vPKPTyFvKa5K6iNI3JatEaz6DJSaEwnnMJ9zQugPMd9lhBra9VsVuXg1a9%2BQK55d6E%3D&webInteractiveContentId=182137959545&portalId=5524414)\n## Pre-MLOps - Move fast and ship it\nIn the early days of ML Systems, it quickly became common knowledge that building ML systems was much more than just training a model.\n[![Hidden Technical Debt in Machine Learning Systems](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4de9_6502ad4eb9b805fbe60281f4_figure%25203_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4de9_6502ad4eb9b805fbe60281f4_figure%25203_lightbox.png)\n**_Figure 3:_**_This diagram from 2015 is from a canonical paper by Google on_[ _Hidden Technical Debt in Machine Learning Systems_](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf) _. [_[_Image Source_](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning) _]_\nThe diagram in Figure 3 spread fast and wide and the message was clear to Data Scientists - building ML systems is hard and much more than just training a model.\nIn the era before MLOps practices merged, the first generation of ML systems [many different architecture patterns were proposed](https://www.oreilly.com/library/view/machine-learning-design/9781098115777/) to help build batch and real-time ML systems. \nBroadly speaking, there are two types of ML systems: batch ML systems and real-time ML systems. An example of a batch ML system would be [Spotify weekly](https://hpac.cs.umu.se/teaching/sem-mus-16/presentations/Pruefer.pdf). It produces predictions for every user once per week on what songs to recommend to each user. The results are stored in a key-value store, and when you login to Spotify, it downloads the predictions to your client and shows you the recommendations. An example of a real-time ML system would be TikTok. Every click or swipe you make is used to [compute features in near real-time about your user history and context](https://arxiv.org/pdf/2209.07663.pdf) (e.g., what\u2019s trending) that are, in turn, used to personalize recommendations for upcoming videos.\n[![A monolithic batch ML system](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e0a_6502ad7d387cfe977467717b_figure%25204_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e0a_6502ad7d387cfe977467717b_figure%25204_lightbox.png)\n**_Figure 4:_**_A monolithic batch ML system that can run in either (1) training mode or (2) inference mode._\nIn figure 4, you can see a quick-and-dirty batch ML system. This anti-pattern is for a production batch ML system that runs on a schedule to make predictions that are consumed by some prediction consumer, such as a dashboard or an operational system. The pattern solves one problem - it ensures consistent features for training and inference by including training and inference in the same program. You run the program with a boolean flag that decides whether it runs in TRAIN or INFERENCE mode. However, features created here are not easily reused in other models and there is very little modularity - if data volumes increase beyond what your Python program can process, you can\u2019t easily switch to PySpark.\n[![ A real-time ML system](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e07_6502adc6d9992658b4fa84b7_figure%25205_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e07_6502adc6d9992658b4fa84b7_figure%25205_lightbox.png)\n**_Figure 5:_**_A real-time ML system requires separate offline training and online inference pipelines._\nIn figure 5, you can see an operational ML system that receives requests from clients and responds with predictions in real-time. This is a very different architecture from our previous batch ML system, which makes it challenging for developers to move from batch to real-time systems. There are also new challenges here. Now, you need two separate systems - an offline [training pipeline](https://www.hopsworks.ai/dictionary/training-pipeline), and an online model serving service. You can no longer ensure consistent features between training and serving by having a single monolithic program. One solution is to version the feature creation source code and ensure both training and serving use the same version. In this architecture, the features in the online [inference pipeline](https://www.hopsworks.ai/dictionary/feature-pipeline) are computed using data provided by the request - the models have no access to history or context, all state used to create features should come in the request. An example of such a system would be a[ LLM](https://www.hopsworks.ai/dictionary/llms-large-language-models) chatbot. The user provides a query and the LLM provides a response, with the online inference pipeline tokenizing the input text.\n[![ML systems also require history and context](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e04_6502adf3ca2eceb7be5b949e_figure%25206_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e04_6502adf3ca2eceb7be5b949e_figure%25206_lightbox.png)\n**_Figure 6_** _: Many real-time ML systems also require history and context, and the feature store provides them as precomputed features to online models. This now requires three separate pipelines - feature creation,_[_model training_](http://www.hopsworks.ai/dictionary/model-training) _, and online_[ _model inference_](http://www.hopsworks.ai/dictionary/model-inference) _._\nIn our first real-time ML system, the online inference pipeline was stateless - it had no access to historical data or context data. Figure 6 shows the architecture of a real-time ML System with the feature store providing history and context, enabling richer [feature vectors](https://www.hopsworks.ai/dictionary/feature-vector) for models. The feature store provides low-latency access to pre-computed features (historical and contextual information) by the online inference pipeline looking features up with [entity](http://www.hopsworks.ai/dictionary/entity) IDs (userID, sessionID, productID, etc) provided by the client at request-time.\nAt this point, we have introduced 3 different architectures for building production ML systems. The challenge for ML platform teams has been how to easily communicate these architectural patterns to the data/ML/ops/product teams who collaborate to build production ML systems. \n## The Kitchen-Sink Mental Maps for MLOps\nIn 2020, the term MLOps came to be adopted as the set of patterns and processes that ML platforms teams should use to productionize ML systems. MLOps was inspired by the DevOps movement which brought software engineers closer to production by ensuring that they automate and test the deployment of their software in production environments. Wouldn\u2019t it be great if we could do the same for ML systems? However, ML systems are more complex than classical software products - they have to manage both the code used to build them and the data used for training and inference. Our nascent MLOps Community badly needed some guidance on best practices and processes for teams - we needed a map to build ML systems.\nThere is a long history of bad maps leading people astray.\n[![Mercator Projection Maps](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e2f_6502ae2467753cc1a6a3370c_figure%25207_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e2f_6502ae2467753cc1a6a3370c_figure%25207_lightbox.png)\n**_Figure 7:_**_Mercator Projection Maps make Greenland look much bigger than it is. Mercator Projection Maps suck._\nPersonally, since a very young age, I have had an unreasonable dislike for Mercator Projection maps (see figure 7).\nThe first MLOps maps for building ML systems have similar deficiencies to Mercator Maps - they try to project a view of ML systems that doesn\u2019t reflect the reality of building a ML system. The maps are a kitchen sink of requirements collected by consultants from all the different stakeholders and thrown unthinkingly into one huge diagram.\n[![Google\u2019s MLOps Map](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e22_6502ae5d69553d37d6d0832e_figure%25208_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e22_6502ae5d69553d37d6d0832e_figure%25208_lightbox.png)\n**_Figure 8:_**_Google\u2019s MLOps Map to help you build ML Systems [_[_Source_](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning) _]_\nAs you can, Google started by asking the operations teams - who said they needed separate dev and prod environments. The data teams wanted clean data. The ML teams want [hyperparameter tuning](https://www.hopsworks.ai/dictionary/hyperparameter-tuning) and models to be validated. The product teams got forgotten in this MLOps map.\n[![Databricks MLOps map ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e13_6502ae84b1e9d013a8dae678_figure%25209_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e13_6502ae84b1e9d013a8dae678_figure%25209_lightbox.png)\n**_Figure 9_** _: Databricks MLOps map to build ML Systems [_[_Source_](https://www.databricks.com/resources/ebook/the-big-book-of-mlops) _]_\nDatabricks, however, were not to be outdone. Google forgot a staging environment! But where is the mythical [ML pipeline](https://www.hopsworks.ai/dictionary/ml-pipeline) here? There are no names for the pipelines, no easy way to build your first ML system in a couple of weeks. The barrier to entry is too high - only a very few people have the software engineering, operations, and data science knowledge to navigate these maps. For other mortals - Data Scientists, Data Engineers, ML Engineers - the maps might as well be a riddle for finding buried treasure.\n[![ML Systems are fed new data with pipelines](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e16_6502aeb3d68f1808344bea82_figure%252010_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e16_6502aeb3d68f1808344bea82_figure%252010_lightbox.png)\n**_Figure 10:_**_ML Systems are fed new data with pipelines enabling them to continually drive business value._\nLet\u2019s take a step back and examine the problem of making ML systems from first principles. We will skip static datasets where you only make one-off predictions on a static dataset - this is not a ML System. \nFirstly, a ML System uses a trained ML model to make predictions on new data to solve a \u201cprediction problem\u201d of interest. That new data and the historical data for training is fed by pipelines. Pipelines continually feed data to models to enable ML Systems to make predictions on new data, and to collect training data. Pipelines also enable automation - to retrain models, to monitor [model performance](http://www.hopsworks.ai/dictionary/model-performance), to log data for debugging. \nBatch pipelines enable both batch ML Systems, such as dashboards, and operational ML systems, such as Spotify weekly, where recommendations for songs are updated once per week and consumed by the user when they login to Spotify. [Streaming feature pipelines](https://www.hopsworks.ai/dictionary/streaming-feature-pipeline) and features computed at request-time ([on-demand features](https://www.hopsworks.ai/dictionary/on-demand-features)) enable interactive ML systems that are updated in real-time, enabling systems such as personalized recommendations or search.\n[ ![hops_tutorials_cta](https://no-cache.hubspot.com/cta/default/5524414/interactive-176354145715.png) ](https://cta-service-cms2.hubspot.com/web-interactives/public/v1/track/redirect?encryptedPayload=AVxigLJi8P%2FSvIxXfqydx54LsVD0gaHJ5E4GvTlLecprN2%2By5AHJncnOg8Uc9akS%2FQ2jcuL4gAqOPf3qmWUownMShKbykjOZgI4RvECKxgBhRbCdBWTA7Bs7NFxK9MZREIlQ5vYALgbNN5GeAuL%2F4XdRJCqCSuxG6yU%2FSrJgDO%2FNHl1u065miOv1UfyKOIj22RFXRyZ4OaWkg92EZ8aGKbgtoEqhtWCk46D2&webInteractiveContentId=176354145715&portalId=5524414)\n## Unified Architecture for ML Systems as Feature/Training/Inference Pipelines\nThere is, however, an easier mental map that can help you build ML systems. This architectural pattern has been used to build hundreds of ML systems by ordinary developers ([here](https://id2223kth.github.io/assignments/project/ServerlessMLProjectsID22232023.html), [here](https://www.serverless-ml.org/sml-coursehttps://www.serverless-ml.org/sml-course), [here](https://github.com/iusztinpaul/energy-forecasting), and [here](https://realworldmachinelearning.carrd.co/)). The pattern is as follows: a ML system consists of three independently developed and operated ML pipelines: \n  * a feature pipeline that takes as input raw data that it transforms into features (and labels)\n  * a training pipeline that takes as input features (and labels) and outputs a trained model, and\n  * an inference pipeline that takes new feature data and a trained model and makes predictions.\n\n\nIn this FTI (feature, training, inference) architecture, there is no single ML pipeline. The confusion about what the ML pipeline does (does it feature engineer and train models or also do inference or just one of those?) disappears. The FTI map is the same for both batch ML systems and real-time ML systems.\n[![The Feature/Training/Inference \\(FTI\\) pipelines ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e0d_6502aeee69553d37d6d1197a_figure%252011_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e0d_6502aeee69553d37d6d1197a_figure%252011_lightbox.png)\n**_Figure 11:_**_The Feature/Training/Inference (FTI) pipelines Mental Map for building ML Systems_\nThe feature pipeline can be a batch program or a streaming program. The training pipeline can output anything from a simple XGBoost model to a fine-tuned large-language model (LLM), trained on many GPUs. Finally, the inference pipeline can be a batch program that produces a batch of predictions to an online service that takes requests from clients and returns predictions in real-time.\n[![Best language/framework for your ML pipelines](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4dfa_6502af32eb0ca70960db6e14_figure%252012_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4dfa_6502af32eb0ca70960db6e14_figure%252012_lightbox.png)\n**_Figure 12:_**_Choose the best language/framework for your ML pipelines._\nOne major advantage of **FTI pipelines is it is an open architecture**. You can use Python, Java or SQL. If you need to do [feature engineering](https://www.hopsworks.ai/dictionary/feature-engineering) on large volumes of data, you can use Spark or DBT or Beam. Training will typically be in Python using some ML framework, and batch inference could be in Python or Spark, depending on your data volumes. Online inference pipelines are, however, nearly always in Python as models are typically training with Python.\n[![Best framework for your feature pipeline based on your requirements.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e19_6502af602cbbd8ba80d46187_figure%252013_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e19_6502af602cbbd8ba80d46187_figure%252013_lightbox.png)\n**_Figure 13:_**_Pick the best framework for your feature pipeline based on your requirements. If you have small data to process, Pandas may be fine, but when data volumes exceed what a single node can process, you may move to PySpark. If you need \u201cfresh\u201d features, use a stream processing engine._\nIn figure 13, we can see the value of pluggable feature pipelines. For a given feature ingestion problem, you can easily pick the best framework, depending on the size of the data that will be ingested in a pipeline run and the freshness requirements for the feature data created - use streaming or on-demand features if you need very fresh (near real-time) feature data.\n[![Best orchestrator for your ML pipeline/service](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e25_6502afafca2eceb7be5d5696_figure%252014_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e25_6502afafca2eceb7be5d5696_figure%252014_lightbox.png)\n**_Figure 14:_**_Choose the best orchestrator for your ML pipeline/service._\nThe FTI pipelines are also modular and there is a clear interface between the different stages. Each FTI pipeline can be operated independently. Compared to the [monolithic ML pipeline](https://www.hopsworks.ai/dictionary/monolithic-ml-pipeline), different teams can now be responsible for developing and operating each pipeline. The impact of this is that for [orchestration](https://www.hopsworks.ai/dictionary/orchestration), for example, one team could use one orchestrator for a feature pipeline and a different team could use a different orchestrator for the batch inference pipeline. Alternatively, you could use the same orchestrator for the three different FTI pipelines for a batch ML system. Some examples of orchestrators that can be used in ML systems include general-purpose, feature-rich orchestrators, such as Airflow, or lightweight orchestrators, such as Modal, or managed orchestrators offered by [feature platform](https://www.hopsworks.ai/dictionary/feature-platform)s. \nSome of our FTI pipelines, however, will not need orchestration. Training pipelines can be run on-demand, when a new model is needed. Streaming feature pipelines and online inference pipelines run continuously as services, and do not require orchestration. Flink, Spark Streaming, and Beam are run as services on platforms such as Kubernetes, Databricks, or Hopsworks. Online inference pipelines are deployed with their model on model serving platforms, such as KServe (Hopsworks), Seldon, Sagemaker, and Ray. The main takeaway here is that the ML pipelines are modular with clear interfaces, enabling you to choose the best technology for running your FTI pipelines.\n[![ML pipelines with a Feature Store and Model Registry](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e1c_6502afddededc5d1aa93a0ec_figure%252015_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e1c_6502afddededc5d1aa93a0ec_figure%252015_lightbox.png)\n**_Figure 15:_**_Connect your ML pipelines with a Feature Store and Model Registry_\nFinally, we show how we connect our FTI pipelines together with a stateful layer to store the [ML artifacts](https://www.hopsworks.ai/dictionary/ml-artifacts) - features, training/test data, and models. Feature pipelines store their output, features, as DataFrames in the feature store. Incremental tables store each new update/append/delete as separate commits using a table format (we use Apache Hudi in Hopsworks). Training pipelines read point-in-time consistent snapshots of training data from Hopsworks to train models with and output the trained model to a model registry. You can include your favorite model registry here, but we are biased towards Hopsworks\u2019 model registry. Batch inference pipelines also read point-in-time consistent snapshots of inference data from the feature store, and produce predictions by applying the model to the inference data. Online inference pipelines compute on-demand features and read [precomputed features](http://www.hopsworks.ai/dictionary/precomputed-features) from the feature store to build feature vectors that are used to make predictions in response to requests by online applications/services.\n### **A Zoomable Map**\nAs with any good map, the FTI pipelines architecture hides complexity at a high level, but allows you to zoom in on any part of the map to discover important implementation details. Here, we enumerate some of the important questions on implementing the different pipelines that can be handled internally within the team implementing the pipeline. In contrast, the MLOps 1.0 mental maps leaked all concepts across all teams making it hard for teams to know where their responsibilities started and ended as part of a very complex bigger picture.\n### Feature Pipelines\nFeature pipelines read data from data sources, compute features and ingest them to the feature store. Some of the questions that need to be answered for any given feature pipeline include:\n  * Is the feature pipeline batch or streaming? \n  * Are feature ingestions incremental or full-load operations?\n  * What framework/language is used to implement the feature pipeline?\n  * Is there [data validation](https://www.hopsworks.ai/dictionary/data-validation-for-features) performed on the feature data before ingestion? \n  * What orchestrator is used to schedule the feature pipeline? \n  * If some features have already been computed by an [upstream](https://www.hopsworks.ai/dictionary/upstream) system (e.g., a data warehouse), how do you prevent duplicating that data, and only read those features when creating training or batch inference data?\n\n\n### Training Pipelines\nIn training pipelines some of the details that can be discovered on double-clicking are:\n  * What framework/language is used to implement the training pipeline?\n  * What experiment tracking platform is used?\n  * Is the training pipeline run on a schedule (if so, what orchestrator is used), or is it run on-demand (e.g., in response to performance degradation of a model)?\n  * Are GPUs needed for training? If yes, how are they allocated to training pipelines?\n  * What feature [encoding](https://www.hopsworks.ai/dictionary/encoding-for-features)/scaling is done on which features? (We typically store feature data unencoded in the feature store, so that it can be used for EDA (exploratory data analysis). Encoding/scaling is performed in a consistent manner training and inference pipelines). Examples of feature encoding techniques include scikit-learn pipelines or declarative transformations in [feature views](https://www.hopsworks.ai/dictionary/feature-view) (Hopsworks).\n  * What [model evaluation and validation](https://www.hopsworks.ai/dictionary/model-evaluation-model-validation) process is used?\n  * What model registry is used to store the trained models?\n\n\n### Inference Pipelines\nInference pipelines are as diverse as the applications they AI-enable. In inference pipelines, some of the details that can be discovered on double-clicking are:\n  * What is the prediction consumer - is it a dashboard, online application - and how does it consume predictions? \n  * Is it a batch or online inference pipeline?\n  * What type of feature encoding/scaling is done on which features? \n  * For batch inference pipelines: what framework/language is used? What orchestrator is used to run it on a schedule? What sink is used to consume the predictions produced?\n  * For online inference pipelines: what model serving server is used to host the deployed model? How is the online inference pipeline implemented - as a predictor class or with a separate transformer step? Are GPUs needed for inference? Is there a SLA (service-level agreements) for how long it takes to respond to prediction requests?\n\n\n## What are the fundamental principles of MLOps?\nThe existing mantra is that MLOps is about automating continuous integration (CI), continuous delivery (CD), and continuous training (CT) for ML systems. But that is too abstract for many developers. MLOps is really about continual development of ML-enabled products that evolve over time. The available input data (features) changes over time, the target you are trying to predict changes over time. You need to make changes to the source code, and you want to ensure that any changes you make do not break your ML system or degrade its performance. And you want to accelerate the time required to make those changes and test before those changes are automatically deployed to production.\nSo, from our perspective, a more pithy definition of MLOps that enables ML Systems to be safely evolved over time is that it requires, at a minimum, automated testing, versioning, and monitoring of ML artifacts. **MLOps is about automated testing, versioning, and monitoring of ML artifacts.**\n[![The testing pyramid for ML Artifacts](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4df3_6502b00c239901ef1c0446a5_figure%252016_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4df3_6502b00c239901ef1c0446a5_figure%252016_lightbox.png)\n**_Figure 16:_**_The testing pyramid for ML Artifacts_\nIn figure 16, we can see that more levels of testing are needed in ML systems than in traditional software systems. Small bugs in data or code can easily cause a ML model to make incorrect predictions. From a testing perspective, if web applications are propeller-driven airplanes, ML systems are jet-engines. It takes significant engineering effort to test and validate ML Systems to make them safe!\nAt a high level, we need to test both the source-code and data for ML Systems. The features created by feature pipelines can have their logic tested with unit tests and their input data checked with data validation tests (e.g., [Great Expectations](https://www.hopsworks.ai/post/data-validation-for-enterprise-ai-using-great-expectations-with-hopsworks)). The models need to be tested for performance, but also for a lack of bias against known groups of vulnerable users. Finally, at the top of the pyramid, ML-Systems need to test their performance with A/B tests before they can switch to use a new model.\n![Monitoring features for batch and real-time systems in Hopsworks.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e78_64f07b3dc4d764029d02c8e2_blog-gif.gif)\n**_Figure 17:_**_Monitoring features for batch and real-time systems in Hopsworks._\nWhen a ML system runs in production, you can also add [feature monitoring](https://www.hopsworks.ai/dictionary/feature-monitoring) and [model monitoring](https://www.hopsworks.ai/dictionary/model-monitoring) support to it to try and identify and correct problems in their performance. For example, monitoring can identify issues such as drift in [feature values](http://www.hopsworks.ai/dictionary/feature-value) or a changing prediction target for a ML model.\n[![Versioning of ML Artifacts](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e1f_6502b0ae202da9cf0ac98548_figure%252018_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66a3437f014c1e1ac78d4e1f_6502b0ae202da9cf0ac98548_figure%252018_lightbox.png)\n**_Figure 18:_**_Versioning of ML Artifacts enables easy upgrade and downgrade of models._\nFinally, we need to version ML artifacts so that the operators of ML systems can safely update and rollback versions of deployed models. System support for the push-button upgrade/downgrade of models is one of the holy grails of MLOps. But models need features to make predictions, so model versions are connected to feature versions and models and features need to be upgraded/downgraded synchronously. Luckily, you don\u2019t need a year in rotation as a Google SRE to easily upgrade/downgrade models - platform support for versioned ML artifacts should make this a straightforward ML system maintenance operation. \n## Example ML Systems built with FTI Pipelines\nHere is a sample of some of the open-source ML systems available built on the FTI architecture. They have been built mostly by practitioners and students.**Batch ML Systems**\n  * [Electricity Demand Prediction](https://github.com/iusztinpaul/energy-forecasting) (452 github stars)\n  * [NBA Game Prediction](https://github.com/cmunch1/nba-prediction) (152 github stars)\n  * [Premier league football score predictions](https://github.com/MatejFrnka/ScalableML-project) (101 github stars)\n  * [Churn prediction](https://github.com/logicalclocks/hopsworks-tutorials/tree/master/churn) (113 github stars)\n\n\n**Real-Time ML System**\n  * [Online Credit Card Fraud](https://github.com/logicalclocks/hopsworks-tutorials/tree/master/fraud_online) (113 github stars)\n  * [Crypto Price Prediction](https://github.com/Paulescu/bytewax-hopsworks-example) (65 github stars)\n  * [Loan application approval](https://github.com/logicalclocks/hopsworks-tutorials/tree/master/loan_approval) (113 github stars)\n\n\n## Summary\nThis blog post introduces a new mental map for MLOps, the FTI pipeline architecture. The architecture has enabled hundreds of developers to build maintainable ML Systems in a short period of time. In our experience, compared to MLOps 1.0, the architecture leads to reduced cognitive load when designing and describing a ML system. In enterprise settings, the architecture enables better communication across teams by providing clear interfaces between teams, leading to better collaboration and higher quality ML systems, faster. The architecture hides complexity at a high level, but you can easily drill down to the different feature/training/inference components to examine the details. Our hope is that the FTI pipeline architecture can help more teams work better together and get more models into production, quicker, accelerating the transformation of society by AI.\n### Resources\n  * The [serverless ML community](https://www.serverless-ml.org/) (with >2k [discord members](https://discord.gg/uRbSdXauk7) in August 2023) dedicated to building ML systems using the FTI pipeline architecture.\n  * There is a [serverless ML course](https://www.serverless-ml.org/sml-course), given by Jim Dowling and Hopsworks, that introduces the FTI pipeline architecture and how to build ML systems using free serverless technologies.\n\n\n## Recommended Reading\n  * [Modularity and Composability for AI Systems with AI Pipelines and Shared Storage](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)\n  * [The Taxonomy for Data Transformations in AI Systems](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)\n  * [The Journey from Star Schema to Snowflake Schema in the Feature Store](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store)\n  * [The Feature Store Makes Your Data Warehouse Easy to Use for AI](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai)\n\n\n## References\n![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/663b8c6a964c76be96237444_written%20by%20human_hops.png)![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/663b8ae5a0a33abc94baf561_written%20by%20ai_hops.png)\n### Interested for more?\n  * \ud83e\udd16 Register for free on [Hopsworks Serverless](https://app.hopsworks.ai/app?utm_source=blog&utm_medium=list&utm_id=backlink)\n  * \ud83c\udf10 Read about the open, disaggregated [AI Lakehouse stack](https://www.hopsworks.ai/post/the-ai-lakehouse)\n  * \ud83d\udcda Get your early copy: O'Reilly's ['**Building Machine Learning Systems'**](https://www.hopsworks.ai/lp/oreilly-book-building-ml-systems-with-a-feature-store)book\n  * \ud83d\udee0\ufe0f Explore all [Hopsworks Integrations](https://www.hopsworks.ai/integrations)\n  * \ud83e\udde9 Get started with [codes and examples](https://www.hopsworks.ai/hopsworks-examples)\n  * \u2696\ufe0f [Compare other Feature Stores](https://www.hopsworks.ai/product-comparison/sagemaker) with Hopsworks\n\n\n### More blogs\n[![Introducing the feature store which is a new data science tool for building and deploying better AI models in the gambling and casino business.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63ac0935acaa438f5b48db96_Betting%20Industry%20Blog.png)](https://www.hopsworks.ai/post/better-ai-models-in-the-betting-industry)[Feature StoreFebruary 20, 20206 minReadTowards better AI-models in the betting industry with a Feature StoreIntroducing the feature store which is a new data science tool for building and deploying better AI models in the gambling and casino business.](https://www.hopsworks.ai/post/better-ai-models-in-the-betting-industry)\nJim Dowling\n[![In this article, we will show the added value and how the feature store is an easy to use layer on top of your data warehouse or ai lakehouse.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/668bf0ab60826eb58e0d6af1_PR%20Blog%204_data%20warehouse.png)](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai)[Feature StoreJuly 10, 202424 minReadThe Feature Store Makes Your Data Warehouse Easy to Use for AIIn this article, we cover the added value of a feature store over a data warehouse when managing offline data for AI. ](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai)\nJim Dowling\n[![Explore the convergence of MLOps vs DevOps. Learn about their purposes, differences, areas of integration as well as discover best practices.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d355fb6ba1878a9afd9066_Mlops%20Devops%20Blog.png)](https://www.hopsworks.ai/post/mlops-vs-devops-best-practices-challenges-and-differences)[MLOpsJuly 11, 202317ReadMLOps vs. DevOps: Best Practices, Challenges and DifferencesExplore the convergence of MLOps and DevOps. Learn about their purposes, differences, and areas of integration and discover best practices, challenges, and their future potential. ](https://www.hopsworks.ai/post/mlops-vs-devops-best-practices-challenges-and-differences)\nPrithivee Ramalingam\n[![Untitled UI logotext](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/630e3413d3fafa0f79c52da2_hopsworks-logo%202022_white.svg)![Logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/666c3cc1cfc4741e6b2d9fe6_untitled-ui-logo.png)The AI Lakehouse](https://www.hopsworks.ai/)\nProduct\n[Hopsworks Enterprise](https://www.hopsworks.ai/try)[Capabilities](https://www.hopsworks.ai/product-capabilities)[Integrations](https://www.hopsworks.ai/integrations)[Examples](https://www.hopsworks.ai/hopsworks-examples)[Pricing](https://www.hopsworks.ai/pricing)[App Status](https://hopsworks.statuspage.io/)[FAQ](https://www.hopsworks.ai/frequently-asked-questions)\nSolutions\n[Industry & Team Solutions](https://www.hopsworks.ai/solutions/all)[Generative AI](https://www.hopsworks.ai/use-case/fine-tuning-llms-rag-for-genai)[Real-time Fraud Detection ](https://www.hopsworks.ai/use-case/realtime-fraud-detection)[Hopsworks Medical Copilot](https://www.hopscopilot.com/)[Customers](https://www.hopsworks.ai/customers)\nResources\n[Blog](https://www.hopsworks.ai/blog)[MLOps Dictionary](https://www.hopsworks.ai/mlops-dictionary)[Events](https://www.hopsworks.ai/events)[Documentation](https://docs.hopsworks.ai/latest/)[Academy](https://www.hopsworks.ai/academy)[Research Papers](https://www.hopsworks.ai/research-papers)[Feature Store Comparison](https://www.hopsworks.ai/product-comparison/sagemaker)[Community](https://community.hopsworks.ai/)[FAQ: EU AI Act](https://www.hopsworks.ai/faq-eu-ai-act)\nCompany\n[About Us](https://www.hopsworks.ai/about-us)[News](https://www.hopsworks.ai/news)[Security & Compliance](https://www.hopsworks.ai/security-compliance)[Contact Us](https://www.hopsworks.ai/contact/main)\nJoin our newsletter\n**Receive the latest product updates, upcoming events, and industry news.**\n\u00a9 Hopsworks 2025. All rights reserved. Various trademarks held by their respective owners.\n[](https://join.slack.com/t/public-hopsworks/shared_invite/zt-1uf21vitz-rhHKNdIf8GEiOf1EJ6Wzsw)[](https://github.com/logicalclocks/hopsworks)[](https://www.linkedin.com/company/hopsworks/)[](https://twitter.com/hopsworks)[](https://www.youtube.com/@hopsworks)\n[Privacy Policy](https://www.iubenda.com/privacy-policy/90800199 \"Privacy Policy \")\n[Cookie Policy](https://www.iubenda.com/privacy-policy/90800199/cookie-policy \"Cookie Policy \")\n[Terms and Conditions](https://www.iubenda.com/terms-and-conditions/90800199 \"Terms and Conditions \")\n",
    "answer": "```markdown\n## TL;DR\nThe article presents a new FTI (Feature/Training/Inference) pipeline architecture for MLOps, enhancing collaboration and efficiency in ML systems. It emphasizes modularity, automated testing, versioning, and monitoring, enabling faster deployment and iterative improvements in ML projects.\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/ggerganov/llama.cpp/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fggml-org%2Fllama.cpp)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fggml-org%2Fllama.cpp)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=ggml-org%2Fllama.cpp) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/ggerganov/llama.cpp/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/ggerganov/llama.cpp/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/ggerganov/llama.cpp/) to refresh your session. Dismiss alert\n{{ message }}\n[ ggml-org ](https://github.com/ggml-org) / **[llama.cpp](https://github.com/ggml-org/llama.cpp) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Fggml-org%2Fllama.cpp) You must be signed in to change notification settings\n  * [ Fork 11.3k ](https://github.com/login?return_to=%2Fggml-org%2Fllama.cpp)\n  * [ Star  77.4k ](https://github.com/login?return_to=%2Fggml-org%2Fllama.cpp)\n\n\nLLM inference in C/C++ \n### License\n[ MIT license ](https://github.com/ggml-org/llama.cpp/blob/master/LICENSE)\n[ 77.4k stars ](https://github.com/ggml-org/llama.cpp/stargazers) [ 11.3k forks ](https://github.com/ggml-org/llama.cpp/forks) [ Branches ](https://github.com/ggml-org/llama.cpp/branches) [ Tags ](https://github.com/ggml-org/llama.cpp/tags) [ Activity ](https://github.com/ggml-org/llama.cpp/activity)\n[ Star  ](https://github.com/login?return_to=%2Fggml-org%2Fllama.cpp)\n[ Notifications ](https://github.com/login?return_to=%2Fggml-org%2Fllama.cpp) You must be signed in to change notification settings\n  * [ Code ](https://github.com/ggml-org/llama.cpp)\n  * [ Issues 347 ](https://github.com/ggml-org/llama.cpp/issues)\n  * [ Pull requests 407 ](https://github.com/ggml-org/llama.cpp/pulls)\n  * [ Discussions ](https://github.com/ggml-org/llama.cpp/discussions)\n  * [ Actions ](https://github.com/ggml-org/llama.cpp/actions)\n  * [ Projects 9 ](https://github.com/ggml-org/llama.cpp/projects)\n  * [ Wiki ](https://github.com/ggml-org/llama.cpp/wiki)\n  * [ Security 5 ](https://github.com/ggml-org/llama.cpp/security)\n  * [ Insights ](https://github.com/ggml-org/llama.cpp/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/ggml-org/llama.cpp)\n  * [ Issues  ](https://github.com/ggml-org/llama.cpp/issues)\n  * [ Pull requests  ](https://github.com/ggml-org/llama.cpp/pulls)\n  * [ Discussions  ](https://github.com/ggml-org/llama.cpp/discussions)\n  * [ Actions  ](https://github.com/ggml-org/llama.cpp/actions)\n  * [ Projects  ](https://github.com/ggml-org/llama.cpp/projects)\n  * [ Wiki  ](https://github.com/ggml-org/llama.cpp/wiki)\n  * [ Security  ](https://github.com/ggml-org/llama.cpp/security)\n  * [ Insights  ](https://github.com/ggml-org/llama.cpp/pulse)\n\n\n# ggml-org/llama.cpp\nmaster\n[Branches](https://github.com/ggml-org/llama.cpp/branches)[Tags](https://github.com/ggml-org/llama.cpp/tags)\n[](https://github.com/ggml-org/llama.cpp/branches)[](https://github.com/ggml-org/llama.cpp/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[4,998 Commits](https://github.com/ggml-org/llama.cpp/commits/master/)[](https://github.com/ggml-org/llama.cpp/commits/master/)  \n[.devops](https://github.com/ggml-org/llama.cpp/tree/master/.devops \".devops\")| [.devops](https://github.com/ggml-org/llama.cpp/tree/master/.devops \".devops\")  \n[.github](https://github.com/ggml-org/llama.cpp/tree/master/.github \".github\")| [.github](https://github.com/ggml-org/llama.cpp/tree/master/.github \".github\")  \n[ci](https://github.com/ggml-org/llama.cpp/tree/master/ci \"ci\")| [ci](https://github.com/ggml-org/llama.cpp/tree/master/ci \"ci\")  \n[cmake](https://github.com/ggml-org/llama.cpp/tree/master/cmake \"cmake\")| [cmake](https://github.com/ggml-org/llama.cpp/tree/master/cmake \"cmake\")  \n[common](https://github.com/ggml-org/llama.cpp/tree/master/common \"common\")| [common](https://github.com/ggml-org/llama.cpp/tree/master/common \"common\")  \n[docs](https://github.com/ggml-org/llama.cpp/tree/master/docs \"docs\")| [docs](https://github.com/ggml-org/llama.cpp/tree/master/docs \"docs\")  \n[examples](https://github.com/ggml-org/llama.cpp/tree/master/examples \"examples\")| [examples](https://github.com/ggml-org/llama.cpp/tree/master/examples \"examples\")  \n[ggml](https://github.com/ggml-org/llama.cpp/tree/master/ggml \"ggml\")| [ggml](https://github.com/ggml-org/llama.cpp/tree/master/ggml \"ggml\")  \n[gguf-py](https://github.com/ggml-org/llama.cpp/tree/master/gguf-py \"gguf-py\")| [gguf-py](https://github.com/ggml-org/llama.cpp/tree/master/gguf-py \"gguf-py\")  \n[grammars](https://github.com/ggml-org/llama.cpp/tree/master/grammars \"grammars\")| [grammars](https://github.com/ggml-org/llama.cpp/tree/master/grammars \"grammars\")  \n[include](https://github.com/ggml-org/llama.cpp/tree/master/include \"include\")| [include](https://github.com/ggml-org/llama.cpp/tree/master/include \"include\")  \n[media](https://github.com/ggml-org/llama.cpp/tree/master/media \"media\")| [media](https://github.com/ggml-org/llama.cpp/tree/master/media \"media\")  \n[models](https://github.com/ggml-org/llama.cpp/tree/master/models \"models\")| [models](https://github.com/ggml-org/llama.cpp/tree/master/models \"models\")  \n[pocs](https://github.com/ggml-org/llama.cpp/tree/master/pocs \"pocs\")| [pocs](https://github.com/ggml-org/llama.cpp/tree/master/pocs \"pocs\")  \n[prompts](https://github.com/ggml-org/llama.cpp/tree/master/prompts \"prompts\")| [prompts](https://github.com/ggml-org/llama.cpp/tree/master/prompts \"prompts\")  \n[requirements](https://github.com/ggml-org/llama.cpp/tree/master/requirements \"requirements\")| [requirements](https://github.com/ggml-org/llama.cpp/tree/master/requirements \"requirements\")  \n[scripts](https://github.com/ggml-org/llama.cpp/tree/master/scripts \"scripts\")| [scripts](https://github.com/ggml-org/llama.cpp/tree/master/scripts \"scripts\")  \n[src](https://github.com/ggml-org/llama.cpp/tree/master/src \"src\")| [src](https://github.com/ggml-org/llama.cpp/tree/master/src \"src\")  \n[tests](https://github.com/ggml-org/llama.cpp/tree/master/tests \"tests\")| [tests](https://github.com/ggml-org/llama.cpp/tree/master/tests \"tests\")  \n[.clang-format](https://github.com/ggml-org/llama.cpp/blob/master/.clang-format \".clang-format\")| [.clang-format](https://github.com/ggml-org/llama.cpp/blob/master/.clang-format \".clang-format\")  \n[.clang-tidy](https://github.com/ggml-org/llama.cpp/blob/master/.clang-tidy \".clang-tidy\")| [.clang-tidy](https://github.com/ggml-org/llama.cpp/blob/master/.clang-tidy \".clang-tidy\")  \n[.dockerignore](https://github.com/ggml-org/llama.cpp/blob/master/.dockerignore \".dockerignore\")| [.dockerignore](https://github.com/ggml-org/llama.cpp/blob/master/.dockerignore \".dockerignore\")  \n[.ecrc](https://github.com/ggml-org/llama.cpp/blob/master/.ecrc \".ecrc\")| [.ecrc](https://github.com/ggml-org/llama.cpp/blob/master/.ecrc \".ecrc\")  \n[.editorconfig](https://github.com/ggml-org/llama.cpp/blob/master/.editorconfig \".editorconfig\")| [.editorconfig](https://github.com/ggml-org/llama.cpp/blob/master/.editorconfig \".editorconfig\")  \n[.flake8](https://github.com/ggml-org/llama.cpp/blob/master/.flake8 \".flake8\")| [.flake8](https://github.com/ggml-org/llama.cpp/blob/master/.flake8 \".flake8\")  \n[.gitignore](https://github.com/ggml-org/llama.cpp/blob/master/.gitignore \".gitignore\")| [.gitignore](https://github.com/ggml-org/llama.cpp/blob/master/.gitignore \".gitignore\")  \n[.gitmodules](https://github.com/ggml-org/llama.cpp/blob/master/.gitmodules \".gitmodules\")| [.gitmodules](https://github.com/ggml-org/llama.cpp/blob/master/.gitmodules \".gitmodules\")  \n[.pre-commit-config.yaml](https://github.com/ggml-org/llama.cpp/blob/master/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/ggml-org/llama.cpp/blob/master/.pre-commit-config.yaml \".pre-commit-config.yaml\")  \n[AUTHORS](https://github.com/ggml-org/llama.cpp/blob/master/AUTHORS \"AUTHORS\")| [AUTHORS](https://github.com/ggml-org/llama.cpp/blob/master/AUTHORS \"AUTHORS\")  \n[CMakeLists.txt](https://github.com/ggml-org/llama.cpp/blob/master/CMakeLists.txt \"CMakeLists.txt\")| [CMakeLists.txt](https://github.com/ggml-org/llama.cpp/blob/master/CMakeLists.txt \"CMakeLists.txt\")  \n[CMakePresets.json](https://github.com/ggml-org/llama.cpp/blob/master/CMakePresets.json \"CMakePresets.json\")| [CMakePresets.json](https://github.com/ggml-org/llama.cpp/blob/master/CMakePresets.json \"CMakePresets.json\")  \n[CODEOWNERS](https://github.com/ggml-org/llama.cpp/blob/master/CODEOWNERS \"CODEOWNERS\")| [CODEOWNERS](https://github.com/ggml-org/llama.cpp/blob/master/CODEOWNERS \"CODEOWNERS\")  \n[CONTRIBUTING.md](https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md \"CONTRIBUTING.md\")  \n[LICENSE](https://github.com/ggml-org/llama.cpp/blob/master/LICENSE \"LICENSE\")| [LICENSE](https://github.com/ggml-org/llama.cpp/blob/master/LICENSE \"LICENSE\")  \n[Makefile](https://github.com/ggml-org/llama.cpp/blob/master/Makefile \"Makefile\")| [Makefile](https://github.com/ggml-org/llama.cpp/blob/master/Makefile \"Makefile\")  \n[README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md \"README.md\")| [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md \"README.md\")  \n[SECURITY.md](https://github.com/ggml-org/llama.cpp/blob/master/SECURITY.md \"SECURITY.md\")| [SECURITY.md](https://github.com/ggml-org/llama.cpp/blob/master/SECURITY.md \"SECURITY.md\")  \n[build-xcframework.sh](https://github.com/ggml-org/llama.cpp/blob/master/build-xcframework.sh \"build-xcframework.sh\")| [build-xcframework.sh](https://github.com/ggml-org/llama.cpp/blob/master/build-xcframework.sh \"build-xcframework.sh\")  \n[convert_hf_to_gguf.py](https://github.com/ggml-org/llama.cpp/blob/master/convert_hf_to_gguf.py \"convert_hf_to_gguf.py\")| [convert_hf_to_gguf.py](https://github.com/ggml-org/llama.cpp/blob/master/convert_hf_to_gguf.py \"convert_hf_to_gguf.py\")  \n[convert_hf_to_gguf_update.py](https://github.com/ggml-org/llama.cpp/blob/master/convert_hf_to_gguf_update.py \"convert_hf_to_gguf_update.py\")| [convert_hf_to_gguf_update.py](https://github.com/ggml-org/llama.cpp/blob/master/convert_hf_to_gguf_update.py \"convert_hf_to_gguf_update.py\")  \n[convert_llama_ggml_to_gguf.py](https://github.com/ggml-org/llama.cpp/blob/master/convert_llama_ggml_to_gguf.py \"convert_llama_ggml_to_gguf.py\")| [convert_llama_ggml_to_gguf.py](https://github.com/ggml-org/llama.cpp/blob/master/convert_llama_ggml_to_gguf.py \"convert_llama_ggml_to_gguf.py\")  \n[convert_lora_to_gguf.py](https://github.com/ggml-org/llama.cpp/blob/master/convert_lora_to_gguf.py \"convert_lora_to_gguf.py\")| [convert_lora_to_gguf.py](https://github.com/ggml-org/llama.cpp/blob/master/convert_lora_to_gguf.py \"convert_lora_to_gguf.py\")  \n[flake.lock](https://github.com/ggml-org/llama.cpp/blob/master/flake.lock \"flake.lock\")| [flake.lock](https://github.com/ggml-org/llama.cpp/blob/master/flake.lock \"flake.lock\")  \n[flake.nix](https://github.com/ggml-org/llama.cpp/blob/master/flake.nix \"flake.nix\")| [flake.nix](https://github.com/ggml-org/llama.cpp/blob/master/flake.nix \"flake.nix\")  \n[mypy.ini](https://github.com/ggml-org/llama.cpp/blob/master/mypy.ini \"mypy.ini\")| [mypy.ini](https://github.com/ggml-org/llama.cpp/blob/master/mypy.ini \"mypy.ini\")  \n[poetry.lock](https://github.com/ggml-org/llama.cpp/blob/master/poetry.lock \"poetry.lock\")| [poetry.lock](https://github.com/ggml-org/llama.cpp/blob/master/poetry.lock \"poetry.lock\")  \n[pyproject.toml](https://github.com/ggml-org/llama.cpp/blob/master/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/ggml-org/llama.cpp/blob/master/pyproject.toml \"pyproject.toml\")  \n[pyrightconfig.json](https://github.com/ggml-org/llama.cpp/blob/master/pyrightconfig.json \"pyrightconfig.json\")| [pyrightconfig.json](https://github.com/ggml-org/llama.cpp/blob/master/pyrightconfig.json \"pyrightconfig.json\")  \n[requirements.txt](https://github.com/ggml-org/llama.cpp/blob/master/requirements.txt \"requirements.txt\")| [requirements.txt](https://github.com/ggml-org/llama.cpp/blob/master/requirements.txt \"requirements.txt\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/ggerganov/llama.cpp/)\n  * [MIT license](https://github.com/ggerganov/llama.cpp/)\n  * [Security](https://github.com/ggerganov/llama.cpp/)\n\n\n# llama.cpp\n[](https://github.com/ggerganov/llama.cpp/#llamacpp)\n[![llama](https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png)](https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png)\n[![License: MIT](https://camo.githubusercontent.com/6581c31c16c1b13ddc2efb92e2ad69a93ddc4a92fd871ff15d401c4c6c9155a4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667)](https://opensource.org/licenses/MIT) [![Server](https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml/badge.svg)](https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml)\n[Roadmap](https://github.com/users/ggerganov/projects/7) / [Project status](https://github.com/ggml-org/llama.cpp/discussions/3471) / [Manifesto](https://github.com/ggml-org/llama.cpp/discussions/205) / [ggml](https://github.com/ggml-org/ggml)\nInference of Meta's [LLaMA](https://arxiv.org/abs/2302.13971) model (and others) in pure C/C++\nImportant\nNew `llama.cpp` package location: [ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp/pkgs/container/llama.cpp)\nUpdate your container URLs to: `ghcr.io/ggml-org/llama.cpp`\nMore info: [#11801](https://github.com/ggml-org/llama.cpp/discussions/11801)\n## Recent API changes\n[](https://github.com/ggerganov/llama.cpp/#recent-api-changes)\n  * [Changelog for `libllama` API](https://github.com/ggml-org/llama.cpp/issues/9289)\n  * [Changelog for `llama-server` REST API](https://github.com/ggml-org/llama.cpp/issues/9291)\n\n\n## Hot topics\n[](https://github.com/ggerganov/llama.cpp/#hot-topics)\n  * **How to use[MTLResidencySet](https://developer.apple.com/documentation/metal/mtlresidencyset?language=objc) to keep the GPU memory active?** [#11427](https://github.com/ggml-org/llama.cpp/pull/11427)\n  * **VS Code extension for FIM completions:** <https://github.com/ggml-org/llama.vscode>\n  * Universal [tool call support](https://github.com/ggml-org/llama.cpp/blob/master/docs/function-calling.md) in `llama-server` [#9639](https://github.com/ggml-org/llama.cpp/pull/9639)\n  * Vim/Neovim plugin for FIM completions: <https://github.com/ggml-org/llama.vim>\n  * Introducing GGUF-my-LoRA [#10123](https://github.com/ggml-org/llama.cpp/discussions/10123)\n  * Hugging Face Inference Endpoints now support GGUF out of the box! [#9669](https://github.com/ggml-org/llama.cpp/discussions/9669)\n  * Hugging Face GGUF editor: [discussion](https://github.com/ggml-org/llama.cpp/discussions/9268) | [tool](https://huggingface.co/spaces/CISCai/gguf-editor)\n\n\n## Description\n[](https://github.com/ggerganov/llama.cpp/#description)\nThe main goal of `llama.cpp` is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud.\n  * Plain C/C++ implementation without any dependencies\n  * Apple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks\n  * AVX, AVX2, AVX512 and AMX support for x86 architectures\n  * 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use\n  * Custom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP and Moore Threads MTT GPUs via MUSA)\n  * Vulkan and SYCL backend support\n  * CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity\n\n\nThe `llama.cpp` project is the main playground for developing new features for the [ggml](https://github.com/ggml-org/ggml) library.\nModels\nTypically finetunes of the base models below are supported as well.\nInstructions for adding support for new models: [HOWTO-add-model.md](https://github.com/ggml-org/llama.cpp/blob/master/docs/development/HOWTO-add-model.md)\n#### Text-only\n[](https://github.com/ggerganov/llama.cpp/#text-only)\n  * LLaMA \ud83e\udd99\n  * LLaMA 2 \ud83e\udd99\ud83e\udd99\n  * LLaMA 3 \ud83e\udd99\ud83e\udd99\ud83e\udd99\n  * [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n  * [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)\n  * [DBRX](https://huggingface.co/databricks/dbrx-instruct)\n  * [Falcon](https://huggingface.co/models?search=tiiuae/falcon)\n  * [Chinese LLaMA / Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) and [Chinese LLaMA-2 / Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)\n  * [Vigogne (French)](https://github.com/bofenghuang/vigogne)\n  * [BERT](https://github.com/ggml-org/llama.cpp/pull/5423)\n  * [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)\n  * [Baichuan 1 & 2](https://huggingface.co/models?search=baichuan-inc/Baichuan) + [derivations](https://huggingface.co/hiyouga/baichuan-7b-sft)\n  * [Aquila 1 & 2](https://huggingface.co/models?search=BAAI/Aquila)\n  * [Starcoder models](https://github.com/ggml-org/llama.cpp/pull/3187)\n  * [Refact](https://huggingface.co/smallcloudai/Refact-1_6B-fim)\n  * [MPT](https://github.com/ggml-org/llama.cpp/pull/3417)\n  * [Bloom](https://github.com/ggml-org/llama.cpp/pull/3553)\n  * [Yi models](https://huggingface.co/models?search=01-ai/Yi)\n  * [StableLM models](https://huggingface.co/stabilityai)\n  * [Deepseek models](https://huggingface.co/models?search=deepseek-ai/deepseek)\n  * [Qwen models](https://huggingface.co/models?search=Qwen/Qwen)\n  * [PLaMo-13B](https://github.com/ggml-org/llama.cpp/pull/3557)\n  * [Phi models](https://huggingface.co/models?search=microsoft/phi)\n  * [PhiMoE](https://github.com/ggml-org/llama.cpp/pull/11003)\n  * [GPT-2](https://huggingface.co/gpt2)\n  * [Orion 14B](https://github.com/ggml-org/llama.cpp/pull/5118)\n  * [InternLM2](https://huggingface.co/models?search=internlm2)\n  * [CodeShell](https://github.com/WisdomShell/codeshell)\n  * [Gemma](https://ai.google.dev/gemma)\n  * [Mamba](https://github.com/state-spaces/mamba)\n  * [Grok-1](https://huggingface.co/keyfan/grok-1-hf)\n  * [Xverse](https://huggingface.co/models?search=xverse)\n  * [Command-R models](https://huggingface.co/models?search=CohereForAI/c4ai-command-r)\n  * [SEA-LION](https://huggingface.co/models?search=sea-lion)\n  * [GritLM-7B](https://huggingface.co/GritLM/GritLM-7B) + [GritLM-8x7B](https://huggingface.co/GritLM/GritLM-8x7B)\n  * [OLMo](https://allenai.org/olmo)\n  * [OLMo 2](https://allenai.org/olmo)\n  * [OLMoE](https://huggingface.co/allenai/OLMoE-1B-7B-0924)\n  * [Granite models](https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330)\n  * [GPT-NeoX](https://github.com/EleutherAI/gpt-neox) + [Pythia](https://github.com/EleutherAI/pythia)\n  * [Snowflake-Arctic MoE](https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520)\n  * [Smaug](https://huggingface.co/models?search=Smaug)\n  * [Poro 34B](https://huggingface.co/LumiOpen/Poro-34B)\n  * [Bitnet b1.58 models](https://huggingface.co/1bitLLM)\n  * [Flan T5](https://huggingface.co/models?search=flan-t5)\n  * [Open Elm models](https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759ca)\n  * [ChatGLM3-6b](https://huggingface.co/THUDM/chatglm3-6b) + [ChatGLM4-9b](https://huggingface.co/THUDM/glm-4-9b) + [GLMEdge-1.5b](https://huggingface.co/THUDM/glm-edge-1.5b-chat) + [GLMEdge-4b](https://huggingface.co/THUDM/glm-edge-4b-chat)\n  * [SmolLM](https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966)\n  * [EXAONE-3.0-7.8B-Instruct](https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct)\n  * [FalconMamba Models](https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a)\n  * [Jais](https://huggingface.co/inceptionai/jais-13b-chat)\n  * [Bielik-11B-v2.3](https://huggingface.co/collections/speakleash/bielik-11b-v23-66ee813238d9b526a072408a)\n  * [RWKV-6](https://github.com/BlinkDL/RWKV-LM)\n  * [QRWKV-6](https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1)\n  * [GigaChat-20B-A3B](https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct)\n\n\n#### Multimodal\n[](https://github.com/ggerganov/llama.cpp/#multimodal)\n  * [LLaVA 1.5 models](https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e), [LLaVA 1.6 models](https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2)\n  * [BakLLaVA](https://huggingface.co/models?search=SkunkworksAI/Bakllava)\n  * [Obsidian](https://huggingface.co/NousResearch/Obsidian-3B-V0.5)\n  * [ShareGPT4V](https://huggingface.co/models?search=Lin-Chen/ShareGPT4V)\n  * [MobileVLM 1.7B/3B models](https://huggingface.co/models?search=mobileVLM)\n  * [Yi-VL](https://huggingface.co/models?search=Yi-VL)\n  * [Mini CPM](https://huggingface.co/models?search=MiniCPM)\n  * [Moondream](https://huggingface.co/vikhyatk/moondream2)\n  * [Bunny](https://github.com/BAAI-DCAI/Bunny)\n  * [GLM-EDGE](https://huggingface.co/models?search=glm-edge)\n  * [Qwen2-VL](https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d)\n\nBindings\n  * Python: [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python)\n  * Go: [go-skynet/go-llama.cpp](https://github.com/go-skynet/go-llama.cpp)\n  * Node.js: [withcatai/node-llama-cpp](https://github.com/withcatai/node-llama-cpp)\n  * JS/TS (llama.cpp server client): [lgrammel/modelfusion](https://modelfusion.dev/integration/model-provider/llamacpp)\n  * JS/TS (Programmable Prompt Engine CLI): [offline-ai/cli](https://github.com/offline-ai/cli)\n  * JavaScript/Wasm (works in browser): [tangledgroup/llama-cpp-wasm](https://github.com/tangledgroup/llama-cpp-wasm)\n  * Typescript/Wasm (nicer API, available on npm): [ngxson/wllama](https://github.com/ngxson/wllama)\n  * Ruby: [yoshoku/llama_cpp.rb](https://github.com/yoshoku/llama_cpp.rb)\n  * Rust (more features): [edgenai/llama_cpp-rs](https://github.com/edgenai/llama_cpp-rs)\n  * Rust (nicer API): [mdrokz/rust-llama.cpp](https://github.com/mdrokz/rust-llama.cpp)\n  * Rust (more direct bindings): [utilityai/llama-cpp-rs](https://github.com/utilityai/llama-cpp-rs)\n  * Rust (automated build from crates.io): [ShelbyJenkins/llm_client](https://github.com/ShelbyJenkins/llm_client)\n  * C#/.NET: [SciSharp/LLamaSharp](https://github.com/SciSharp/LLamaSharp)\n  * C#/VB.NET (more features - community license): [LM-Kit.NET](https://docs.lm-kit.com/lm-kit-net/index.html)\n  * Scala 3: [donderom/llm4s](https://github.com/donderom/llm4s)\n  * Clojure: [phronmophobic/llama.clj](https://github.com/phronmophobic/llama.clj)\n  * React Native: [mybigday/llama.rn](https://github.com/mybigday/llama.rn)\n  * Java: [kherud/java-llama.cpp](https://github.com/kherud/java-llama.cpp)\n  * Zig: [deins/llama.cpp.zig](https://github.com/Deins/llama.cpp.zig)\n  * Flutter/Dart: [netdur/llama_cpp_dart](https://github.com/netdur/llama_cpp_dart)\n  * Flutter: [xuegao-tzx/Fllama](https://github.com/xuegao-tzx/Fllama)\n  * PHP (API bindings and features built on top of llama.cpp): [distantmagic/resonance](https://github.com/distantmagic/resonance) [(more info)](https://github.com/ggml-org/llama.cpp/pull/6326)\n  * Guile Scheme: [guile_llama_cpp](https://savannah.nongnu.org/projects/guile-llama-cpp)\n  * Swift [srgtuszy/llama-cpp-swift](https://github.com/srgtuszy/llama-cpp-swift)\n  * Swift [ShenghaiWang/SwiftLlama](https://github.com/ShenghaiWang/SwiftLlama)\n  * Delphi [Embarcadero/llama-cpp-delphi](https://github.com/Embarcadero/llama-cpp-delphi)\n\nUIs\n_(to have a project listed here, it should clearly state that it depends on`llama.cpp`)_\n  * [AI Sublime Text plugin](https://github.com/yaroslavyaroslav/OpenAI-sublime-text) (MIT)\n  * [cztomsik/ava](https://github.com/cztomsik/ava) (MIT)\n  * [Dot](https://github.com/alexpinel/Dot) (GPL)\n  * [eva](https://github.com/ylsdamxssjxxdd/eva) (MIT)\n  * [iohub/collama](https://github.com/iohub/coLLaMA) (Apache-2.0)\n  * [janhq/jan](https://github.com/janhq/jan) (AGPL)\n  * [johnbean393/Sidekick](https://github.com/johnbean393/Sidekick) (MIT)\n  * [KanTV](https://github.com/zhouwg/kantv?tab=readme-ov-file) (Apache-2.0)\n  * [KodiBot](https://github.com/firatkiral/kodibot) (GPL)\n  * [llama.vim](https://github.com/ggml-org/llama.vim) (MIT)\n  * [LARS](https://github.com/abgulati/LARS) (AGPL)\n  * [Llama Assistant](https://github.com/vietanhdev/llama-assistant) (GPL)\n  * [LLMFarm](https://github.com/guinmoon/LLMFarm?tab=readme-ov-file) (MIT)\n  * [LLMUnity](https://github.com/undreamai/LLMUnity) (MIT)\n  * [LMStudio](https://lmstudio.ai/) (proprietary)\n  * [LocalAI](https://github.com/mudler/LocalAI) (MIT)\n  * [LostRuins/koboldcpp](https://github.com/LostRuins/koboldcpp) (AGPL)\n  * [MindMac](https://mindmac.app) (proprietary)\n  * [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT)\n  * [Mobile-Artificial-Intelligence/maid](https://github.com/Mobile-Artificial-Intelligence/maid) (MIT)\n  * [Mozilla-Ocho/llamafile](https://github.com/Mozilla-Ocho/llamafile) (Apache-2.0)\n  * [nat/openplayground](https://github.com/nat/openplayground) (MIT)\n  * [nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all) (MIT)\n  * [ollama/ollama](https://github.com/ollama/ollama) (MIT)\n  * [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) (AGPL)\n  * [PocketPal AI](https://github.com/a-ghorbani/pocketpal-ai) (MIT)\n  * [psugihara/FreeChat](https://github.com/psugihara/FreeChat) (MIT)\n  * [ptsochantaris/emeltal](https://github.com/ptsochantaris/emeltal) (MIT)\n  * [pythops/tenere](https://github.com/pythops/tenere) (AGPL)\n  * [ramalama](https://github.com/containers/ramalama) (MIT)\n  * [semperai/amica](https://github.com/semperai/amica) (MIT)\n  * [withcatai/catai](https://github.com/withcatai/catai) (MIT)\n  * [Autopen](https://github.com/blackhole89/autopen) (GPL)\n\nTools\n  * [akx/ggify](https://github.com/akx/ggify) \u2013 download PyTorch models from HuggingFace Hub and convert them to GGML\n  * [akx/ollama-dl](https://github.com/akx/ollama-dl) \u2013 download models from the Ollama library to be used directly with llama.cpp\n  * [crashr/gppm](https://github.com/crashr/gppm) \u2013 launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption\n  * [gpustack/gguf-parser](https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser) - review/check the GGUF file and estimate the memory usage\n  * [Styled Lines](https://marketplace.unity.com/packages/tools/generative-ai/styled-lines-llama-cpp-model-292902) (proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example)\n\nInfrastructure\n  * [Paddler](https://github.com/distantmagic/paddler) - Stateful load balancer custom-tailored for llama.cpp\n  * [GPUStack](https://github.com/gpustack/gpustack) - Manage GPU clusters for running LLMs\n  * [llama_cpp_canister](https://github.com/onicai/llama_cpp_canister) - llama.cpp as a smart contract on the Internet Computer, using WebAssembly\n  * [llama-swap](https://github.com/mostlygeek/llama-swap) - transparent proxy that adds automatic model switching with llama-server\n  * [Kalavai](https://github.com/kalavai-net/kalavai-client) - Crowdsource end to end LLM deployment at any scale\n  * [llmaz](https://github.com/InftyAI/llmaz) - \u2638\ufe0f Easy, advanced inference platform for large language models on Kubernetes.\n\nGames\n  * [Lucy's Labyrinth](https://github.com/MorganRO8/Lucys_Labyrinth) - A simple maze game where agents controlled by an AI model will try to trick you.\n\n\n## Supported backends\n[](https://github.com/ggerganov/llama.cpp/#supported-backends)\nBackend | Target devices  \n---|---  \n[Metal](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#metal-build) | Apple Silicon  \n[BLAS](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#blas-build) | All  \n[BLIS](https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/BLIS.md) | All  \n[SYCL](https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md) | Intel and Nvidia GPU  \n[MUSA](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#musa) | Moore Threads MTT GPU  \n[CUDA](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#cuda) | Nvidia GPU  \n[HIP](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#hip) | AMD GPU  \n[Vulkan](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#vulkan) | GPU  \n[CANN](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#cann) | Ascend NPU  \n[OpenCL](https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md) | Adreno GPU  \n## Building the project\n[](https://github.com/ggerganov/llama.cpp/#building-the-project)\nThe main product of this project is the `llama` library. Its C-style interface can be found in [include/llama.h](https://github.com/ggml-org/llama.cpp/blob/master/include/llama.h). The project also includes many example programs and tools using the `llama` library. The examples range from simple, minimal code snippets to sophisticated sub-projects such as an OpenAI-compatible HTTP server. Possible methods for obtaining the binaries:\n  * Clone this repository and build locally, see [how to build](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md)\n  * On MacOS or Linux, install `llama.cpp` via [brew, flox or nix](https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md)\n  * Use a Docker image, see [documentation for Docker](https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md)\n  * Download pre-built binaries from [releases](https://github.com/ggml-org/llama.cpp/releases)\n\n\n## Obtaining and quantizing models\n[](https://github.com/ggerganov/llama.cpp/#obtaining-and-quantizing-models)\nThe [Hugging Face](https://huggingface.co) platform hosts a [number of LLMs](https://huggingface.co/models?library=gguf&sort=trending) compatible with `llama.cpp`:\n  * [Trending](https://huggingface.co/models?library=gguf&sort=trending)\n  * [LLaMA](https://huggingface.co/models?sort=trending&search=llama+gguf)\n\n\nYou can either manually download the GGUF file or directly use any `llama.cpp`-compatible models from Hugging Face by using this CLI argument: `-hf <user>/<model>[:quant]`\nAfter downloading a model, use the CLI tools to run it locally - see below.\n`llama.cpp` requires the model to be stored in the [GGUF](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md) file format. Models in other data formats can be converted to GGUF using the `convert_*.py` Python scripts in this repo.\nThe Hugging Face platform provides a variety of online tools for converting, quantizing and hosting models with `llama.cpp`:\n  * Use the [GGUF-my-repo space](https://huggingface.co/spaces/ggml-org/gguf-my-repo) to convert to GGUF format and quantize model weights to smaller sizes\n  * Use the [GGUF-my-LoRA space](https://huggingface.co/spaces/ggml-org/gguf-my-lora) to convert LoRA adapters to GGUF format (more info: [#10123](https://github.com/ggml-org/llama.cpp/discussions/10123))\n  * Use the [GGUF-editor space](https://huggingface.co/spaces/CISCai/gguf-editor) to edit GGUF meta data in the browser (more info: [#9268](https://github.com/ggml-org/llama.cpp/discussions/9268))\n  * Use the [Inference Endpoints](https://ui.endpoints.huggingface.co/) to directly host `llama.cpp` in the cloud (more info: [#9669](https://github.com/ggml-org/llama.cpp/discussions/9669))\n\n\nTo learn more about model quantization, [read this documentation](https://github.com/ggml-org/llama.cpp/blob/master/examples/quantize/README.md)\n## [`llama-cli`](https://github.com/ggml-org/llama.cpp/blob/master/examples/main)\n[](https://github.com/ggerganov/llama.cpp/#llama-cli)\n#### A CLI tool for accessing and experimenting with most of `llama.cpp`'s functionality.\n[](https://github.com/ggerganov/llama.cpp/#a-cli-tool-for-accessing-and-experimenting-with-most-of-llamacpps-functionality)\n  * Run in conversation mode\nModels with a built-in chat template will automatically activate conversation mode. If this doesn't occur, you can manually enable it by adding `-cnv` and specifying a suitable chat template with `--chat-template NAME`\n```\nllama-cli -m model.gguf\n# > hi, who are you?\n# Hi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist and provide information to users like you. I'm here to help answer your questions, provide guidance, and offer support on a wide range of topics. I'm a friendly and knowledgeable AI, and I'm always happy to help with anything you need. What's on your mind, and how can I assist you today?\n#\n# > what is 1+1?\n# Easy peasy! The answer to 1+1 is... 2!\n```\n\n  * Run in conversation mode with custom chat template\n```\n# use the \"chatml\" template (use -h to see the list of supported templates)\nllama-cli -m model.gguf -cnv --chat-template chatml\n# use a custom template\nllama-cli -m model.gguf -cnv --in-prefix 'User: ' --reverse-prompt 'User:'\n```\n\n  * Run simple text completion\nTo disable conversation mode explicitly, use `-no-cnv`\n```\nllama-cli -m model.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv\n# I believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don't align with societal expectations. I think that's what I love about yoga \u2013 it's not just a physical practice, but a spiritual one too. It's about connecting with yourself, listening to your inner voice, and honoring your own unique journey.\n```\n\n  * Constrain the output with a custom grammar\n```\nllama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p 'Request: schedule a call at 8pm; Command:'\n# {\"appointmentTime\": \"8pm\", \"appointmentDetails\": \"schedule a a call\"}\n```\n\nThe [grammars/](https://github.com/ggml-org/llama.cpp/blob/master/grammars) folder contains a handful of sample grammars. To write your own, check out the [GBNF Guide](https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md).\nFor authoring more complex JSON grammars, check out <https://grammar.intrinsiclabs.ai/>\n\n\n## [`llama-server`](https://github.com/ggml-org/llama.cpp/blob/master/examples/server)\n[](https://github.com/ggerganov/llama.cpp/#llama-server)\n#### A lightweight, [OpenAI API](https://github.com/openai/openai-openapi) compatible, HTTP server for serving LLMs.\n[](https://github.com/ggerganov/llama.cpp/#a-lightweight-openai-api-compatible-http-server-for-serving-llms)\n  * Start a local HTTP server with default configuration on port 8080\n```\nllama-server -m model.gguf --port 8080\n# Basic web UI can be accessed via browser: http://localhost:8080\n# Chat completion endpoint: http://localhost:8080/v1/chat/completions\n```\n\n  * Support multiple-users and parallel decoding\n```\n# up to 4 concurrent requests, each with 4096 max context\nllama-server -m model.gguf -c 16384 -np 4\n```\n\n  * Enable speculative decoding\n```\n# the draft.gguf model should be a small variant of the target model.gguf\nllama-server -m model.gguf -md draft.gguf\n```\n\n  * Serve an embedding model\n```\n# use the /embedding endpoint\nllama-server -m model.gguf --embedding --pooling cls -ub 8192\n```\n\n  * Serve a reranking model\n```\n# use the /reranking endpoint\nllama-server -m model.gguf --reranking\n```\n\n  * Constrain all outputs with a grammar\n```\n# custom grammar\nllama-server -m model.gguf --grammar-file grammar.gbnf\n# JSON\nllama-server -m model.gguf --grammar-file grammars/json.gbnf\n```\n\n\n\n## [`llama-perplexity`](https://github.com/ggml-org/llama.cpp/blob/master/examples/perplexity)\n[](https://github.com/ggerganov/llama.cpp/#llama-perplexity)\n#### A tool for measuring the perplexity [1](https://github.com/ggerganov/llama.cpp/#user-content-fn-1-97c613c5f775dae8f9c71fd304b459b4)[2](https://github.com/ggerganov/llama.cpp/#user-content-fn-2-97c613c5f775dae8f9c71fd304b459b4) (and other quality metrics) of a model over a given text.\n[](https://github.com/ggerganov/llama.cpp/#a-tool-for-measuring-the-perplexity-12-and-other-quality-metrics-of-a-model-over-a-given-text)\n  * Measure the perplexity over a text file\n```\nllama-perplexity -m model.gguf -f file.txt\n# [1]15.2701,[2]5.4007,[3]5.3073,[4]6.2965,[5]5.8940,[6]5.6096,[7]5.7942,[8]4.9297, ...\n# Final estimate: PPL = 5.4007 +/- 0.67339\n```\n\n  * Measure KL divergence\n```\n# TODO\n```\n\n\n\n## [`llama-bench`](https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench)\n[](https://github.com/ggerganov/llama.cpp/#llama-bench)\n#### Benchmark the performance of the inference for various parameters.\n[](https://github.com/ggerganov/llama.cpp/#benchmark-the-performance-of-the-inference-for-various-parameters)\n  * Run default benchmark\n```\nllama-bench -m model.gguf\n# Output:\n# | model        |    size |   params | backend  | threads |     test |         t/s |\n# | ------------------- | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\n# | qwen2 1.5B Q4_0   | 885.97 MiB |   1.54 B | Metal,BLAS |   16 |     pp512 |   5765.41 \u00b1 20.55 |\n# | qwen2 1.5B Q4_0   | 885.97 MiB |   1.54 B | Metal,BLAS |   16 |     tg128 |    197.71 \u00b1 0.81 |\n#\n# build: 3e0ba0e60 (4229)\n```\n\n\n\n## [`llama-run`](https://github.com/ggml-org/llama.cpp/blob/master/examples/run)\n[](https://github.com/ggerganov/llama.cpp/#llama-run)\n#### A comprehensive example for running `llama.cpp` models. Useful for inferencing. Used with RamaLama [3](https://github.com/ggerganov/llama.cpp/#user-content-fn-3-97c613c5f775dae8f9c71fd304b459b4).\n[](https://github.com/ggerganov/llama.cpp/#a-comprehensive-example-for-running-llamacpp-models-useful-for-inferencing-used-with-ramalama-3)\n  * Run a model with a specific prompt (by default it's pulled from Ollama registry)\n```\nllama-run granite-code\n```\n\n\n\n## [`llama-simple`](https://github.com/ggml-org/llama.cpp/blob/master/examples/simple)\n[](https://github.com/ggerganov/llama.cpp/#llama-simple)\n#### A minimal example for implementing apps with `llama.cpp`. Useful for developers.\n[](https://github.com/ggerganov/llama.cpp/#a-minimal-example-for-implementing-apps-with-llamacpp-useful-for-developers)\n  * Basic text completion\n```\nllama-simple -m model.gguf\n# Hello my name is Kaitlyn and I am a 16 year old girl. I am a junior in high school and I am currently taking a class called \"The Art of\n```\n\n\n\n## Contributing\n[](https://github.com/ggerganov/llama.cpp/#contributing)\n  * Contributors can open PRs\n  * Collaborators can push to branches in the `llama.cpp` repo and merge PRs into the `master` branch\n  * Collaborators will be invited based on contributions\n  * Any help with managing issues, PRs and projects is very appreciated!\n  * See [good first issues](https://github.com/ggml-org/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) for tasks suitable for first contributions\n  * Read the [CONTRIBUTING.md](https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md) for more information\n  * Make sure to read this: [Inference at the edge](https://github.com/ggml-org/llama.cpp/discussions/205)\n  * A bit of backstory for those who are interested: [Changelog podcast](https://changelog.com/podcast/532)\n\n\n## Other documentation\n[](https://github.com/ggerganov/llama.cpp/#other-documentation)\n  * [main (cli)](https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md)\n  * [server](https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md)\n  * [GBNF grammars](https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md)\n\n\n#### Development documentation\n[](https://github.com/ggerganov/llama.cpp/#development-documentation)\n  * [How to build](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md)\n  * [Running on Docker](https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md)\n  * [Build on Android](https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md)\n  * [Performance troubleshooting](https://github.com/ggml-org/llama.cpp/blob/master/docs/development/token_generation_performance_tips.md)\n  * [GGML tips & tricks](https://github.com/ggml-org/llama.cpp/wiki/GGML-Tips-&-Tricks)\n\n\n#### Seminal papers and background on the models\n[](https://github.com/ggerganov/llama.cpp/#seminal-papers-and-background-on-the-models)\nIf your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:\n  * LLaMA: \n    * [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)\n    * [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)\n  * GPT-3 \n    * [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n  * GPT-3.5 / InstructGPT / ChatGPT: \n    * [Aligning language models to follow instructions](https://openai.com/research/instruction-following)\n    * [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\n\n\n## Completions\n[](https://github.com/ggerganov/llama.cpp/#completions)\nCommand-line completion is available for some environments.\n#### Bash Completion\n[](https://github.com/ggerganov/llama.cpp/#bash-completion)\n```\n$ build/bin/llama-cli --completion-bash > ~/.llama-completion.bash\n$ source ~/.llama-completion.bash\n```\n\nOptionally this can be added to your `.bashrc` or `.bash_profile` to load it automatically. For example:\n```\n$ echo \"source ~/.llama-completion.bash\" >> ~/.bashrc\n```\n\n## References\n[](https://github.com/ggerganov/llama.cpp/#references)\n## Footnotes\n  1. [examples/perplexity/README.md](https://github.com/ggerganov/llama.cpp/examples/perplexity/README.md) [\u21a9](https://github.com/ggerganov/llama.cpp/#user-content-fnref-1-97c613c5f775dae8f9c71fd304b459b4)\n  2. <https://huggingface.co/docs/transformers/perplexity> [\u21a9](https://github.com/ggerganov/llama.cpp/#user-content-fnref-2-97c613c5f775dae8f9c71fd304b459b4)\n  3. [RamaLama](https://github.com/containers/ramalama) [\u21a9](https://github.com/ggerganov/llama.cpp/#user-content-fnref-3-97c613c5f775dae8f9c71fd304b459b4)\n\n\n## About\nLLM inference in C/C++ \n### Topics\n[ llama ](https://github.com/topics/llama \"Topic: llama\") [ ggml ](https://github.com/topics/ggml \"Topic: ggml\")\n### Resources\n[ Readme ](https://github.com/ggerganov/llama.cpp/#readme-ov-file)\n### License\n[ MIT license ](https://github.com/ggerganov/llama.cpp/#MIT-1-ov-file)\n### Security policy\n[ Security policy ](https://github.com/ggerganov/llama.cpp/#security-ov-file)\n[ Activity](https://github.com/ggml-org/llama.cpp/activity)\n[ Custom properties](https://github.com/ggml-org/llama.cpp/custom-properties)\n### Stars\n[ **77.4k** stars](https://github.com/ggml-org/llama.cpp/stargazers)\n### Watchers\n[ **580** watching](https://github.com/ggml-org/llama.cpp/watchers)\n### Forks\n[ **11.3k** forks](https://github.com/ggml-org/llama.cpp/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fggml-org%2Fllama.cpp&report=ggml-org+%28user%29)\n##  [Releases 3,134](https://github.com/ggml-org/llama.cpp/releases)\n[ b4998 Latest  Mar 30, 2025 ](https://github.com/ggml-org/llama.cpp/releases/tag/b4998)\n[+ 3,133 releases](https://github.com/ggml-org/llama.cpp/releases)\n##  [Packages 0](https://github.com/orgs/ggml-org/packages?repo_name=llama.cpp)\n##  [Contributors 1,097](https://github.com/ggml-org/llama.cpp/graphs/contributors)\n  * [ ![@ggerganov](https://avatars.githubusercontent.com/u/1991296?s=64&v=4) ](https://github.com/ggerganov)\n  * [ ![@slaren](https://avatars.githubusercontent.com/u/2141330?s=64&v=4) ](https://github.com/slaren)\n  * [ ![@JohannesGaessler](https://avatars.githubusercontent.com/u/18492268?s=64&v=4) ](https://github.com/JohannesGaessler)\n  * [ ![@ngxson](https://avatars.githubusercontent.com/u/7702203?s=64&v=4) ](https://github.com/ngxson)\n  * [ ![@cebtenzzre](https://avatars.githubusercontent.com/u/14168726?s=64&v=4) ](https://github.com/cebtenzzre)\n  * [ ![@danbev](https://avatars.githubusercontent.com/u/432351?s=64&v=4) ](https://github.com/danbev)\n  * [ ![@ikawrakow](https://avatars.githubusercontent.com/u/48489457?s=64&v=4) ](https://github.com/ikawrakow)\n  * [ ![@Kawrakow](https://avatars.githubusercontent.com/u/31961568?s=64&v=4) ](https://github.com/Kawrakow)\n  * [ ![@compilade](https://avatars.githubusercontent.com/u/113953597?s=64&v=4) ](https://github.com/compilade)\n  * [ ![@phymbert](https://avatars.githubusercontent.com/u/5741141?s=64&v=4) ](https://github.com/phymbert)\n  * [ ![@jeffbolznv](https://avatars.githubusercontent.com/u/8260565?s=64&v=4) ](https://github.com/jeffbolznv)\n  * [ ![@ochafik](https://avatars.githubusercontent.com/u/273860?s=64&v=4) ](https://github.com/ochafik)\n  * [ ![@KerfuffleV2](https://avatars.githubusercontent.com/u/44031344?s=64&v=4) ](https://github.com/KerfuffleV2)\n  * [ ![@0cc4m](https://avatars.githubusercontent.com/u/11707594?s=64&v=4) ](https://github.com/0cc4m)\n\n\n[+ 1,083 contributors](https://github.com/ggml-org/llama.cpp/graphs/contributors)\n## Languages\n  * [ C++ 60.7% ](https://github.com/ggml-org/llama.cpp/search?l=c%2B%2B)\n  * [ C 16.7% ](https://github.com/ggml-org/llama.cpp/search?l=c)\n  * [ Cuda 6.8% ](https://github.com/ggml-org/llama.cpp/search?l=cuda)\n  * [ Python 6.4% ](https://github.com/ggml-org/llama.cpp/search?l=python)\n  * [ Objective-C 2.4% ](https://github.com/ggml-org/llama.cpp/search?l=objective-c)\n  * [ Metal 2.4% ](https://github.com/ggml-org/llama.cpp/search?l=metal)\n  * Other 4.6%\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary of llama.cpp\n\n`llama.cpp` enables efficient LLM inference in C/C++ with minimal setup. It supports various models like LLaMA and Mistral, optimized for multiple architectures (ARM, x86, CUDA). Key features include quantization for faster inference, a CLI tool for interaction, and a lightweight HTTP server for serving models.\n\n**Tags:** Generative AI, LLMs, C++, Inference\n```"
  },
  {
    "instruction": "[Skip to main content](https://arxiv.org/abs/1911.02150/#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/) > [cs](https://arxiv.org/list/cs/recent) > arXiv:1911.02150 \n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[ ![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg) ](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n## quick links\n  * [Login](https://arxiv.org/login)\n  * [Help Pages](https://info.arxiv.org/help)\n  * [About](https://info.arxiv.org/about)\n\n\n# Computer Science > Neural and Evolutionary Computing\n**arXiv:1911.02150** (cs) \n[Submitted on 6 Nov 2019]\n# Title:Fast Transformer Decoding: One Write-Head is All You Need\nAuthors:[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer,+N)\nView a PDF of the paper titled Fast Transformer Decoding: One Write-Head is All You Need, by Noam Shazeer\n[View PDF](https://arxiv.org/pdf/1911.02150)\n> Abstract:Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large \"keys\" and \"values\" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention \"heads\", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline. \nSubjects: |  Neural and Evolutionary Computing (cs.NE); Computation and Language (cs.CL); Machine Learning (cs.LG)  \n---|---  \nCite as: | [arXiv:1911.02150](https://arxiv.org/abs/1911.02150) [cs.NE]  \n(or  [arXiv:1911.02150v1](https://arxiv.org/abs/1911.02150v1) [cs.NE] for this version)   \n<https://doi.org/10.48550/arXiv.1911.02150> Focus to learn more arXiv-issued DOI via DataCite  \n## Submission history\nFrom: Noam Shazeer [[view email](https://arxiv.org/show-email/2abb0461/1911.02150)] **[v1]** Wed, 6 Nov 2019 00:19:05 UTC (10 KB) \nFull-text links:\n## Access Paper:\nView a PDF of the paper titled Fast Transformer Decoding: One Write-Head is All You Need, by Noam Shazeer\n  * [View PDF](https://arxiv.org/pdf/1911.02150)\n  * [TeX Source](https://arxiv.org/src/1911.02150)\n  * [Other Formats](https://arxiv.org/format/1911.02150)\n\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \"Rights to this article\")\nCurrent browse context: \ncs.NE\n[< prev](https://arxiv.org/prevnext?id=1911.02150&function=prev&context=cs.NE \"previous in cs.NE \\(accesskey p\\)\") |  [next >](https://arxiv.org/prevnext?id=1911.02150&function=next&context=cs.NE \"next in cs.NE \\(accesskey n\\)\")\n[new](https://arxiv.org/list/cs.NE/new) |  [recent](https://arxiv.org/list/cs.NE/recent) | [2019-11](https://arxiv.org/list/cs.NE/2019-11)\nChange to browse by: \n[cs](https://arxiv.org/abs/1911.02150?context=cs) [cs.CL](https://arxiv.org/abs/1911.02150?context=cs.CL) [cs.LG](https://arxiv.org/abs/1911.02150?context=cs.LG)\n### References & Citations\n  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1911.02150)\n  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1911.02150)\n  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:1911.02150)\n\n\n### [ 2 blog links](https://arxiv.org/tb/1911.02150)\n([what is this?](https://info.arxiv.org/help/trackback.html)) \n### [DBLP](https://dblp.uni-trier.de) - CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1911.html#abs-1911-02150 \"listing on DBLP\") | [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1911-02150 \"DBLP bibtex record\")\n[Noam Shazeer](https://dblp.uni-trier.de/search/author?author=Noam%20Shazeer \"DBLP author search\")\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citation Loading...\n## BibTeX formatted citation\n\u00d7\nloading...\nData provided by: \n### Bookmark\n[ ![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png) ](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1911.02150&description=Fast Transformer Decoding: One Write-Head is All You Need \"Bookmark on BibSonomy\") [ ![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png) ](https://reddit.com/submit?url=https://arxiv.org/abs/1911.02150&title=Fast Transformer Decoding: One Write-Head is All You Need \"Bookmark on Reddit\")\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\nConnected Papers Toggle\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\nLitmaps Toggle\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\nscite.ai Toggle\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_\nDagsHub Toggle\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\nGotitPub Toggle\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\nHuggingface Toggle\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\nLinks to Code Toggle\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\nScienceCast Toggle\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\nDemos\n# Demos\nReplicate Toggle\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\nSpaces Toggle\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\nSpaces Toggle\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\nCore recommender toggle\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n\nAbout arXivLabs \n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1911.02150) | [Disable MathJax](javascript:setMathjaxCookie\\(\\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n  * [About](https://info.arxiv.org/about)\n  * [Help](https://info.arxiv.org/help)\n\n\n  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)\n  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)\n\n\n  * [Copyright](https://info.arxiv.org/help/license/index.html)\n  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n\n  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n  * [arXiv Operational Status ](https://status.arxiv.org) Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n\n\n",
    "answer": "```markdown\n# TL;DR: Fast Transformer Decoding: One Write-Head is All You Need\n\n**Author:** Noam Shazeer  \n**Published:** 6 Nov 2019  \n**Tags:** Generative AI, LLMs, Neural Networks\n\nThis paper introduces a multi-query attention variant for Transformer models, sharing keys and values across attention heads. This approach significantly reduces memory bandwidth requirements during incremental decoding, leading to faster performance with minimal quality loss.\n```"
  },
  {
    "instruction": "  * [Skip to main content](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#content)\n  * [Skip to search](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#top-nav-search-input)\n  * [Skip to select language](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#languages-switcher-button)\n\n\n[MDN Web Docs](https://developer.mozilla.org/en-US/)Open main menu\n  * References[References](https://developer.mozilla.org/en-US/docs/Web)\n    * [Overview / Web TechnologyWeb technology reference for developers](https://developer.mozilla.org/en-US/docs/Web)\n    * [HTMLStructure of content on the web](https://developer.mozilla.org/en-US/docs/Web/HTML)\n    * [CSSCode used to describe document style](https://developer.mozilla.org/en-US/docs/Web/CSS)\n    * [JavaScriptGeneral-purpose scripting language](https://developer.mozilla.org/en-US/docs/Web/JavaScript)\n    * [HTTPProtocol for transmitting web resources](https://developer.mozilla.org/en-US/docs/Web/HTTP)\n    * [Web APIsInterfaces for building web applications](https://developer.mozilla.org/en-US/docs/Web/API)\n    * [Web ExtensionsDeveloping extensions for web browsers](https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions)\n    * [AccessibilityBuild web projects usable for all](https://developer.mozilla.org/en-US/docs/Web/Accessibility)\n    * [Web TechnologyWeb technology reference for developers](https://developer.mozilla.org/en-US/docs/Web)\n  * Learn[Learn](https://developer.mozilla.org/en-US/docs/Learn_web_development)\n    * [Overview / MDN Learning AreaLearn web development](https://developer.mozilla.org/en-US/docs/Learn_web_development)\n    * [MDN Learning AreaLearn web development](https://developer.mozilla.org/en-US/docs/Learn_web_development)\n    * [HTMLLearn to structure web content with HTML](https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Structuring_content)\n    * [CSSLearn to style content using CSS](https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Styling_basics)\n    * [JavaScriptLearn to run scripts in the browser](https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Scripting)\n    * [AccessibilityLearn to make the web accessible to all](https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Accessibility)\n  * Plus[Plus](https://developer.mozilla.org/en-US/plus)\n    * [OverviewA customized MDN experience](https://developer.mozilla.org/en-US/plus)\n    * [AI HelpGet real-time assistance and support](https://developer.mozilla.org/en-US/plus/ai-help)\n    * [UpdatesAll browser compatibility updates at a glance](https://developer.mozilla.org/en-US/plus/updates)\n    * [DocumentationLearn how to use MDN Plus](https://developer.mozilla.org/en-US/plus/docs/features/overview)\n    * [FAQFrequently asked questions about MDN Plus](https://developer.mozilla.org/en-US/plus/docs/faq)\n  * [Curriculum New](https://developer.mozilla.org/en-US/curriculum/)\n  * [Blog](https://developer.mozilla.org/en-US/blog/)\n  * Tools\n    * [PlaygroundWrite, test and share your code](https://developer.mozilla.org/en-US/play)\n    * [HTTP ObservatoryScan a website for free](https://developer.mozilla.org/en-US/observatory)\n    * [AI HelpGet real-time assistance and support](https://developer.mozilla.org/en-US/plus/ai-help)\n\n\nSearch MDNClear search inputSearch\nTheme\n  * [Log in](https://developer.mozilla.org/users/fxa/login/authenticate/?next=%2Fen-US%2Fdocs%2FWeb%2FMedia%2FGuides%2FFormats%2FVideo_codecs)\n  * [Sign up for free](https://developer.mozilla.org/users/fxa/login/authenticate/?next=%2Fen-US%2Fdocs%2FWeb%2FMedia%2FGuides%2FFormats%2FVideo_codecs)\n\n\n  1. [References](https://developer.mozilla.org/en-US/docs/Web)\n  2. [Media](https://developer.mozilla.org/en-US/docs/Web/Media)\n  3. [Guides](https://developer.mozilla.org/en-US/docs/Web/Media/Guides)\n  4. [Media types and formats for image, audio, and video content](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats)\n  5. [Web video codec guide](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs)\n\n\nArticle Actions\n  * English (US)\n    * Remember language[](https://github.com/orgs/mdn/discussions/739 \"Enable this setting to automatically switch to this language when it's available. \\(Click to learn more.\\)\")\nIs this useful?This feature is useful.This feature is not useful.\n    * [Deutsch](https://developer.mozilla.org/de/docs/Web/Media/Guides/Formats/Video_codecs)\n    * [\u65e5\u672c\u8a9e](https://developer.mozilla.org/ja/docs/Web/Media/Guides/Formats/Video_codecs)\n    * [\ud55c\uad6d\uc5b4](https://developer.mozilla.org/ko/docs/Web/Media/Guides/Formats/Video_codecs)\n    * [\u4e2d\u6587 (\u7b80\u4f53)](https://developer.mozilla.org/zh-CN/docs/Web/Media/Guides/Formats/Video_codecs)\n\n\nFilter sidebarClear filter input\n## In this article\n  * [Common codecs](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#common_codecs)\n  * [Factors affecting the encoded video](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#factors_affecting_the_encoded_video)\n  * [Compression artifacts](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#compression_artifacts)\n  * [Codec details](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#codec_details)\n  * [Choosing a video codec](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#choosing_a_video_codec)\n  * [See also](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#see_also)\n\n\n  1. [Media](https://developer.mozilla.org/en-US/docs/Web/Media)\n  2. [Guides](https://developer.mozilla.org/en-US/docs/Web/Media/Guides)\n  3. [Using images in HTML](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Images)\n  4. [Audio and video manipulation](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Audio_and_video_manipulation)\n  5. [Autoplay guide for media and Web Audio APIs](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Autoplay)\n  6. [Streaming audio and video](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Streaming)\n  7. [Audio and video delivery](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Audio_and_video_delivery)\n    1. [Adding captions and subtitles to HTML video](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Audio_and_video_delivery/Adding_captions_and_subtitles_to_HTML5_video)\n    2. [Creating a cross-browser video player](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Audio_and_video_delivery/cross_browser_video_player)\n    3. [Cross-browser audio basics](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Audio_and_video_delivery/Cross-browser_audio_basics)\n    4. [Livestreaming web audio and video](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Audio_and_video_delivery/Live_streaming_web_audio_and_video)\n    5. [Media buffering, seeking, and time ranges](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Audio_and_video_delivery/buffering_seeking_time_ranges)\n    6. [Setting up adaptive streaming media sources](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Audio_and_video_delivery/Setting_up_adaptive_streaming_media_sources)\n    7. [Video player styling basics](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Audio_and_video_delivery/Video_player_styling_basics)\n    8. [Web audio playbackRate explained](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Audio_and_video_delivery/WebAudio_playbackRate_explained)\n  8. [Media formats](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats)\n    1. [Codecs in common media types](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/codecs_parameter)\n    2. [Codecs used by WebRTC](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/WebRTC_codecs)\n    3. [Configuring servers for Ogg media](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Configuring_servers_for_Ogg_media)\n    4. [Digital audio concepts](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Audio_concepts)\n    5. [Digital video concepts](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_concepts)\n    6. [Handling unsupported media](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Support_issues)\n    7. [Image file type and format guide](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Image_types)\n    8. [Media container formats (file types)](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers)\n    9. [Web audio codec guide](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Audio_codecs)\n    10.  _[Web video codec guide](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs)_\n\n\n## In this article\n  * [Common codecs](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#common_codecs)\n  * [Factors affecting the encoded video](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#factors_affecting_the_encoded_video)\n  * [Compression artifacts](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#compression_artifacts)\n  * [Codec details](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#codec_details)\n  * [Choosing a video codec](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#choosing_a_video_codec)\n  * [See also](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#see_also)\n\n\n# Web video codec guide\nThis guide introduces the video codecs you're most likely to encounter or consider using on the web, summaries of their capabilities and any compatibility and utility concerns, and advice to help you choose the right codec for your project's video.\nDue to the sheer size of uncompressed video data, it's necessary to compress it significantly in order to store it, let alone transmit it over a network. Imagine the amount of data needed to store uncompressed video:\n  * A single frame of high definition (1920x1080) video in full color (4 bytes per pixel) is 8,294,400 bytes.\n  * At a typical 30 frames per second, each second of HD video would occupy 248,832,000 bytes (~249 MB).\n  * A minute of HD video would need 14.93 GB of storage.\n  * A fairly typical 30 minute video conference would need about 447.9 GB of storage, and a 2-hour movie would take _almost 1.79**TB** (that is, 1790 GB)_.\n\n\nNot only is the required storage space enormous, but the network bandwidth needed to transmit an uncompressed video like that would be enormous, at 249 MB/sec\u2014not including audio and overhead. This is where video codecs come in. Just as audio codecs do for the sound data, video codecs compress the video data and encode it into a format that can later be decoded and played back or edited.\nMost video codecs are **lossy** , in that the decoded video does not precisely match the source. Some details may be lost; the amount of loss depends on the codec and how it's configured, but as a general rule, the more compression you achieve, the more loss of detail and fidelity will occur. Some lossless codecs do exist, but they are typically used for archival and storage for local playback rather than for use on a network.\n## [Common codecs](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#common_codecs)\nThe following video codecs are those which are most commonly used on the web. For each codec, the containers (file types) that can support them are also listed. Each codec provides a link to a section below which offers additional details about the codec, including specific capabilities and compatibility issues you may need to be aware of.\nCodec name (short) | Full codec name | Container support  \n---|---|---  \n[AV1](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#av1) | AOMedia Video 1 |  [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4), [WebM](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#webm)  \n[AVC (H.264)](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#avc_h.264) | Advanced Video Coding |  [3GP](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#3gp), [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4)  \n[H.263](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#h.263) | H.263 Video | [3GP](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#3gp)  \n[HEVC (H.265)](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#hevc_h.265) | High Efficiency Video Coding | [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4)  \n[MP4V-ES](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#mp4v-es) | MPEG-4 Video Elemental Stream |  [3GP](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#3gp), [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4)  \n[MPEG-1](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#mpeg-1_part_2_video) | MPEG-1 Part 2 Visual |  [MPEG](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpegmpeg-2), [QuickTime](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#quicktime)  \n[MPEG-2](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#mpeg-2_part_2_video) | MPEG-2 Part 2 Visual |  [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4), [MPEG](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpegmpeg-2), [QuickTime](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#quicktime)  \n[Theora](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#theora) Deprecated | Theora | [Ogg](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#ogg)  \n[VP8](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#vp8) | Video Processor 8 |  [3GP](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#3gp), [Ogg](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#ogg), [WebM](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#webm)  \n[VP9](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#vp9) | Video Processor 9 |  [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4), [Ogg](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#ogg), [WebM](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#webm)  \n## [Factors affecting the encoded video](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#factors_affecting_the_encoded_video)\nAs is the case with any encoder, there are two basic groups of factors affecting the size and quality of the encoded video: specifics about the source video's format and contents, and the characteristics and configuration of the codec used while encoding the video.\nThe simplest guideline is this: anything that makes the encoded video look more like the original, uncompressed, video will generally make the resulting data larger as well. Thus, it's always a tradeoff of size versus quality. In some situations, a greater sacrifice of quality in order to bring down the data size is worth that lost quality; other times, the loss of quality is unacceptable and it's necessary to accept a codec configuration that results in a correspondingly larger file.\n### [Effect of source video format on encoded output](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#effect_of_source_video_format_on_encoded_output)\nThe degree to which the format of the source video will affect the output varies depending on the codec and how it works. If the codec converts the media into an internal pixel format, or otherwise represents the image using a means other than simple pixels, the format of the original image doesn't make any difference. However, things such as frame rate and, obviously, resolution will always have an impact on the output size of the media.\nAdditionally, all codecs have their strengths and weaknesses. Some have trouble with specific kinds of shapes and patterns, or aren't good at replicating sharp edges, or tend to lose detail in dark areas, or any number of possibilities. It all depends on the underlying algorithms and mathematics.\nThe potential effect of source video format and contents on the encoded video quality and size  Feature | Effect on quality | Effect on size  \n---|---|---  \nColor depth (bit depth) |  The higher the color bit depth, the higher the quality of color fidelity is achieved in the video. Additionally, in saturated portions of the image (that is, where colors are pure and intense, such as a bright, pure red: `rgb(255 0 0 / 100%)`), color depths below 10 bits per component (10-bit color) allow banding, where gradients cannot be represented without visible stepping of the colors.  |  Depending on the codec, higher color depths may result in larger compressed file sizes. The determining factor is what internal storage format is used for the compressed data.   \nFrame rate |  Primarily affects the perceived smoothness of the motion in the image. To a point, the higher the frame rate, the smoother and more realistic the motion will appear. Eventually the point of diminishing returns is reached. See [Frame rate](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#reduced_frame_rate) below for details.  |  Assuming the frame rate is not reduced during encoding, higher frame rates cause larger compressed video sizes.   \nMotion |  Compression of video typically works by comparing frames, finding where they differ, and constructing records containing enough information to update the previous frame to approximate the appearance of the following frame. The more successive frames differ from one another, the larger these differences are, and the less effective the compression is at avoiding the introduction of artifacts into the compressed video.  |  The complexity introduced by motion results in larger intermediate frames due to the higher number of differences between frames). For this and other reasons, the more motion there is in a video, the larger the output file will typically be.   \nNoise |  Picture noise (such as film grain effects, dust, or other grittiness to the image) introduces variability. Variability generally makes compression more difficult, resulting in more lost quality due to the need to drop details to achieve the same level of compression.  |  The more variability\u2014such as noise\u2014there is in the image, the more complex the compression process and the less success the algorithm is likely to have compressing the image to the same degree. Unless you configure the encoder in a way that ignores some or all of the variations caused by noise, the compressed video will be larger.   \nResolution (width and height) |  Higher resolution video, presented in the same screen size, will typically be able to more accurately portray the original scene, barring effects introduced during compression.  |  The higher the resolution of a video, the larger it gets. This plays a key role in the final size of the video.   \nThe degree to which these affect the resulting encoded video will vary depending on the precise details of the situation, including which encoder you use and how it's configured. In addition to general codec options, the encoder could be configured to reduce the frame rate, to clean up noise, and/or to reduce the overall resolution of the video during encoding.\n### [Effect of codec configuration on encoded output](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#effect_of_codec_configuration_on_encoded_output)\nThe algorithms used do encode video typically use one or more of a number of general techniques to perform their encoding. Generally speaking, any configuration option that is intended to reduce the output size of the video will probably have a negative impact on the overall quality of the video, or will introduce certain types of artifacts into the video. It's also possible to select a lossless form of encoding, which will result in a much larger encoded file but with perfect reproduction of the original video upon decoding.\nIn addition, each encoder utility may have variations in how they process the source video, resulting in differences in the output quality and/or size.\nVideo encoder configuration effects on quality and size  Feature | Effect on quality | Effect on size  \n---|---|---  \nLossless compression | No loss of quality |  Lossless compression cannot reduce the overall video size nearly as much as lossy compression; the resulting files are likely to still be too large for general usage.   \nLossy compression |  To some degree, artifacts and other forms of quality degradation will occur, depending on the specific codec and how much compression is being applied  |  The more the encoded video is allowed to deviate from the source, the easier it is to accomplish higher compression rates   \nQuality setting |  The higher the quality configuration, the more like the original media the encoded video will look  |  In general, higher quality settings will result in larger encoded video files; the degree to which this is true varies depending on the codec   \nBit rate | Quality generally improves with higher bit rates | Higher bit rates inherently lead to larger output files  \nThe options available when encoding video, and the values to be assigned to those options, will vary not only from one codec to another but depending on the encoding software you use. The documentation included with your encoding software will help you to understand the specific impact of these options on the encoded video.\n## [Compression artifacts](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#compression_artifacts)\n**Artifacts** are side effects of a lossy encoding process in which the lost or rearranged data results in visibly negative effects. Once an artifact has appeared, it may linger for a while, because of how video is displayed. Each frame of video is presented by applying a set of changes to the currently-visible frame. This means that any errors or artifacts will compound over time, resulting in glitches or otherwise strange or unexpected deviations in the image that linger for a time.\nTo resolve this, and to improve seek time through the video data, periodic **key frames** (also known as **intra-frames** or **i-frames**) are placed into the video file. The key frames are full frames, which are used to repair any damage or artifact residue that's currently visible.\n### [Aliasing](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#aliasing)\nAliasing is a general term for anything that upon being reconstructed from the encoded data does not look the same as it did before compression. There are many forms of aliasing; the most common ones you may see include:\n#### Moir\u00e9 patterns\nA [**Moir\u00e9 pattern**](https://en.wikipedia.org/wiki/Moir%C3%A9_pattern) is a large-scale spatial interference pattern produced when a pattern in the source image and the manner in which the encoder operates are slightly out of alignment spatially. The artifacts generated by the encoder then introduce strange, swirling effects in the source image's pattern upon decoding.  |  ![a brick wall showing swirling effect similar to waves due to the moire pattern](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs/moire-pattern.jpg)  \n---|---  \n#### Staircase effect\nThe **staircase effect** is a spatial artifact that occurs when diagonal straight or curved edges that should be smooth take on a jagged appearance, looking somewhat like a set of stair steps. This is the effect that is being reduced by \"anti-aliasing\" filters.  |  ![Photo of diagonal lines that look like a staircase due to aliasing causing a staircase effect](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs/staircase-effect.jpg)  \n#### Wagon-wheel effect\nThe **wagon-wheel effect** (or **[stroboscopic effect](https://en.wikipedia.org/wiki/Stroboscopic_effect)**) is the visual effect that's commonly seen in film, in which a turning wheel appears to rotate at the wrong speed, or even in reverse, due to an interaction between the frame rate and the compression algorithm. The same effect can occur with any repeating pattern that moves, such as the ties on a railway line, posts along the side of a road, and so forth. This is a temporal (time-based) aliasing issue; the speed of the rotation interferes with the frequency of the sampling performed during compression or encoding.  |  ![Turning wheel due to aliasing causing a wagon wheel effect.](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs/stroboscopic-effect.gif)  \n### [Color edging](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#color_edging)\n**Color edging** is a type of visual artifact that presents as spurious colors introduced along the edges of colored objects within the scene. These colors have no intentional color relationship to the contents of the frame.\n### [Loss of sharpness](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#loss_of_sharpness)\nThe act of removing data in the process of encoding video requires that some details be lost. If enough compression is applied, parts or potentially all of the image could lose sharpness, resulting in a slightly fuzzy or hazy appearance.\nLost sharpness can make text in the image difficult to read, as text\u2014especially small text\u2014is very detail-oriented content, where minor alterations can significantly impact legibility.\n### [Ringing](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#ringing)\nLossy compression algorithms can introduce **[ringing](https://en.wikipedia.org/wiki/Ringing_artifacts)** , an effect where areas outside an object are contaminated with colored pixels generated by the compression algorithm. This happens when an algorithm that uses blocks that span across a sharp boundary between an object and its background. This is particularly common at higher compression levels.\n![Example of the ringing effect](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs/ringing-effects.png)\nNote the blue and pink fringes around the edges of the star above (as well as the stepping and other significant compression artifacts). Those fringes are the ringing effect. Ringing is similar in some respects to [mosquito noise](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#mosquito_noise), except that while the ringing effect is more or less steady and unchanging, mosquito noise shimmers and moves.\nRinging is another type of artifact that can make it particularly difficult to read text contained in your images.\n### [Posterizing](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#posterizing)\n**Posterization** occurs when the compression results in the loss of color detail in gradients. Instead of smooth transitions through the various colors in a region, the image becomes blocky, with blobs of color that approximate the original appearance of the image.\n![Bald eagle photo with blotchy resolution.](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs/posterize-effect.jpg)\nNote the blockiness of the colors in the plumage of the bald eagle in the photo above (and the snowy owl in the background). The details of the feathers is largely lost due to these posterization artifacts.\n### [Contouring](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#contouring)\n**Contouring** or **color banding** is a specific form of posterization in which the color blocks form bands or stripes in the image. This occurs when the video is encoded with too coarse a quantization configuration. As a result, the video's contents show a \"layered\" look, where instead of smooth gradients and transitions, the transitions from color to color are abrupt, causing strips of color to appear.\n![Example of an image whose compression has introduced contouring](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs/contouring-effect.jpg)\nIn the example image above, note how the sky has bands of different shades of blue, instead of being a consistent gradient as the sky color changes toward the horizon. This is the contouring effect.\n### [Mosquito noise](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#mosquito_noise)\n**Mosquito noise** is a temporal artifact which presents as noise or **edge busyness** that appears as a flickering haziness or shimmering that roughly follows outside the edges of objects with hard edges or sharp transitions between foreground objects and the background. The effect can be similar in appearance to [ringing](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#ringing).\n![Example of an image whose compression has introduced mosquito noise.](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs/mosquito-effect-sm.png)\nThe photo above shows mosquito noise in a number of places, including in the sky surrounding the bridge. In the upper-right corner, an inset shows a close-up of a portion of the image that exhibits mosquito noise.\nMosquito noise artifacts are most commonly found in MPEG video, but can occur whenever a discrete cosine transform (DCT) algorithm is used; this includes, for example, JPEG still images.\n### [Motion compensation block boundary artifacts](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#motion_compensation_block_boundary_artifacts)\nCompression of video generally works by comparing two frames and recording the differences between them, one frame after another, until the end of the video. This technique works well when the camera is fixed in place, or the objects in the frame are relatively stationary, but if there is a great deal of motion in the frame, the number of differences between frames can be so great that compression doesn't do any good.\n**[Motion compensation](https://en.wikipedia.org/wiki/Motion_compensation)** is a technique that looks for motion (either of the camera or of objects in the frame of view) and determines how many pixels the moving object has moved in each direction. Then that shift is stored, along with a description of the pixels that have moved that can't be described just by that shift. In essence, the encoder finds the moving objects, then builds an internal frame of sorts that looks like the original but with all the objects translated to their new locations. In theory, this approximates the new frame's appearance. Then, to finish the job, the remaining differences are found, then the set of object shifts and the set of pixel differences are stored in the data representing the new frame. This object that describes the shift and the pixel differences is called a **residual frame**.\nOriginal frame | Inter-frame differences |  Difference after motion compensation   \n---|---|---  \n![Original frame of video](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs/motion-comp-orig.jpg) | ![Differences between the first frame and the following frame.](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs/motion-comp-diff.jpg) |  ![Differences between the frames after shifting two pixels right](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs/motion-comp-compensated.jpg)  \nThe first full frame as seen by the viewer.  |  Here, only the differences between the first frame and the following frame are seen. Everything else is black. Looking closely, we can see that the majority of these differences come from a horizontal camera move, making this a good candidate for motion compensation.  |  To minimize the number of pixels that are different, here we take into account the panning of the camera by first shifting the first frame to the right by two pixels, then by taking the difference. This compensates for the panning of the camera, allowing for more overlap between the two frames.   \nImages from [Wikipedia](https://en.wikipedia.org/wiki/Motion_compensation#Illustrated_example)  \nThere are two general types of motion compensation: **global motion compensation** and **block motion compensation**. Global motion compensation generally adjusts for camera movements such as tracking, dolly movements, panning, tilting, rolling, and up and down movements. Block motion compensation handles localized changes, looking for smaller sections of the image that can be encoded using motion compensation. These blocks are normally of a fixed size, in a grid, but there are forms of motion compensation that allow for variable block sizes, and even for blocks to overlap.\nThere are, however, artifacts that can occur due to motion compensation. These occur along block borders, in the form of sharp edges that produce false ringing and other edge effects. These are due to the mathematics involved in the coding of the residual frames, and can be easily noticed before being repaired by the next key frame.\n### [Reduced frame size](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#reduced_frame_size)\nIn certain situations, it may be useful to reduce the video's dimensions in order to improve the final size of the video file. While the immediate loss of size or smoothness of playback may be a negative factor, careful decision-making can result in a good end result. If a 1080p video is reduced to 720p prior to encoding, the resulting video can be much smaller while having much higher visual quality; even after scaling back up during playback, the result may be better than encoding the original video at full size and accepting the quality hit needed to meet your size requirements.\n### [Reduced frame rate](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#reduced_frame_rate)\nSimilarly, you can remove frames from the video entirely and decrease the frame rate to compensate. This has two benefits: it makes the overall video smaller, and that smaller size allows motion compensation to accomplish even more for you. For example, instead of computing motion differences for two frames that are two pixels apart due to inter-frame motion, skipping every other frame could lead to computing a difference that comes out to four pixels of movement. This lets the overall movement of the camera be represented by fewer residual frames.\nThe absolute minimum frame rate that a video can be before its contents are no longer perceived as motion by the human eye is about 12 frames per second. Less than that, and the video becomes a series of still images. Motion picture film is typically 24 frames per second, while standard definition television is about 30 frames per second (slightly less, but close enough) and high definition television is between 24 and 60 frames per second. Anything from 24 FPS upward will generally be seen as satisfactorily smooth; 30 or 60 FPS is an ideal target, depending on your needs.\nIn the end, the decisions about what sacrifices you're able to make are entirely up to you and/or your design team.\n## [Codec details](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#codec_details)\n### [AV1](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#av1)\nThe **AOMedia Video 1** (**AV1**) codec is an open format designed by the [Alliance for Open Media](https://aomedia.org/) specifically for internet video. It achieves higher data compression rates than [VP9](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#vp9) and [H.265/HEVC](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#hevc_h.265), and as much as 50% higher rates than [AVC](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#avc_h.264). AV1 is fully royalty-free and is designed for use by both the [`<video>`](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video) element and by [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API).\nAV1 currently offers three profiles: **main** , **high** , and **professional** with increasing support for color depths and chroma subsampling. In addition, a series of **levels** are specified, each defining limits on a range of attributes of the video. These attributes include frame dimensions, image area in pixels, display and decode rates, average and maximum bit rates, and limits on the number of tiles and tile columns used in the encoding/decoding process.\nFor example, AV1 level 2.0 offers a maximum frame width of 2048 pixels and a maximum height of 1152 pixels, but its maximum frame size in pixels is 147,456, so you can't actually have a 2048x1152 video at level 2.0. It's worth noting, however, that at least for Firefox and Chrome, the levels are actually ignored at this time when performing software decoding, and the decoder just does the best it can to play the video given the settings provided. For compatibility's sake going forward, however, you should stay within the limits of the level you choose.\nThe primary drawback to AV1 at this time is that it is very new, and support is still in the process of being integrated into most browsers. Additionally, encoders and decoders are still being optimized for performance, and hardware encoders and decoders are still mostly in development rather than production. For this reason, encoding a video into AV1 format takes a very long time, since all the work is done in software.\nFor the time being, because of these factors, AV1 is not yet ready to be your first choice of video codec, but you should watch for it to be ready to use in the future.\nSupported bit rates |  Varies depending on the video's level; theoretical maximum reaches 800 Mbps at level 6.3  See the AV1 specification's [tables of levels](https://aomediacodec.github.io/av1-spec/#levels), which describe the maximum resolutions and rates at each level.   \n---|---  \nSupported frame rates |  Varies by level; for example, level 2.0 has a maximum of 30 FPS while level 6.3 can reach 120 FPS   \nCompression |  Lossy [DCT-based algorithm](https://en.wikipedia.org/wiki/Discrete_cosine_transform)  \nSupported frame sizes |  8 x 8 pixels to 65,535 x 65535 pixels with each dimension allowed to take any value between these   \nSupported color modes |  | Profile | Color depths | Chroma subsampling  \n---|---|---  \nMain | 8 or 10 | 4:0:0 (greyscale) or 4:2:0  \nHigh | 8 or 10 | 4:0:0 (greyscale), 4:2:0, or 4:4:4  \nProfessional | 8, 10, or 12 | 4:0:0 (greyscale), 4:2:0, 4:2:2, or 4:4:4  \nHDR support | Yes  \nVariable Frame Rate (VFR) support | Yes  \nBrowser compatibility |  | Feature | Chrome | Edge | Firefox | Firefox Android | Opera | Safari  \n---|---|---|---|---|---|---  \nAV1 support | 70 | 75 | 67 | 113 | 57 | 17  \nContainer support |  [ISOBMFF](https://en.wikipedia.org/wiki/ISO/IEC_base_media_file_format), MPEG-TS, [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4), [WebM](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#webm)  \n[RTP](https://developer.mozilla.org/en-US/docs/Glossary/RTP) / [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API) compatible  | Yes  \nSupporting/Maintaining organization | [Alliance for Open Media](https://aomedia.org/)  \nSpecification |  <https://aomediacodec.github.io/av1-spec/av1-spec.pdf>  \nLicensing | Royalty-free, open standard  \n### [AVC (H.264)](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#avc_h.264)\nThe MPEG-4 specification suite's **Advanced Video Coding** (**AVC**) standard is specified by the identical ITU H.264 specification and the MPEG-4 Part 10 specification. It's a motion compensation based codec that is widely used today for all sorts of media, including broadcast television, [RTP](https://developer.mozilla.org/en-US/docs/Glossary/RTP) videoconferencing, and as the video codec for Blu-Ray discs.\nAVC is highly flexible, with a number of profiles with varying capabilities; for example, the Constrained Baseline Profile is designed for use in videoconferencing and mobile scenarios, using less bandwidth than the Main Profile (which is used for standard definition digital TV in some regions) or the High Profile (used for Blu-Ray Disc video). Most of the profiles use 8-bit color components and 4:2:0 chroma subsampling; The High 10 Profile adds support for 10-bit color, and advanced forms of High 10 add 4:2:2 and 4:4:4 chroma subsampling.\nAVC also has special features such as support for multiple views of the same scene (Multiview Video Coding), which allows, among other things, the production of stereoscopic video.\nAVC is a proprietary format, however, and numerous patents are owned by multiple parties regarding its technologies. Commercial use of AVC media requires a license, though the Via LA patent pool does not require license fees for streaming internet video in AVC format as long as the video is free for end users.\nNon-web browser implementations of WebRTC (any implementation which doesn't include the JavaScript APIs) are _required_ to support AVC as a codec in WebRTC calls. While web browsers are not required to do so, some do.\nIn HTML content for web browsers, AVC is broadly compatible and many platforms support hardware encoding and decoding of AVC media. However, be aware of its [licensing requirements](https://www.via-la.com/licensing-2/avc-h-264/) before choosing to use AVC in your project!\nSupported bit rates | Varies by level  \n---|---  \nSupported frame rates | Varies by level; up to 300 FPS is possible  \nCompression |  Lossy [DCT-based algorithm](https://en.wikipedia.org/wiki/Discrete_cosine_transform), though it's possible to create lossless macroblocks within the image   \nSupported frame sizes | Up to 8,192 x 4,320 pixels  \nSupported color modes |  Some of the more common or interesting profiles: | Profile | Color depths | Chroma subsampling  \n---|---|---  \nConstrained Baseline (CBP) | 8 | 4:2:0  \nBaseline (BP) | 8 | 4:2:0  \nExtended (XP) | 8 | 4:2:0  \nMain (MP) | 8 | 4:2:0  \nHigh (HiP) | 8 | 4:0:0 (greyscale) and 4:2:0  \nProgressive High (ProHiP) | 8 | 4:0:0 (greyscale) and 4:2:0  \nHigh 10 (Hi10P) | 8 to 10 | 4:0:0 (greyscale) and 4:2:0  \nHigh 4:2:2 (Hi422P) | 8 to 10 | 4:0:0 (greyscale), 4:2:0, and 4:2:2  \nHigh 4:4:4 Predictive | 8 to 14 | 4:0:0 (greyscale), 4:2:0, 4:2:2, and 4:4:4  \nHDR support |  Yes; [Hybrid Log-Gamma](https://en.wikipedia.org/wiki/Hybrid_Log-Gamma) or Advanced HDR/SL-HDR; both are part of ATSC   \nVariable Frame Rate (VFR) support | Yes  \nBrowser compatibility |  All versions of Chrome, Edge, Firefox, Opera, and Safari  Firefox support for AVC is dependent upon the operating system's built-in or preinstalled codecs for AVC and its container in order to avoid patent concerns.   \nContainer support |  [3GP](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#3gp), [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4)  \n[RTP](https://developer.mozilla.org/en-US/docs/Glossary/RTP) / [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API) compatible  | Yes  \nSupporting/Maintaining organization |  [MPEG](https://mpeg.chiariglione.org/) / [ITU](https://www.itu.int/)  \nSpecification |  <https://mpeg.chiariglione.org/standards/mpeg-4/advanced-video-coding.html><https://www.itu.int/rec/T-REC-H.264>  \nLicensing |  Proprietary with numerous patents. Commercial use [requires a license](https://www.via-la.com/licensing-2/avc-h-264/). Note that multiple patent pools may apply.   \n### [H.263](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#h.263)\nITU's **H.263** codec was designed primarily for use in low-bandwidth situations. In particular, its focus is for video conferencing on PSTN (Public Switched Telephone Networks), [RTSP](https://developer.mozilla.org/en-US/docs/Glossary/RTSP), and SIP (IP-based videoconferencing) systems. Despite being optimized for low-bandwidth networks, it is fairly CPU intensive and may not perform adequately on lower-end computers. The data format is similar to that of MPEG-4 Part 2.\nH.263 has never been widely used on the web. Variations on H.263 have been used as the basis for other proprietary formats, such as Flash video or the Sorenson codec. However, no major browser has ever included H.263 support by default. Certain media plugins have enabled support for H.263 media.\nUnlike most codecs, H.263 defines fundamentals of an encoded video in terms of the maximum bit rate per frame (picture), or **BPPmaxKb**. During encoding, a value is selected for BPPmaxKb, and then the video cannot exceed this value for each frame. The final bit rate will depend on this, the frame rate, the compression, and the chosen resolution and block format.\nH.263 has been superseded by H.264 and is therefore considered a legacy media format which you generally should avoid using if you can. The only real reason to use H.263 in new projects is if you require support on very old devices on which H.263 is your best choice.\nH.263 is a proprietary format, with [patents](https://www.itu.int/ITU-T/recommendations/related_ps.aspx?id_prod=4242) held by a number of organizations and companies, including Telenor, Fujitsu, Motorola, Samsung, Hitachi, Polycom, Qualcomm, and so on. To use H.263, you are legally obligated to obtain the appropriate licenses.\nSupported bit rates | Unrestricted, but typically below 64 kbps  \n---|---  \nSupported frame rates | Any  \nCompression |  Lossy [DCT-based algorithm](https://en.wikipedia.org/wiki/Discrete_cosine_transform)  \nSupported frame sizes |  Up to 1408 x 1152 pixels. Version 1 of H.263 specifies a set of picture sizes which are supported. Later versions may support additional resolutions.   \nSupported color modes |  YCbCr; each picture format (sub-QCIF, QCIF, CIF, 4CIF, or 16CIF) defines the frame size in pixels as well as how many rows each of luminance and chrominance samples are used for each frame   \nHDR support | No  \nVariable Frame Rate (VFR) support | No  \nBrowser compatibility |  | Feature | Chrome | Edge | Firefox | Opera | Safari  \n---|---|---|---|---|---  \nH.263 support | No | No | No | No | No  \nContainer support |  [3GP](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#3gp), [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4), [QuickTime](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#quicktime)  \n[RTP](https://developer.mozilla.org/en-US/docs/Glossary/RTP) / [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API) compatible  | No  \nSupporting/Maintaining organization | [ITU](https://www.itu.int/)  \nSpecification |  <https://www.itu.int/rec/T-REC-H.263/>  \nLicensing |  Proprietary; appropriate license or licenses are required. Note that multiple patent pools may apply.   \n### [HEVC (H.265)](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#hevc_h.265)\nThe **[High Efficiency Video Coding](http://hevc.info/)** (**HEVC**) codec is defined by ITU's **H.265** as well as by MPEG-H Part 2 (the still in-development follow-up to MPEG-4). HEVC was designed to support efficient encoding and decoding of video in sizes including very high resolutions (including 8K video), with a structure specifically designed to let software take advantage of modern processors. Theoretically, HEVC can achieve compressed file sizes half that of [AVC](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#avc_h.264) but with comparable image quality.\nFor example, each coding tree unit (CTU)\u2014similar to the macroblock used in previous codecs\u2014consists of a tree of luma values for each sample as well as a tree of chroma values for each chroma sample used in the same coding tree unit, as well as any required syntax elements. This structure supports easy processing by multiple cores.\nAn interesting feature of HEVC is that the main profile supports only 8-bit per component color with 4:2:0 chroma subsampling. Also interesting is that 4:4:4 video is handled specially. Instead of having the luma samples (representing the image's pixels in grayscale) and the Cb and Cr samples (indicating how to alter the grays to create color pixels), the three channels are instead treated as three monochrome images, one for each color, which are then combined during rendering to produce a full-color image.\nHEVC is a proprietary format and is covered by a number of patents. Licensing is [managed by Via LA](https://www.via-la.com/licensing-2/hevc-vvc/); fees are charged to developers rather than to content producers and distributors. Be sure to review the latest license terms and requirements before making a decision on whether or not to use HEVC in your app or website!\nSupported bit rates | Up to 800,000 kbps  \n---|---  \nSupported frame rates | Varies by level; up to 300 FPS is possible  \nCompression |  Lossy [DCT-based algorithm](https://en.wikipedia.org/wiki/Discrete_cosine_transform)  \nSupported frame sizes | 128 x 96 to 8,192 x 4,320 pixels; varies by profile and level  \nSupported color modes |  Information below is provided for the major profiles. There are a number of other profiles available that are not included here.  | Profile | Color depths | Chroma subsampling  \n---|---|---  \nMain | 8 | 4:2:0  \nMain 10 | 8 to 10 | 4:2:0  \nMain 12 | 8 to 12 | 4:0:0 and 4:2:0  \nMain 4:2:2 10 | 8 to 10 | 4:0:0, 4:2:0, and 4:2:2  \nMain 4:2:2 12 | 8 to 12 | 4:0:0, 4:2:0, and 4:2:2  \nMain 4:4:4 | 8 | 4:0:0, 4:2:0, 4:2:2, and 4:4:4  \nMain 4:4:4 10 | 8 to 10 | 4:0:0, 4:2:0, 4:2:2, and 4:4:4  \nMain 4:4:4 12 | 8 to 12 | 4:0:0, 4:2:0, 4:2:2, and 4:4:4  \nMain 4:4:4 16 Intra | 8 to 16 | 4:0:0, 4:2:0, 4:2:2, and 4:4:4  \nHDR support | Yes  \nVariable Frame Rate (VFR) support | Yes  \nBrowser compatibility |  | Feature | Chrome | Edge | Firefox | Opera | Safari  \n---|---|---|---|---|---  \nHEVC / H.265 support | 107 | 18 | 120 | 94 | 11  \nChrome support HEVC for devices with hardware support on Windows 8+, Linux and ChromeOS, for all devices on macOS Big Sur 11+ and Android 5.0+.\nEdge (Chromium) supports HEVC for devices with hardware support on Windows 10 1709+ when [HEVC video extensions from the Microsoft Store](https://apps.microsoft.com/detail/9nmzlz57r3t7) is installed, and has the same support status as Chrome on other platforms. Edge (Legacy) only supports HEVC for devices with a hardware decoder. \nFirefox enables HEVC on: \n  * Windows from Firefox 134 using either hardware (on devices that support it, where the range is the same as Edge) or software (on Windows the user must pay for and install an extension)\n  * macOS from Firefox 136 using either hardware or software.\n  * Linux from Firefox 137 using either hardware or software (via the system ffmpeg).\n  * Android from Firefox 137 using hardware only.\n\n\nOpera and other Chromium based browsers have the same support status as Chrome.\nSafari supports HEVC for all devices on macOS High Sierra or later.  \nContainer support |  [ISOBMFF](https://en.wikipedia.org/wiki/ISO/IEC_base_media_file_format), MPEG-TS, [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4) [QuickTime](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#quicktime)  \n[RTP](https://developer.mozilla.org/en-US/docs/Glossary/RTP) / [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API) compatible  | No  \nSupporting/Maintaining organization |  [ITU](https://www.itu.int/) / [MPEG](https://mpeg.chiariglione.org/)  \nSpecifications |  [http://www.itu.int/rec/T-REC-H.265](https://www.itu.int/rec/T-REC-H.265)<https://www.iso.org/standard/69668.html>  \nLicensing |  Proprietary; confirm your compliance with the [licensing requirements](https://www.via-la.com/licensing-2/hevc-vvc/). Note that multiple patent pools may apply.   \n### [MP4V-ES](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#mp4v-es)\nThe **MPEG-4 Video Elemental Stream** (**MP4V-ES**) format is part of the MPEG-4 Part 2 Visual standard. While in general, MPEG-4 part 2 video is not used by anyone because of its lack of compelling value related to other codecs, MP4V-ES does have some usage on mobile. MP4V is essentially H.263 encoding in an MPEG-4 container.\nIts primary purpose is to be used to stream MPEG-4 audio and video over an [RTP](https://developer.mozilla.org/en-US/docs/Glossary/RTP) session. However, MP4V-ES is also used to transmit MPEG-4 audio and video over a mobile connection using [3GP](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#3gp).\nYou almost certainly don't want to use this format, since it isn't supported in a meaningful way by any major browsers, and is quite obsolete. Files of this type should have the extension `.mp4v`, but sometimes are inaccurately labeled `.mp4`.\nSupported bit rates | 5 kbps to 1 Gbps and more  \n---|---  \nSupported frame rates | No specific limit; restricted only by the data rate  \nCompression |  Lossy [DCT-based algorithm](https://en.wikipedia.org/wiki/Discrete_cosine_transform)  \nSupported frame sizes | Up to 4,096 x 4,096 pixels  \nSupported color modes |  YCrCb with chroma subsampling (4:2:0, 4:2:2, and 4:4:4) supported; up to 12 bits per component   \nHDR support | No  \nVariable Frame Rate (VFR) support | Yes  \nBrowser compatibility |  | Feature | Chrome | Edge | Firefox | Opera | Safari  \n---|---|---|---|---|---  \nMP4V-ES support | No | No | Yes | No | No  \nFirefox supports MP4V-ES in [3GP](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#3gp) containers only. \nChrome does not support MP4V-ES; however, ChromeOS does.  \nContainer support |  [3GP](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#3gp), [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4)  \n[RTP](https://developer.mozilla.org/en-US/docs/Glossary/RTP) / [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API) compatible  | No  \nSupporting/Maintaining organization | [MPEG](https://mpeg.chiariglione.org/)  \nSpecification | [RFC 6416](https://datatracker.ietf.org/doc/html/rfc6416)  \nLicensing |  Proprietary; [obtain a license](https://www.via-la.com/licensing-2/mpeg-4-visual/) through [Via LA](https://www.via-la.com/) and/or [AT&T](https://about.att.com/innovation/ip/patents) as needed   \n### [MPEG-1 Part 2 Video](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#mpeg-1_part_2_video)\n**MPEG-1 Part 2 Video** was unveiled at the beginning of the 1990s. Unlike the later MPEG video standards, MPEG-1 was created solely by MPEG, without the [ITU's](https://developer.mozilla.org/en-US/docs/Glossary/ITU) involvement.\nBecause any MPEG-2 decoder can also play MPEG-1 video, it's compatible with a wide variety of software and hardware devices. There are no active patents remaining in relation to MPEG-1 video, so it may be used free of any licensing concerns. However, few web browsers support MPEG-1 video without the support of a plugin, and with plugin use deprecated in web browsers, these are generally no longer available. This makes MPEG-1 a poor choice for use in websites and web applications.\nSupported bit rates | Up to 1.5 Mbps  \n---|---  \nSupported frame rates |  23.976 FPS, 24 FPS, 25 FPS, 29.97 FPS, 30 FPS, 50 FPS, 59.94 FPS, and 60 FPS   \nCompression |  Lossy [DCT-based algorithm](https://en.wikipedia.org/wiki/Discrete_cosine_transform)  \nSupported frame sizes | Up to 4,095 x 4,095 pixels  \nSupported color modes |  Y'CbCr with 4:2:0 chroma subsampling with up to 12 bits per component   \nHDR support | No  \nVariable Frame Rate (VFR) support | No  \nBrowser compatibility |  | Feature | Chrome | Edge | Firefox | Opera | Safari  \n---|---|---|---|---|---  \nMPEG-1 support | No | No | No | No | Yes  \nContainer support | MPEG  \n[RTP](https://developer.mozilla.org/en-US/docs/Glossary/RTP) / [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API) compatible  | No  \nSupporting/Maintaining organization | [MPEG](https://mpeg.chiariglione.org/)  \nSpecification |  <https://www.iso.org/standard/22411.html>  \nLicensing |  Proprietary; however, all patents have expired, so MPEG-1 may be used freely   \n### [MPEG-2 Part 2 Video](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#mpeg-2_part_2_video)\n**[MPEG-2 Part 2](https://en.wikipedia.org/wiki/H.262/MPEG-2_Part_2)** is the video format defined by the MPEG-2 specification, and is also occasionally referred to by its [ITU](https://developer.mozilla.org/en-US/docs/Glossary/ITU) designation, H.262. It is very similar to MPEG-1 video\u2014in fact, any MPEG-2 player can automatically handle MPEG-1 without any special work\u2014except it has been expanded to support higher bit rates and enhanced encoding techniques.\nThe goal was to allow MPEG-2 to compress standard definition television, so interlaced video is also supported. The standard definition compression rate and the quality of the resulting video met needs well enough that MPEG-2 is the primary video codec used for DVD video media.\nMPEG-2 has several profiles available with different capabilities. Each profile is then available four levels, each of which increases attributes of the video, such as frame rate, resolution, bit rate, and so forth. Most profiles use Y'CbCr with 4:2:0 chroma subsampling, but more advanced profiles support 4:2:2 as well. In addition, there are four levels, each of which offers support for larger frame dimensions and bit rates. For example, the [ATSC](https://en.wikipedia.org/wiki/ATSC_standards) specification for television used in North America supports MPEG-2 video in high definition using the Main Profile at High Level, allowing 4:2:0 video at both 1920 x 1080 (30 FPS) and 1280 x 720 (60 FPS), at a maximum bit rate of 80 Mbps.\nHowever, few web browsers support MPEG-2 without the support of a plugin, and with plugin use deprecated in web browsers, these are generally no longer available. This makes MPEG-2 a poor choice for use in websites and web applications.\nSupported bit rates | Up to 100 Mbps; varies by level and profile  \n---|---  \nSupported frame rates |  | Abbr. | Level name | Frame rates supported  \n---|---|---  \nLL | Low Level | 23.9, 24, 25, 29.97, 30  \nML | Main Level | 23.976, 24, 25, 29.97, 30  \nH-14 | High 1440 | 23.976, 24, 26, 29.97, 30, 50, 59.94, 60  \nHL | High Level | 23.976, 24, 26, 29.97, 30, 50, 59.94, 60  \nCompression |  Lossy [DCT-based algorithm](https://en.wikipedia.org/wiki/Discrete_cosine_transform)  \nSupported frame sizes |  | Abbr. | Level name | Maximum frame size  \n---|---|---  \nLL | Low Level | 352 x 288 pixels  \nML | Main Level | 720 x 576 pixels  \nH-14 | High 1440 | 1440 x 1152 pixels  \nHL | High Level | 1920 x 1152 pixels  \nSupported color modes |  Y'CbCr with 4:2:0 chroma subsampling in most profiles; the \"High\" and \"4:2:2\" profiles support 4:2:2 chroma subsampling as well.   \nHDR support | No  \nVariable Frame Rate (VFR) support | No  \nBrowser compatibility |  | Feature | Chrome | Edge | Firefox | Opera | Safari  \n---|---|---|---|---|---  \nMPEG-2 support | No | No | No | No | Yes  \nContainer support |  [MPEG](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpegmpeg-2), MPEG-TS (MPEG Transport Stream), [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4), [QuickTime](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#quicktime)  \n[RTP](https://developer.mozilla.org/en-US/docs/Glossary/RTP) / [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API) compatible  | No  \nSupporting/Maintaining organization |  [MPEG](https://mpeg.chiariglione.org/) / [ITU](https://www.itu.int/)  \nSpecification |  <https://www.itu.int/rec/T-REC-H.262><https://www.iso.org/standard/61152.html>  \nLicensing |  Proprietary; all patents have expired worldwide with the exception of in Malaysia (as of October 1, 2024), so MPEG-2 can be used freely outside of Malaysia. Patents are licensed by [Via LA](https://www.via-la.com/licensing-2/mpeg-2/).   \n### [Theora](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#theora)\n**Warning:** This codec is no longer recommended. It has extremely small usage, and support is being removed from browsers.\n**[Theora](https://en.wikipedia.org/wiki/Theora)** , developed by [Xiph.org](https://xiph.org/), is an open and free video codec which may be used without royalties or licensing. Theora is comparable in quality and compression rates to MPEG-4 Part 2 Visual and AVC, making it a very good if not top-of-the-line choice for video encoding. But its status as being free from any licensing concerns and its relatively low CPU resource requirements make it a popular choice for many software and web projects. The low CPU impact is particularly useful since there are no hardware decoders available for Theora.\nTheora was originally based upon the VC3 codec by On2 Technologies. The codec and its specification were released under the LGPL license and entrusted to Xiph.org, which then developed it into the Theora standard.\nOne drawback to Theora is that it only supports 8 bits per color component, with no option to use 10 or more in order to avoid color banding. That said, 8 bits per component is still the most commonly-used color format in use today, so this is only a minor inconvenience in most cases. Also, Theora can only be used in an Ogg container. The biggest drawback of all, however, is that it is not supported by Safari, leaving Theora unavailable not only on macOS but on all those millions and millions of iPhones and iPads.\nThe [Theora Cookbook](https://en.flossmanuals.net/ogg-theora/_full/) offers additional details about Theora as well as the Ogg container format it is used within.\nSupported bit rates | Up to 2 Gbps  \n---|---  \nSupported frame rates |  Arbitrary; any non-zero value is supported. The frame rate is specified as a 32-bit numerator and a 32-bit denominator, to allow for non-integer frame rates.   \nCompression |  Lossy [DCT-based algorithm](https://en.wikipedia.org/wiki/Discrete_cosine_transform)  \nSupported frame sizes |  Any combination of width and height up to 1,048,560 x 1,048,560 pixels   \nSupported color modes |  Y'CbCr with 4:2:0, 4:2:2, and 4:4:4 chroma subsampling at 8 bits per component   \nHDR support | No  \nVariable Frame Rate (VFR) support |  Yes While Theora doesn't support Variable Frame Rate (VFR) within a single stream, multiple streams can be chained together within a single file, and each of those can have its own frame rate, thus allowing what is essentially VFR. However, this is impractical if the frame rate needs to change frequently.   \nBrowser compatibility |  | Feature | Chrome | Edge | Firefox | Opera | Safari  \n---|---|---|---|---|---  \nTheora support | 3 to 121 | 12 to 121 | 3.5 to 126 | 10.5 to 107 | No  \nEdge supports Theora with the optional [Web Media Extensions](https://apps.microsoft.com/detail/9n5tdp8vcmhs) add-on.   \nContainer support | [Ogg](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#ogg)  \n[RTP](https://developer.mozilla.org/en-US/docs/Glossary/RTP) / [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API) compatible  | No  \nSupporting/Maintaining organization | [Xiph.org](https://www.xiph.org/)  \nSpecification |  <https://www.theora.org/doc/>  \nLicensing | Open and free of royalties and any other licensing requirements  \n### [VP8](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#vp8)\nThe **Video Processor 8** (**VP8**) codec was initially created by On2 Technologies. Following their purchase of On2, Google released VP8 as an open and royalty-free video format under a promise not to enforce the relevant patents. In terms of quality and compression rate, VP8 is comparable to [AVC](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#avc_h.264).\nIf supported by the browser, VP8 allows video with an alpha channel, allowing the video to play with the background able to be seen through the video to a degree specified by each pixel's alpha component.\nThere is good browser support for VP8 in HTML content, especially within [WebM](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#webm) files. This makes VP8 a good candidate for your content, although VP9 is an even better choice if available to you. Web browsers are _required_ to support VP8 for WebRTC, but not all browsers that do so also support it in HTML audio and video elements.\nSupported bit rates | Arbitrary; no maximum unless level-based limitations are enforced  \n---|---  \nSupported frame rates | Arbitrary  \nCompression |  Lossy [DCT-based algorithm](https://en.wikipedia.org/wiki/Discrete_cosine_transform)  \nSupported frame sizes | Up to 16,384 x 16,384 pixels  \nSupported color modes | Y'CbCr with 4:2:0 chroma subsampling at 8 bits per component  \nHDR support | No  \nVariable Frame Rate (VFR) support | Yes  \nBrowser compatibility |  All versions of Chrome, Edge, Firefox, Opera, and Safari[iOS: Safari 12.1](https://webkit.org/blog/8672/on-the-road-to-webrtc-1-0-including-vp8/) and later support VP8 in WebRTC connections only. Firefox only supports VP8 in MSE when no H.264 hardware decoder is available. Use [`MediaSource.isTypeSupported()`](https://developer.mozilla.org/en-US/docs/Web/API/MediaSource/isTypeSupported_static \"MediaSource.isTypeSupported\\(\\)\") to check for availability.  \nContainer support |  [3GP](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#3gp), [Ogg](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#ogg), [WebM](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#webm)  \n[RTP](https://developer.mozilla.org/en-US/docs/Glossary/RTP) / [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API) compatible  | Yes; VP8 is one of the spec-required codecs for WebRTC  \nSupporting/Maintaining organization | [Google](https://www.google.com/)  \nSpecification | [RFC 6386](https://datatracker.ietf.org/doc/html/rfc6386)  \nLicensing | Open and free of royalties and any other licensing requirements  \n### [VP9](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#vp9)\n**Video Processor 9** (**VP9**) is the successor to the older VP8 standard developed by Google. Like VP8, VP9 is entirely open and royalty-free. Its encoding and decoding performance is comparable to or slightly faster than that of AVC, but with better quality. VP9's encoded video quality is comparable to that of HEVC at similar bit rates.\nVP9's main profile supports only 8-bit color depth at 4:2:0 chroma subsampling levels, but its profiles include support for deeper color and the full range of chroma subsampling modes. It supports several HDR implementations, and offers substantial freedom in selecting frame rates, aspect ratios, and frame sizes.\nVP9 is widely supported by browsers, and hardware implementations of the codec are fairly common. VP9 is one of the two video codecs mandated by [WebM](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#webm) (the other being [VP8](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#vp8)). Note however that Safari support for WebM and VP9 was only introduced in version 14.1, so if you choose to use VP9, consider offering a fallback format such as AVC or HEVC for iPhone, iPad, and Mac users.\nVP9 is a good choice if you are able to use a WebM container (and can provide fallback video when needed). This is especially true if you wish to use an open codec rather than a proprietary one.\nSupported bit rates | Arbitrary; no maximum unless level-based limitations are enforced  \n---|---  \nSupported frame rates | Arbitrary  \nCompression |  Lossy [DCT-based algorithm](https://en.wikipedia.org/wiki/Discrete_cosine_transform)  \nSupported frame sizes | Up to 65,536 x 65,536 pixels  \nSupported color modes |  | Profile | Color depths | Chroma subsampling  \n---|---|---  \nProfile 0 | 8 | 4:2:0  \nProfile 1 | 8 | 4:2:0, 4:2:2, and 4:4:4  \nProfile 2 | 10 to 12 | 4:2:0  \nProfile 3 | 10 to 12 | 4:2:0, 4:2:2, and f:4:4  \nColor spaces supported: [Rec. 601](https://en.wikipedia.org/wiki/Rec._601), [Rec. 709](https://en.wikipedia.org/wiki/Rec._709), [Rec. 2020](https://en.wikipedia.org/wiki/Rec._2020), [SMPTE C](https://en.wikipedia.org/wiki/SMPTE_C), SMPTE-240M (obsolete; replaced by Rec. 709), and [sRGB](https://en.wikipedia.org/wiki/SRGB).   \nHDR support |  Yes; HDR10+, [HLG](https://en.wikipedia.org/wiki/Hybrid_Log-Gamma), and [PQ](https://en.wikipedia.org/wiki/Perceptual_Quantizer)  \nVariable Frame Rate (VFR) support | Yes  \nBrowser compatibility |  All versions of Chrome, Edge, Firefox, Opera, and Safari Firefox only supports VP8 in MSE when no H.264 hardware decoder is available. Use [`MediaSource.isTypeSupported()`](https://developer.mozilla.org/en-US/docs/Web/API/MediaSource/isTypeSupported_static \"MediaSource.isTypeSupported\\(\\)\") to check for availability.   \nContainer support |  [MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4), [Ogg](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#ogg), [WebM](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#webm)  \n[RTP](https://developer.mozilla.org/en-US/docs/Glossary/RTP) / [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API) compatible  | Yes  \nSupporting/Maintaining organization | [Google](https://www.google.com/)  \nSpecification |  <https://www.webmproject.org/vp9/>  \nLicensing | Open and free of royalties and any other licensing requirements  \n## [Choosing a video codec](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#choosing_a_video_codec)\nThe decision as to which codec or codecs to use begins with a series of questions to prepare yourself:\n  * Do you wish to use an open format, or are proprietary formats also to be considered?\n  * Do you have the resources to produce more than one format for each of your videos? The ability to provide a fallback option vastly simplifies the decision-making process.\n  * Are there any browsers you're willing to sacrifice compatibility with?\n  * How old is the oldest version of web browser you need to support? For example, do you need to work on every browser shipped in the past five years, or just the past one year?\n\n\nIn the sections below, we offer recommended codec selections for specific use cases. For each use case, you'll find up to two recommendations. If the codec which is considered best for the use case is proprietary or may require royalty payments, then two options are provided: first, an open and royalty-free option, followed by the proprietary one.\nIf you are only able to offer a single version of each video, you can choose the format that's most appropriate for your needs. The first one is recommended as being a good combination of quality, performance, and compatibility. The second option will be the most broadly compatible choice, at the expense of some amount of quality, performance, and/or size.\n### [Recommendations for everyday videos](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#recommendations_for_everyday_videos)\nFirst, let's look at the best options for videos presented on a typical website such as a blog, informational site, small business website where videos are used to demonstrate products (but not where the videos themselves are a product), and so forth.\n  1. A **[WebM](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#webm)** container using the **[VP9](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#vp9)** codec for video and the **[Opus](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Audio_codecs#opus)** codec for audio. These are all open, royalty-free formats which are generally well-supported, although only in quite recent browsers, which is why a fallback is a good idea.\nhtmlCopy to Clipboard\n```\n<video controls src=\"filename.webm\"></video>\n\n```\n\n  2. An **[MP4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers#mpeg-4_mp4)** container and the **[AVC](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#avc_h.264)** (**H.264**) video codec, ideally with **[AAC](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Audio_codecs#aac_advanced_audio_coding)** as your audio codec. This is because the MP4 container with AVC and AAC codecs within is a broadly-supported combination\u2014by every major browser, in fact\u2014and the quality is typically good for most use cases. Make sure you verify your compliance with the license requirements, however.\nhtmlCopy to Clipboard\n```\n<video controls>\n <source type=\"video/webm\" src=\"filename.webm\" />\n <source type=\"video/mp4\" src=\"filename.mp4\" />\n</video>\n\n```\n\n\n\n**Note:** The [`<video>`](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video) element requires a closing `</video>` tag, whether or not you have any [`<source>`](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/source) elements inside it.\n### [Recommendations for high-quality video presentation](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#recommendations_for_high-quality_video_presentation)\nIf your mission is to present video at the highest possible quality, you will probably benefit from offering as many formats as possible, as the codecs capable of the best quality tend also to be the newest, and thus the most likely to have gaps in browser compatibility.\n  1. A WebM container using AV1 for video and Opus for audio. If you're able to use the High or Professional profile when encoding AV1, at a high level like 6.3, you can get very high bit rates at 4K or 8K resolution, while maintaining excellent video quality. Encoding your audio using Opus's Fullband profile at a 48 kHz sample rate maximizes the audio bandwidth captured, capturing nearly the entire frequency range that's within human hearing.\nhtmlCopy to Clipboard\n```\n<video controls src=\"filename.webm\"></video>\n\n```\n\n  2. An MP4 container using the [HEVC](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#hevc_h.265) codec using one of the advanced Main profiles, such as Main 4:2:2 with 10 or 12 bits of color depth, or even the Main 4:4:4 profile at up to 16 bits per component. At a high bit rate, this provides excellent graphics quality with remarkable color reproduction. In addition, you can optionally include HDR metadata to provide high dynamic range video. For audio, use the AAC codec at a high sample rate (at least 48 kHz but ideally 96kHz) and encoded with complex encoding rather than fast encoding.\nhtmlCopy to Clipboard\n```\n<video controls>\n <source type=\"video/webm\" src=\"filename.webm\" />\n <source type=\"video/mp4\" src=\"filename.mp4\" />\n</video>\n\n```\n\n\n\n### [Recommendations for archival, editing, or remixing](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#recommendations_for_archival_editing_or_remixing)\nThere are not currently any lossless\u2014or even near-lossless\u2014video codecs generally available in web browsers. The reason for this is simple: video is huge. Lossless compression is by definition less effective than lossy compression. For example, uncompressed 1080p video (1920 by 1080 pixels) with 4:2:0 chroma subsampling needs at least 1.5 Gbps. Using lossless compression such as FFV1 (which is not supported by web browsers) could perhaps reduce that to somewhere around 600 Mbps, depending on the content. That's still a huge number of bits to pump through a connection every second, and is not currently practical for any real-world use.\nThis is the case even though some of the lossy codecs have a lossless mode available; the lossless modes are not implemented in any current web browsers. The best you can do is to select a high-quality codec that uses lossy compression and configure it to perform as little compression as possible. One way to do this is to configure the codec to use \"fast\" compression, which inherently means less compression is achieved.\n#### Preparing video externally\nTo prepare video for archival purposes from outside your website or app, use a utility that performs compression on the original uncompressed video data. For example, the free [x264](https://www.videolan.org/developers/x264.html) utility can be used to encode video in [AVC](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#avc_h.264) format using a very high bit rate:\nbashCopy to Clipboard\n```\nx264 --crf 18 -preset ultrafast --output out-file.mp4 infile\n\n```\n\nWhile other codecs may have better best-case quality levels when compressing the video by a significant margin, their encoders tend to be slow enough that the nearly-lossless encoding you get with this compression is vastly faster at about the same overall quality level.\n#### Recording video\nGiven the constraints on how close to lossless you can get, you might consider using [AVC](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#avc_h.264) or [AV1](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#av1). For example, if you're using the [MediaStream Recording API](https://developer.mozilla.org/en-US/docs/Web/API/MediaStream_Recording_API) to record video, you might use code like the following when creating your [`MediaRecorder`](https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder) object:\njsCopy to Clipboard\n```\nconst kbps = 1024;\nconst Mbps = kbps * kbps;\nconst options = {\n mimeType: 'video/webm; codecs=\"av01.2.19H.12.0.000.09.16.09.1, flac\"',\n bitsPerSecond: 800 * Mbps,\n};\nlet recorder = new MediaRecorder(sourceStream, options);\n\n```\n\nThis example creates a `MediaRecorder` configured to record [AV1](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#av1) video using BT.2100 HDR in 12-bit color with 4:4:4 chroma subsampling and [FLAC](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Audio_codecs#flac_free_lossless_audio_codec) for lossless audio. The resulting file will use a bit rate of no more than 800 Mbps shared between the video and audio tracks. You will likely need to adjust these values depending on hardware performance, your requirements, and the specific codecs you choose to use. This bit rate is obviously not realistic for network transmission and would likely only be used locally.\nBreaking down the value of the `codecs` parameter into its dot-delineated properties, we see the following:\nValue | Description  \n---|---  \n`av01` | The four-character code (4CC) designation identifying the [AV1](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#av1) codec.  \n`2` | The profile. A value of 2 indicates the Professional profile. A value of 1 is the High profile, while a value of 0 would specify the Main profile.  \n`19H` | The level and tier. This value comes from the table in section [A.3](https://aomediacodec.github.io/av1-spec/#levels) of the AV1 specification, and indicates the high tier of Level 6.3.  \n`12` | The color depth. This indicates 12 bits per component. Other possible values are 8 and 10, but 12 is the highest-accuracy color representation available in AV1.  \n`0` | The monochrome mode flag. If 1, then no chroma planes would be recorded, and all data should be strictly luma data, resulting in a greyscale image. We've specified 0 because we want color.  \n`000` | The chroma subsampling mode, taken from [section 6.4.2](https://aomediacodec.github.io/av1-spec/#color-config-semantics) in the AV1 specification. A value of 000, combined with the monochrome mode value 0, indicates that we want 4:4:4 chroma subsampling, or no loss of color data.  \n`09` | The color primaries to use. This value comes from [section 6.4.2](https://aomediacodec.github.io/av1-spec/#color-config-semantics) in the AV1 specification; 9 indicates that we want to use BT.2020 color, which is used for HDR.  \n`16` | The transfer characteristics to use. This comes from [section 6.4.2](https://aomediacodec.github.io/av1-spec/#color-config-semantics) as well; 16 indicates that we want to use the characteristics for BT.2100 PQ color.  \n`09` | The matrix coefficients to use, from the [section 6.4.2](https://aomediacodec.github.io/av1-spec/#color-config-semantics) again. A value of 9 specifies that we want to use BT.2020 with variable luminance; this is also known as BT.2010 YbCbCr.  \n`1` | The video \"full range\" flag. A value of 1 indicates that we want the full color range to be used.  \nThe documentation for your codec choices will probably offer information you'll use when constructing your `codecs` parameter.\n## [See also](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_codecs/#see_also)\n  * [Web audio codec guide](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Audio_codecs)\n  * [Media container formats (file types)](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Containers)\n  * [Handling media support issues in web content](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Support_issues)\n  * [Codecs used by WebRTC](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/WebRTC_codecs)\n  * [RFC 6381](https://datatracker.ietf.org/doc/html/rfc6381): The \"Codecs\" and \"Profiles\" parameters for \"Bucket\" media types\n  * [RFC 5334](https://datatracker.ietf.org/doc/html/rfc5334): Ogg Media Types\n  * [RFC 3839](https://datatracker.ietf.org/doc/html/rfc3839): MIME Type Registrations for 3GPP Multimedia Files\n  * [RFC 4381](https://datatracker.ietf.org/doc/html/rfc4381): MIME Type Registrations for 3GPP2 Multimedia Files\n  * [RFC 4337](https://datatracker.ietf.org/doc/html/rfc4337): MIME Type Registrations for MPEG-4\n  * [Video and audio codecs in Chrome](https://www.chromium.org/audio-video/)\n\n\n## Help improve MDN\nWas this page helpful to you?\nYesNo\n[Learn how to contribute](https://github.com/mdn/content/blob/main/CONTRIBUTING.md \"This will take you to our contribution guidelines on GitHub.\").\nThis page was last modified on Mar 27, 2025 by [MDN contributors](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Video_codecs/contributors.txt).\n[View this page on GitHub](https://github.com/mdn/content/blob/main/files/en-us/web/media/guides/formats/video_codecs/index.md?plain=1 \"Folder: en-us/web/media/guides/formats/video_codecs \\(Opens in a new tab\\)\") \u2022 [Report a problem with this content](https://github.com/mdn/content/issues/new?template=page-report.yml&mdn-url=https%3A%2F%2Fdeveloper.mozilla.org%2Fen-US%2Fdocs%2FWeb%2FMedia%2FGuides%2FFormats%2FVideo_codecs&metadata=%3C%21--+Do+not+make+changes+below+this+line+--%3E%0A%3Cdetails%3E%0A%3Csummary%3EPage+report+details%3C%2Fsummary%3E%0A%0A*+Folder%3A+%60en-us%2Fweb%2Fmedia%2Fguides%2Fformats%2Fvideo_codecs%60%0A*+MDN+URL%3A+https%3A%2F%2Fdeveloper.mozilla.org%2Fen-US%2Fdocs%2FWeb%2FMedia%2FGuides%2FFormats%2FVideo_codecs%0A*+GitHub+URL%3A+https%3A%2F%2Fgithub.com%2Fmdn%2Fcontent%2Fblob%2Fmain%2Ffiles%2Fen-us%2Fweb%2Fmedia%2Fguides%2Fformats%2Fvideo_codecs%2Findex.md%0A*+Last+commit%3A+https%3A%2F%2Fgithub.com%2Fmdn%2Fcontent%2Fcommit%2F3d5a03ea2b7fefbd4428e582bd14c44177c4ba2f%0A*+Document+last+modified%3A+2025-03-27T13%3A21%3A38.000Z%0A%0A%3C%2Fdetails%3E \"This will take you to GitHub to file a new issue.\")\n[MDN logo](https://developer.mozilla.org/)\nYour blueprint for a better internet.\n  * [MDN on Bluesky](https://bsky.app/profile/developer.mozilla.org)\n  * [MDN on Mastodon](https://mastodon.social/@mdn)\n  * [MDN on X (formerly Twitter)](https://twitter.com/mozdevnet)\n  * [MDN on GitHub](https://github.com/mdn/)\n  * [MDN Blog RSS Feed](https://developer.mozilla.org/en-US/blog/rss.xml)\n\n\n## MDN\n  * [About](https://developer.mozilla.org/en-US/about)\n  * [Blog](https://developer.mozilla.org/en-US/blog/)\n  * [Careers](https://www.mozilla.org/en-US/careers/listings/?team=ProdOps)\n  * [Advertise with us](https://developer.mozilla.org/en-US/advertising)\n\n\n## Support\n  * [Product help](https://support.mozilla.org/products/mdn-plus)\n  * [Report an issue](https://developer.mozilla.org/en-US/docs/MDN/Community/Issues)\n\n\n## Our communities\n  * [MDN Community](https://developer.mozilla.org/en-US/community)\n  * [MDN Forum](https://discourse.mozilla.org/c/mdn/236)\n  * [MDN Chat](https://developer.mozilla.org/discord)\n\n\n## Developers\n  * [Web Technologies](https://developer.mozilla.org/en-US/docs/Web)\n  * [Learn Web Development](https://developer.mozilla.org/en-US/docs/Learn)\n  * [MDN Plus](https://developer.mozilla.org/en-US/plus)\n  * [Hacks Blog](https://hacks.mozilla.org/)\n\n\n[](https://www.mozilla.org/)\n  * [Website Privacy Notice](https://www.mozilla.org/privacy/websites/)\n  * [Cookies](https://www.mozilla.org/privacy/websites/#cookies)\n  * [Legal](https://www.mozilla.org/about/legal/terms/mozilla)\n  * [Community Participation Guidelines](https://www.mozilla.org/about/governance/policies/participation/)\n\n\nVisit [Mozilla Corporation\u2019s](https://www.mozilla.org) not-for-profit parent, the [Mozilla Foundation](https://foundation.mozilla.org/).Portions of this content are \u00a91998\u20132025 by individual mozilla.org contributors. Content available under [a Creative Commons license](https://developer.mozilla.org/en-US/docs/MDN/Writing_guidelines/Attrib_copyright_license).\n  *[\nDeprecated\n]: Deprecated. Not for use in new websites.\n",
    "answer": "```markdown\n# TL;DR: Web Video Codec Guide\n\nThis guide covers common video codecs for web use, detailing their capabilities, compatibility, and factors affecting video quality and size. Key codecs include AV1, AVC (H.264), HEVC (H.265), VP8, and VP9. Choosing the right codec involves balancing quality, compatibility, and licensing considerations.\n```"
  },
  {
    "instruction": "A Metrics-First Approach to LLM Evaluation - Galileo AI\n[![](https://cdn.sanity.io/images/tf66morw/production/b6ba804260aa8c85927b129e1fba84a92ea3ad87-3400x744.svg?w=3400&h=744&auto=format)](https://www.rungalileo.io/)\n  * Products\n    * [Agents](https://www.rungalileo.io/agentic-evaluations)\n    * [Evaluate](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-evaluate)\n    * [Observe](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-observe)\n    * [Protect](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-protect)\n  * [Docs](https://docs.galileo.ai/galileo)\n  * [Pricing](https://www.rungalileo.io/pricing)\n  * [Blog](https://www.rungalileo.io/blog)\n  * Resources\n    * [Agent Leaderboard](https://www.rungalileo.io/agent-leaderboard)\n    * [Mastering Agents eBook](https://www.rungalileo.io/ebook-mastering-agents)\n    * [Hallucination Index](https://www.rungalileo.io/hallucinationindex)\n    * [Mastering RAG eBook](https://www.rungalileo.io/mastering-rag)\n    * [Chain of Thought podcast](https://pod.link/1776879655)\n    * [Case Studies](https://www.rungalileo.io/case-studies)\n    * [Research](https://www.rungalileo.io/research)\n    * [GenAI Productionize 2.0](https://www.rungalileo.io/genai-productionize-2-0)\n  * Company\n    * [Team](https://www.rungalileo.io/team)\n    * [Careers](https://ats.rippling.com/galileo/jobs)\n[Login](https://app.galileo.ai/sign-in)[Contact Sales](https://www.rungalileo.io/get-started)[Sign up](https://app.galileo.ai/sign-up)\n\n\n[![](https://cdn.sanity.io/images/tf66morw/production/b6ba804260aa8c85927b129e1fba84a92ea3ad87-3400x744.svg?w=3400&h=744&auto=format)](https://www.rungalileo.io/)\nProducts\n    * [Agents](https://www.rungalileo.io/agentic-evaluations)\n    * [Evaluate](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-evaluate)\n    * [Observe](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-observe)\n    * [Protect](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-protect)\n[Docs](https://docs.galileo.ai/galileo)[Pricing](https://www.rungalileo.io/pricing)[Blog](https://www.rungalileo.io/blog)\nResources\n    * [Agent Leaderboard](https://www.rungalileo.io/agent-leaderboard)\n    * [Mastering Agents eBook](https://www.rungalileo.io/ebook-mastering-agents)\n    * [Hallucination Index](https://www.rungalileo.io/hallucinationindex)\n    * [Mastering RAG eBook](https://www.rungalileo.io/mastering-rag)\n    * [Chain of Thought podcast](https://pod.link/1776879655)\n    * [Case Studies](https://www.rungalileo.io/case-studies)\n    * [Research](https://www.rungalileo.io/research)\n    * [GenAI Productionize 2.0](https://www.rungalileo.io/genai-productionize-2-0)\nCompany\n    * [Team](https://www.rungalileo.io/team)\n    * [Careers](https://ats.rippling.com/galileo/jobs)\n\n\n[Login](https://app.galileo.ai/sign-in)[Contact Sales](https://www.rungalileo.io/get-started)[Sign up](https://app.galileo.ai/sign-up)\n# A Metrics-First Approach to LLM Evaluation\n![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='56'%20width='56'%20xmlns='http://www.w3.org/2000/svg'%20version='1.1'%3E%3C/svg%3E)\n![Pratik Bhavsar](https://cdn.sanity.io/images/tf66morw/production/06a39f484014944a7a05678b71ab2cf008adf1dd-394x394.jpg?w=56&h=56&auto=format)\nPratik BhavsarGalileo Labs\n![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='655'%20width='1280'%20xmlns='http://www.w3.org/2000/svg'%20version='1.1'%3E%3C/svg%3E)\n![Learn about different types of LLM evaluation metrics](https://cdn.sanity.io/images/tf66morw/production/adc8a29efafd3ac996e3153c443ef6c93457c95c-1228x628.png?w=1228&h=628&auto=format)\n10 min readSeptember 19 2023\nTable of contents\nShow\n  1. [ Need for new LLM evaluation metrics](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#need-for-new-llm-evaluation-metrics)\n  2. [Types of LLM Evaluation Metrics](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#types-of-llm-evaluation-metrics)\n    1. [Top Level Metrics for LLM Evaluation](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#top-level-metrics-for-llm-evaluation)\n    2. [Metrics for Evaluating RAG Effectiveness](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#metrics-for-evaluating-rag-effectiveness)\n    3. [Metrics for Evaluating Safety ](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#metrics-for-evaluating-safety)\n    4. [Custom Evaluation Metrics](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#custom-evaluation-metrics)\n    5. [Final thoughts](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#final-thoughts)\n\n\nThere has been tremendous progress in the world of Large Language Models (LLMs). We have seen a series of blockbuster models like GPT3, GPT3.5, GPT4, Falcon, MPT and Llama pushing the state of the art. The industry has started adopting them for various applications but there's a big problem. It's hard to figure out how well these models are performing. Companies are struggling to compare different LLMs for generative applications. The tendency of LLMs to hallucinate requires us to measure them carefully. In this blog, we discuss helpful metrics for evaluating LLMs and understanding their suitability for your use cases. \n##  Need for new LLM evaluation metrics\nIt is easy to start building with Large Language Models (LLMs) such as OpenAI's ChatGPT, but they can be very difficult to evaluate. The main concerns around evaluations are:\n  1. Human evaluation is costly and prone to errors: LLMs are used for varied generative tasks which [cannot be judged](https://arxiv.org/abs/2008.12009) by these metrics. In desperation, companies often rely on human vibe check. But human annotations are costly and full of biases which make evaluation slow and unreliable.\n  2. Poor correlations with human judgment: Traditional metrics like BLEU / ROUGE have shown [poor correlation](https://arxiv.org/abs/2303.16634) with how humans evaluate model output. Without reliable metrics it becomes hard to know if the LLM app is release worthy.\n  3. Absence of reliable benchmarks: Researchers have come up with benchmarks like [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), but they do not evaluate the generative capability since the tasks contain multiple choice questions. The datasets used are also limited and might not have domain coverage for the target use case.\n\n\nGiven these challenges, companies should prioritize investments in the development of evaluation metrics. These metrics will enable them to make data-driven decisions without depending solely on human judgment. Let's explore some key metrics that can assist companies in designing an evaluation system to enhance their generative applications.\n## Types of LLM Evaluation Metrics\nOver time, many metrics have been proposed to measure the quality of LLM outputs. How to best evaluate LLMs is still an active research work, but we have found some which are more useful than others. We categorize them below to make them easier to understand.\n![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='769.9999999999999'%20width='1654'%20xmlns='http://www.w3.org/2000/svg'%20version='1.1'%3E%3C/svg%3E)\n![LLM Evaluation Metrics](https://cdn.sanity.io/images/tf66morw/production/53f3548af41c40a2fba24d4e94f337d304850656-1654x770.png?w=1654&h=770&auto=format)\nLLM Evaluation Metrics\n### Top Level Metrics for LLM Evaluation\nThese are well known metrics which can be used for any application. They work for any input/output of the LLMs.\n#### 1. Context Adherence\nContext adherence measures whether the model\u2019s response is supported by the context given to it. This checks if the model is stating facts that are out of context\u2014hallucinations. A score of 1 means it\u2019s grounded, and there is a lower chance of hallucination.\nMetric signal: Higher context adherence is correlated with higher output quality.\nExamples:\nQuery: \"What is the population of Paris, France?\"\nRetrieved docs:\nDoc1: Census 2023 reported population of Paris to be 2.2 million.\nDoc2: Census 2022 reported population of Paris to be 2.1 million.\nDoc3: The population of Paris is more than 2 million.\nHigh context adherence:\nResponse: \"The population of Paris, France, according to the most recent census report, is approximately 2.2 million people.\"\nIn this example, the model's response is directly supported by the information present in the retrieved documents. It provides a specific population figure based on a reliable source, demonstrating groundedness.\nLow context adherence:\nResponse: \"Paris, France has a population of 10 million people.\"\nIn this example, the model's response is not grounded in the provided documents, and it seems to have fabricated a population figure that is not supported by the context.\n#### 2. Correctness\nAs the name suggests, correctness measures the factual correctness of the output. It was developed by Galileo, which leverages GPT-3.5 with chain of thought(CoT) prompting and self-consistency. It surfaces errors of precision and not recall. It is very useful for [detecting hallucinations](https://www.rungalileo.io/blog/5-techniques-for-detecting-llm-hallucinations) in different scenarios, like summarisation or open-domain QA.\nMetric signal: Higher factuality is correlated with higher output quality.\nPrompt: When did Aliens invade earth?\nHigh factuality:\nResponse: Aliens have never invaded earth.\nLow factuality:\nResponse: Aliens invaded earth on July 4th 2020.\n#### 3. LLM Uncertainty\nA recent [study](https://aclanthology.org/2023.eacl-main.75/) has shown that log probability can help us find low quality generations. Uncertainty leverages the same philosophy as prompt perplexity but on the generated text. It is calculated by leveraging log probability given by the LLM for each generated token. For models like GPT-3.5 and GPT4 which do not give logprob, we use other models as proxy.\nMetric signal: Lower LLM uncertainty is correlated with higher output quality.\nLow uncertainty:\nPrompt: \u201cWhere did the inventors of GPT3 architecture work?\u201d\nResponse: \u201cOpenAI\u201d\nThe response here is correct and contains low uncertainty.\nHigh uncertainty:\nPrompt: \u201cWhere did the inventors of GPT5 architecture work?\u201d\nResponse: \u201cDeepmind\u201d\nThe response here is incorrect and contains high uncertainty.\n#### 4. Prompt Perplexity\nPrompt perplexity is simply the perplexity of the prompt given as input to the LLM. A recent [study](https://arxiv.org/abs/2212.04037) showed that the lower the perplexity of the prompt, the better suited the prompt is for the given task. High perplexity indicates lower understanding of the text which means the model has not understood the prompt. If the model is unable to understand its input (the prompt) it is more likely to generate poor outputs.\nMetric signal: Lower prompt perplexity is correlated with higher output quality.\nLow perplexity:\nTranslate the following English sentence into French: 'The quick brown fox jumps over the lazy dog.'\nIn this case, the prompt is clear and directly instructs the model on what task to perform. The model is likely to have a low perplexity because it can confidently understand and execute the translation task.\nHigh perplexity:\n\"Can you, like, if you don't mind, convert to French for me? The quick brown fox jumps over the lazy dog.\nIn this example, the instruction is not very clear that it\u2019s a translation task and does not highlight what is to be translated.\n### Metrics for Evaluating RAG Effectiveness\n![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='1255'%20width='2398'%20xmlns='http://www.w3.org/2000/svg'%20version='1.1'%3E%3C/svg%3E)\n![Galileo RAG Analytics](https://cdn.sanity.io/images/tf66morw/production/3f94840ff011ee42b76c784cc7b9cbbcf97fc80e-2398x1255.png?w=2398&h=1255&auto=format)\nGalileo RAG Analytics\n[RAG](https://www.rungalileo.io/blog/mastering-rag-how-to-architect-an-enterprise-rag-system) refers to retrieval augmented generation where we add domain specific knowledge(DSK) in the prompt with the help of a search engine. This is required to make LLMs work with DSK which can be totally missing during its training on open web data. Let\u2019s discuss metrics which help with improving the RAG system.\n#### 1. Context Relevance\nContext relevance measures how relevant the context fetched was to the user query. Low score could be a sign of a bad doc chunking/retrieval strategy or of missing info. To address this, one would add more documents to context DB or change the retrieval indexing/retrieval strategy. This can help debug reasons for bad generations.\nMetric signal: Higher context relevance is correlated with higher output quality.\nQuery: Please provide information about the impact of climate change on polar bears.\nHigh context relevance:\nRetrieved Documents:\nDoc 1 title: \"The Effects of Climate Change on Polar Bear Populations\"\nDoc 2 title: \"Arctic Ecosystems and Climate Change\"\nDoc 3 title: \"Polar Bear Conservation Efforts in a Warming World\"\nIn this case, the context relevance is good because all three retrieved documents are directly related to the query about the impact of climate change on polar bears. T\nLow context relevance:\nRetrieved Documents:\nDoc 1 title: \"Polar bears are fascinating creatures living in the Arctic.\"\nDoc 2 title: \"Climate change is a global issue affecting various ecosystems.\"\nDoc 3 title: \"The significance of bears in cultural folklore.\"\nIn this example, there is an overlap of words like \"polar bears\" and \"climate change\" between the query and documents. However, the context relevance is low because none of the retrieved documents directly address the impact of climate change on polar bears.\n#### 2. Completeness\nCompleteness measures how thoroughly your model's response covers the relevant information available in the provided context. Completeness and Context Adherence work in tandem to ensure that the model's response is both consistent with the context and comprehensive in addressing all relevant information.\nContext Adherence vs. Completeness: Context Adherence evaluates whether the model's response is consistent with the information in the context, while Completeness assesses whether all relevant information in the context is adequately addressed in the response. In essence, Context Adherence represents precision, whereas Completeness represents recall.\nMetric signal: Higher context similarity is correlated with higher output quality.\nExamples:\nUser Query: \"Who was Galileo Galilei?\"\nContext: \"Galileo Galilei was an Italian astronomer. Galileo was the first to see the Milky Way Galaxy in 1610 as individual stars through the telescope.\"\nLow completeness\nGenerated Answer: \"Galileo Galilei was Italian.\"\nIn this example, the model's response achieves perfect Context Adherence because the context supports everything it states. However, the response lacks completeness because it fails to mention that Galileo Galilei was an astronomer, which is relevant information provided in the context.\nHigh completeness\nGenerated Answer: \"Galileo Galilei was Italian astronomer.\"\nIn this example, the model's response achieves perfect completeness.\n#### 3. Chunk Attribution\nChunk Attribution measures whether each retrieved chunk affected the model's response. It is a binary metric, classifying each chunk as either Attributed or Not Attributed.\nChunk Attribution offers valuable insights for optimizing RAG pipelines in various ways:\n1. Tuning the number of retrieved chunks: If the system generates satisfactory responses but many chunks are Not Attributed, reducing the number of retrieved chunks per example may enhance efficiency without compromising response quality. This optimization can lead to cost savings and reduced latency.\n2. \"Debugging\" anomalous model behavior in individual examples: When analyzing specific model responses that are unsatisfactory or unusual, Chunk Attribution helps identify the chunks that influenced the response. This facilitates a quicker understanding of the underlying factors contributing to the issue, streamlining the debugging process for individual examples.\nMetric signal: Higher chunk attribution is correlated with higher output quality.\nExample:\nQuery: \"Tell me about the discoveries of Galileo Galilei.\"\nRetrieved Chunks:\nChunk 1: \"Galileo Galilei was an Italian physicist, mathematician, and astronomer.\"\nChunk 2: \"Galileo's telescopic observations supported the Copernican heliocentric model of the solar system.\"\nChunk 3: \"He discovered the four largest moons of Jupiter, now known as the Galilean moons.\"\nHigh chunk attribution:\nGenerated Answer: \"Galileo Galilei, an Italian physicist, mathematician, and astronomer, made significant discoveries that reshaped our understanding of the universe. His telescopic observations provided evidence for the Copernican heliocentric model and led to the discovery of the four largest moons of Jupiter, now known as the Galilean moons.\"\nIn this case, all retrieved chunks are attributed since each contributes essential information to the model's response. Therefore, each chunk affected the generated answer, indicating high Chunk Attribution.\nLow chunk attribution:\nQuery: \"What was Galileo Galilei's nationality?\"\nRetrieved Chunks:\nChunk 1: \"Galileo Galilei was an Italian physicist, mathematician, and astronomer.\"\nChunk 2: \"Galileo's telescopic observations supported the Copernican heliocentric model of the solar system.\"\nChunk 3: \"He discovered the four largest moons of Jupiter, now known as the Galilean moons.\"\nGenerated Answer: \"Galileo Galilei was born in Pisa in 1564.\"\nIn this example, none of the retrieved chunks directly addresses Galileo Galilei's nationality. Although Chunk 1 mentions that he was Italian, the model's response does not incorporate this information. Therefore, none of the retrieved chunks significantly influenced the model's response regarding Galileo Galilei's nationality, indicating that they are Not Attributed.\n#### 4. Chunk Utilization\nChunk Utilization measures the fraction of text in each retrieved chunk that is used for model's response. It ranges from 0 to 1, where a value of 1 indicates that the entire chunk affected the response, while a lower value, such as 0.5, suggests that some text within the chunk did not contribute to the response. Low chunk utilization scores indicate that the chunks retrieved may be longer than necessary.\nMetric signal: Higher chunk utilization is correlated with higher output quality.\nHigh chunk utilization\nUser Query: \"What are the main ingredients used in making pizza?\"\nRetrieved Chunk:\nChunk 1: \"Pizza, a popular Italian dish, typically consists of a flattened bread base topped with tomato sauce, cheese, and various toppings. The base is traditionally made from wheat flour, water, yeast, salt, and olive oil. Tomato sauce, often seasoned with herbs and spices, serves as the primary topping, while mozzarella cheese is commonly used for its melting properties. Additional toppings may include meats such as pepperoni, vegetables like mushrooms and bell peppers, and other ingredients based on regional preferences.\"\nGenerated Answer: \"Pizza typically consists of a flattened bread base topped with tomato sauce, cheese, and various toppings. The base is traditionally made from wheat flour, water, yeast, salt, and olive oil. Tomato sauce, often seasoned with herbs and spices, serves as the primary topping, while mozzarella cheese is commonly used for its melting properties. Additional toppings may include meats such as pepperoni, vegetables like mushrooms and bell peppers, and other ingredients based on regional preferences.\"\nIn this example, the retrieved chunk provides comprehensive information about the main ingredients used in making pizza, including details about the bread base, tomato sauce, cheese, and various toppings. The model's response utilizes the entire chunk effectively, as all the information contributes directly to answering the user query about pizza ingredients. Therefore, the chunk utilization is high.\nLow chunk utilization\nUser Query: \"What is the population of Tokyo, Japan?\"\nRetrieved Chunk:\nChunk 1: \"Tokyo, officially Tokyo Metropolis, is the capital and most populous prefecture of Japan. It is the political, economic, and cultural center of the country. Tokyo is one of the 47 prefectures of Japan, consisting of 23 central city wards and multiple cities, towns, and villages west of the city center. The population of the Tokyo Metropolis is approximately 14 million people, making it one of the most populous urban areas in the world.\"\nGenerated Answer: \"The population of the Tokyo Metropolis is approximately 14 million people.\"\nIn this example, the model's response only utilizes a small portion of the chunk, focusing solely on the population figure. Despite the chunk being attributed to the response, the utilization is low because the majority of the text in the chunk is not directly relevant to answering the user query.\n### Metrics for Evaluating Safety \n![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='1254'%20width='2398'%20xmlns='http://www.w3.org/2000/svg'%20version='1.1'%3E%3C/svg%3E)\n![Galileo Protect](https://cdn.sanity.io/images/tf66morw/production/8d4314e57d008e25a7f477011d8b085b48b7923a-2398x1254.png?w=2398&h=1254&auto=format)\nGalileo Protect\nSafety metrics are essential tools used to evaluate and maintain the safety and integrity of AI models and systems. These metrics encompass various aspects of safety, including the identification and mitigation of potential risks such as the presence of sensitive information, toxic content, inappropriate tones, sexism, and malicious prompt injections. By systematically assessing these factors, [safety metrics](https://www.rungalileo.io/blog/introducing-protect-realtime-hallucination-firewall) play a crucial role in safeguarding users and promoting responsible AI deployment.\n#### PII\nThe PII (Private Identifiable Information) metric identifies instances of PII within a model's responses, specifically flagging sensitive information such as addresses, credit card details, dates, emails, names, network information, personal identification (ID), passwords, phone numbers, social security numbers (SSN), and usernames.\nThis metric automatically detects and flags responses containing PII, enabling the implementation of guardrails or other preventative measures to protect user privacy and prevent potential security breaches.\n#### Toxicity\nThe Toxicity metric flags whether a response contains hateful or toxic content. It provides a binary classification indicating whether a response is toxic or not.\nBy identifying responses with toxic comments, this metric facilitates the implementation of measures such as fine-tuning models or implementing guardrails to flag and mitigate toxic responses, ensuring a safer and more respectful user experience.\n#### Tone\nThe Tone metric classifies the emotional tone of a response into nine different categories, including neutral, joy, love, fear, surprise, sadness, anger, annoyance, and confusion.\nBy categorizing the emotional tone of responses, this metric enables the alignment of responses with user preferences. It allows for optimization by discouraging undesirable tones and promoting preferred emotional responses, thereby enhancing user satisfaction.\n#### Sexism\nThe Sexism metric flags whether a response contains sexist content. It provides a binary classification indicating whether a response is sexist or not.\nThis metric helps identify responses containing sexist comments, allowing for preventive measures such as fine-tuning models or implementing guardrails to flag and mitigate sexist responses, fostering a more inclusive and respectful environment.\n#### Prompt Injection\nThe Prompt Injection metric identifies instances of prompt injection within a model's input, including attacks such as simple instruction attacks, few-shot attacks, impersonation, obfuscation, and context switching.\nBy automatically detecting and classifying user queries with prompt injection attacks, this metric enables the implementation of guardrails or other preventative measures to mitigate potential risks and ensure the model operates within intended parameters, enhancing safety and reliability.\n### Custom Evaluation Metrics\nWe acknowledge that the metrics listed above may not cover all scenarios comprehensively. For instance, LLMs can produce outputs with multiple predictions, necessitating custom metrics for evaluating various aspects of these outputs. Certain applications demand LLM-based chatbots to maintain a specific tone, as deviations can prompt user dissatisfaction. Additionally, developers may seek to identify specific words associated with errors within the outputs. These specialized requirements fall under the umbrella of custom metrics, addressing the diverse range of use cases beyond standard evaluation metrics.\n### Final thoughts\nOver the span of three years, Galileo has been deeply immersed in the study of language model metrics. With Galileo Evaluate, harnessing these metrics for swift experimentation is effortless. You gain immediate access to a concise overview of all runs, facilitating the identification of your top-performing setup. Moreover, the option to craft custom metrics tailored to your precise requirements is at your fingertips for unique applications.\n![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='2070'%20width='2974'%20xmlns='http://www.w3.org/2000/svg'%20version='1.1'%3E%3C/svg%3E)\n![Galileo GenAI Studio](https://cdn.sanity.io/images/tf66morw/production/82130a417b2027cc00983da2ccb86884bbd1a904-2974x2070.png?w=2974&h=2070&auto=format)\nGalileo GenAI Studio\nGalileo [GenAI Studio](https://docs.rungalileo.io/galileo#llm-studio) is the leading platform for rapid evaluation, experimentation and observability for teams building LLM powered applications. It is powered by a [suite of metrics](https://docs.rungalileo.io/galileo/llm-studio/prompt-inspector/choosing-your-guardrail-metrics) to identify and mitigate hallucinations. [Get started](https://www.rungalileo.io/get-started) today!\nExplore our research-backed evaluation metric for hallucination \u2013 read our paper on [Chainpoll](https://arxiv.org/abs/2310.18344).\nTable of contents\nHide\n  1. [ Need for new LLM evaluation metrics](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#need-for-new-llm-evaluation-metrics)\n  2. [Types of LLM Evaluation Metrics](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#types-of-llm-evaluation-metrics)\n    1. [Top Level Metrics for LLM Evaluation](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#top-level-metrics-for-llm-evaluation)\n    2. [Metrics for Evaluating RAG Effectiveness](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#metrics-for-evaluating-rag-effectiveness)\n    3. [Metrics for Evaluating Safety ](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#metrics-for-evaluating-safety)\n    4. [Custom Evaluation Metrics](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#custom-evaluation-metrics)\n    5. [Final thoughts](https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation?utm_medium=email&_hsmi=304542585&utm_content=304542585&utm_source=hs_automation/#final-thoughts)\n\n\n### Subscribe to Newsletter\n[![](https://cdn.sanity.io/images/tf66morw/production/b6ba804260aa8c85927b129e1fba84a92ea3ad87-3400x744.svg?w=3400&h=744&auto=format)](https://www.rungalileo.io/)\nSubscribe to our newsletter\nresearch\n  * [Evaluation Efficiency](https://www.rungalileo.io/research#evaluation-efficiency)\n  * [Hallucination Detection](https://www.rungalileo.io/research#hallucination-detection)\n  * [AI System Diagnostics](https://www.rungalileo.io/research#al-system-diagnostics)\n  * [High Quality Fine-Tuning](https://www.rungalileo.io/research#high-quality-fine-tuning)\n  * [Powerful Metrics](https://www.rungalileo.io/research#powerful-metrics)\n\n\nmodules\n  * [Evaluate](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-evaluate?_gl=1*1rhbtfm*_gcl_au*MTU0MzAwOTU5NS4xNzM2ODY0NDAx)\n  * [Observe](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-observe?_gl=1*1rhbtfm*_gcl_au*MTU0MzAwOTU5NS4xNzM2ODY0NDAx)\n  * [Protect](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-protect)\n  * [Guardrail Metric](https://docs.galileo.ai/galileo/gen-ai-studio-products/galileo-guardrail-metrics)\n\n\nresources\n  * [Docs](https://docs.rungalileo.io/galileo)\n  * [Blog](https://www.rungalileo.io/blog)\n  * [Hallucination Index](https://www.rungalileo.io/hallucinationindex)\n  * [Mastering RAG eBook](https://www.rungalileo.io/mastering-rag)\n  * [Podcast](https://pod.link/1776879655)\n  * [Request a Demo](https://www.rungalileo.io/get-started)\n\n\ncompany\n  * [Careers](https://www.rungalileo.io/team)\n  * [Privacy Policy](https://drive.google.com/file/d/1kIVXS3F6YAg7KUpZwCpB0BB5vikt2cy8/view)\n  * [Term of Service](https://docs.google.com/document/d/e/2PACX-1vRANTV4gmxpLFggXZRxGofzj65o0bRs8Bp8he2_3psEEPg113D0HD0krqydg-rk-g/pub)\n  * [Case Studies](https://www.rungalileo.io/case-studies)\n\n\n  * [![](https://cdn.sanity.io/images/tf66morw/production/9d89afe946dee14fe310a7aff00a3171d3321340-24x24.svg)![](https://cdn.sanity.io/images/tf66morw/production/2dd33ae72f1ecc3ca644c05b6d8ffcea8def210f-24x24.svg)](https://www.linkedin.com/company/galileo-ai)\n  * [![](https://cdn.sanity.io/images/tf66morw/production/61322ff99c5872369cad5c94091866e4662579dd-24x24.svg)![](https://cdn.sanity.io/images/tf66morw/production/fde049779533431f0cecc3ba12f45cc5c6222bd0-24x24.svg)](https://x.com/rungalileo)\n  * [![](https://cdn.sanity.io/images/tf66morw/production/e74b8448396b837a533c09d917591e50d343d760-24x24.svg)![](https://cdn.sanity.io/images/tf66morw/production/4d7bdaa3d3f37ec29118e7fee4008655d319b838-24x24.svg)](https://www.youtube.com/@rungalileo)\n  * [![](https://cdn.sanity.io/images/tf66morw/production/51fd03602d9c78f66d6cad3095e025d8bc82f0b6-24x24.svg)![](https://cdn.sanity.io/images/tf66morw/production/2ac9c6c1ebacb7536f421200a5f0fdfa51dfe452-24x24.svg)](https://pod.link/1776879655)\n\n\nContact us at info@galileo.ai\n\u00a9 2025 Galileo. All rights reserved.\n![AICPA SOC Logo](https://www.rungalileo.io/footer-logo-right-corner.svg)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nGalileo AI emphasizes the need for new metrics to evaluate Large Language Models (LLMs) due to challenges like high costs of human evaluation and poor correlation with human judgment. Key metrics include Context Adherence, Correctness, and Safety metrics, which help assess LLM performance and mitigate hallucinations.\n```"
  },
  {
    "instruction": "[Skip to main content](https://www.databricks.com/product/unity-catalog/#main)\n[![](https://www.databricks.com/product/unity-catalog/)](https://www.databricks.com/)\n[Login](https://login.databricks.com/?dbx_source=www&itm=main-cta-login&l=en-EN)\n[![](https://www.databricks.com/product/unity-catalog/)](https://www.databricks.com/)\n  * Why Databricks \n    *       * Discover\n        * [For Executives](https://www.databricks.com/why-databricks/executives)\n        * [For Startups ](https://www.databricks.com/product/startups)\n        * [Lakehouse Architecture ](https://www.databricks.com/product/data-lakehouse)\n        * [Mosaic Research](https://www.databricks.com/research/mosaic)\n      * Customers\n        * [Featured Stories](https://www.databricks.com/customers)\n        * [See All Customers](https://www.databricks.com/customers/all)\n      * Partners\n        * [Cloud ProvidersDatabricks on AWS, Azure, and GCP](https://www.databricks.com/company/partners/cloud-partners)\n        * [Consulting & System IntegratorsExperts to build, deploy and migrate to Databricks](https://www.databricks.com/company/partners/consulting-and-si)\n        * [Technology PartnersConnect your existing tools to your Lakehouse](https://www.databricks.com/company/partners/technology-partner-program)\n        * [C&SI Partner ProgramBuild, deploy or migrate to the Lakehouse](https://www.databricks.com/company/partners/consulting-and-si/candsi-partner-program)\n        * [Data PartnersAccess the ecosystem of data consumers](https://www.databricks.com/company/partners/data-partner-program)\n        * [Partner SolutionsFind custom industry and migration solutions](https://www.databricks.com/company/partners/consulting-and-si/partner-solutions)\n        * [Built on DatabricksBuild, market and grow your business](https://www.databricks.com/company/partners/built-on-partner-program)\n  * Product \n    *       * Databricks Platform\n        * [Platform OverviewA unified platform for data, analytics and AI](https://www.databricks.com/product/data-intelligence-platform)\n        * [Data ManagementData reliability, security and performance](https://www.databricks.com/product/delta-lake-on-databricks)\n        * [SharingAn open, secure, zero-copy sharing for all data](https://www.databricks.com/product/delta-sharing)\n        * [Data WarehousingServerless data warehouse for SQL analytics](https://www.databricks.com/product/databricks-sql)\n        * [GovernanceUnified governance for all data, analytics and AI assets](https://www.databricks.com/product/unity-catalog)\n        * [Real-Time AnalyticsReal-time analytics, AI and applications made simple](https://www.databricks.com/product/data-streaming)\n        * [Artificial IntelligenceBuild and deploy ML and GenAI applications](https://www.databricks.com/product/artificial-intelligence)\n        * [Data EngineeringETL and orchestration for batch and streaming data](https://www.databricks.com/solutions/data-engineering)\n        * [Business IntelligenceIntelligent analytics for real-world data](https://www.databricks.com/product/business-intelligence)\n        * [Data ScienceCollaborative data science at scale](https://www.databricks.com/product/data-science)\n      * Integrations and Data\n        * [MarketplaceOpen marketplace for data, analytics and AI](https://www.databricks.com/product/marketplace)\n        * [IDE IntegrationsBuild on the Lakehouse in your favorite IDE](https://www.databricks.com/product/data-science/ide-integrations)\n        * [Partner ConnectDiscover and integrate with the Databricks ecosystem](https://www.databricks.com/partnerconnect)\n      * Pricing\n        * [Databricks PricingExplore product pricing, DBUs and more](https://www.databricks.com/product/pricing)\n        * [Cost CalculatorEstimate your compute costs on any cloud](https://www.databricks.com/product/pricing/product-pricing/instance-types)\n      * Open Source\n        * [Open Source TechnologiesLearn more about the innovations behind the platform](https://www.databricks.com/product/open-source)\n  * Solutions \n    *       * Databricks for Industries\n        * [Communications](https://www.databricks.com/solutions/industries/communications)\n        * [Media and Entertainment](https://www.databricks.com/solutions/industries/media-and-entertainment)\n        * [Financial Services](https://www.databricks.com/solutions/industries/financial-services)\n        * [Public Sector](https://www.databricks.com/solutions/industries/public-sector)\n        * [Healthcare & Life Sciences](https://www.databricks.com/solutions/industries/healthcare-and-life-sciences)\n        * [Retail](https://www.databricks.com/solutions/industries/retail-industry-solutions)\n        * [Manufacturing](https://www.databricks.com/solutions/industries/manufacturing-industry-solutions)\n        * [See All Industries](https://www.databricks.com/solutions)\n      * Cross Industry Solutions\n        * [Customer Data Platform](https://www.databricks.com/solutions/industries/customer-experience)\n        * [Cyber Security](https://www.databricks.com/solutions/industries/cybersecurity)\n      * Migration & Deployment\n        * [Data Migration](https://www.databricks.com/solutions/migration)\n        * [Professional Services](https://www.databricks.com/professional-services)\n      * Solution Accelerators\n        * [Explore AcceleratorsMove faster toward outcomes that matter](https://www.databricks.com/solutions/accelerators)\n  * Resources \n    *       * Training and Certification\n        * [Learning OverviewHub for training, certification, events and more](https://www.databricks.com/learn)\n        * [Training OverviewDiscover curriculum tailored to your needs](https://www.databricks.com/learn/training/home)\n        * [Databricks AcademySign in to the Databricks learning platform](https://www.databricks.com/learn/training/login)\n        * [CertificationGain recognition and differentiation](https://www.databricks.com/learn/training/certification)\n        * [University AllianceWant to teach Databricks? See how.](https://www.databricks.com/university)\n      * Events\n        * [Data + AI Summit](https://www.databricks.com/dataaisummit)\n        * [Data + AI World Tour](https://www.databricks.com/dataaisummit/worldtour)\n        * [Data Intelligence Days](https://www.databricks.com/lp/data-intelligence-days)\n        * [Event Calendar](https://www.databricks.com/events)\n      * Blog and Podcasts\n        * [Databricks BlogExplore news, product announcements, and more](https://www.databricks.com/blog)\n        * [Databricks Mosaic Research BlogDiscover the latest in our Gen AI research](https://www.databricks.com/blog/category/generative-ai/mosaic-research)\n        * [Data Brew PodcastLet\u2019s talk data!](https://www.databricks.com/discover/data-brew)\n        * [Champions of Data + AI PodcastInsights from data leaders powering innovation](https://www.databricks.com/discover/champions-of-data-and-ai)\n      * Get Help\n        * [Customer Support](https://www.databricks.com/support)\n        * [Documentation](https://docs.databricks.com/en/index.html)\n        * [Community](https://community.databricks.com/s/)\n      * Dive Deep\n        * [Resource Center](https://www.databricks.com/resources)\n        * [Demo Center](https://www.databricks.com/resources/demos)\n  * About \n    *       * Company\n        * [Who We Are](https://www.databricks.com/company/about-us)\n        * [Our Team](https://www.databricks.com/company/leadership-team)\n        * [Databricks Ventures](https://www.databricks.com/databricks-ventures)\n        * [Contact Us](https://www.databricks.com/company/contact)\n      * Careers\n        * [Working at Databricks](https://www.databricks.com/company/careers)\n        * [Open Jobs](https://www.databricks.com/company/careers/open-positions)\n      * Press\n        * [Awards and Recognition](https://www.databricks.com/company/awards-and-recognition)\n        * [Newsroom](https://www.databricks.com/company/newsroom)\n      * Security and Trust\n        * [Security and Trust](https://www.databricks.com/trust)\n\n\n  * Ready to get started?\n  * [Get a Demo](https://www.databricks.com/resources/demos)\n\n\n  * [Login](https://login.databricks.com/?dbx_source=www&itm=main-cta-login&l=en-EN)\n  * [Contact Us](https://www.databricks.com/company/contact)\n  * [Try Databricks](https://login.databricks.com/?dbx_source=www&itm_data=databricks-web-nav&intent=SIGN_UP&l=en-EN)\n\n\n# \nUnity Catalog\n### Unified and open governance for data and AI\n[Get started](https://login.databricks.com/?dbx_source=www&intent=SIGN_UP \"Get started\")[Watch demo](https://www.databricks.com/resources/demos/videos/data-governance/unity-catalog-overview \"Watch demo\")\n[![](https://www.databricks.com/sites/default/files/2025-02/2023-10-EB-Data-and-AI-Governance-PC-107x107-2x.png?v=1740752502) A Comprehensive Guide to Data and AI Governance Build a unified governance strategy for your data and AI estate. Get the eBook](https://www.databricks.com/resources/ebook/data-analytics-and-ai-governance?itm_data=product-unity-catalog-pc-feb25)\nDatabricks Unity Catalog is the industry\u2019s only unified and open governance solution for data and AI, built into the Databricks Data Intelligence Platform. With Unity Catalog, organizations can seamlessly govern both structured and unstructured data in any format, as well as machine learning models, notebooks, dashboards and files across any cloud or platform. Data scientists, analysts and engineers can securely discover, access and collaborate on trusted data and AI assets across platforms, leveraging AI to boost productivity and unlock the full potential of the lakehouse environment. This unified and open approach to governance promotes interoperability and accelerates data and AI initiatives while simplifying regulatory compliance.\n### \u201cDatabricks Unity Catalog is now an integral part of the PepsiCo Data Foundation, our centralized global system that consolidates over 6 petabytes of data worldwide. It streamlines the onboarding process for more than 1,500 active users and enables unified data discovery for our 30+ digital product teams across the globe, supporting both business intelligence and artificial intelligence applications.\u201d\n\u2014 Bhaskar Palit, Senior Director, Data and Analytics\n[Learn more](https://www.databricks.com/blog/how-pepsico-established-enterprise-grade-data-intelligence-platform-powered-databricks-unity)\n![pepsico](https://www.databricks.com/sites/default/files/2024-06/pepsico-customer-image.png?v=1717761445)\n### \u201cUnity Catalog distills permissioning into its important pieces by removing the complexity introduced by IAM policies and other data control platforms, allowing us to focus on what is important to our business use case.\u201d\n\u2014 Joseph Kesting, Software Engineer\n[Learn more](https://www.databricks.com/customers/block/unity-catalog)\n![Block](https://www.databricks.com/sites/default/files/2023-06/Block.png?v=1687639080)\n### \u201cUnity Catalog has enabled us to centralize our data governance, providing robust security features and streamlined compliance processes. This centralization reduced the complexities associated with managing permissions across disparate systems and helped ensure that only authorized personnel had access to sensitive data, which is crucial for adhering to stringent data protection laws, including GDPR.\u201d\n\u2014 Michael Ewins, Director of Engineering\n[Learn more](https://www.databricks.com/blog/data-destinations-how-skyscanner-optimizes-traveler-experiences-databricks-unity-catalog)\n![Skyscanner](https://www.databricks.com/sites/default/files/2024-06/skyscanner-customer-image.png?v=1717760942)\n### \u201cWith Unity Catalog, we are centralizing data catalog and access management across various teams and workspaces, which has simplified governance.\u201d\n\u2014 Venkatesh Sivasubramanian, Senior Director, Big Data\n[Learn more](https://www.databricks.com/customers/rivian)\n![Rivian](https://www.databricks.com/sites/default/files/2023-06/Rivian.png?v=1687639181)\n### \u201cDistributing data governance through Databricks Unity Catalog transformed Akamai\u2019s approach to managing and governing data. With Unity Catalog, we are now managing and governing over 6 petabytes of data with fine-grained access controls on rows and columns.\u201d\n\u2014 Gilad Asulin, Big Data Team Leader\n[Learn more](https://medium.com/@agilad_3118/how-akamai-leverages-databricks-unity-catalog-for-distributed-data-governance-d1eda1c5851)\n![Akamai](https://www.databricks.com/sites/default/files/2023-06/Akamai.png?v=1687639211)\n![PepsiCo logo](https://www.databricks.com/sites/default/files/2024-06/logo-color-pepsico.svg?v=1717720292)\n![block](https://www.databricks.com/sites/default/files/2023-09/logo-color-block-image.png?v=1724434337)\n![Skyscanner](https://www.databricks.com/sites/default/files/2024-06/logo-color-skyscanner.svg?v=1724667400)\n![Rivian logo](https://www.databricks.com/sites/default/files/2023-03/logo-color-rivian.svg?v=1678822598)\n![Akamai](https://www.databricks.com/sites/default/files/2023-03/akamai-logo.svg?v=1724432005)\n![UC](https://www.databricks.com/sites/default/files/2023-06/unity-catalog1.png?v=1687797543)\n### Unified visibility into data and AI\nEasily discover and classify both structured and unstructured data in any format, including machine learning models, notebooks, dashboards and files across all cloud platforms. Seamlessly manage, govern and query data from external databases and data warehouses like MySQL, PostgreSQL, Amazon Redshift, Snowflake, Azure SQL, Azure Synapse, Google BigQuery and catalogs such as HMS and AWS Glue in one place. Accelerate your data and AI initiatives with a single point of access for data exploration. Improve productivity with intelligent search, discovery and automatically generated data insights and documentation.\n![Single permission model for data and AI](https://www.databricks.com/sites/default/files/2024-06/uc-image-02.png?v=1717757695)\n### Single permission model for data and AI\nSimplify access management with a unified interface to define access policies on data and AI assets and consistently apply and audit these policies on any cloud or data platform. Securely access data from other computing platforms using open interfaces, with consistent permissions managed in one place. Enhance security with fine-grained control on rows and columns, while efficiently managing access through low-code attribute-based access policies that scale seamlessly.\n![A dashboard displaying data and graphs.](https://www.databricks.com/sites/default/files/2023-06/unity-catalog-3.png?v=1703004975)\n### AI-powered monitoring and observability\nHarness the power of AI to automate monitoring, diagnose errors and uphold data and ML model quality. Benefit from proactive alerts that automatically detect personally identifiable information (PII) data, track model drift, and effectively resolve issues within your data and AI pipelines to maintain accuracy and integrity. Streamline debugging, root cause analysis, and impact assessment with automated column-level data lineage. Gain comprehensive observability into your data and AI with operational intelligence utilizing built-in system tables for billing, auditing, lineage and more.\n![Open accessibility](https://www.databricks.com/sites/default/files/2024-06/uc-image-04.png?v=1717759484)\n### Open accessibility\nSecurely access your data and AI assets from any compute engine using open APIs and standard interfaces. Share data and AI assets across clouds, regions and platforms with open source Delta Sharing. Securely collaborate with anyone, anywhere to unlock new revenue streams and drive business value, without relying on proprietary formats, complex ETL processes or costly data replication.\n## Here's more to explore\n[![](https://www.databricks.com/sites/default/files/2024-03/2023-05-wb-introducing-generative-ai-ty-tn-362x190-2x.png?v=1711095832)TrainingGenerative AI FundamentalsStart now](https://www.databricks.com/resources/learn/training/generative-ai-fundamentals?itm_data=cro-fpd-target-360)[![](https://www.databricks.com/sites/default/files/2023-11/2023-04-lakehouse-fundamentals-build-your-skill-ty-tn-362x190-2x.png?v=1700688856)TrainingLakehouse FundamentalsStart now](https://www.databricks.com/resources/learn/training/lakehouse-fundamentals?itm_data=cro-fpd-target-360)[![](https://www.databricks.com/sites/default/files/2024-10/2022-09-Databricks-on-Azure-3-part-training-series-TY-TN-360x188-2x.jpg?v=1729099703)TrainingFree Azure Databricks TrainingWatch now](https://www.databricks.com/resources/webinar/azure-databricks-free-training-series?itm_data=cro-fpd-target-360)\n## Here's more to explore\n[![](https://www.databricks.com/sites/default/files/2024-03/2023-05-wb-introducing-generative-ai-ty-tn-362x190-2x.png?v=1711095832)TrainingGenerative AI FundamentalsStart now](https://www.databricks.com/resources/learn/training/generative-ai-fundamentals?itm_data=cro-fpd-target-360)[![](https://www.databricks.com/sites/default/files/2023-11/2023-04-lakehouse-fundamentals-build-your-skill-ty-tn-362x190-2x.png?v=1700688856)TrainingLakehouse FundamentalsStart now](https://www.databricks.com/resources/learn/training/lakehouse-fundamentals?itm_data=cro-fpd-target-360)[![](https://www.databricks.com/sites/default/files/2024-07/2024-07-eb-big-book-of-data-engineering-3rd-ed-ty-tn-360x188-2x.png?v=1721029189)eBookBig Book of Data EngineeringRead now](https://www.databricks.com/resources/ebook/big-book-of-data-engineering?itm_data=cro-fpd-target-360)\n## Resources\n[![](https://www.databricks.com/sites/default/files/2023-10/2023-10-eb-data-and-ai-governance-ad-1200x628.png?v=1702986485)eBookA Comprehensive Guide to Data and AI GovernanceRead now](https://www.databricks.com/resources/ebook/data-analytics-and-ai-governance)[![](https://www.databricks.com/sites/default/files/2023-06/2022-06-EB-New-Approach-to-Data-Sharing-Ad-1200x628.png?v=1687654866)eBookA New Approach to Data SharingRead now](https://www.databricks.com/resources/ebook/a-new-approach-to-data-sharing)[![](https://www.databricks.com/sites/default/files/2024-04/2024-02-wp-databricks-ai-security-framework-cover-ty-tn-360x188-2x.png?v=1712042992)WhitepaperDatabricks AI Security FrameworkRead now](https://www.databricks.com/resources/whitepaper/databricks-ai-security-framework-dasf)\n## Integrations\nUnity Catalog works with your existing data catalogs, data storage systems and governance solutions so you can leverage your existing investments and build a future-proof governance model without expensive migration costs.\n[![](https://www.databricks.com/sites/default/files/2025-02/LP-hero-image-2023-09-EB-A-New-Approach-to-Data-Sharing-460x460-2x.jpg?v=1738573853) Share data and AI securely Learn the easiest way to share across platforms. Download now](https://www.databricks.com/resources/ebook/a-new-approach-to-data-sharing?itm_data=product-unity-catalog-pc2-feb5)\n## Resources\n[![ebook: A Comprehensive Guide to Data and AI Governance](https://www.databricks.com/sites/default/files/2023-10/2023-10-eb-data-and-ai-governance-ad-1200x628.png)EbookA Comprehensive Guide to Data and AI Governance Read now](https://www.databricks.com/resources/ebook/data-analytics-and-ai-governance)\n[![eBook: A New Approach to Data Sharing](https://www.databricks.com/sites/default/files/2023-06/2022-06-EB-New-Approach-to-Data-Sharing-Ad-1200x628.png)eBookA New Approach to Data Sharing Read now](https://www.databricks.com/resources/ebook/a-new-approach-to-data-sharing)\n[![Databricks AI Security Framework](https://www.databricks.com/sites/default/files/2024-04/2024-02-wp-databricks-ai-security-framework-cover-ty-tn-360x188-2x.png)WhitepaperDatabricks AI Security Framework Read now](https://www.databricks.com/resources/whitepaper/databricks-ai-security-framework-dasf)\n### Featured Talks\n[[Keynote] Evolving Data Governance With Unity Catalog Presented by Matei Zaharia at Data + AI Summit 2024](https://www.youtube.com/watch?v=lZbmBAxa3O4&list=PLTPXxbhUt-YXh57uL3QUQiKLwTf39IoT1&index=17)\n[What\u2019s New in Unity Catalog - With Live Demos](https://youtu.be/vnznr18oGfI)\n[Technical Deep Dive for Practitioners: Databricks Unity Catalog from A-Z](https://www.youtube.com/watch?v=iMGG1Ng7sy4)\n[Introducing Unity Catalog Open Source: Unified Governance and Interoperability for Data + AI ](https://youtu.be/COnxxM0gJmw)\n[Attribute-Based Access Controls in Unity Catalog - Building a Scalable Access Management Framework](https://www.youtube.com/watch?v=KufwTp7MQD4)\n[Lakehouse Federation: Discover, Query and Govern Any Data with Unity Catalog](https://www.youtube.com/watch?v=xGImEGpw0uE)\n[Lakehouse Monitoring GA: Profiling, Diagnosing, and Enforcing Data Quality with Intelligence](https://www.youtube.com/watch?v=aDBPoKyA0DQ)\n[Upgrading to Unity Catalog With Ease, Using UCX](https://www.youtube.com/watch?v=pmW9jOFE0qI)\n### Blogs\n[What\u2019s New with Databricks Unity Catalog at Data + AI Summit 2024](https://www.databricks.com/blog/whats-new-databricks-unity-catalog-data-ai-summit-2024)\n[Open Sourcing Unity Catalog](https://www.databricks.com/blog/open-sourcing-unity-catalog)\n[What\u2019s New with Data Sharing and Collaboration](https://www.databricks.com/blog/whats-new-data-sharing-and-collaboration)\n[Unity Catalog Lakeguard: Industry-first and only data governance for multi-user Apache Spark\u2122 clusters](https://www.databricks.com/blog/unity-catalog-lakeguard-industry-first-and-only-data-governance-multi-user-apachetm-spark)\n[Introducing Lakehouse Federation Capabilities in Unity Catalog](https://www.databricks.com/blog/introducing-lakehouse-federation-capabilities-unity-catalog)\n[Built-In Governance for Your Databricks Workspace](https://www.databricks.com/blog/built-governance-your-databricks-workspace)\n### Getting Started\n[AWS Documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n[Azure Documentation](https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/)\n[GCP Documentation](https://docs.gcp.databricks.com/data-governance/unity-catalog/index.html)\n## Ready to get started?\n[Try Databricks for free](https://login.databricks.com/?dbx_source=www&intent=SIGN_UP)\n[![databricks logo](https://www.databricks.com/sites/default/files/2023-08/databricks-default.png?v=1712162038)](https://www.databricks.com/)\nWhy Databricks\nDiscover\n  * [For Executives](https://www.databricks.com/why-databricks/executives)\n  * [For Startups](https://www.databricks.com/product/startups)\n  * [Lakehouse Architecture](https://www.databricks.com/product/data-lakehouse)\n  * [Mosaic Research](https://www.databricks.com/research/mosaic)\n\n\nCustomers\n  * [Featured](https://www.databricks.com/customers)\n  * [See All](https://www.databricks.com/customers/all)\n\n\nPartners\n  * [Cloud Providers](https://www.databricks.com/company/partners/cloud-partners)\n  * [Technology Partners](https://www.databricks.com/company/partners/technology-partner-program)\n  * [Data Partners](https://www.databricks.com/company/partners/data-partner-program)\n  * [Built on Databricks](https://www.databricks.com/company/partners/built-on-partner-program)\n  * [Consulting & System Integrators](https://www.databricks.com/company/partners/consulting-and-si)\n  * [C&SI Partner Program](https://www.databricks.com/company/partners/consulting-and-si/candsi-partner-program)\n  * [Partner Solutions](https://www.databricks.com/company/partners/consulting-and-si/partner-solutions)\n\n\nWhy Databricks\nDiscover\n  * [For Executives](https://www.databricks.com/why-databricks/executives)\n  * [For Startups](https://www.databricks.com/product/startups)\n  * [Lakehouse Architecture](https://www.databricks.com/product/data-lakehouse)\n  * [Mosaic Research](https://www.databricks.com/research/mosaic)\n\n\nCustomers\n  * [Featured](https://www.databricks.com/customers)\n  * [See All](https://www.databricks.com/customers/all)\n\n\nPartners\n  * [Cloud Providers](https://www.databricks.com/company/partners/cloud-partners)\n  * [Technology Partners](https://www.databricks.com/company/partners/technology-partner-program)\n  * [Data Partners](https://www.databricks.com/company/partners/data-partner-program)\n  * [Built on Databricks](https://www.databricks.com/company/partners/built-on-partner-program)\n  * [Consulting & System Integrators](https://www.databricks.com/company/partners/consulting-and-si)\n  * [C&SI Partner Program](https://www.databricks.com/company/partners/consulting-and-si/candsi-partner-program)\n  * [Partner Solutions](https://www.databricks.com/company/partners/consulting-and-si/partner-solutions)\n\n\nProduct\nDatabricks Platform\n  * [Platform Overview](https://www.databricks.com/product/data-intelligence-platform)\n  * [Sharing](https://www.databricks.com/product/delta-sharing)\n  * [Governance](https://www.databricks.com/product/unity-catalog)\n  * [Artificial Intelligence](https://www.databricks.com/product/artificial-intelligence)\n  * [Business Intelligence](https://www.databricks.com/product/business-intelligence)\n  * [Data Management](https://www.databricks.com/product/delta-lake-on-databricks)\n  * [Data Warehousing](https://www.databricks.com/product/databricks-sql)\n  * [Real-Time Analytics](https://www.databricks.com/product/data-streaming)\n  * [Data Engineering](https://www.databricks.com/solutions/data-engineering)\n  * [Data Science](https://www.databricks.com/product/data-science)\n\n\nPricing\n  * [Pricing Overview](https://www.databricks.com/product/pricing)\n  * [Pricing Calculator](https://www.databricks.com/product/pricing/product-pricing/instance-types)\n\n\n[Open Source](https://www.databricks.com/product/open-source)\nIntegrations and Data\n  * [Marketplace](https://www.databricks.com/product/marketplace)\n  * [IDE Integrations](https://www.databricks.com/product/data-science/ide-integrations)\n  * [Partner Connect](https://www.databricks.com/partnerconnect)\n\n\nProduct\nDatabricks Platform\n  * [Platform Overview](https://www.databricks.com/product/data-intelligence-platform)\n  * [Sharing](https://www.databricks.com/product/delta-sharing)\n  * [Governance](https://www.databricks.com/product/unity-catalog)\n  * [Artificial Intelligence](https://www.databricks.com/product/artificial-intelligence)\n  * [Business Intelligence](https://www.databricks.com/product/business-intelligence)\n  * [Data Management](https://www.databricks.com/product/delta-lake-on-databricks)\n  * [Data Warehousing](https://www.databricks.com/product/databricks-sql)\n  * [Real-Time Analytics](https://www.databricks.com/product/data-streaming)\n  * [Data Engineering](https://www.databricks.com/solutions/data-engineering)\n  * [Data Science](https://www.databricks.com/product/data-science)\n\n\nPricing\n  * [Pricing Overview](https://www.databricks.com/product/pricing)\n  * [Pricing Calculator](https://www.databricks.com/product/pricing/product-pricing/instance-types)\n\n\nOpen Source\nIntegrations and Data\n  * [Marketplace](https://www.databricks.com/product/marketplace)\n  * [IDE Integrations](https://www.databricks.com/product/data-science/ide-integrations)\n  * [Partner Connect](https://www.databricks.com/partnerconnect)\n\n\nSolutions\nDatabricks For Industries\n  * [Communications](https://www.databricks.com/solutions/industries/communications)\n  * [Financial Services](https://www.databricks.com/solutions/industries/financial-services)\n  * [Healthcare and Life Sciences](https://www.databricks.com/solutions/industries/healthcare-and-life-sciences)\n  * [Manufacturing](https://www.databricks.com/solutions/industries/manufacturing-industry-solutions)\n  * [Media and Entertainment](https://www.databricks.com/solutions/industries/media-and-entertainment)\n  * [Public Sector](https://www.databricks.com/solutions/industries/federal-government)\n  * [Retail](https://www.databricks.com/solutions/industries/retail-industry-solutions)\n  * [View All](https://www.databricks.com/solutions)\n\n\nCross Industry Solutions\n  * [Customer Data Platform](https://www.databricks.com/solutions/industries/customer-experience)\n  * [Cyber Security](https://www.databricks.com/solutions/industries/cybersecurity)\n\n\n[Data Migration](https://www.databricks.com/solutions/migration)\n[Professional Services](https://www.databricks.com/professional-services)\n[Solution Accelerators](https://www.databricks.com/solutions/accelerators)\nSolutions\nDatabricks For Industries\n  * [Communications](https://www.databricks.com/solutions/industries/communications)\n  * [Financial Services](https://www.databricks.com/solutions/industries/financial-services)\n  * [Healthcare and Life Sciences](https://www.databricks.com/solutions/industries/healthcare-and-life-sciences)\n  * [Manufacturing](https://www.databricks.com/solutions/industries/manufacturing-industry-solutions)\n  * [Media and Entertainment](https://www.databricks.com/solutions/industries/media-and-entertainment)\n  * [Public Sector](https://www.databricks.com/solutions/industries/federal-government)\n  * [Retail](https://www.databricks.com/solutions/industries/retail-industry-solutions)\n  * [View All](https://www.databricks.com/solutions)\n\n\nCross Industry Solutions\n  * [Customer Data Platform](https://www.databricks.com/solutions/industries/customer-experience)\n  * [Cyber Security](https://www.databricks.com/solutions/industries/cybersecurity)\n\n\nData Migration\nProfessional Services\nSolution Accelerators\nResources\n[Documentation](https://docs.databricks.com/en/index.html)\n[Customer Support](https://www.databricks.com/support)\n[Community](https://community.databricks.com/)\nTraining and Certification\n  * [Learning Overview](https://www.databricks.com/learn)\n  * [Training Overview](https://www.databricks.com/learn/training/home)\n  * [Certification](https://www.databricks.com/learn/training/certification)\n  * [University Alliance](https://www.databricks.com/university)\n  * [Databricks Academy Login](https://www.databricks.com/learn/training/login)\n\n\nEvents\n  * [Data + AI Summit](https://www.databricks.com/dataaisummit)\n  * [Data + AI World Tour](https://www.databricks.com/dataaisummit/worldtour)\n  * [Data Intelligence Days](https://www.databricks.com/lp/data-intelligence-days)\n  * [Full Calendar](https://www.databricks.com/events)\n\n\nBlog and Podcasts\n  * [Databricks Blog](https://www.databricks.com/blog)\n  * [Databricks Mosaic Research Blog](https://www.databricks.com/blog/category/generative-ai/mosaic-research)\n  * [Data Brew Podcast](https://www.databricks.com/discover/data-brew)\n  * [Champions of Data & AI Podcast](https://www.databricks.com/discover/champions-of-data-and-ai)\n\n\nResources\nDocumentation\nCustomer Support\nCommunity\nTraining and Certification\n  * [Learning Overview](https://www.databricks.com/learn)\n  * [Training Overview](https://www.databricks.com/learn/training/home)\n  * [Certification](https://www.databricks.com/learn/training/certification)\n  * [University Alliance](https://www.databricks.com/university)\n  * [Databricks Academy Login](https://www.databricks.com/learn/training/login)\n\n\nEvents\n  * [Data + AI Summit](https://www.databricks.com/dataaisummit)\n  * [Data + AI World Tour](https://www.databricks.com/dataaisummit/worldtour)\n  * [Data Intelligence Days](https://www.databricks.com/lp/data-intelligence-days)\n  * [Full Calendar](https://www.databricks.com/events)\n\n\nBlog and Podcasts\n  * [Databricks Blog](https://www.databricks.com/blog)\n  * [Databricks Mosaic Research Blog](https://www.databricks.com/blog/category/generative-ai/mosaic-research)\n  * [Data Brew Podcast](https://www.databricks.com/discover/data-brew)\n  * [Champions of Data & AI Podcast](https://www.databricks.com/discover/champions-of-data-and-ai)\n\n\nAbout\nCompany\n  * [Who We Are](https://www.databricks.com/company/about-us)\n  * [Our Team](https://www.databricks.com/company/leadership-team)\n  * [Databricks Ventures](https://www.databricks.com/databricks-ventures)\n  * [Contact Us](https://www.databricks.com/company/contact)\n\n\nCareers\n  * [Open Jobs](https://www.databricks.com/company/careers/open-positions)\n  * [Working at Databricks](https://www.databricks.com/company/careers)\n\n\nPress\n  * [Awards and Recognition](https://www.databricks.com/company/awards-and-recognition)\n  * [Newsroom](https://www.databricks.com/company/newsroom)\n\n\n[Security and Trust](https://www.databricks.com/trust)\nAbout\nCompany\n  * [Who We Are](https://www.databricks.com/company/about-us)\n  * [Our Team](https://www.databricks.com/company/leadership-team)\n  * [Databricks Ventures](https://www.databricks.com/databricks-ventures)\n  * [Contact Us](https://www.databricks.com/company/contact)\n\n\nCareers\n  * [Open Jobs](https://www.databricks.com/company/careers/open-positions)\n  * [Working at Databricks](https://www.databricks.com/company/careers)\n\n\nPress\n  * [Awards and Recognition](https://www.databricks.com/company/awards-and-recognition)\n  * [Newsroom](https://www.databricks.com/company/newsroom)\n\n\nSecurity and Trust\n[![databricks logo](https://www.databricks.com/sites/default/files/2023-08/databricks-default.png?v=1712162038)](https://www.databricks.com/)\nDatabricks Inc. 160 Spear Street, 15th Floor San Francisco, CA 94105 1-866-330-0121\n  * [](https://www.linkedin.com/company/databricks)\n  * [](https://www.facebook.com/pages/Databricks/560203607379694)\n  * [](https://twitter.com/databricks)\n  * [](https://www.databricks.com/feed)\n  * [](https://www.glassdoor.com/Overview/Working-at-Databricks-EI_IE954734.11,21.htm)\n  * [](https://www.youtube.com/@Databricks)\n\n\n![](https://www.databricks.com/sites/default/files/2021/02/telco-icon-2.png?v=1715274112)\n[See Careers at Databricks](https://www.databricks.com/company/careers)\n  * [](https://www.linkedin.com/company/databricks)\n  * [](https://www.facebook.com/pages/Databricks/560203607379694)\n  * [](https://twitter.com/databricks)\n  * [](https://www.databricks.com/feed)\n  * [](https://www.glassdoor.com/Overview/Working-at-Databricks-EI_IE954734.11,21.htm)\n  * [](https://www.youtube.com/@Databricks)\n\n\n\u00a9 Databricks 2025. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the [Apache Software Foundation.](https://www.apache.org/)\n  * [Privacy Notice](https://www.databricks.com/legal/privacynotice)\n  * |[Terms of Use](https://www.databricks.com/terms-of-use)\n  * |[Modern Slavery Statement](https://www.databricks.com/legal/modern-slavery-policy-statement)\n  * |[California Privacy](https://www.databricks.com/legal/supplemental-privacy-notice-california-residents)\n  * |[Your Privacy Choices](https://www.databricks.com/product/unity-catalog/#yourprivacychoices)\n  * ![](https://www.databricks.com/sites/default/files/2022-12/gpcicon_small.png)\n\n\n",
    "answer": "```markdown\n# TL;DR Summary\n\nDatabricks Unity Catalog offers unified governance for data and AI, enabling secure access and collaboration across platforms. It simplifies data management, enhances compliance, and supports interoperability, making it essential for organizations like PepsiCo and Akamai.\n```"
  },
  {
    "instruction": "[Skip to main content](https://www.datacamp.com/tutorial/speculative-decoding/#main)\n[](https://www.datacamp.com/)[**Write for us**](https://datacamp-1.gitbook.io/community-submissions)\nEN\n[Log In](https://www.datacamp.com/users/sign_in?redirect=%2Ftutorial%2Fspeculative-decoding)[Get Started](https://www.datacamp.com/users/sign_up?redirect=%2Ftutorial%2Fspeculative-decoding)\nTutorials\n[Blogs](https://www.datacamp.com/blog)\n[Tutorials](https://www.datacamp.com/tutorial)\n[docs](https://www.datacamp.com/doc)\n[Podcasts](https://www.datacamp.com/podcast)\n[Cheat Sheets](https://www.datacamp.com/cheat-sheet)\n[code-alongs](https://www.datacamp.com/code-along)\nCategory\nCategory\nTechnologies \nDiscover content by tools and technology\n[Artificial Intelligence](https://www.datacamp.com/tutorial/category/ai)[AWS](https://www.datacamp.com/tutorial/category/aws)[Azure](https://www.datacamp.com/tutorial/category/microsoft-azure)[Business Intelligence](https://www.datacamp.com/tutorial/category/learn-business-intelligence)[ChatGPT](https://www.datacamp.com/tutorial/category/chatgpt)[Databricks](https://www.datacamp.com/tutorial/category/databricks)[dbt](https://www.datacamp.com/tutorial/category/dbt)[Docker](https://www.datacamp.com/tutorial/category/docker)[Excel](https://www.datacamp.com/tutorial/category/excel)[Generative AI](https://www.datacamp.com/tutorial/category/generative-ai)[Git](https://www.datacamp.com/tutorial/category/git)[Google Cloud Platform](https://www.datacamp.com/tutorial/category/google-cloud-platform)[Hugging Face](https://www.datacamp.com/tutorial/category/Hugging-Face)[Java](https://www.datacamp.com/tutorial/category/java)[Julia](https://www.datacamp.com/tutorial/category/julia)[Kubernetes](https://www.datacamp.com/tutorial/category/kubernetes)[Large Language Models](https://www.datacamp.com/tutorial/category/large-language-models)[OpenAI](https://www.datacamp.com/tutorial/category/OpenAI)[PostgreSQL](https://www.datacamp.com/tutorial/category/postgresql)[Power BI](https://www.datacamp.com/tutorial/category/power-bi)[Python](https://www.datacamp.com/tutorial/category/python)[R](https://www.datacamp.com/tutorial/category/r-programming)[Scala](https://www.datacamp.com/tutorial/category/scala)[Snowflake](https://www.datacamp.com/tutorial/category/snowflake)[Spreadsheets](https://www.datacamp.com/tutorial/category/spreadsheets)[SQL](https://www.datacamp.com/tutorial/category/sql)[SQLite](https://www.datacamp.com/tutorial/category/sqlite)[Tableau](https://www.datacamp.com/tutorial/category/tableau)\nCategory\nTopics \nDiscover content by data science topics\n[AI for Business](https://www.datacamp.com/tutorial/category/ai-for-business)[Big Data](https://www.datacamp.com/tutorial/category/big-data)[Career Services](https://www.datacamp.com/tutorial/category/career-services)[Cloud](https://www.datacamp.com/tutorial/category/cloud)[Data Analysis](https://www.datacamp.com/tutorial/category/data-analysis)[Data Engineering](https://www.datacamp.com/tutorial/category/data-engineering)[Data Literacy](https://www.datacamp.com/tutorial/category/data-literacy)[Data Science](https://www.datacamp.com/tutorial/category/data-science)[Data Visualization](https://www.datacamp.com/tutorial/category/data-visualization)[DataLab](https://www.datacamp.com/tutorial/category/datalab)[Deep Learning](https://www.datacamp.com/tutorial/category/deep-learning)[Machine Learning](https://www.datacamp.com/tutorial/category/machine-learning)[MLOps](https://www.datacamp.com/tutorial/category/mlops)[Natural Language Processing](https://www.datacamp.com/tutorial/category/natural-language-processing)\n[Request a Demo](https://www.datacamp.com/business)[](https://www.datacamp.com/search-resources)\ncategory\n  1. [Home](https://www.datacamp.com)\n  2. [Tutorials](https://www.datacamp.com/tutorial)\n  3. [Artificial Intelligence](https://www.datacamp.com/tutorial/category/ai)\n\n\n# Speculative Decoding: A Guide With Implementation Examples\nLearn what speculative decoding is, how it works, when to use it, and how to implement it using Gemma2 models.\nList Contents\nNov 8, 2024 \u00b7 12 min read\nContents\n  * [What Is Speculative Decoding?](https://www.datacamp.com/tutorial/speculative-decoding/#what-is-speculative-decoding?-<span)\n\n\n  * [Traditional Decoding vs. Speculative Decoding](https://www.datacamp.com/tutorial/speculative-decoding/#traditional-decoding-vs.-speculative-decoding-<span)\n\n\n  * [Hands-On Example: Speculative Decoding With Gemma2 Models](https://www.datacamp.com/tutorial/speculative-decoding/#hands-on-example:-speculative-decoding-with-gemma2-models-<span)\n  * [Step 1: Model and Tokenizer setup](https://www.datacamp.com/tutorial/speculative-decoding/#step-1:-model-and-tokenizer-setup-<span)\n  * [Step 2: Autoregressive (normal) inference ](https://www.datacamp.com/tutorial/speculative-decoding/#step-2:-autoregressive-\\(normal\\)-inference\u00a0-<span)\n  * [Step 3: Speculative decoding ](https://www.datacamp.com/tutorial/speculative-decoding/#step-3:-speculative-decoding\u00a0-next,)\n  * [Step 4: Measuring latency](https://www.datacamp.com/tutorial/speculative-decoding/#step-4:-measuring-latency-after)\n\n\n  * [Testing Speculative Decoding on Five Prompts](https://www.datacamp.com/tutorial/speculative-decoding/#testing-speculative-decoding-on-five-prompts-<span)\n\n\n  * [Optimized Speculative Decoding With Quantization ](https://www.datacamp.com/tutorial/speculative-decoding/#optimized-speculative-decoding-with-quantization\u00a0\u00a0-theab)\n  * [4-bit quantization (weight compression)](https://www.datacamp.com/tutorial/speculative-decoding/#4-bit-quantization-\\(weight-compression\\)-<span)\n  * [bfloat16 for computation (efficient use of Tensor Cores)](https://www.datacamp.com/tutorial/speculative-decoding/#bfloat16-for-computation-\\(efficient-use-of-tensor-cores\\)-<span)\n  * [NF4 quantization type (optimized precision)](https://www.datacamp.com/tutorial/speculative-decoding/#nf4-quantization-type-\\(optimized-precision\\)-<span)\n  * [Double quantization disabled ](https://www.datacamp.com/tutorial/speculative-decoding/#double-quantization-disabled\u00a0-<span)\n\n\n  * [Applications of Speculative Decoding](https://www.datacamp.com/tutorial/speculative-decoding/#applications-of-speculative-decoding-<span)\n\n\n  * [Challenges of Speculative Decoding](https://www.datacamp.com/tutorial/speculative-decoding/#challenges-of-speculative-decoding-<span)\n\n\n  * [Conclusion](https://www.datacamp.com/tutorial/speculative-decoding/#conclusion-<span)\n\n\n[LLMs](https://www.datacamp.com/courses/large-language-models-llms-concepts) are so powerful, but they can often be a bit slow, and this is not ideal in scenarios where we need speed. Speculative decoding is a technique designed to speed LLMs by generating responses faster without compromising quality.\nIn essence, it\u2019s a way to \u201cguess ahead\u201d in the process of text generation, making predictions about the words that might come next while still allowing for the accuracy and depth we expect from LLMs.\nIn this blog, I\u2019ll explain what speculative decoding is, how it works, and how to implement it with Gemma 2 models.\n## What Is Speculative Decoding?\nSpeculative decoding accelerates LLMs by incorporating a smaller, faster model that generates preliminary predictions. This smaller model, often called the \u201cdraft\u201d model, generates a batch of tokens that the main, more powerful LLM can either confirm or refine. The draft model acts as a first pass, producing multiple tokens that speed up the generation process.\nInstead of the main LLM generating tokens sequentially, the draft model provides a set of likely candidates, and the main model evaluates them in parallel. This method reduces the computational load on the main LLM by offloading initial predictions, allowing it to focus only on corrections or validations.\n![Speculative Decoding pipeline](https://media.datacamp.com/cms/google/ad_4nxe9skexjvdpmgmrvqelgqhpfalhztym3paxvicwvo3znaeemswzztbayouvgg-laionqivqzlyj1aapawwbmg0vwu5uk9q-hj0sezmf9wyachjhgqbzrtyd2zgg1dmocddpkjn7rbuawnhei-vxbogguixu.png)\nThink of it like a writer with an editor. The main LLM is the writer, capable of producing high-quality text but at a slower pace. A smaller, faster \"draft\" model acts as the editor, quickly generating potential continuations of the text. The main LLM then evaluates these suggestions, incorporating the accurate ones and discarding the rest. This allows the LLM to process multiple tokens concurrently, speeding up text generation.\nLet's break down the process of speculative decoding into simple steps:\n  1. \u200bDraft generation: The smaller model (e.g., Gemma2-2B-it) generates multiple token suggestions based on the input prompt. These tokens are generated speculatively, meaning the model is not certain they are correct but provides them as \u201cdraft\u201d tokens.\n  2. Parallel \u200bverification: The larger model (e.g., Gemma2-9B-it) verifies these tokens in parallel, checking their probability against the model\u2019s learned distribution. If the tokens are deemed acceptable, they are used in the final output. If not, the larger model corrects them.\n  3. \u200bFinal output: Once the tokens are verified (or corrected), they are passed on to the user as the final output. This entire process happens much faster than traditional one-token-at-a-time decoding.\n\n\n## Traditional Decoding vs. Speculative Decoding\nTraditional decoding processes tokens one at a time, leading to high latency, but speculative decoding allows a smaller model to generate tokens in bulk, with the larger model verifying them. This can reduce response time by 30-40%, cutting latency from 25-30 seconds to as little as 15-18 seconds.\n![Traditional decoding vs speculative decoding](https://media.datacamp.com/cms/google/ad_4nxfg3vmjzp91nr8ry9x5_1y20vp_ly0osb_i5c_hrgm6ad-kwolpzc2iyq_c8ce1uwzxf_yerqqejsg-ptsocrsmb9zm2i-uc-5wp81j3-1vcf8jklmmegfgwthw_3elilwy_giitg.png)\nAdditionally, speculative decoding optimizes memory usage by shifting most of the token generation to the smaller model, reducing memory requirements from 26 GB to around 14 GB and making [on-device](https://www.datacamp.com/blog/edge-ai) inference more accessible.\nFinally, it lowers compute demands by 50%, as the larger model only verifies rather than generates tokens, enabling smoother performance on mobile devices with limited power and preventing overheating.\n## Hands-On Example: Speculative Decoding With Gemma2 Models\nTo implement a hands-on example of speculative decoding using the Gemma2 models. We\u2019ll explore how speculative decoding compares with standard inference in terms of both latency and performance.\n### Step 1: Model and Tokenizer setup\nTo get started, import the dependencies and set the seed.\nNext, check if GPU is available on the machine you are operating on. This is mainly required for big models such as Gemma 2-9B-it or LLama2-13B.\nFinally, we load both small and big model along with their tokenizers. Here, we are using the Gemma2-2b-it (instruct) model for the draft model and the Gemma2-9b-it model for verification.\nThere are a few other models that can be used alternatively as well. For example:\n  * Gemma 7B (main) & Gemma 2B (draft)\n  * Mixtral-8x7B (main) & Mistral 7B (draft)\n  * Pythia 12B (main) & Pythia 70M (draft)\n  * Llama 2 13B (main) & TinyLlama 1.1B (draft)\n\n```\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n# Set Seed\nset_seed(42)\n# Check if GPU is available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Load the smaller Gemma2 model (draft generation)\nsmall_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\")\nsmall_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n# Load the larger Gemma2 model (verification)\nbig_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\", device_map=\"auto\")\nbig_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\n### Step 2: Autoregressive (normal) inference \nFirst, we perform inference on just the large model (Gemma2-9b-it) and generate output. Start by [tokenizing](https://www.datacamp.com/tutorial/tiktoken-library-python) the input prompt and moving the tokens to the correct device (GPU if available). The `generate` method produces the model's output, generating up to `max_new_tokens`. The result is then decoded from token IDs back into human-readable text.\n```\n\ndef normal_inference(big_model, big_tokenizer, prompt, max_new_tokens=50):\n  inputs = big_tokenizer(prompt, return_tensors='pt').to(device)\n  outputs = big_model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens)\n  return big_tokenizer.decode(outputs[0], skip_special_tokens=True)\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\n### Step 3: Speculative decoding \nNext, we try the speculative decoding method, where we take the following steps:\n  1. Draft generation: The smaller model generates a draft of the text from the given prompt.\n  2. Verification: The larger model then verifies the draft by computing the log-likelihood for each token in the draft.\n  3. Log-likelihood calculation: We compute an average log-likelihood to determine how likely the large model considers the small model\u2019s draft to be correct.\n\n```\n\ndef speculative_decoding(small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens=50):\n  # Step 1: Use the small model to generate the draft\n  inputs = small_tokenizer(prompt, return_tensors='pt').to(device)\n  small_outputs = small_model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens)\n  draft = small_tokenizer.decode(small_outputs[0], skip_special_tokens=True)\n  # Step 2: Verify the draft with the big model\n  big_inputs = big_tokenizer(draft, return_tensors='pt').to(device)\n  # Step 3: Calculate log-likelihood of the draft tokens under the large model\n  with torch.no_grad():\n    outputs = big_model(big_inputs['input_ids'])\n    log_probs = torch.log_softmax(outputs.logits, dim=-1)\n  draft_token_ids = big_inputs['input_ids']\n  log_likelihood = 0\n  for i in range(draft_token_ids.size(1) - 1):\n    token_id = draft_token_ids[0, i + 1]\n    log_likelihood += log_probs[0, i, token_id].item()\n  avg_log_likelihood = log_likelihood / (draft_token_ids.size(1) - 1)\n  # Return the draft and its log-likelihood score\n  return draft, avg_log_likelihood\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\nNote: Log-likelihood is the logarithm of the probability that a model assigns to a specific sequence of tokens. Here, it reflects how likely or \"confident\" the model is that the sequence of tokens (the generated text) is valid given the previous tokens.\n### Step 4: Measuring latency\nAfter implementing both techniques, we can measure their respective latencies. For speculative decoding, we assess performance by examining the log-likelihood value. A log-likelihood value approaching zero, particularly in the negative range, indicates that the generated text is accurate.\n```\n\ndef measure_latency(small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens=50):\n  # Measure latency for normal inference (big model only)\n  start_time = time.time()\n  normal_output = normal_inference(big_model, big_tokenizer, prompt, max_new_tokens)\n  normal_inference_latency = time.time() - start_time\n  print(f\"Normal Inference Output: {normal_output}\")\n  print(f\"Normal Inference Latency: {normal_inference_latency:.4f} seconds\")\n  print(\"\\n\\n\")\n  # Measure latency for speculative decoding\n  start_time = time.time()\n  speculative_output, log_likelihood = speculative_decoding(\n    small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens\n  )\n  speculative_decoding_latency = time.time() - start_time\n  print(f\"Speculative Decoding Output: {speculative_output}\")\n  print(f\"Speculative Decoding Latency: {speculative_decoding_latency:.4f} seconds\")\n  print(f\"Log Likelihood (Verification Score): {log_likelihood:.4f}\")\n  return normal_inference_latency, speculative_decoding_latency\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\nThis returns:\n  * Log-likelihood (verification score): -0.5242\n  * Normal inference latency: 17.8899 seconds\n  * Speculative decoding latency: 10.5291 seconds (about 70% faster)\n\n\nThe lower latency is thanks to less time taken by the smaller model for text generation and less time taken by the larger model for just verifying the generated text.\n## Testing Speculative Decoding on Five Prompts\nLet's compare speculative decoding with autoregressive inference by using five prompts:\n```\n\n# List of 5 prompts\nprompts = [\n  \"The future of artificial intelligence is \",\n  \"Machine learning is transforming the world by \",\n  \"Natural language processing enables computers to understand \",\n  \"Generative models like GPT-3 can create \",\n  \"AI ethics and fairness are important considerations for \"\n]\n# Inference settings\nmax_new_tokens = 200\n# Initialize accumulators for latency, memory, and tokens per second\ntotal_tokens_per_sec_normal = 0\ntotal_tokens_per_sec_speculative = 0\ntotal_normal_latency = 0\ntotal_speculative_latency = 0\n# Perform inference on each prompt and accumulate the results\nfor prompt in prompts:\n  normal_latency, speculative_latency, _, _, tokens_per_sec_normal, tokens_per_sec_speculative = measure_latency_and_memory(\n    small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens\n  )\n  total_tokens_per_sec_normal += tokens_per_sec_normal\n  total_tokens_per_sec_speculative += tokens_per_sec_speculative\n  total_normal_latency += normal_latency\n  total_speculative_latency += speculative_latency\n# Calculate averages\naverage_tokens_per_sec_normal = total_tokens_per_sec_normal / len(prompts)\naverage_tokens_per_sec_speculative = total_tokens_per_sec_speculative / len(prompts)\naverage_normal_latency = total_normal_latency / len(prompts)\naverage_speculative_latency = total_speculative_latency / len(prompts)\n# Output the averages\nprint(f\"Average Normal Inference Latency: {average_normal_latency:.4f} seconds\")\nprint(f\"Average Speculative Decoding Latency: {average_speculative_latency:.4f} seconds\")\nprint(f\"Average Normal Inference Tokens per second: {average_tokens_per_sec_normal:.2f} tokens/second\")\nprint(f\"Average Speculative Decoding Tokens per second: {average_tokens_per_sec_speculative:.2f} tokens/second\")\nPowered By [](https://www.datacamp.com/datalab)\n\n```\n```\n\nAverage Normal Inference Latency: 25.0876 seconds\nAverage Speculative Decoding Latency: 15.7802 seconds\nAverage Normal Inference Tokens per second: 7.97 tokens/second\nAverage Speculative Decoding Tokens per second: 12.68 tokens/second\nPowered By [](https://www.datacamp.com/datalab)\n\n```\n\nThis shows that speculative decoding is more efficient, generating more tokens per second than normal inference. This improvement is because the smaller model handles the majority of the text generation, while the larger model's role is limited to verification, reducing the overall computational load in terms of latency and memory. \nWith these memory requirements, we can easily deploy speculative decoding techniques on edge devices and gain speed-ups on our on-device applications such as chatbots, language translators, games, and more.\n## Optimized Speculative Decoding With Quantization \nThe above approach is efficient, but there is a trade-off between latency and memory optimization for on-device inference. To address this, let\u2019s apply [quantization](https://www.datacamp.com/tutorial/quantization-for-large-language-models) to both small and big models. You can experiment and try applying quantization to only the large model since the small model already takes the least space.\nQuantification is applied to smaller and larger models using the `BitsAndBytesConfig` configuration from the Hugging Face `transformers` library. Quantization allows us to significantly reduce memory usage and, in many cases, improve inference speed by converting the model\u2019s weights to a more compact form.\nAdd the following code snippet to the above code to witness the effects of quantization.\n```\n\nbnb_config = BitsAndBytesConfig(\n  load_in_4bit=True, # Enables 4-bit quantization\n  bnb_4bit_quant_type=\"nf4\", # Specifies the quantization type (nf4)\n  bnb_4bit_compute_dtype=torch.bfloat16, # Uses bfloat16 for computation\n  bnb_4bit_use_double_quant=False, # Disables double quantization to avoid additional overhead\n)\n# Load the smaller and larger Gemma2 models with quantization applied\nsmall_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\", quantization_config=bnb_config)\nbig_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b-it\", device_map=\"auto\", quantization_config=bnb_config)\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\nHere is a quick comparison of outputs to showcase the effects of speculative decoding with and without quantization:\n![Comparisions with and without quantization](https://media.datacamp.com/cms/google/ad_4nxfup6giqvls9r9_fdmzkn_z7_1cuegd1ewdsrgbcipvva4xj5nlcgnpmtsadf1ehznithnkv4etzd6md0wk8zuipyk4nyp6wvlqgxhlpuwumx68rilmg8dq14m_punl35svrzz9t8hyuhoqx9ek2ptxzfm.png)\n### 4-bit quantization (weight compression)\nThe configuration specifies `load_in_4bit=True`, which means the model\u2019s weights are quantized from their original 32-bit or 16-bit floating-point format to 4-bit integers. This reduces the model's memory footprint. Quantization compresses the model\u2019s weights, allowing us to store and operate on them more efficiently. These are the concrete memory savings:\n  * By reducing the precision from 32-bit or 16-bit floats to 4-bit integers, each weight now takes up 1/4th or 1/8th of the original space, significantly reducing memory usage.\n  * This is reflected in the memory usage as:\n    * Normal inference memory usage: 26,458 MB\n    * Speculative decoding memory usage: 8,993 MB.\n\n\n### bfloat16 for computation (efficient use of Tensor Cores)\nThe configuration `bnb_4bit_compute_dtype=torch.bfloat16` specifies that the computation is performed in bfloat16 (BF16), a more efficient floating-point format. BF16 has a wider dynamic range than FP16 but takes up half the memory compared to FP32, making it a good balance between precision and performance.\nUsing BF16, especially on NVIDIA GPUs (like A100), uses Tensor Cores, which are optimized for BF16 operations. This results in faster matrix multiplications and other computations during inference.\nFor speculative decoding, we see improved latency:\n  * Normal inference latency: 27.65 seconds\n  * Speculative decoding latency: 15.56 seconds\n\n\nThe smaller memory footprint means faster memory access and more efficient use of GPU resources, leading to faster generation.\n### NF4 quantization type (optimized precision)\nThe `bnb_4bit_quant_type=\"nf4\"` option specifies Norm-Four Quantization (NF4), which is optimized for neural networks. NF4 quantization helps retain the precision of important parts of the model, even though the weights are represented in 4 bits. This minimizes the degradation in model performance compared to simple 4-bit quantization.\nNF4 helps achieve a balance between the compactness of 4-bit quantization and the accuracy of the model's predictions, ensuring that performance remains close to the original while drastically reducing memory usage.\n### Double quantization disabled \nDouble quantization (`bnb_4bit_use_double_quant=False`) introduces an additional layer of quantization on top of the 4-bit weights, which can further reduce memory usage but also add computation overhead. In this case, double quantization is disabled to avoid slowing down the inference.\n## Applications of Speculative Decoding\nThe potential applications of speculative decoding are vast and exciting. Here are a few examples:\n  * Chatbots and virtual assistants: To make those conversations with AI feel more natural and fluid with faster response times.\n  * Real-time translation: Speculative decoding reduces latency in real-time translation.\n  * Content generation: Speculative decoding speeds up content creation.\n  * Gaming and interactive applications: To improve the responsiveness of AI-powered characters or game elements for a more immersive experience, speculative decoding can help us attain almost real-time responses.\n\n\n![Applications of Speculative Decoding](https://media.datacamp.com/cms/google/ad_4nxdxjtdtb_3gotq7mu_ye_cb1wuva_dopcu3qjjrnkcwlbtchnxk1qssdq8oox6lgjuf0om_n86ffucxli8soawyypivtfxwltwaafyuuwdjc3uk2jsxysinerxwjsap-wb4yfjnl0v-a30-jog49fwy5xs.png)\n## Challenges of Speculative Decoding\nWhile speculative decoding holds immense promise, it's not without its challenges:\n  * Memory overhead: Maintaining multiple model states (for both the draft and main LLM) can increase memory usage, especially when larger models are used for verification.\n  * Tuning the draft model: Choosing the right draft model and tuning its parameters is crucial to strike the right balance between speed and accuracy, as an overly simplistic model may lead to frequent verification failures.\n  * Implementation complexity: Implementing speculative decoding is more complex than traditional methods, requiring careful synchronization between the small draft model and the larger verification model, as well as efficient error handling.\n  * Compatibility with decoding strategies: Speculative decoding currently supports only greedy search and sampling, limiting its use to more sophisticated decoding strategies such as beam search or diverse sampling.\n  * Verification overhead: If the smaller draft model generates tokens that frequently fail verification, the efficiency gains can be diminished, as the larger model will need to regenerate parts of the output, potentially negating the speed advantages.\n  * Limited support for batch processing: Speculative decoding typically doesn't support batched inputs, which can reduce its effectiveness in high-throughput systems requiring parallel processing of multiple requests.\n\n\n## Conclusion\nSpeculative decoding is a powerful technique that has the potential to revolutionize the way we interact with large language models. It can significantly speed up LLM inference without compromising the quality of the generated text. While there are challenges to overcome, the benefits of speculative decoding are undeniable, and we can expect to see its adoption grow in the coming years, enabling a new generation of faster, more responsive, and more efficient AI applications.\n![Aashi Dutt's photo](https://media.datacamp.com/legacy/v1725454401/aashi_pic_746310ff15.jpg?w=128)\nAuthor\n[Aashi Dutt](https://www.datacamp.com/portfolio/aashidutt3)\n[LinkedIn](https://www.linkedin.com/in/aashi-dutt)[Twitter](https://twitter.com/aashidutt?lang=en)\nI am a Google Developers Expert in ML(Gen AI), a Kaggle 3x Expert, and a Women Techmakers Ambassador with 3+ years of experience in tech. I co-founded a health-tech startup in 2020 and am pursuing a master's in computer science at Georgia Tech, specializing in machine learning.\nTopics\n[Artificial Intelligence](https://www.datacamp.com/tutorial/category/ai)[Large Language Models](https://www.datacamp.com/tutorial/category/large-language-models)\n![Aashi Dutt's photo](https://media.datacamp.com/legacy/v1725454401/aashi_pic_746310ff15.jpg?w=128)\n[Aashi Dutt](https://www.datacamp.com/portfolio/aashidutt3) Google Developers Expert in ML | Speaker | Kaggle 3x Expert \nTopics\n[Artificial Intelligence](https://www.datacamp.com/tutorial/category/ai)[Large Language Models](https://www.datacamp.com/tutorial/category/large-language-models)\n### [Gemini 2.0 Flash Thinking Experimental: A Guide With Examples](https://www.datacamp.com/blog/gemini-2-0-flash-experimental)\n### [Small Language Models: A Guide With Examples](https://www.datacamp.com/blog/small-language-models)\n### [AI Hallucination: A Guide With Examples](https://www.datacamp.com/blog/ai-hallucination)\n### [Speculative RAG Implementation With Transformers](https://www.datacamp.com/tutorial/speculative-rag)\n### [Fine Tuning Google Gemma: Enhancing LLMs with Customized Instructions](https://www.datacamp.com/tutorial/fine-tuning-google-gemma)\n### [Introduction to Large Language Models with GPT & LangChain](https://www.datacamp.com/code-along/introduction-to-large-language-models-gpt-langchain)\nLearn AI with these courses!\nTrack\n### [Developing AI Applications](https://www.datacamp.com/tracks/developing-ai-applications)\n23hrs hr\nLearn to create AI-powered applications with the latest AI developer tools, including the OpenAI API, Hugging Face, and LangChain.\n[See DetailsRight Arrow](https://www.datacamp.com/tracks/developing-ai-applications)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Ftracks%2Fdeveloping-ai-applications%2Fcontinue)\nCourse\n### [Developing AI Systems with the OpenAI API](https://www.datacamp.com/courses/developing-ai-systems-with-the-openai-api)\n3 hr\n5.2K\nLeverage the OpenAI API to get your AI applications ready for production.\n[See DetailsRight Arrow](https://www.datacamp.com/courses/developing-ai-systems-with-the-openai-api)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Fcourses%2Fdeveloping-ai-systems-with-the-openai-api%2Fcontinue)\nCourse\n### [AI Security and Risk Management](https://www.datacamp.com/courses/ai-security-and-risk-management)\n2 hr\n2.5K\nLearn the fundamentals of AI security to protect systems from threats, align security with business goals, and mitigate key risks. \n[See DetailsRight Arrow](https://www.datacamp.com/courses/ai-security-and-risk-management)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Fcourses%2Fai-security-and-risk-management%2Fcontinue)\n[See MoreRight Arrow](https://www.datacamp.com/category/artificial-intelligence)\nRelated\n[blogGemini 2.0 Flash Thinking Experimental: A Guide With Examples](https://www.datacamp.com/blog/gemini-2-0-flash-experimental)\nLearn about Gemini 2.0 Flash Thinking Experimental, including its features, benchmarks, limitations, and how it compares to other reasoning models.\n![Alex Olteanu's photo](https://media.datacamp.com/legacy/v1718983935/DSC_00941_5_square_42e8868173.jpg?w=48)\nAlex Olteanu \n8 min\n[blogSmall Language Models: A Guide With Examples](https://www.datacamp.com/blog/small-language-models)\nLearn about small language models (SLMs), their benefits and applications, and how they compare to large language models (LLMs).\n![Dr Ana Rojo-Echebur\u00faa's photo](https://media.datacamp.com/cms/ana_rojo.png?w=48)\nDr Ana Rojo-Echebur\u00faa \n8 min\n[blogAI Hallucination: A Guide With Examples](https://www.datacamp.com/blog/ai-hallucination)\nLearn about AI hallucinations, their types, why they occur, their potential negative impacts, and how to mitigate them.\n[![Tom Farnschl\u00e4der's photo](https://media.datacamp.com/legacy/v1719243659/1698334986991_adfcb9390d.jpg?w=48)](https://www.datacamp.com/portfolio/tomfarnschlaeder)\nTom Farnschl\u00e4der \n8 min\n[TutorialSpeculative RAG Implementation With Transformers](https://www.datacamp.com/tutorial/speculative-rag)\nLearn Speculative RAG, a technique that improves RAG through a two-step drafting and verification process, and apply your skills with a hands-on implementation using Hugging Face Transformers.\n![Bhavishya Pandit's photo](https://media.datacamp.com/legacy/v1725454467/Unknown_f0f804ba73.jpg?w=48)\nBhavishya Pandit \n8 min\n[TutorialFine Tuning Google Gemma: Enhancing LLMs with Customized Instructions](https://www.datacamp.com/tutorial/fine-tuning-google-gemma)\nLearn how to run inference on GPUs/TPUs and fine-tune the latest Gemma 7b-it model on a role-play dataset.\n[![Abid Ali Awan's photo](https://media.datacamp.com/legacy/v1658155691/Abid_Ali_Awan_415cc44670.jpg?w=48)](https://www.datacamp.com/portfolio/kingabzpro)\nAbid Ali Awan \n12 min\n[code-alongIntroduction to Large Language Models with GPT & LangChain](https://www.datacamp.com/code-along/introduction-to-large-language-models-gpt-langchain)\nLearn the fundamentals of working with large language models and build a bot that analyzes data.\n[![Richie Cotton's photo](https://media.datacamp.com/legacy/v1658155296/richie_new_barnet_be7b436d1f.jpg?w=48)](https://www.datacamp.com/portfolio/richie)\nRichie Cotton \n[See More](https://www.datacamp.com/tutorial/category/ai)[See More](https://www.datacamp.com/tutorial/category/ai)\n## Grow your data skills with DataCamp for Mobile\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\n[Download on the App Store](https://datacamp.onelink.me/xztQ/45dozwue?deep_link_sub1=%7B%22src_url%22%3A%22https%3A%2F%2Fwww.datacamp.com%2Ftutorial%2Fspeculative-decoding%22%7D)[Get it on Google Play](https://datacamp.onelink.me/xztQ/go2f19ij?deep_link_sub1=%7B%22src_url%22%3A%22https%3A%2F%2Fwww.datacamp.com%2Ftutorial%2Fspeculative-decoding%22%7D)\n**Learn**\n[Learn Python](https://www.datacamp.com/blog/how-to-learn-python-expert-guide)[Learn AI](https://www.datacamp.com/blog/how-to-learn-ai)[Learn Power BI](https://www.datacamp.com/learn/power-bi)[Learn Data Engineering](https://www.datacamp.com/category/data-engineering)[Assessments](https://www.datacamp.com/signal)[Career Tracks](https://www.datacamp.com/tracks/career)[Skill Tracks](https://www.datacamp.com/tracks/skill)[Courses](https://www.datacamp.com/courses-all)[Data Science Roadmap](https://www.datacamp.com/blog/data-science-roadmap)\n**Data Courses**\n[Python Courses](https://www.datacamp.com/category/python)[R Courses](https://www.datacamp.com/category/r)[SQL Courses](https://www.datacamp.com/category/sql)[Power BI Courses](https://www.datacamp.com/category/power-bi)[Tableau Courses](https://www.datacamp.com/category/tableau)[Alteryx Courses](https://www.datacamp.com/category/alteryx)[Azure Courses](https://www.datacamp.com/category/azure)[AWS Courses](https://www.datacamp.com/category/aws)[Google Sheets Courses](https://www.datacamp.com/category/google-sheets)[Excel Courses](https://www.datacamp.com/category/excel)[AI Courses](https://www.datacamp.com/category/artificial-intelligence)[Data Analysis Courses](https://www.datacamp.com/category/data-analysis)[Data Visualization Courses](https://www.datacamp.com/category/data-visualization)[Machine Learning Courses](https://www.datacamp.com/category/machine-learning)[Data Engineering Courses](https://www.datacamp.com/category/data-engineering)[Probability & Statistics Courses](https://www.datacamp.com/category/probability-and-statistics)\n**DataLab**\n[Get Started](https://www.datacamp.com/datalab)[Pricing](https://www.datacamp.com/datalab/pricing)[Security](https://www.datacamp.com/datalab/security)[Documentation](https://datalab-docs.datacamp.com)\n**Certification**\n[Certifications](https://www.datacamp.com/certification)[Data Scientist](https://www.datacamp.com/certification/data-scientist)[Data Analyst](https://www.datacamp.com/certification/data-analyst)[Data Engineer](https://www.datacamp.com/certification/data-engineer)[SQL Associate](https://www.datacamp.com/certification/sql-associate)[Power BI Data Analyst](https://www.datacamp.com/certification/data-analyst-in-power-bi)[Tableau Certified Data Analyst](https://www.datacamp.com/certification/data-analyst-in-tableau)[Azure Fundamentals](https://www.datacamp.com/certification/azure-fundamentals)[AI Fundamentals](https://www.datacamp.com/certification/ai-fundamentals)\n**Resources**\n[Resource Center](https://www.datacamp.com/resources)[Upcoming Events](https://www.datacamp.com/webinars)[Blog](https://www.datacamp.com/blog)[Code-Alongs](https://www.datacamp.com/code-along)[Tutorials](https://www.datacamp.com/tutorial)[Docs](https://www.datacamp.com/doc)[Open Source](https://www.datacamp.com/open-source)[RDocumentation](https://www.rdocumentation.org)[Book a Demo with DataCamp for Business](https://www.datacamp.com/business/demo)[Data Portfolio](https://www.datacamp.com/data-portfolio)\n**Plans**\n[Pricing](https://www.datacamp.com/pricing)[For Students](https://www.datacamp.com/pricing/student)[For Business](https://www.datacamp.com/business)[For Universities](https://www.datacamp.com/universities)[Discounts, Promos & Sales](https://www.datacamp.com/promo)[DataCamp Donates](https://www.datacamp.com/donates)\n**For Business**\n[Business Pricing](https://www.datacamp.com/business/compare-plans)[Teams Plan](https://www.datacamp.com/business/learn-teams)[Data & AI Unlimited Plan](https://www.datacamp.com/business/data-unlimited)[Customer Stories](https://www.datacamp.com/business/customer-stories)[Partner Program](https://www.datacamp.com/business/partner-program)\n**About**\n[About Us](https://www.datacamp.com/about)[Learner Stories](https://www.datacamp.com/stories)[Careers](https://www.datacamp.com/careers)[Become an Instructor](https://www.datacamp.com/learn/create)[Press](https://www.datacamp.com/press)[Leadership](https://www.datacamp.com/about/leadership)[Contact Us](https://support.datacamp.com/hc/en-us/articles/360021185634)[DataCamp Espa\u00f1ol](https://www.datacamp.com/es)[DataCamp Portugu\u00eas](https://www.datacamp.com/pt)[DataCamp Deutsch](https://www.datacamp.com/de)[DataCamp Fran\u00e7ais](https://www.datacamp.com/fr)\n**Support**\n[Help Center](https://support.datacamp.com/hc/en-us)[Become an Affiliate](https://www.datacamp.com/affiliates)\n[Facebook](https://www.facebook.com/datacampinc/)[Twitter](https://twitter.com/datacamp)[LinkedIn](https://www.linkedin.com/school/datacampinc/)[YouTube](https://www.youtube.com/channel/UC79Gv3mYp6zKiSwYemEik9A)[Instagram](https://www.instagram.com/datacamp/)\n[Privacy Policy](https://www.datacamp.com/privacy-policy)[Cookie Notice](https://www.datacamp.com/cookie-notice)[Do Not Sell My Personal Information](https://www.datacamp.com/do-not-sell-my-personal-information)[Accessibility](https://www.datacamp.com/accessibility)[Security](https://www.datacamp.com/security)[Terms of Use](https://www.datacamp.com/terms-of-use)\n\u00a9 2025 DataCamp, Inc. All Rights Reserved.\n",
    "answer": "```markdown\n## TL;DR Summary\n\nSpeculative decoding enhances LLMs by using a smaller draft model to generate token predictions, reducing latency by 30-40%. It allows faster responses in applications like chatbots and translation while optimizing memory usage. Challenges include implementation complexity and verification overhead.\n```"
  },
  {
    "instruction": "![MLOps Logo](https://ml-ops.org/assets/mlops_header_logo.svg)\n# Machine Learning Operations\nWith Machine Learning Model Operationalization Management (MLOps), we want to provide an end-to-end machine learning development process to design, build and manage reproducible, testable, and evolvable ML-powered software.\n![MLOps Logo](https://ml-ops.org/assets/mlops_visual.svg)\n[![Please scroll](https://ml-ops.org/assets/icons/Icon__ButtonDown.svg)](https://ml-ops.org/#gettingstarted)\n## Getting started\nBeing an emerging field, MLOps is rapidly gaining momentum amongst Data Scientists, ML Engineers and AI enthusiasts. Following this trend, the [Continuous Delivery Foundation SIG MLOps](https://github.com/cdfoundation/sig-mlops) differentiates the ML models management from traditional software engineering and suggests the following MLOps capabilities:\n  * MLOps aims to unify the release cycle for machine learning and software application release. \n  * MLOps enables automated testing of machine learning artifacts (e.g. data validation, ML model testing, and ML model integration testing) \n  * MLOps enables the application of agile principles to machine learning projects.\n  * MLOps enables supporting machine learning models and datasets to build these models as first-class citizens within CI/CD systems. \n  * MLOps reduces technical debt across machine learning models.\n  * MLOps must be a language-, framework-, platform-, and infrastructure-agnostic practice.\n\n\n![](https://ml-ops.org/assets/icons/Icon__Motivation.svg)\n### Motivation for MLOps\nYou will learn for what to use Machine Learning, about various scenarios of change that need to be managed and the iterative nature of ML-based software development. Finally, we provide the MLOps definition and show the evolution of MLOps.\n[Read more](https://ml-ops.org/content/motivation)\n![](https://ml-ops.org/assets/icons/Icon__DesigningSoftware.svg)\n### Designing ML-powered Software\nThis part is devoted to one of the most important phase in any software project \u2014 understanding the business problem and requirements. As these equally apply to ML-based software you need to make sure to have a good understanding before setting out designing things.\n[Read more](https://ml-ops.org/content/phase-zero)\n![](https://ml-ops.org/assets/icons/Icon__Lifecycle.svg)\n### End-to-End ML Workflow Lifecycle\nIn this section, we provide a high-level overview of a typical workflow for machine learning-based software development.\n[Read more](https://ml-ops.org/content/end-to-end-ml-workflow)\n![](https://ml-ops.org/assets/icons/Icon__ThreeLevels.svg)\n### Three Levels of ML-based Software\nYou will learn about three core elements of ML-based software \u2014 Data, ML models, and Code. In particular, we will talk about\n  * Data Engineering Pipelines\n  * ML Pipelines and ML workflows.\n  * Model Serving Patterns and Deployment Strategies\n\n\n[Read more](https://ml-ops.org/content/three-levels-of-ml-software)\n![](https://ml-ops.org/assets/icons/Icon__Principles.svg)\n### MLOps Principles\nIn this part, we describe principles and established practices to test, deploy, manage, and monitor ML models in production.\n[Read more](https://ml-ops.org/content/mlops-principles)\n![](https://ml-ops.org/assets/icons/Icon__Crisp.svg)\n### CRISP-ML(Q)\nYou will learn about the standard process model for machine learning development.\n[Read more](https://ml-ops.org/content/crisp-ml)\n![](https://ml-ops.org/assets/icons/Icon__StackCanvas.svg)\n### MLOps Stack Canvas\nIn this part, you will learn how to specify an architecture and infrastructure stack for MLOps by applying a general MLOps Stack Canvas framework, which is designed to be application- and industry-neutral.\n[Read more](https://ml-ops.org/content/mlops-stack-canvas)\n![](https://ml-ops.org/assets/icons/Icon__ModelGovernance.svg)\n### ML Model Governance\nThis part presents an overview of governance processes, which are an integral part of MLOps.\n[Read more](https://ml-ops.org/content/model-governance)\n![](https://ml-ops.org/assets/icons/Icon__Help.svg)\n### Need Help?\nMLOps consulting services by INNOQ\n[Read more](https://data-ai.innoq.com/en)\nThe content of this site was created by Dr. Larysa Visengeriyeva, Anja Kammer, Isabel B\u00e4r, Alexander Kniesz, and Michael Pl\u00f6d (DDD Advisor).\nIt is published under [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/) and can therefore be shared and adapted with attribution (\"INNOQ\").\nMade and maintained by\n[ ![INNOQ](https://ml-ops.org/assets/innoq-logo_mlops.svg) ](https://data-ai.innoq.com/en)\n[Privacy](https://www.innoq.com/en/datenschutz) | [Site notice](https://www.innoq.com/en/impressum/)\n",
    "answer": "```markdown\n# TL;DR Summary of MLOps\n\nMLOps integrates machine learning into software development, emphasizing automated testing, agile principles, and model governance. Key components include data engineering, ML pipelines, and a standardized development process (CRISP-ML). It aims to reduce technical debt and unify release cycles.\n```"
  },
  {
    "instruction": "[ Tell 120+K peers about your AI research \u2192 Learn more \ud83d\udca1 ![](https://neptune.ai/wp-content/themes/neptune/img/icon-cancel.svg) ](https://neptune.ai/neurips-2024)\n[ ![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg) ](https://neptune.ai \"neptune.ai\")\n  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [Walkthrough [2 min]](https://neptune.ai/walkthrough)\n      * [Play with public sandbox](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n      * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [Neptune vs WandB](https://neptune.ai/vs/wandb)\n      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n      * [Other comparisons](https://neptune.ai/vs)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune Scale projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [AI Researcher](https://neptune.ai/product/ai-researcher)\n      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n      * [Academia & Kagglers](https://neptune.ai/research)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [Monitor training](https://neptune.ai/product/monitor-training)\n      * [Compare experiments](https://neptune.ai/product/compare-experiments)\n      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)\n      * [Reports](https://neptune.ai/product/reports)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/deepsense-ai)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2025/01/Artera-color-signet.png?fit=129%2C97&ssl=1)Case studyHow Neptune Helps Artera Bring AI Solutions to Market Faster ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/artera)\n[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](https://neptune.ai/customers)\n  * [Documentation](https://docs.neptune.ai/)\n  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [Menu Item](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [Blog](https://neptune.ai/blog)\n      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Grici\u016bnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)\n  * [Pricing](https://neptune.ai/pricing)\n  * [Enterprise](https://neptune.ai/product/enterprise)\n  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [Menu Item](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [About us](https://neptune.ai/about-us)\n      * [Customers](https://neptune.ai/customers)\n      * [Careers](https://neptune.ai/jobs)\n      * [Security](https://security.neptune.ai/)\n      * [Contact us](https://neptune.ai/contact-us)\n\n\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find? \nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/) > [Blog](https://neptune.ai/blog) > [MLOps](https://neptune.ai/blog/category/mlops)\n  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [Categories](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n      * [MLOps](https://neptune.ai/blog/category/mlops)\n      * [LLMOps](https://neptune.ai/blog/category/llmops)\n      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)\n    * [Categories](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)\n      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)\n      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)\n\n\nSearch in Blog... \n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg) ![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog... \n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [Categories](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n      * [MLOps](https://neptune.ai/blog/category/mlops)\n      * [LLMOps](https://neptune.ai/blog/category/llmops)\n      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)\n    * [Categories](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)\n      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)\n      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)\n\n\n  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [Walkthrough [2 min]](https://neptune.ai/walkthrough)\n      * [Play with public sandbox](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n      * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [Neptune vs WandB](https://neptune.ai/vs/wandb)\n      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n      * [Other comparisons](https://neptune.ai/vs)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune Scale projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [AI Researcher](https://neptune.ai/product/ai-researcher)\n      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n      * [Academia & Kagglers](https://neptune.ai/research)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [Monitor training](https://neptune.ai/product/monitor-training)\n      * [Compare experiments](https://neptune.ai/product/compare-experiments)\n      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)\n      * [Reports](https://neptune.ai/product/reports)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/deepsense-ai)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2025/01/Artera-color-signet.png?fit=129%2C97&ssl=1)Case studyHow Neptune Helps Artera Bring AI Solutions to Market Faster ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/artera)\n[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](https://neptune.ai/customers)\n  * [Documentation](https://docs.neptune.ai/)\n  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [Menu Item](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [Blog](https://neptune.ai/blog)\n      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Grici\u016bnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)\n  * [Pricing](https://neptune.ai/pricing)\n  * [Enterprise](https://neptune.ai/product/enterprise)\n  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [Menu Item](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n      * [About us](https://neptune.ai/about-us)\n      * [Customers](https://neptune.ai/customers)\n      * [Careers](https://neptune.ai/jobs)\n      * [Security](https://security.neptune.ai/)\n      * [Contact us](https://neptune.ai/contact-us)\n\n\n[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)\n[Neptune Blog](https://neptune.ai/blog)\n#  ML Pipeline Architecture Design Patterns (With 10 Real-World Examples) \n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/03/Richmond-Alake-.jpeg?fit=800%2C800&ssl=1)\n[ Richmond Alake  ](https://neptune.ai/blog/author/richmond-alake)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg) 17 min \n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg) 1st December, 2023 \n[MLOps](https://neptune.ai/blog/category/mlops)\n![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 3 2'%3E%3C/svg%3E)\nThere comes a time when every ML practitioner realizes that training a model in Jupyter Notebook is just one small part of the entire project. Getting a workflow ready which takes your data from its raw form to predictions while maintaining responsiveness and flexibility is the real deal.\nAt that point, the Data Scientists or ML Engineers become curious and start looking for such implementations. Many questions regarding [building machine learning pipelines](https://neptune.ai/blog/building-end-to-end-ml-pipeline) and systems have already been answered and come from industry best practices and patterns. But some of these queries are still recurrent and haven\u2019t been explained well.\nHow should the machine learning pipeline operate? How should they be implemented to accommodate scalability and adaptability whilst maintaining an infrastructure that\u2019s easy to troubleshoot?\nML pipelines usually consist of interconnected infrastructure that enables an organization or machine learning team to enact a consistent, modularized, and structured approach to building, training, and [deploying ML systems](https://neptune.ai/blog/deploying-ml-models-make-sure-new-model-is-better). However, this efficient system does not just operate independently \u2013 it necessitates a comprehensive architectural approach and thoughtful design consideration.\nBut what do these terms \u2013 machine learning design and architecture mean, and how can a complex software system such as an ML pipeline mechanism work proficiently? This blog will answer these questions by exploring the following:\n  * 1 What is pipeline architecture and design consideration, and what are the advantages of understanding it? \n  * 2 Exploration of standard ML pipeline/system design and architectural practices in prominent tech companies \n  * 3 Explanation of common ML pipeline architecture design patterns \n  * 4 Introduction to common components of ML pipelines \n  * 5 Introduction to tools, techniques and software used to implement and maintain ML pipelines \n  * 6 ML pipeline architecture examples \n  * 7 Common best practices to consider when designing and developing ML pipelines \n\n\nSo let\u2019s dive in!\n## What are ML pipeline architecture design patterns?\nThese two terms are often used interchangeably, yet they hold distinct meanings.\n**ML pipeline architecture** is like the high-level musical score for the symphony. It**outlines the components, stages, and workflows within the ML pipeline**. The architectural considerations primarily focus on the arrangement of the components in relation to each other and the involved processes and stages. It answers the question: \u201cWhat ML processes and components will be included in the pipeline, and how are they structured?\u201d\nIn contrast, **ML pipeline design** is a deep dive into the composition of the ML pipeline,**dealing with the tools, paradigms, techniques, and programming languages used to implement the pipeline and its components**. It is the composer\u2019s touch that answers the question: \u201cHow will the components and processes in the pipeline be implemented, tested, and maintained?\u201d\nAlthough there are a number of technical information concerning machine learning pipeline design and architectural patterns, this post primarily covers the following:\n## Advantages of understanding ML pipeline architecture\n![The four pillars of the ML pipeline architecture](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/The-four-pillars-of-a-successful-project-.png?resize=1800%2C942&ssl=1)The four pillars of the**ML pipeline architecture** | Source: Author\nThere are several reasons why ML Engineers, Data Scientists and ML practitioners should be aware of the patterns that exist in ML pipeline architecture and design, some of which are:\n  * **Efficiency** : understanding patterns in ML pipeline architecture and design enables practitioners to identify technical resources required for quick project delivery.\n\n\n  * **Scalability** : ML pipeline architecture and design patterns allow you to prioritize scalability, enabling practitioners to build ML systems with a scalability-first approach. These patterns introduce solutions that deal with model training on large volumes of data, low-latency model inference and more.\n\n\n  * **Templating and reproducibility** : typical pipeline stages and components become reproducible across teams utilizing familiar patterns, enabling members to replicate ML projects efficiently.\n\n\n  * **Standardization** : n organization that utilizes the same patterns for ML pipeline architecture and design, is able to update and maintain pipelines more easily across the entire organization.\n\n\n## Common ML pipeline architecture steps\nHaving touched on the importance of understanding ML pipeline architecture and design patterns, the following sections introduce a number of common architecture and design approaches found in ML pipelines at various stages or components.\nML pipelines are segmented into sections referred to as stages, consisting of one or several components or processes that operate in unison to produce the output of the ML pipeline. Over the years, the stages involved within an ML pipeline have increased.\nLess than a decade ago, when the machine learning industry was primarily research-focused, stages such as model monitoring, deployment, and maintenance were nonexistent or low-priority considerations. Fast forward to current times, the monitoring, maintaining, and deployment stages within an ML pipeline have taken priority, as models in production systems require upkeep and updating. These stages are primarily considered in the domain of [MLOps (machine learning operations)](https://neptune.ai/blog/mlops).\nToday different stages exist within ML pipelines built to meet technical, industrial, and business requirements. This section delves into the common stages in most ML pipelines, regardless of industry or business function.\n  * 1 Data Ingestion (e.g., Apache Kafka, Amazon Kinesis) \n  * 2 Data Preprocessing (e.g., pandas, NumPy) \n  * 3 Feature Engineering and Selection (e.g., Scikit-learn, Feature Tools) \n  * 4 Model Training (e.g., TensorFlow, PyTorch) \n  * 5 Model Evaluation (e.g., Scikit-learn, MLflow) \n  * 6 Model Deployment (e.g., TensorFlow Serving, TFX) \n  * 7 Monitoring and Maintenance (e.g., Prometheus, Grafana) \n\n\nNow that we understand the components within a standard ML pipeline, below are sub-pipelines or systems you\u2019ll come across within the entire ML pipeline.\n  * Data Engineering Pipeline\n  * Feature Engineering Pipeline\n  * Model Training and Development Pipeline\n  * Model Deployment Pipeline\n  * Production Pipeline\n\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  Building ML Pipeline: 6 Problems & Solutions  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/building-ml-pipeline-problems-solutions)\n## 10 ML pipeline architecture examples\nLet\u2019s dig deeper into some of the most common architecture and design patterns and explore their examples, advantages, and drawbacks in more detail.\n### Single leader architecture\n#### What is single leader architecture?\n**The exploration of common machine learning pipeline architecture and patterns starts with a pattern** found in not just machine learning systems but also database systems, streaming platforms, web applications, and modern computing infrastructure. The Single Leader architecture is a pattern leveraged in developing machine learning pipelines designed to operate at scale whilst providing a manageable infrastructure of individual components.\nThe Single Leader Architecture utilises the master-slave paradigm; in this architecture, the leader or master node is aware of the system\u2019s overall state, manages the execution and distribution of tasks according to resource availability, and handles write operations.\nThe follower or slave nodes primarily execute read operations. In the context of ML pipelines, the leader node would be responsible for orchestrating the execution of various tasks, distributing the workload among the follower nodes based on resource availability, and managing the system\u2019s overall state.\nMeanwhile, the follower nodes carry out the tasks the leader node assign, such as data preprocessing, feature extraction, model training, and validation.\n[![ML pipeline architecture design patterns: single leader architecture](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/single-leader-architecture.png?resize=1800%2C942&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/single-leader-architecture.png?ssl=1)ML pipeline architecture design patterns: single leader architecture | Source: Author\n#### A real-world example of single leader architecture\nIn order to see the Single Leader Architecture utilised at scale within a machine learning pipeline, we have to look at one of the biggest streaming platforms that provide personalised video recommendations to millions of users around the globe, Netflix.\nInternally within Netflix\u2019s engineering team, Meson was built to manage, orchestrate, schedule, and execute workflows within ML/Data pipelines. Meson managed the lifecycle of ML pipelines, providing functionality such as recommendations and content analysis, and leveraged the Single Leader Architecture.\nMeson had 70,000 workflows scheduled, with over 500,000 jobs executed daily. Within Meson, the leader node tracked and managed the state of each job execution assigned to a follower node provided fault tolerance by identifying and rectifying failed jobs, and handled job execution and scheduling. \n![A real-world example of the single leader architecture \\(illustrated as a workflow within Meson\\)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/ML-pipeline-architectures-and-design-patterns-3.png?resize=1920%2C1011&ssl=1)A real-world example of the single leader architecture | [Source](https://www.google.com/url?q=https://netflixtechblog.com/meson-workflow-orchestration-for-netflix-recommendations-fc932625c1d9&sa=D&source=docs&ust=1691141770489187&usg=AOvVaw0W7OROgWGKCS_Z_u81jc0h)\n#### Advantages and disadvantages of single leader architecture\nIn order to understand when to leverage the Single Leader Architecture within machine learning pipeline components, it helps to explore its key advantages and disadvantages.\n  * Notable advantages of the Single Leader Arthcutecture are fault tolerance, scalability, consistency, and decentralization. \n  * With one node or part of the system responsible for workflow operations and management, identifying points of failure within pipelines that adopt Single Leader architecture is straightforward. \n  * It effectively handles unexpected processing failures by redirecting/redistributing the execution of jobs, providing consistency of data and state within the entire ML pipeline, and acting as a single source of truth for all processes. \n  * ML pipelines that adopt the Single Leader Architecture can scale horizontally for additional read operations by increasing the number of follower nodes.\n\n\n[![ML pipeline architecture design patterns: scaling single leader architecture](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/scaling-single-leader-architecture.png?resize=1920%2C1005&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/scaling-single-leader-architecture.png?ssl=1)ML pipeline architecture design patterns: scaling single leader architecture | Source: Author\nHowever, in all its advantages, the single leader architecture for ML pipelines can present issues such as scaling, data loss, and availability. \n  * Write scalability within the single leader architecture is limited, and this limitation can act as a bottleneck to the speed of the overall job/workflow orchestration and management. \n  * All write operations are handled by the single leader node in the architecture, which means that although read operations can scale horizontally, the write operation handled by the leader node does not scale proportionally or at all.\n  * The single leader architecture can have significant downtime if the leader node fails; this presents pipeline availability issues and causes entire system failure due to the architecture\u2019s reliance on the leader node.\n\n\nAs the number of workflows managed by Meson grew, the single-leader architecture started showing signs of scale issues. For instance, it experienced slowness during peak traffic moments and required close monitoring during non-business hours. As usage increased, the system had to be scaled vertically, approaching AWS instance-type limits. \nThis led to the development of **Maestro, which uses a shared-nothing architecture to horizontally scale** and manage the states of millions of workflow and step instances simultaneously.\nMaestro incorporates several architectural patterns in modern applications powered by machine learning functionalities. These include **shared-nothing architecture, event-driven architecture, and directed acyclic graphs (DAGs)**. Each of these architectural patterns plays a crucial role in enhancing the efficiency of machine learning pipelines. \nThe next section delves into these architectural patterns, exploring how they are leveraged in machine learning pipelines to streamline data ingestion, processing, model training, and deployment.\n### Directed acyclic graphs (DAG)\n#### What is directed acyclic graphs architecture?\nDirected graphs are made up of nodes, edges, and directions. The nodes represent processes; edges in graphs depict relationships between processes, and the direction of the edges signifies the flow of process execution or data/signal transfer within the graph.\nApplying constraints to graphs allows for the expression and implementation of systems with a sequential execution flow. **For instance, a condition in graphs where loops between vertices or nodes are disallowed. This type of graph is called an acyclic graph** , meaning there are no circular relationships (directed cycles) among one or more nodes. \nAcyclic graphs eliminate repetition between nodes, points, or processes by avoiding loops between two nodes. We get the directed acyclic graph by combining the features of directed edges and non-circular relationships between nodes.\n**A directed acyclic graph (DAG) represents activities in a manner that depicts activities as nodes and dependencies between nodes as edges directed to another node.** Notably, within a DAG, cycles or loops are avoided in the direction of the edges between nodes.\nDAGs have a topological property, which implies that nodes in a DAG are ordered linearly, with nodes arranged sequentially. \nIn this ordering, a node connecting to other nodes is positioned before the nodes it points to. This linear arrangement ensures that the directed edges only move forward in the sequence, preventing any cycles or loops from occurring.\n[![ML pipeline architecture design patterns: directed acyclic graphs \\(DAG\\)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Directed-Acyclic-Graphs-1.png?resize=1800%2C942&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Directed-Acyclic-Graphs-1.png?ssl=1)ML pipeline architecture design patterns: directed acyclic graphs (DAG) | Source: Author\n#### A real-world example of directed acyclic graphs architecture\n[![A real-world example of the directed acyclic graphs architecture](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Directed-Acyclic-Graphs-2.png?resize=1800%2C942&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Directed-Acyclic-Graphs-2.png?ssl=1)A real-world example of the directed acyclic graphs architecture | Source: Author\nA fitting real-world example illustrating the use of DAGs is the process within ride-hailing apps like Uber or Lyft. In this context, **a DAG represents the sequence of activities, tasks, or jobs as nodes, and the directed edges connecting each node indicate the execution order or flow.** For instance, a user must request a driver through the app before the driver can proceed to the user\u2019s location.\nFurthermore, Netflix\u2019s Maestro platform uses DAGs to orchestrate and manage workflows within machine learning/data pipelines. Here, the DAGs represent workflows comprising units embodying job definitions for operations to be carried out, known as Steps.\nPractitioners looking to leverage the DAG architecture within ML pipelines and projects can do so by utilizing the architectural characteristics of DAGs to enforce and manage a description of a sequence of operations that is to be executed in a predictable and efficient manner. \nThis main characteristic of DAGs enables the definition of the execution of workflows in complex ML pipelines to be more manageable, especially where there are high levels of dependencies between processes, jobs, or operations within the ML pipelines.\nFor example, the image below depicts a standard ML pipeline that includes data ingestion, preprocessing, feature extraction, model training, model validation, and prediction. The stages in the pipeline are executed consecutively, one after the other, when the previous stage is marked as complete and provides an output. Each of the stages within can again be defined as nodes within DAGs, with the directed edges indicating the dependencies between the pipeline stages/components.\n[![Standard ML pipeline](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Directed-Acyclic-Graphs.png?resize=1800%2C942&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Directed-Acyclic-Graphs.png?ssl=1)Standard ML pipeline | Source: Author\n#### Advantages and disadvantages of directed acyclic graphs architecture\n  * Using DAGs provides an efficient way to execute processes and tasks in various applications, including big data analytics, machine learning, and artificial intelligence, where task dependencies and the order of execution are crucial.\n\n\n  * In the case of ride-hailing apps, each activity outcome contributes to completing the ride-hailing process. The topological ordering of DAGs ensures the correct sequence of activities, thus facilitating a smoother process flow.\n\n\n  * For machine learning pipelines like those in Netflix\u2019s Maestro, DAGs offer a logical way to illustrate and organize the sequence of process operations. The nodes in a DAG representation correspond to standard components or stages such as data ingestion, data preprocessing, feature extraction, etc. \n\n\n  * The directed edges denote the dependencies between processes and the sequence of process execution. This feature ensures that all operations are executed in the correct order and can also identify opportunities for parallel execution, reducing overall execution time.\n\n\nAlthough DAGs provide the advantage of visualizing interdependencies between tasks, this advantage can become disadvantageous in a large complex machine-learning pipeline that consists of numerous nodes and dependencies between tasks. \n  * Machine learning systems that eventually reach a high level of complexity and are modelled by DAGs become challenging to manage, understand and visualize.\n\n\n  * In modern machine learning pipelines that are expected to be adaptable and operate within dynamic environments or workflows, DAGs are unsuitable for modelling and managing these systems or pipelines, primarily because DAGs are ideal for static workflows with predefined dependencies. \n\n\nHowever, there may be better choices for today\u2019s dynamic Machine Learning pipelines. For example, imagine a pipeline that detects real-time anomalies in network traffic. This pipeline has to adapt to constant changes in network structure and traffic. A static DAG might struggle to model such dynamic dependencies.\n[ ![](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Podcast-logo-1-e1691414642676.png?fit=200%2C119&ssl=1) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) You may also like  Learnings From Building the ML Platform at Stitch Fix and More With Stefan Krawczyk (DAGWorks)  See more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/learnings-from-building-ml-platform-at-stitch-fix)\n### Foreach pattern\n#### What is foreach pattern?\nArchitectural and design patterns in machine learning pipelines can be found in operation implementation within the pipeline phases. Implemented patterns are leveraged within the machine learning pipeline, enabling sequential and efficient execution of operations that act on datasets. One such pattern is the foreach pattern.\n**The foreach pattern is a code execution paradigm that iteratively executes a piece of code for the number of times an item appears within a collection or set of data.** This pattern is particularly useful in processes, components, or stages within machine learning pipelines that are executed sequentially and recursively. This means that the same process can be executed a certain number of times before providing output and progressing to the next process or stage.\nFor example, a standard dataset comprises several data points that must go through the same data preprocessing script to be transformed into a desired data format. In this example, the foreach pattern lends itself as a method of repeatedly calling the processing function \u2018n\u2019 a number of times. Typically \u2018n\u2019 corresponds to the number of data points. \nAnother application of the foreach pattern can be observed in the model training stage, where a model is repeatedly exposed to different partitions of the dataset for training and others for testing for a specified amount of time.\n[![ML pipeline architecture design patterns: foreach pattern](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Foreach-Pattern-1.png?resize=942%2C1800&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Foreach-Pattern-1.png?ssl=1)ML pipeline architecture design patterns: foreach pattern | Source: Author\n#### A real-world example of foreach pattern\n**A real-world application of the foreach pattern is in Netflix\u2019s ML/Data pipeline orchestrator and scheduler, Maestro.** Maestro workflows consist of job definitions that contain steps/jobs executed in an order defined by the DAG (Directed Acyclic Graph) architecture. Within Maestro, the foreach pattern is leveraged internally as a sub-workflow consisting of defined steps/jobs, where steps are executed repeatedly.\nAs mentioned earlier, the foreach pattern can be used in the model training stage of ML pipelines, where a model is repeatedly exposed to different partitions of the dataset for training and others for testing over a specified amount of time.\n[![Foreach ML pipeline architecture pattern in the model training stage of ML pipelines](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Foreach-Pattern-3.png?resize=1800%2C1800&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Foreach-Pattern-3.png?ssl=1)Foreach ML pipeline architecture pattern in the model training stage of ML pipelines | Source: Author\n#### Advantages and disadvantages of foreach pattern\n  * Utilizing the DAG architecture and foreach pattern in an ML pipeline enables a robust, scalable, and manageable ML pipeline solution. \n  * The foreach pattern can then be utilized within each pipeline stage to apply an operation in a repeated manner, such as repeatedly calling a processing function a number of times in a dataset preprocessing scenario. \n  * This setup offers efficient management of complex workflows in ML pipelines.\n\n\nBelow is an illustration of an ML pipeline leveraging DAG and foreach pattern. The flowchart represents a machine learning pipeline where each stage (Data Collection, Data Preprocessing, Feature Extraction, Model Training, Model Validation, and Prediction Generation) is represented as a Directed Acyclic Graph (DAG) node. Within each stage, the \u201cforeach\u201d pattern is used to apply a specific operation to each item in a collection. \nFor instance, each data point is cleaned and transformed during data preprocessing. The directed edges between the stages represent the dependencies, indicating that a stage cannot start until the preceding stage has been completed. This flowchart illustrates the efficient management of complex workflows in machine learning pipelines using the DAG architecture and the foreach pattern.\n[![ML pipeline leveraging DAG and foreach pattern](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Foreach-Pattern-2.png?resize=942%2C1800&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Foreach-Pattern-2.png?ssl=1) ML pipeline leveraging DAG and foreach pattern | Source: Author\nBut there are some disadvantages to it as well.\nWhen utilizing the foreach pattern in data or feature processing stages, all data must be loaded into memory before the operations can be executed. This can lead to poor computational performance, mainly when processing large volumes of data that may exceed available memory resources. For instance, in a use-case where the dataset is several terabytes large, the system may run out of memory, slow down, or even crash if it attempts to load all the data simultaneously.\nAnother limitation of the foreach pattern lies in the execution order of elements within a data collection. The foreach pattern does not guarantee a consistent order of execution or order in the same form the data was loaded. \nInconsistent order of execution within foreach patterns can be problematic in scenarios where the sequence in which data or features are processed is significant. For example, if processing a time-series dataset where the order of data points is critical to understanding trends or patterns, an unordered execution could lead to inaccurate model training and predictions.\n### Embeddings\n#### What is embeddings design pattern?\nEmbeddings are a design pattern present in traditional and modern machine learning pipelines and are defined as low-dimensional representations of high-dimensional data, capturing the key features, relationships, and characteristics of the data\u2019s inherent structures. \nEmbeddings are typically presented as vectors of floating-point numbers, and the relationships or similarities between two embeddings vectors can be deduced using various distance measurement techniques.\nIn machine learning, embeddings play a significant role in various areas, such as model training, computation efficiency, model interpretability, and dimensionality reduction.\n#### A real-world example of embeddings design pattern\nNotable companies such as Google and OpenAI utilize embeddings for several tasks present in processes within machine learning pipelines. Google\u2019s flagship product, Google Search, leverages embeddings in its search engine and recommendation engine, transforming high-dimensional vectors into lower-level vectors that capture the semantic meaning of words within the text. This leads to improved search result performance regarding the relevance of search results to search queries.\nOpenAI, on the other hand, has been at the forefront of advancements in generative AI models, such as GPT-3, which heavily rely on embeddings. In these models, embeddings represent words or tokens in the input text, capturing the semantic and syntactic relationships between words, thereby enabling the model to generate coherent and contextually relevant text. OpenAI also uses embeddings in reinforcement learning tasks, where they represent the state of the environment or the actions of an agent.\n#### Advantages and disadvantages of embeddings design pattern\nThe advantages of the embedding method of data representation in machine learning pipelines lie in its applicability to several ML tasks and ML pipeline components. Embeddings are utilized in computer vision tasks, NLP tasks, and statistics. More specifically, embeddings enable neural networks to consume training data in formats that allow extracting features from the data, which is particularly important in tasks such as natural language processing (NLP) or image recognition. Additionally, embeddings play a significant role in model interpretability, a fundamental aspect of Explainable AI, and serve as a strategy employed to demystify the internal processes of a model, thereby fostering a deeper understanding of the model\u2019s decision-making process. They also act as a data representation form that retains the key information, patterns, and features, providing a lower-dimensional representation of high-dimensional data that retains key patterns and information.\nWithin the context of machine learning, embeddings play a significant role in a number of areas.\n  1. **Model Training** : Embeddings enable neural networks to consume training data in formats that extract features from the data. In machine learning tasks such as natural language processing (NLP) or image recognition, the initial format of the data \u2013 whether it is words or sentences in text or pixels in images and videos \u2013 is not directly conducive to training neural networks. This is where embeddings come into play. By transforming this high-dimensional data into dense vectors of real numbers, embeddings provide a format that allows the network\u2019s parameters, such as weights and biases, to adapt appropriately to the dataset.\n  2. **Model Interpretability** : The models\u2019 capacity to generate prediction results and provide accompanying insights detailing how these predictions were inferred based on the model\u2019s internal parameters, training dataset, and heuristics can significantly enhance the adoption of AI systems. The concept of Explainable AI revolves around developing models that offer inference results and a form of explanation detailing the process behind the prediction. Model interpretability is a fundamental aspect of Explainable AI, serving as a strategy employed to demystify the internal processes of a model, thereby fostering a deeper understanding of the model\u2019s decision-making process. This transparency is crucial in building trust among users and stakeholders, facilitating the debugging and improvement of the model, and ensuring compliance with regulatory requirements. Embeddings provide an approach to model interpretability, especially in NLP tasks where visualizing the semantic relationship between sentences or words in a sentence provides an understanding of how a model understands the text content it has been provided with.\n  3. **Dimensionality Reduction** : Embeddings form data representation that retains key information, patterns, and features. In machine learning pipelines, data contain a vast amount of information captured in varying levels of dimensionality. This means that the vast amount of data increases compute cost, storage requirements, model training, and data processing, all pointing to items found in the[ curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) scenario. Embeddings provide a lower-dimensional representation of high-dimensional data that retains key patterns and information.\n  4. Other areas in ML pipelines: **transfer learning, anomaly detection, vector similarity search, clustering, etc.**\n\n\nAlthough embeddings are useful data representation approaches for many ML tasks, there are a few scenarios where the representational power of embeddings is limited due to sparse data and the lack of inherent patterns in the dataset. This is known as the \u201ccold start\u201d problem, an embedding is a data representation approach that\u2019s generated by identifying the patterns and correlations within elements of datasets, but in situations where there are scarce patterns or insufficient amounts of data, the representational benefits of embeddings can be lost, which results in poor performance in machine learning systems such as recommender and ranking systems.\nAn expected disadvantage of lower dimensional data representation is loss of information; embeddings generated from high dimensional data might sometimes succumb to loss of information in the dimensionality reduction process, contributing to poor performance of machine learning systems and pipelines.\n### Data parallelism\n#### What is data parallelism?\n**D\u0430t\u0430 \u0440\u0430r\u0430llelism is \u0430 str\u0430tegy use\u0501 in \u0430 m\u0430\u0441hine le\u0430rning \u0440i\u0440eline with \u0430\u0441\u0441ess to multi\u0440le \u0441om\u0440ute resour\u0441es, su\u0441h \u0430s CPUs \u0430n\u0501 GPUs \u0430n\u0501 \u0430 l\u0430rge dataset.** This strategy involves dividing the l\u0430rge dataset into sm\u0430ller b\u0430t\u0441hes, e\u0430\u0441h \u0440ro\u0441esse\u0501 on \u0430 different \u0441om\u0440uting resources. \nAt the st\u0430rt of tr\u0430ining, the s\u0430me initi\u0430l mo\u0501el \u0440\u0430r\u0430meters \u0430n\u0501 weights \u0430re \u0441o\u0440ie\u0501 to e\u0430\u0441h \u0441om\u0440ute resour\u0441e. As e\u0430\u0441h resour\u0441e \u0440ro\u0441esses its b\u0430t\u0441h of data, it independently updates these \u0440\u0430r\u0430meters \u0430n\u0501 weights. After e\u0430\u0441h b\u0430t\u0441h is \u0440ro\u0441esse\u0501, these \u0440\u0430r\u0430meters\u2019 gr\u0430\u0501ients (or \u0441h\u0430nges) \u0430re \u0441om\u0440ute\u0501 \u0430n\u0501 shared \u0430\u0441ross \u0430ll resour\u0441es. This ensures that \u0430ll \u0441o\u0440ies of the mo\u0501el remain synchronized during training.\n[![ML pipeline architecture design patterns: data parallelism](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Data-Parallelism.png?resize=942%2C1800&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Data-Parallelism.png?ssl=1)**ML pipeline architecture design patterns:** d**\u0430t\u0430 \u0440\u0430r\u0430llelism** | Source: Author\n#### A real-world example of data parallelism\n**A real-world scenario of how the principles of data parallelism are embodied in real-life applications is the groundbreaking work by Facebook AI Research (FAIR) Engineering** with their novel system \u2013 the [Fully Sharded Data Parallel (FSDP) system](https://engineering.fb.com/2021/07/15/open-source/fsdp/). \nThis innovative creation has the sole purpose of enhancing the training process of massive AI models. It does so by disseminating an AI model\u2019s variables over data parallel operators while also optionally offloading a fraction of the training computation to CPUs.\nFSDP sets itself apart by its unique approach to sharding parameters. It takes a more balanced approach which results in superior performance. This is achieved by allowing training-related communication and computation to overlap. **What is exciting about FSDP is how it optimizes the training of vastly larger models but uses fewer GPUs in the process.**\nThis optimization becomes particularly relevant and valuable in specialized areas such as Natural Language Processing (NLP) and computer vision. Both these areas often demand large-scale model training.\nA practical application of FSDP is evident within the operations of Facebook. **They have incorporated FSDP in the training process of some of their NLP and Vision models, a testament to its effectiveness.** Moreover, it is a part of the FairScale library, providing a straightforward API to enable developers and engineers to improve and scale their model training.\nThe influence of FSDP extends to numerous machine learning frameworks, like fairseq for language models, VISSL for computer vision models, and PyTorch Lightning for a wide range of other applications. This broad integration showcases the applicability and usability of data parallelism in modern machine learning pipelines.\n#### Advantages and disadvantages of data parallelism\n  * The concept of data parallelism presents a compelling approach to reducing training time in machine learning models. \n  * The fundamental idea is to subdivide the dataset and then concurrently process these divisions on various computing platforms, be it multiple CPUs or GPUs. As a result, you get the most out of the available computing resources.\n  * Integrating data parallelism into your processes and ML pipeline is challenging. For instance, synchronizing model parameters across diverse computing resources has added complexity. Particularly in distributed systems, this synchronization may incur overhead costs due to possible communication latency issues. \n  * Moreover, it is essential to note that the utility of data parallelism only extends to some machine learning models or datasets. There are models with sequential dependencies, like certain types of recurrent neural networks, which might not align well with a data parallel approach.\n\n\n### Model parallelism\n#### What is model parallelism?\n**Model parallelism is used within machine learning pipelines to efficiently utilize compute resources when the deep learning model is too large to be held on a single instance of GPU or CPU.** This compute efficiency is achieved by splitting the initial model into subparts and holding those parts on different GPUs, CPUs, or machines. \nThe model parallelism strategy hosts different parts of the model on different computing resources. Additionally, the computations of model gradients and training are executed on each machine for their respective segment of the initial model. This strategy was born in the era of deep learning, where models are large enough to contain billions of parameters, meaning they cannot be held or stored on a single GPU.\n[![ML pipeline architecture design patterns: model parallelism](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Model-Parallelism.png?resize=942%2C1800&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Model-Parallelism.png?ssl=1)ML pipeline architecture design patterns: model parallelism | Source: Author\n#### A real-world example of model parallelism\nDeep learning models today are inherently large in terms of the number of internal parameters; this results in needing scalable computing resources to hold and calculate model parameters during training and inference phases in ML pipeline. For example, GPT-3 has 175 billion parameters and requires 800GB of memory space, and other foundation models, such as LLaMA, created by Meta, have parameters ranging from 7 billion to 70 billion. \nThese models require significant computational resources during the training phase. Model parallelism offers a method of training parts of the model across different compute resources, where each resource trains the model on a mini-batch of the training data and computes the gradients for their allocated part of the original model.\n#### Advantages and disadvantages of model parallelism\nImplementing model parallelism within ML pipelines comes with unique challenges. \n  * There\u2019s a requirement for constant communication between machines holding parts of the initial model as the output of one part of the model is used as input for another. \n  * In addition, understanding what part of the model to split into segments requires a deep understanding and experience with complex deep learning models and, in most cases, the particular model itself. \n  * One key advantage is the efficient use of compute resources to handle and train large models.\n\n\n### Federated learning\n#### What is federated learning architecture?\n**Federated Learning is an approach to distributed learning that attempts to enable innovative advancements made possible through machine learning while also considering the evolving perspective of privacy and sensitive data.**\nA relatively new method, Federated Learning decentralizes the model training processes across devices or machines so that the data doesn\u2019t have to leave the premises of the machine. Instead, only the updates to the model\u2019s internal parameters, which are trained on a copy of the model using unique user-centric data stored on the device, are transferred to a central server. This central server accumulates all updates from other local devices and applies the changes to a model residing on the centralised server.\n#### A real-world example of federated learning architecture\nWithin the Federated Learning approach to distributed machine learning, the user\u2019s privacy and data are preserved as they never leave the user\u2019s device or machine where the data is stored. This approach is a strategic model training method in ML pipelines where data sensitivity and access are highly prioritized. **It allows for machine learning functionality without transmitting user data across devices or to centralized systems such as cloud storage solutions.**\n[![ML pipeline architecture design patterns: federated learning architecture](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Federated-Learning.png?resize=942%2C1800&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Federated-Learning.png?ssl=1)ML pipeline architecture design patterns: federated learning architecture | Source: Author\n#### Advantages and disadvantages of federated learning architecture\nFederated Learning steers an organization toward a more data-friendly future by ensuring user privacy and preserving data. However, it does have limitations. \n  * Federated learning is still in its infancy, which means a limited number of tools and technologies are available to facilitate the implementation of efficient, federated learning procedures. \n  * Adopting federated learning in a fully matured organization with a standardized ML pipeline requires significant effort and investment as it introduces a new approach to model training, implementation, and evaluation that requires a complete restructuring of existing ML infrastructure. \n  * Additionally, the central model\u2019s overall performance relies on several user-centric factors, such as data quality and transmission speed.\n\n\n### Synchronous training\n#### What is synchronous training architecture?\nSynchronous Training is a machine learning pipeline strategy that comes into play when complex deep learning models are partitioned or distributed across different compute resources, and there is an increased requirement for consistency during the training process. \nIn this context, synchronous training involves a coordinated effort among all independent computational units, referred to as \u2018workers\u2019. Each worker holds a partition of the model and updates its parameters using its portion of the evenly distributed data. \n**The key characteristic of synchronous training is that all workers operate in synchrony, which means that every worker must complete the training phase before any of them can proceed to the next operation or training step.**\n[![ML pipeline architecture design patterns: synchronous training](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Synchronous-Training.png?resize=942%2C1800&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Synchronous-Training.png?ssl=1)ML pipeline architecture design patterns: synchronous training | Source: Author\n#### A real-world example of synchronous training architecture\nSynchronous Training is relevant to scenarios or use cases where there is a need for even distribution of training data across compute resources, uniform computational capacity across all resources, and low latency communication between these independent resources. \n#### Advantages and disadvantages of synchronous training architecture\n  * The advantages of synchronous training are consistency, uniformity, improved accuracy and simplicity.\n  * All workers conclude their training phases before progressing to the next step, thereby retaining consistency across all units\u2019 model parameters. \n  * Compared to asynchronous methods, synchronous training often achieves superior results as workers\u2019 synchronized and uniform operation reduces variance in parameter updates at each step.\n  * One major disadvantage is the longevity of the training phase within synchronous training. \n  * Synchronous training may pose time efficiency issues as it requires the completion of tasks by all workers before proceeding to the next step. \n  * This could introduce inefficiencies, especially in systems with heterogeneous computing resources.\n\n\n### Parameter server architecture\n#### What is parameter server architecture?\n**The Parameter Server Architecture is designed to tackle distributed machine learning problems such as worker interdependencies, complexity in implementing strategies, consistency, and synchronization.**\nThis architecture operates on the principle of server-client relationships, where the client nodes, referred to as \u2018workers\u2019, are assigned specific tasks such as handling data, managing model partitions, and executing defined operations. \nOn the other hand, the server node plays a central role in managing and aggregating the updated model parameters and is also responsible for communicating these updates to the client nodes.\n#### A real-world example of parameter server architecture\nIn the context of distributed machine learning systems, the Parameter Server Architecture is used to facilitate efficient and coordinated learning. The server node in this architecture ensures consistency in the model\u2019s parameters across the distributed system, making it a viable choice for handling large-scale machine-learning tasks that require careful management of model parameters across multiple nodes or workers.\n[![ML pipeline architecture design patterns: parameter server architecture](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Parameter-Server-Architecture-1.png?resize=1800%2C942&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Parameter-Server-Architecture-1.png?ssl=1)ML pipeline architecture design patterns: parameter server architecture | Source: Author\n#### Advantages and disadvantages of parameter server architecture\n  * The Parameter Server Architecture facilitates a high level of organization within machine learning pipelines and workflows, mainly due to servers\u2019 and client nodes\u2019 distinct, defined responsibilities. \n  * This clear distinction simplifies the operation, streamlines problem-solving, and optimizes pipeline management. \n  * Centralizing the upkeep and consistency of model parameters at the server node ensures the transmission of the most recent updates to all client nodes or workers, reinforcing the performance and trustworthiness of the model\u2019s output.\n\n\nHowever, this architectural approach has its drawbacks. \n  * A significant downside is its vulnerability to a total system failure, stemming from its reliance on the server node. \n  * Consequently, if the server node experiences any malfunction, it could potentially cripple the entire system, underscoring the inherent risk of single points of failure in this architecture.\n\n\n### Ring-AllReduce architecture\n#### What is ring-allreduce architecture?\nThe Ring-AllReduce Architecture is a distributed machine learning training architecture leveraged in modern machine learning pipelines. It provides a method to manage the gradient computation and model parameter updates made through backpropagation in large complex machine learning models training on extensive datasets. Each worker node is provided with a copy of the complete model\u2019s parameters and a subset of the training data in this architecture. \nThe workers independently compute their gradients during backward propagation on their own partition of the training data. A ring-like structure is applied to ensure each worker on a device has a model with parameters that include the gradient updates made on all other independent workers. \nThis is achieved by passing the sum of gradients from one worker to the next worker in the ring, which then adds its own computed gradient to the sum and passes it on to the following worker. This process is repeated until all the workers have the complete sum of the gradients aggregated from all workers in the ring.\n[![ML pipeline architecture design patterns: ring-allreduce architecture](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Parameter-Server-Architecture-2.png?resize=1800%2C942&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Parameter-Server-Architecture-2.png?ssl=1)ML pipeline architecture design patterns: ring-allreduce architecture | Source: Author\n#### A real-world example of ring-allreduce architecture\nThe Ring-AllReduce Architecture has proven instrumental in various real-world applications involving distributed machine learning training, particularly in scenarios requiring handling extensive datasets. **For instance, leading tech companies like Facebook and Google successfully integrated this architecture into their machine learning pipelines.**\nFacebook\u2019s AI Research (FAIR) team utilizes the[ Ring-AllReduce architecture for distributed deep learning](https://pytorch.org/tutorials/intermediate/dist_tuto.html?highlight=ring%20all%20reduce), helping to enhance the training efficiency of their models and effectively handle extensive and complex datasets. Google also incorporates this architecture into its [TensorFlow machine learning framework](https://www.tensorflow.org/guide/distributed_training), thus enabling efficient multi-node training of deep learning models.\n#### Advantages and disadvantages of ring-allreduce architecture\n  * The advantage of the Ring-AllReduce architecture is that it is an efficient strategy for managing distributed machine learning tasks, especially when dealing with large datasets. \n  * It enables effective data parallelism by ensuring optimal utilization of computational resources. Each worker node holds a complete copy of the model and is responsible for training on its subset of the data. \n  * Another advantage of Ring-AllReduce is that it allows for the aggregation of model parameter updates across multiple devices. While each worker trains on a subset of the data, it also benefits from gradient updates computed by other workers. \n  * This approach accelerates the model training phase and enhances the scalability of the machine learning pipeline, allowing for an increase in the number of models as demand grows.\n\n\n## Conclusion\nThis article covered various aspects, including pipeline architecture, design considerations, standard practices in leading tech corporations, common patterns, and typical components of ML pipelines.\nWe also introduced tools, methodologies, and software essential for constructing and maintaining ML pipelines, alongside discussing best practices. We provided illustrated overviews of architecture and design patterns like Single Leader Architecture, Directed Acyclic Graphs, and the Foreach Pattern.\nAdditionally, we examined various distribution strategies offering unique solutions to distributed machine learning problems, including Data Parallelism, Model Parallelism, Federated Learning, Synchronous Training, and Parameter Server Architecture.\nFor ML practitioners who are focused on career longevity, it is crucial to recognize how an ML pipeline should function and how it can scale and adapt while maintaining a troubleshoot-friendly infrastructure. I hope this article brought you much-needed clarity around the same.\n### References\n  * Netflix Tech Blog: Meson Workflow Orchestration for Netflix Recommendations\n  * Netflix. (2022, January 18). Meson workflow orchestration for Netflix recommendations. [Online]. Retrieved from <https://netflixtechblog.com/meson-workflow-orchestration-for-netflix-recommendations-fc932625c1d9>\n  * Facebook Engineering: fSDP: A Fast and Scalable Distributed Parallel Programming Framework\n  * Facebook. (2021, July 15). fSDP: A Fast and Scalable Distributed Parallel Programming Framework. [Online]. Retrieved from <https://engineering.fb.com/2021/07/15/open-source/fsdp/>\n\n\n##  Was the article useful? \n![yes](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--yes.svg) Yes  ![no](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--no.svg) No \n![](https://neptune.ai/wp-content/themes/neptune/img/icon-bulb.svg) Suggest changes \nYour email Your message (optional)\nThis site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply. \nI am familiar with the [Privacy Policy](https://neptune.staginglab.eu/privacy-policy)*\nSubmit\n\u0394\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/i-box/header-icon.svg)\n### **More about** ML Pipeline Architecture Design Patterns (With 10 Real-World Examples) \n####  Check out our  **product resources** and  **related articles** below: \n[ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  Real-World MLOps Examples: End-To-End MLOps Pipeline for Visual Search at Brainly  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/mlops-examples-pipeline-for-visual-search-brainly)\n###  Explore more content topics: \n[ Computer Vision ](https://neptune.ai/blog/category/computer-vision) [ General ](https://neptune.ai/blog/category/general) [ LLMOps ](https://neptune.ai/blog/category/llmops) [ ML Model Development ](https://neptune.ai/blog/category/machine-learning-model-development) [ ML Tools ](https://neptune.ai/blog/category/machine-learning-tools) [ MLOps ](https://neptune.ai/blog/category/mlops) [ Natural Language Processing ](https://neptune.ai/blog/category/natural-language-processing) [ Paper Reflections ](https://neptune.ai/blog/category/paper-reflections) [ Product Updates ](https://neptune.ai/blog/category/product-updates) [ Reinforcement Learning ](https://neptune.ai/blog/category/reinforcement-learning) [ Tabular Data ](https://neptune.ai/blog/category/tabular-data) [ Time Series ](https://neptune.ai/blog/category/time-series-forecasting)\nAbout neptune.ai ![chevron](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1920 1080'%3E%3C/svg%3E) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-play.svg)\nNeptune is the experiment tracker purpose-built for foundation model training. \nIt lets you monitor and visualize thousands of per-layer metrics\u2014losses, gradients, and activations\u2014at any scale. Drill down into logs and debug training issues fast. Keep your model training stable while reducing wasted GPU cycles. \n[ Play with a live project  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://demo.neptune.ai/) [ See Docs  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://docs.neptune.ai/)\nTable of contents\n![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-small.svg)\n  1. What are ML pipeline architecture design patterns?\n  2. Advantages of understanding ML pipeline architecture\n  3. Common ML pipeline architecture steps\n  4. 10 ML pipeline architecture examples\n     * Single leader architecture\n     * Directed acyclic graphs (DAG)\n     * Foreach pattern\n     * Embeddings\n     * Data parallelism\n     * Model parallelism\n     * Federated learning\n     * Synchronous training\n     * Parameter server architecture\n     * Ring-AllReduce architecture\n  5. Conclusion\n\n\n![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-cancel.svg) ![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-more.svg)\nCheck also: [Learnings From Building the ML Platform at Stitch Fix](https://neptune.ai/blog/learnings-from-building-ml-platform-at-stitch-fix)\n##  **Monitor your model training at scale**\nJoin 60,000+ researchers and practitioners who use Neptune to debug training failures, spot anomalies, and compare experiments.\n[ Try for free  ](https://app.neptune.ai/register) [ Get early access to Neptune Scale  ](https://neptune.ai/early-access)\n[ Take an interactive product tour  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)\n![](https://neptune.ai/wp-content/themes/neptune/img/footer-post-illustration.svg)\n#### Newsletter\nTop articles, case studies, events (and more) in your inbox every month.\nGet Newsletter\n  * [Product](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [Walkthrough](https://neptune.ai/walkthrough)\n    * [Documentation](https://docs.neptune.ai/)\n    * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [Pricing](https://neptune.ai/pricing)\n    * [Service status](https://status.neptune.ai/)\n\n\n  * [Solutions](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [AI Researcher](https://neptune.ai/product/ai-researcher)\n    * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n    * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n    * [Enterprise](https://neptune.ai/product/enterprise)\n    * [Academic Research](https://neptune.ai/research)\n\n\n  * [Compare](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [Neptune vs Weights & Biases](https://neptune.ai/vs/wandb)\n    * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n    * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n    * [Other comparisons](https://neptune.ai/vs)\n    * [ML experiment tracking tools](https://neptune.ai/blog/best-ml-experiment-tracking-tools)\n\n\n  * [Community](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [Blog](https://neptune.ai/blog)\n    * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n    * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n    * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n    * [How to Build an Experiment Tracker](https://neptune.ai/blog/build-experiment-tracking-tool)\n\n\n  * [Company](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/)\n    * [Customers](https://neptune.ai/customers)\n    * [About us](https://neptune.ai/about-us)\n    * [Careers](https://neptune.ai/jobs)\n    * [Security portal and SOC 2](https://security.neptune.ai/)\n    * [Contact us](https://neptune.ai/contact-us)\n\n\n[ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-linked-in.svg) ](https://www.linkedin.com/company/neptuneai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-twitter.svg) ](https://twitter.com/neptune_ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-github.svg) ](https://github.com/neptune-ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-facebook.svg) ](https://www.facebook.com/neptuneAI) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-youtube.svg) ](https://www.youtube.com/channel/UCvOJU-ubyUqxGSDRN7xK4Ng) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-spotify.svg) ](https://open.spotify.com/show/4kGi82i4wTYgHbWmVMri5x) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-apple-podcast.svg) ](https://podcasts.apple.com/us/podcast/mlops-live/id1634179447)\n[ Terms of Service ](https://neptune.ai/terms-of-service) [ Privacy Policy ](https://neptune.ai/privacy-policy) [ SLA ](https://neptune.ai/service-level-agreement)\nCopyright \u00a9 2025 Neptune Labs. All rights reserved.\n![](https://neptune.ai/wp-content/themes/neptune/img/footer-stars--414w.png) ![](https://pixel.wp.com/g.gif?v=ext&blog=211928962&post=27632&tz=0&srv=neptune.ai&hp=atomic&ac=2&amp=0&j=1%3A14.5-a.9&host=neptune.ai&ref=&fcp=0&rand=0.696484607754676) ![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=85a8326f-ba58-4970-817c-35779c7f8de4&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=4c02b405-1808-4468-965a-6761ebe7cffe&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-pipeline-architecture-design-patterns&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=85a8326f-ba58-4970-817c-35779c7f8de4&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=4c02b405-1808-4468-965a-6761ebe7cffe&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-pipeline-architecture-design-patterns&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)\n![](https://bat.bing.com/action/0?ti=97011748&tm=gtm002&Ver=2&mid=b448af9d-30a1-48f4-9445-490029517012&bo=1&sid=37ef08200d8e11f0a36ed785bff3ae58&vid=37ef08800d8e11f089240dd13df19027&vids=0&msclkid=N&pi=918639831&lg=en-US&sw=1080&sh=600&sc=30&tl=ML%20Pipeline%20Architecture%20Design%20Patterns%20\\(With%20Examples\\)&p=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-pipeline-architecture-design-patterns&r=&lt=794&evt=pageLoad&sv=1&cdb=AQkR&rn=744197)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe article discusses ML pipeline architecture and design patterns, emphasizing their importance for scalability, efficiency, and reproducibility. It outlines common ML pipeline stages, examples like Single Leader Architecture and DAGs, and strategies such as Data and Model Parallelism, Federated Learning, and Synchronous Training. Understanding these patterns enhances the development and maintenance of robust ML systems.\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\n# \n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg)](https://huggingface.co/tiiuae)\n[tiiuae](https://huggingface.co/tiiuae)\n/\n[falcon-40b](https://huggingface.co/tiiuae/falcon-40b)\nlike 2.42k\nFollow\n![](https://cdn-avatars.huggingface.co/v1/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg) Technology Innovation Institute 1.27k\n[ Text Generation ](https://huggingface.co/models?pipeline_tag=text-generation)[ Transformers ](https://huggingface.co/models?library=transformers)[ PyTorch ](https://huggingface.co/models?library=pytorch)[ Safetensors ](https://huggingface.co/models?library=safetensors)\ntiiuae/falcon-refinedweb\n4 languages\n[ falcon ](https://huggingface.co/models?other=falcon)[ custom_code ](https://huggingface.co/models?other=custom_code)[ text-generation-inference ](https://huggingface.co/models?other=text-generation-inference)\narxiv: 6 papers\nLicense: apache-2.0\n[ Model card ](https://huggingface.co/tiiuae/falcon-40b)[ Files Files and versions ](https://huggingface.co/tiiuae/falcon-40b/tree/main)[ Community 115 ](https://huggingface.co/tiiuae/falcon-40b/discussions)\nTrain \nDeploy \nUse this model \n  * [\ud83d\ude80 Falcon-40B](https://huggingface.co/tiiuae/falcon-40b/#\ud83d\ude80-falcon-40b \"\ud83d\ude80 Falcon-40B\")\n    * [Why use Falcon-40B?](https://huggingface.co/tiiuae/falcon-40b/#why-use-falcon-40b \"Why use Falcon-40B?\")\n  * [Model Card for Falcon-40B](https://huggingface.co/tiiuae/falcon-40b/#model-card-for-falcon-40b \"Model Card for Falcon-40B\")\n    * [Model Details](https://huggingface.co/tiiuae/falcon-40b/#model-details \"Model Details\")\n      * [Model Description](https://huggingface.co/tiiuae/falcon-40b/#model-description \"Model Description\")\n      * [Model Source](https://huggingface.co/tiiuae/falcon-40b/#model-source \"Model Source\")\n    * [Uses](https://huggingface.co/tiiuae/falcon-40b/#uses \"Uses\")\n      * [Direct Use](https://huggingface.co/tiiuae/falcon-40b/#direct-use \"Direct Use\")\n      * [Out-of-Scope Use](https://huggingface.co/tiiuae/falcon-40b/#out-of-scope-use \"Out-of-Scope Use\")\n    * [Bias, Risks, and Limitations](https://huggingface.co/tiiuae/falcon-40b/#bias-risks-and-limitations \"Bias, Risks, and Limitations\")\n      * [Recommendations](https://huggingface.co/tiiuae/falcon-40b/#recommendations \"Recommendations\")\n    * [How to Get Started with the Model](https://huggingface.co/tiiuae/falcon-40b/#how-to-get-started-with-the-model \"How to Get Started with the Model\")\n    * [Training Details](https://huggingface.co/tiiuae/falcon-40b/#training-details \"Training Details\")\n      * [Training Data](https://huggingface.co/tiiuae/falcon-40b/#training-data \"Training Data\")\n      * [Training Procedure](https://huggingface.co/tiiuae/falcon-40b/#training-procedure \"Training Procedure\")\n    * [Evaluation](https://huggingface.co/tiiuae/falcon-40b/#evaluation \"Evaluation\")\n    * [Technical Specifications](https://huggingface.co/tiiuae/falcon-40b/#technical-specifications \"Technical Specifications\")\n      * [Model Architecture and Objective](https://huggingface.co/tiiuae/falcon-40b/#model-architecture-and-objective \"Model Architecture and Objective\")\n      * [Compute Infrastructure](https://huggingface.co/tiiuae/falcon-40b/#compute-infrastructure \"Compute Infrastructure\")\n    * [Citation](https://huggingface.co/tiiuae/falcon-40b/#citation \"Citation\")\n    * [License](https://huggingface.co/tiiuae/falcon-40b/#license \"License\")\n    * [Contact](https://huggingface.co/tiiuae/falcon-40b/#contact \"Contact\")\n\n\n#  [ ](https://huggingface.co/tiiuae/falcon-40b/#\ud83d\ude80-falcon-40b) \ud83d\ude80 Falcon-40B \n**Falcon-40B is a 40B parameters causal decoder-only model built by[TII](https://www.tii.ae) and trained on 1,000B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. It is made available under the Apache 2.0 license.**\n_Paper coming soon \ud83d\ude0a._\n\ud83e\udd17 To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading [this great blogpost fron HF](https://huggingface.co/blog/falcon)!\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#why-use-falcon-40b) Why use Falcon-40B? \n  * **It is the best open-source model currently available.** Falcon-40B outperforms [LLaMA](https://github.com/facebookresearch/llama), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1), [MPT](https://huggingface.co/mosaicml/mpt-7b), etc. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n  * **It features an architecture optimized for inference** , with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \n  * **It is made available under a permissive Apache 2.0 license allowing for commercial use** , without any royalties or restrictions.\n  * \u26a0\ufe0f **This is a raw, pretrained model, which should be further finetuned for most usecases.** If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct).\n\n\n\ud83d\udcb8 **Looking for a smaller, less expensive model?** [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) is Falcon-40B's little brother!\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nmodel = \"tiiuae/falcon-40b\"\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n  \"text-generation\",\n  model=model,\n  tokenizer=tokenizer,\n  torch_dtype=torch.bfloat16,\n  trust_remote_code=True,\n  device_map=\"auto\",\n)\nsequences = pipeline(\n  \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n  max_length=200,\n  do_sample=True,\n  top_k=10,\n  num_return_sequences=1,\n  eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n  print(f\"Result: {seq['generated_text']}\")\n\n```\n\n\ud83d\udca5 **Falcon LLMs require PyTorch 2.0 for use with`transformers`!**\nFor fast inference with Falcon, check-out [Text Generation Inference](https://github.com/huggingface/text-generation-inference)! Read more in this [blogpost](https://huggingface.co/tiiuae/falcon-40b/blob/main/\\(https://huggingface.co/blog/falcon). \nYou will need **at least 85-100GB of memory** to swiftly run inference with Falcon-40B.\n#  [ ](https://huggingface.co/tiiuae/falcon-40b/#model-card-for-falcon-40b) Model Card for Falcon-40B \n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#model-details) Model Details \n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#model-description) Model Description \n  * **Developed by:** <https://www.tii.ae>;\n  * **Model type:** Causal decoder-only;\n  * **Language(s) (NLP):** English, German, Spanish, French (and limited capabilities in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish);\n  * **License:** Apache 2.0 license.\n\n\n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#model-source) Model Source \n  * **Paper:** _coming soon_.\n\n\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#uses) Uses \n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#direct-use) Direct Use \nResearch on large language models; as a foundation for further specialization and finetuning for specific usecases (e.g., summarization, text generation, chatbot, etc.)\n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#out-of-scope-use) Out-of-Scope Use \nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. \n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#bias-risks-and-limitations) Bias, Risks, and Limitations \nFalcon-40B is trained mostly on English, German, Spanish, French, with limited capabilities also in in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish. It will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#recommendations) Recommendations \nWe recommend users of Falcon-40B to consider finetuning it for the specific set of tasks of interest, and for guardrails and appropriate precautions to be taken for any production use.\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#how-to-get-started-with-the-model) How to Get Started with the Model \n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nmodel = \"tiiuae/falcon-40b\"\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n  \"text-generation\",\n  model=model,\n  tokenizer=tokenizer,\n  torch_dtype=torch.bfloat16,\n  trust_remote_code=True,\n  device_map=\"auto\",\n)\nsequences = pipeline(\n  \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n  max_length=200,\n  do_sample=True,\n  top_k=10,\n  num_return_sequences=1,\n  eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n  print(f\"Result: {seq['generated_text']}\")\n\n```\n\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#training-details) Training Details \n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#training-data) Training Data \nFalcon-40B was trained on 1,000B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile ([Gao et al., 2020](https://arxiv.org/abs/2101.00027)). \n**Data source** | **Fraction** | **Tokens** | **Sources**  \n---|---|---|---  \n[RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | 75% | 750B | massive web crawl  \nRefinedWeb-Europe | 7% | 70B | European massive web crawl  \nBooks | 6% | 60B  \nConversations | 5% | 50B | Reddit, StackOverflow, HackerNews  \nCode | 5% | 50B  \nTechnical | 2% | 20B | arXiv, PubMed, USPTO, etc.  \nRefinedWeb-Europe is made of the following languages:\n**Language** | **Fraction of multilingual data** | **Tokens**  \n---|---|---  \nGerman | 26% | 18B  \nSpanish | 24% | 17B  \nFrench | 23% | 16B  \n_Italian_ | 7% | 5B  \n_Portuguese_ | 4% | 3B  \n_Polish_ | 4% | 3B  \n_Dutch_ | 4% | 3B  \n_Romanian_ | 3% | 2B  \n_Czech_ | 3% | 2B  \n_Swedish_ | 2% | 1B  \nThe data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.\n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#training-procedure) Training Procedure \nFalcon-40B was trained on 384 A100 40GB GPUs, using a 3D parallelism strategy (TP=8, PP=4, DP=12) combined with ZeRO.\n####  [ ](https://huggingface.co/tiiuae/falcon-40b/#training-hyperparameters) Training Hyperparameters \n**Hyperparameter** | **Value** | **Comment**  \n---|---|---  \nPrecision | `bfloat16`  \nOptimizer | AdamW  \nLearning rate | 1.85e-4 | 4B tokens warm-up, cosine decay to 1.85e-5  \nWeight decay | 1e-1  \nZ-loss | 1e-4  \nBatch size | 1152 | 100B tokens ramp-up  \n####  [ ](https://huggingface.co/tiiuae/falcon-40b/#speeds-sizes-times) Speeds, Sizes, Times \nTraining started in December 2022 and took two months. \n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#evaluation) Evaluation \n_Paper coming soon._\nSee the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#technical-specifications) Technical Specifications \n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#model-architecture-and-objective) Model Architecture and Objective \nFalcon-40B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).\nThe architecture is broadly adapted from the GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the following differences:\n  * **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));\n  * **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));\n  * **Decoder-block:** parallel attention/MLP with a two layer norms.\n\n\nFor multiquery, we are using an internal variant which uses independent key and values per tensor parallel degree.\n**Hyperparameter** | **Value** | **Comment**  \n---|---|---  \nLayers | 60  \n`d_model` | 8192  \n`head_dim` | 64 | Reduced to optimise for FlashAttention  \nVocabulary | 65024  \nSequence length | 2048  \n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#compute-infrastructure) Compute Infrastructure \n####  [ ](https://huggingface.co/tiiuae/falcon-40b/#hardware) Hardware \nFalcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances. \n####  [ ](https://huggingface.co/tiiuae/falcon-40b/#software) Software \nFalcon-40B was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.)\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#citation) Citation \n_Paper coming soon_ \ud83d\ude0a. In the meanwhile, you can use the following information to cite: \n```\n@article{falcon40b,\n title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n year={2023}\n}\n\n```\n\nTo learn more about the pretraining dataset, see the \ud83d\udcd3 [RefinedWeb paper](https://arxiv.org/abs/2306.01116).\n```\n@article{refinedweb,\n title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n journal={arXiv preprint arXiv:2306.01116},\n eprint={2306.01116},\n eprinttype = {arXiv},\n url={https://arxiv.org/abs/2306.01116},\n year={2023}\n}\n\n```\n\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#license) License \nFalcon-40B is made available under the Apache 2.0 license.\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#contact) Contact \nfalconllm@tii.ae \n\nDownloads last month\n    144,698 \nSafetensors[](https://huggingface.co/docs/safetensors)\nModel size\n41.8B params\nTensor type\nBF16 \n\u00b7\nInference Providers [NEW](https://huggingface.co/blog/inference-providers)\n[ Text Generation](https://huggingface.co/tasks/text-generation \"Learn more about text-generation\")\nThis model isn't deployed by any Inference Provider. [\ud83d\ude4b Ask for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=tiiuae/falcon-40b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btiiuae%2Ffalcon-40b%5D\\(%2Ftiiuae%2Ffalcon-40b\\)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A\\(optional\\)%20Which%20providers%20are%20you%20interested%20in%3F%20\\(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6\\)%0A)\nHF Inference deployability: The model authors have turned it off explicitly.\n##  Model tree for tiiuae/falcon-40b [](https://huggingface.co/docs/hub/model-cards#specifying-a-base-model)\nAdapters\n[10 models](https://huggingface.co/models?other=base_model:adapter:tiiuae/falcon-40b)\nFinetunes\n[4 models](https://huggingface.co/models?other=base_model:finetune:tiiuae/falcon-40b)\nQuantizations\n[4 models](https://huggingface.co/models?other=base_model:quantized:tiiuae/falcon-40b)\n##  Dataset used to train tiiuae/falcon-40b\n#### [tiiuae/falcon-refinedweb Viewer \u2022  Updated Jun 20, 2023 \u2022  968M \u2022  41.3k \u2022  842  ](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n##  Spaces using tiiuae/falcon-40b 100\n[\ud83d\udcac HuggingFaceH4/falcon-chat](https://huggingface.co/spaces/HuggingFaceH4/falcon-chat)[\ud83c\udfc6 eduagarcia/open_pt_llm_leaderboard](https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard)[\ud83c\udfc6 BAAI/open_cn_llm_leaderboard](https://huggingface.co/spaces/BAAI/open_cn_llm_leaderboard)[\ud83d\ude3b Sharathhebbar24/One-stop-for-Open-source-models](https://huggingface.co/spaces/Sharathhebbar24/One-stop-for-Open-source-models)[\ud83d\udc83 EvanTHU/MotionLLM](https://huggingface.co/spaces/EvanTHU/MotionLLM)[\ud83d\ude3b GTBench/GTBench](https://huggingface.co/spaces/GTBench/GTBench)[\ud83c\udfc6 Vikhrmodels/small-shlepa-lb](https://huggingface.co/spaces/Vikhrmodels/small-shlepa-lb)[\ud83d\ude80 Justinrune/LLaMA-Factory](https://huggingface.co/spaces/Justinrune/LLaMA-Factory)[\ud83c\udfc6 kz-transformers/kaz-llm-lb](https://huggingface.co/spaces/kz-transformers/kaz-llm-lb)[\ud83d\ude80 awacke1/Deepseek-HPC-GPU-KEDA](https://huggingface.co/spaces/awacke1/Deepseek-HPC-GPU-KEDA)[\ud83d\udd25 HemaAM/GPT_train_on_LLaMa](https://huggingface.co/spaces/HemaAM/GPT_train_on_LLaMa)[\ud83c\udfa8 OPTML-Group/UnlearnCanvas-Benchmark](https://huggingface.co/spaces/OPTML-Group/UnlearnCanvas-Benchmark) + 95 Spaces + 88 Spaces\n##  Collection including tiiuae/falcon-40b\n#### [Falcon Collection  8 items \u2022  Updated Feb 13 \u2022 17](https://huggingface.co/collections/tiiuae/falcon-64fb432660017eeec9837b5a)\nSystem theme \nCompany\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/) [](https://huggingface.co/)\nWebsite\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)\n",
    "answer": "```markdown\n# TL;DR Summary of Falcon-40B Model\n\n**Falcon-40B** is a 40B parameter causal decoder-only model by **TII**, trained on 1,000B tokens from **RefinedWeb**. It excels in text generation, outperforming models like **LLaMA** and **StableLM**. Available under **Apache 2.0**, it requires fine-tuning for specific tasks and is optimized for inference with **PyTorch 2.0**.\n```"
  },
  {
    "instruction": "[Home](https://martinheinz.dev/)[Contact](https://martinheinz.dev/contact)[Subscribe](https://martinheinz.dev/subscribe)[Tip Jar](https://ko-fi.com/martinheinz)\n[](https://martinheinz.dev/blog/14/)\n  * [](https://martinheinz.dev/blog/13)\n  * [](https://martinheinz.dev/blog/15)\n\n\nPrevious post\nNext post\nBack to top\n# Ultimate Setup for Your Next Python Project\nMartin\nJan 15, 2020\n[Python](https://martinheinz.dev/tag/python/)[Blueprint](https://martinheinz.dev/tag/blueprint/)[Boilerplate](https://martinheinz.dev/tag/boilerplate/)[Makefile](https://martinheinz.dev/tag/makefile/)\nWhether you are working on some machine learning/AI stuff, building web apps in Flask or just writing some quick Python script, it's always useful to have some template for your project that satisfies all your needs, namely: predefined directory structure, all necessary config files like `pytest.ini` or `requirements.txt`, Testing, linting and static code analysis setup, CI/CD tooling, Dockerization of your app and on top of that automation with _Makefile_. So, here I bring you exactly that in this _\"Ultimate\" all-purpose setup for your Python projects_. \n_TL;DR: Here is my repository with full source code and docs:<https://github.com/MartinHeinz/python-project-blueprint>_\n## Directory Structure\nWhen I was writing this kind of an article for _Golang_ ([here](https://martinheinz.dev/blog/5)), I had hard time figuring out _ideal_ project structure, with _Python_ however, it's pretty simple: \n```\n\u251c\u2500\u2500 blueprint # Our source code - name of the application/module\n\u2502  \u251c\u2500\u2500 app.py\n\u2502  \u251c\u2500\u2500 __init__.py\n\u2502  \u251c\u2500\u2500 __main__.py\n\u2502  \u2514\u2500\u2500 resources\n\u251c\u2500\u2500 tests\n\u2502  \u251c\u2500\u2500 conftest.py\n\u2502  \u251c\u2500\u2500 context.py\n\u2502  \u251c\u2500\u2500 __init__.py\n\u2502  \u2514\u2500\u2500 test_app.py\n\u251c\u2500\u2500 .github # GitHub Actions\n\u2502  \u2514\u2500\u2500 workflows\n\u2502    \u251c\u2500\u2500 build-test.yml\n\u2502    \u2514\u2500\u2500 push.yml\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 configure_project.sh\n\u251c\u2500\u2500 setup.cfg\n\u251c\u2500\u2500 pytest.ini\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 dev.Dockerfile\n\u2514\u2500\u2500 prod.Dockerfile\n```\n\nLet's outline what we have here, starting from the top: \n  * `blueprint` - This is our source code directory, which should be named by your application or package you are working on. Inside we have the usual `__init__.py` file signifying that it's a _Python_ package, next there is `__main__.py` which is used when we want to run our application directly with `python -m blueprint`. Last source file here is the `app.py` which is here really just for demonstration purposes. In real project instead of this `app.py` you would have few top level source files and more directories (internal packages). We will get to contents of these files a little later. Finally, we also have `resources` directory here, which is used for any static content your application might need, e.g. images, keystore, etc.\n  * `tests` - In this directory resides our test suite. I'm not gonna go into too much detail here as we will dedicate whole section to testing, but just briefly: \n    * `test_app.py` is a test file corresponding to `app.py` in source directory\n    * `conftest.py` is probably familiar to you if you ever used _Pytest_ - it's a file used for specifying _Pytest fixtures_ , hooks or loading external plugins.\n    * `context.py` helps with imports of source code files from `blueprint` directory by manipulating class path. We will see how that works in sec.\n  * `.github` - This is last directory we have in this project. It holds configurations for _GitHub Actions_ which we use for CI/CD. We have two files, first of them - `build-test.yml` is responsible for building, testing and linting our source code on every push. Second file - `push.yml` pushes our built application to _GitHub Package Registry_ every time we create tag/release on GitHub. More on this in separate blog post. \n  * `Makefile` - Apart from directories, we also have few top level files in our project, first of them - `Makefile` contains target that will help us automate commonly performed tasks like building, testing, linting or cleaning our project \n  * `configure_project.sh` - This one is a convenience script that sets up project for you. It essentially renames and substitutes dummy values in this project template for real values like name of your project or name of your package. Pretty handy, right? \n\n\nRest of the files we have here are configuration files for all tools we will use in this project. Let's jump over to next section and explore what they do and what's in them. \n## Config Files\nOne thing that can get pretty messy when setting up _Python_ project is the config file soup that you will end-up with when you use bunch of tools like, _pylint_ , _coverage.py_ , _flake8_ and so on. Each of these tools would like to have it's own file, usually something like `.flake8` or `.coveragerc`, which creates lots of unnecessary clutter in root of your project. To avoid this, I merged all these files into single one - `setup.cfg`: \n```\n[flake8]\nexclude =\n  .git,\n  __pycache__,\n  .pytest_cache,\n  venv\nignore =\n  # Put Error/Style codes here e.g. H301\nmax-line-length = 120\nmax-complexity = 10\n[bandit]\ntargets: blueprint\n[coverage:run]\nbranch = True\nomit =\n  */__main__.py\n  */tests/*\n  */venv/*\n[coverage:report]\nexclude_lines =\n  pragma: no cover\n  if __name__ == .__main__.:\n[coverage:html]\ndirectory = reports\n[pylint]\n... # 100 lines of config...\n```\n\nIn case you are not familiar with all of the tools used here, I will give quick description: \n  * _Flake8_ - is a tool for enforcing code style in your projects - in other words - it's linter similar to _pylint_ , which we will use as well. Why use both? It's true that they overlap, but both of them have some rules that the other doesn't, so in my experience it's worth to use them both.\n  * _Bandit_ - is a tool for finding common security issues in _Python_ code. It works by creating AST (abstract syntax tree) from your code and running plugins against its nodes. Developers are generally not security experts and also all of us make mistakes here-and-there, so it's always nice to have tool that can spot at least some of those security mistakes for us.\n  * _Coverage.py_ - is a tool for measuring code coverage of _Python_ programs. It gets triggered when we run our test suite with _Pytest_ and generates coverage report from the test run. These reports can be in form of terminal output, but also XML format which then can be consumed by CI tools.\n\n\nWith that out of the way, let's go over what we have in `setup.cfg`. For _Flake8_ we define exclusion patterns, so that we don't lint code that we don't care about. Below that is an empty `ignore` section in case we need to ignore some rule globally. We also set max line length to 120, as keeping line length to 80 is in my opinion unreasonable with size of today's screens. Final line sets _McCabe_ complexity threshold to 10, if you are not familiar with _cyclomatic complexity_ you can find out more [here](https://en.wikipedia.org/wiki/Cyclomatic_complexity). \nNext up is _Bandit_ , all we configure here is target directory, which is name of our package. We do this so that we can avoid specifying targets on command line. \nAfter that follows _Coverage.py_. First we enable _branch coverage_ , which means that in places where a line in your program could jump to more than one next line, _Coverage.py_ tracks which of those destination lines are actually visited. Next, we omit some files that shouldn't or can't be included in coverage measurement, like tests themselves or virtual environment files. We also exclude specific lines, e.g. lines that are labeled with `pragma: no cover` comment. Last _Coverage.py_ config line tells the tool to store generated reports in `reports` directory. This directory is created automatically, if it doesn't exist already. \nFinal tool we need to configure is _Pylint_ , the configuration though, is _very_ extensive, like more than 100 lines... So, I will leave this one out and point you the source [here](https://github.com/MartinHeinz/python-project-blueprint/blob/master/setup.cfg) as well as commented and explained `pylintrc` in _Pylint_ repository [here](https://github.com/PyCQA/pylint/blob/master/pylintrc). \nWe went through all the tools in `setup.cfg` but there is one more that cannot be added to `setup.cfg` and that is _Pytest_ - even though _Pytest_ docs tell you that you can use `setup.cfg`, it's not exactly true... As per [this issue](https://github.com/pytest-dev/pytest/issues/3062#issuecomment-393523260), the option to use `setup.cfg` is being deprecated and there are some bugs like interpolation errors, that won't be fixed, therefore we will also need `pytest.ini` file for configuration of _Pytest_ : \n```\n[pytest]\naddopts = --color=yes --cov=blueprint --cov-report=xml --cov-report=term -ra\nfilterwarnings =\nlog_cli = 1\nlog_cli_level = INFO\nlog_cli_format = %(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)\nlog_cli_date_format = %Y-%m-%d %H:%M:%S\n```\n\nFirst thing we do here, is set bunch of commandline arguments - we enable colors in terminal output, then we enable coverage reporting for `blueprint` directory, after that we enable both generation of XML and stdout (`term`) coverage reports. Final 2 arguments (`-ra`) tell _Pytest_ to output short summary for non-passing tests. \nOn the next line we have `filterwarnings` option which allows us to disable some annoying warnings in the output, for example deprecation warnings coming out of some library which we have no control over. \nRest of the config sets up logging. First one just turns it on and other 3 configure level, format and datetime format. Easier than explaining the format config is just seeing the output itself, which is shown in next section. \nWith all the configurations in `pytest.ini`, all we will need to do to run our test suite is run `pytest`, not even package argument needed! \nLast actual configuration file we have is `requirement.txt`, which contains list of our dependencies. All you can find in this file is list of _Python_ packages, one per line with _optional_ version of the package. As noted, the package version is optional, but I strongly suggest you lock versions in `requirements.txt` to avoid situations, where you might download newer, _incompatible_ package during build and deployment, and end-up breaking your application. \nThere are 2 remaining files which aren't actually config files - our _Dockerfiles_ , namely, `dev.Dockerfile` and `prod.Dockerfile` used for development and production images respectively. I will leave those out for time being as we will explore those in separate article where we will talk about CI/CD and deployment. You can however check those files out already in _GitHub_ repository here - <https://github.com/MartinHeinz/python-project-blueprint/blob/master/dev.Dockerfile>. \n## Actual Source Code\nWe have done quite a lot without even mentioning source code of our application, but I think it's time to look at those few lines of code that are in the project skeleton: \n```\n# app.py\nclass Blueprint:\n  @staticmethod\n  def run():\n    print(\"Hello World...\")\n```\n\nOnly actual source code in this blueprint is this one class with static method. This is really on needed so that we can run something, get some output and test it. This also works as entrypoint to the whole application. In real project you could use the `run()` method to initialize your application or webserver. \nSo, how do we actually run this piece of code? \n```\n# __main__.py\nfrom .app import Blueprint\nif __name__ == '__main__':\n  Blueprint.run()\n```\n\nThis short snippet in specially named file `__main__.py` is what we need in our project, so that we can run whole package using `python -m blueprint`. Nice thing about this file and it's contents is that it will _only_ be ran with that command, therefore if we want to just import something from source of this package without running the whole thing, then we can do so without triggering `Blueprint.run()`. \nThere's one more special file in our package and that's the `__init__.py` file. Usually you would leave it empty a use it only to tell _Python_ that the directory is package. Here however, we will use it to export classes, variables and functions from our package. \n```\n# __init__.py\nfrom .app import Blueprint\n```\n\nWithout this one line above you wouldn't be able to call `Blueprint.run()` from outside of this package. This way we can avoid people using internal parts of our code that should not be exposed. \nThat's all for the code of our package, but what about the tests? First, let's look at the `context.py`. \n```\n# context.py\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\nimport blueprint # noqa # pylint: disable=unused-import, wrong-import-position\n```\n\nNormally when you use someones package, then you import it like `import blueprint` or `from blueprint import Blueprint`, to imitate this in our tests and therefore make it as close as possible to real usage we use `context.py` file to import the package into our test context. We also insert our project root directory into system path. This is not actually necessary when running tests with `pytest`, but if you for example ran `context.py` directly with `python ./tests/context.py` or possibly with `unittest` without including the `sys.path.insert...`, then you would get `ModuleNotFoundError: No module named 'blueprint'`, so this one line is a little bit of _insurance policy_. \nNow, let's see the example test: \n```\n# test_app.py\nfrom .context import blueprint\n\ndef test_app(capsys, example_fixture):\n  # pylint: disable=W0612,W0613\n  blueprint.Blueprint.run()\n  captured = capsys.readouterr()\n  assert \"Hello World...\" in captured.out\n```\n\nWhat we have here is just single test that checks standard output of `Blueprint.run()` using built-in _Pytest_ fixture called `capsys` (capture system output). So, what happens when we run the test suite? \n```\n~ $ pytest\n=========================================================== test session starts =========================================\ncollected 1 item\ntests/test_app.py::test_app\n-------------------------------------------------------------- live log setup -------------------------------------------\n2020-01-04 12:22:00 [  INFO] Setting Up Example Fixture... (conftest.py:9)\nPASSED                                                       [100%]\n------------------------------------------------------------ live log teardown ------------------------------------------\n2020-01-04 12:22:00 [  INFO] Tearing Down Example Fixture... (conftest.py:11)\n\n----------- coverage: platform linux, python 3.7.5-final-0 -----------\nName          Stmts  Miss Branch BrPart Cover\n---------------------------------------------------------\nblueprint/__init__.py    1   0   0   0  100%\nblueprint/app.py      3   0   0   0  100%\n---------------------------------------------------------\nTOTAL            4   0   0   0  100%\nCoverage XML written to file coverage.xml\n```\n\nI trimmed few lines from the output so that you can better see the relevant parts of it. What's to note here? Well, our test passed! Other than that, we can see coverage report and we can also see that the report got written to `coverage.xml` as configured in `pytest.ini` One more thing that we have here in the output are 2 log messages coming from `conftest.py`. What is that about? \nYou might have noticed that apart from `capsys` fixture, we also used `example_fixture` in parameters of our small test. This fixture resides in `conftest.py` as should all custom fixtures we make: \n```\n# conftest.py\nimport logging\nimport pytest\nLOGGER = logging.getLogger(__name__)\n\n@pytest.fixture(scope='function')\ndef example_fixture():\n  LOGGER.info(\"Setting Up Example Fixture...\")\n  yield\n  LOGGER.info(\"Tearing Down Example Fixture...\")\n```\n\nAs the name implies, this really is just example fixture. All it does is log one message, then it lets the test run and finally it logs one more message. Nice thing about `conftest.py` file is that it gets automatically discovered by _Pytest_ , so you don't even need to import it to your test files. If you want to find out more about it, then you can check out my previous post about _Pytest_ [here](https://martinheinz.dev/blog/7) or docs [here](https://docs.pytest.org/en/latest/fixture.html#conftest-py-sharing-fixture-functions). \n## One Command for Everything\nIt would be quite laborious if we were to run each of our tools separately and had to remember their arguments, even though they are always the same. Also it would be equally annoying if later we decided to put all these tools into CI/CD (next article!), right? So, let's simplify things with `Makefile`: \n```\nMODULE := blueprint\nBLUE='\\033[0;34m'\nNC='\\033[0m' # No Color\nrun:\n\t@python -m $(MODULE)\ntest:\n\t@pytest\nlint:\n\t@echo \"\\n${BLUE}Running Pylint against source and test files...${NC}\\n\"\n\t@pylint --rcfile=setup.cfg **/*.py\n\t@echo \"\\n${BLUE}Running Flake8 against source and test files...${NC}\\n\"\n\t@flake8\n\t@echo \"\\n${BLUE}Running Bandit against source files...${NC}\\n\"\n\t@bandit -r --ini setup.cfg\nclean:\n\trm -rf .pytest_cache .coverage .pytest_cache coverage.xml\n.PHONY: clean test\n```\n\nIn this `Makefile` we have 4 targets. First of them - `run` runs our application using `__main__.py` we created in root of our source folder. Next, `test` just runs `pytest`. It's that simple thanks to all the configs in `pytest.ini`. The longest target here - `lint` - runs all our linting tool. First it runs `pylint` against all `.py` files in the project, including test files. After that it runs `flake8` and finally `bandit`. For these 2 it runs only against sources in `blueprint` directory. If any of those tools find some problem with our code, it will exit with non-zero code, meaning the target will fail, which will be useful in CI/CD. Last target in this file is `clean`, which well... cleans our projects - it removes all the files generated by previously mentioned tools. \n## Conclusion\nIn this article we've built project skeleton, that's ready to be used for any kind of _Python_ project you might be working on or thinking about, so if you want play with or dig a little deeper, then check out the source code which is available in my repository here: <https://github.com/MartinHeinz/python-project-blueprint>. Repo also includes information on how to setup your project using convenience script, plus some more docs. Feel free to leave feedback/suggestions in form of issue or just star it if you like this kind of content. \ud83d\ude42 \nIn the next one we will look into adding CI/CD into the mix with _GitHub Actions_ and _GitHub Package Registry_. We will also Dockerize our project and create both debuggable and optimized production ready Docker images and add some more code quality tooling using _CodeClimate_ and _SonarCloud_. \n### Resources\n  * [Sample Python Module Repository](https://github.com/navdeep-G/samplemod)\n  * [Pytest Docs](https://docs.pytest.org/en/latest/contents.html)\n  * [Python Code Quality Authority](https://github.com/PyCQA)\n\n\n[PreviousPrev.](https://martinheinz.dev/blog/13)[NextNext](https://martinheinz.dev/blog/15)TopTop[SubscribeSub](https://martinheinz.dev/subscribe)[Tip JarTips](https://ko-fi.com/martinheinz)\nI'm currently looking for a new role. If you're hiring, feel free to reach out at `martin7.heinz@gmail.com` or on [_LinkedIn_](https://www.linkedin.com/in/heinz-martin). \n##  Subscribe:\nCopyright \u00a9 2024 Martin Heinz \n  * [Home](https://martinheinz.dev/)\n  * [Contact](https://martinheinz.dev/contact)\n  * [Subscribe](https://martinheinz.dev/subscribe)\n  * [Tip Jar](https://ko-fi.com/martinheinz)\n\n\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis article presents an \"Ultimate Setup\" for Python projects, featuring a structured directory, essential config files, and automation via a Makefile. Key tools include Pytest, Flake8, and Bandit for testing and linting. The complete setup is available on [GitHub](https://github.com/MartinHeinz/python-project-blueprint).\n```"
  },
  {
    "instruction": "[Chip Huyen](https://huyenchip.com/)\n[Blog](https://huyenchip.com/blog/) [Books](https://huyenchip.com/books/) [Events](https://huyenchip.com/events/)\nAI Guide\n  * [AI Roadmap](https://huyenchip.com/mlops/)\n  * [Llama Police](https://huyenchip.com/llama-police)\n  * [ML Interviews](https://huyenchip.com/ml-interviews-book/)\n\n\n[List 100](https://huyenchip.com/list-100/) [VN](https://huyenchip.com/vn/)\n[ ![Chip Huyen](https://huyenchip.com/assets/profile.jpeg) ](https://www.instagram.com/huyenchip19/)\nI\u2019m Chip Huyen, a writer and computer scientist. I grew up chasing grasshoppers in a small rice-farming village in Vietnam.\nMy focus is on ML/AI systems in production. I\u2019m currently exploring creative use cases of AI in education and entertainment.\nPreviously, I worked on ML tooling at NVIDIA (core dev of [**NeMo**](https://github.com/NVIDIA/NeMo)), Snorkel AI, and Netflix. I also founded and sold an AI infra startup.\nRunning a startup taught me that being a founder is incredibly hard, so I try to support founders in any way I can. Some startups I\u2019ve worked with included [Convai](https://convai.com/), [OctoAI](https://octo.ai/), and [Photoroom](https://www.photoroom.com/).\nI graduated from Stanford, where I taught [ML Systems](https://cs329s.stanford.edu). The lectures became the foundation for the book [**Designing Machine Learning Systems**](https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969?&_encoding=UTF8&tag=chiphuyen-20&linkCode=ur2&linkId=0a1dbab0e76f5996e29e1a97d45f14a5&camp=1789&creative=9325), which is an Amazon #1 bestseller in AI and has been translated into 10+ languages (very proud)!\nMy new book **AI Engineering** (2025) is currently the most read book on the [O\u2019Reilly](https://oreillymedia.pxf.io/c/5719111/2146021/15173) platform. It\u2019s also available on [Amazon](https://amzn.to/49j1cGS) and [Kindle](https://amzn.to/3Vq2ryu).\nI\u2019m active on [GitHub](https://github.com/chiphuyen) and aspire to become a [Goodreads](https://www.goodreads.com/chiphuyen) influencer. You can also find me on [Google Scholar](https://scholar.google.com/citations?user=pwg2DGwAAAAJ&hl=en). I frequently speak at and occasionally host [events](https://huyenchip.com/events).\nI enjoy learning about fun technical challenges and collaborating with great teams. [Reach out](https://huyenchip.com/communication) if you want to find a way to work together!\n  * hi@[thiswebsite]\n  * [chiphuyen](https://github.com/chiphuyen)\n  * [chipro](https://twitter.com/chipro)\n  * [chipiscrazy](https://facebook.com/chipiscrazy)\n  * [huyenchip19](https://instagram.com/huyenchip19)\n  * [chiphuyen](https://www.linkedin.com/in/chiphuyen)\n\n\nI help companies deploy machine learning into production. I write about AI applications, tooling, and best practices. \n![](https://huyenchip.com/)\n",
    "answer": "```markdown\n## TL;DR Summary\n\nChip Huyen is a writer and computer scientist focused on ML/AI systems in production, with experience at NVIDIA, Snorkel AI, and Netflix. He authored the bestseller **Designing Machine Learning Systems** and is working on **AI Engineering** (2025). Huyen supports startups and shares insights on AI applications and tooling.\n```"
  },
  {
    "instruction": "[Skip to main content](https://www.datacamp.com/tutorial/speculative-decoding/#main)\n[](https://www.datacamp.com/)[**Write for us**](https://datacamp-1.gitbook.io/community-submissions)\nEN\n[Log In](https://www.datacamp.com/users/sign_in?redirect=%2Ftutorial%2Fspeculative-decoding)[Get Started](https://www.datacamp.com/users/sign_up?redirect=%2Ftutorial%2Fspeculative-decoding)\nTutorials\n[Blogs](https://www.datacamp.com/blog)\n[Tutorials](https://www.datacamp.com/tutorial)\n[docs](https://www.datacamp.com/doc)\n[Podcasts](https://www.datacamp.com/podcast)\n[Cheat Sheets](https://www.datacamp.com/cheat-sheet)\n[code-alongs](https://www.datacamp.com/code-along)\nCategory\nCategory\nTechnologies \nDiscover content by tools and technology\n[Artificial Intelligence](https://www.datacamp.com/tutorial/category/ai)[AWS](https://www.datacamp.com/tutorial/category/aws)[Azure](https://www.datacamp.com/tutorial/category/microsoft-azure)[Business Intelligence](https://www.datacamp.com/tutorial/category/learn-business-intelligence)[ChatGPT](https://www.datacamp.com/tutorial/category/chatgpt)[Databricks](https://www.datacamp.com/tutorial/category/databricks)[dbt](https://www.datacamp.com/tutorial/category/dbt)[Docker](https://www.datacamp.com/tutorial/category/docker)[Excel](https://www.datacamp.com/tutorial/category/excel)[Generative AI](https://www.datacamp.com/tutorial/category/generative-ai)[Git](https://www.datacamp.com/tutorial/category/git)[Google Cloud Platform](https://www.datacamp.com/tutorial/category/google-cloud-platform)[Hugging Face](https://www.datacamp.com/tutorial/category/Hugging-Face)[Java](https://www.datacamp.com/tutorial/category/java)[Julia](https://www.datacamp.com/tutorial/category/julia)[Kubernetes](https://www.datacamp.com/tutorial/category/kubernetes)[Large Language Models](https://www.datacamp.com/tutorial/category/large-language-models)[OpenAI](https://www.datacamp.com/tutorial/category/OpenAI)[PostgreSQL](https://www.datacamp.com/tutorial/category/postgresql)[Power BI](https://www.datacamp.com/tutorial/category/power-bi)[Python](https://www.datacamp.com/tutorial/category/python)[R](https://www.datacamp.com/tutorial/category/r-programming)[Scala](https://www.datacamp.com/tutorial/category/scala)[Snowflake](https://www.datacamp.com/tutorial/category/snowflake)[Spreadsheets](https://www.datacamp.com/tutorial/category/spreadsheets)[SQL](https://www.datacamp.com/tutorial/category/sql)[SQLite](https://www.datacamp.com/tutorial/category/sqlite)[Tableau](https://www.datacamp.com/tutorial/category/tableau)\nCategory\nTopics \nDiscover content by data science topics\n[AI for Business](https://www.datacamp.com/tutorial/category/ai-for-business)[Big Data](https://www.datacamp.com/tutorial/category/big-data)[Career Services](https://www.datacamp.com/tutorial/category/career-services)[Cloud](https://www.datacamp.com/tutorial/category/cloud)[Data Analysis](https://www.datacamp.com/tutorial/category/data-analysis)[Data Engineering](https://www.datacamp.com/tutorial/category/data-engineering)[Data Literacy](https://www.datacamp.com/tutorial/category/data-literacy)[Data Science](https://www.datacamp.com/tutorial/category/data-science)[Data Visualization](https://www.datacamp.com/tutorial/category/data-visualization)[DataLab](https://www.datacamp.com/tutorial/category/datalab)[Deep Learning](https://www.datacamp.com/tutorial/category/deep-learning)[Machine Learning](https://www.datacamp.com/tutorial/category/machine-learning)[MLOps](https://www.datacamp.com/tutorial/category/mlops)[Natural Language Processing](https://www.datacamp.com/tutorial/category/natural-language-processing)\n[Request a Demo](https://www.datacamp.com/business)[](https://www.datacamp.com/search-resources)\ncategory\n  1. [Home](https://www.datacamp.com)\n  2. [Tutorials](https://www.datacamp.com/tutorial)\n  3. [Artificial Intelligence](https://www.datacamp.com/tutorial/category/ai)\n\n\n# Speculative Decoding: A Guide With Implementation Examples\nLearn what speculative decoding is, how it works, when to use it, and how to implement it using Gemma2 models.\nList Contents\nNov 8, 2024 \u00b7 12 min read\nContents\n  * [What Is Speculative Decoding?](https://www.datacamp.com/tutorial/speculative-decoding/#what-is-speculative-decoding?-<span)\n\n\n  * [Traditional Decoding vs. Speculative Decoding](https://www.datacamp.com/tutorial/speculative-decoding/#traditional-decoding-vs.-speculative-decoding-<span)\n\n\n  * [Hands-On Example: Speculative Decoding With Gemma2 Models](https://www.datacamp.com/tutorial/speculative-decoding/#hands-on-example:-speculative-decoding-with-gemma2-models-<span)\n  * [Step 1: Model and Tokenizer setup](https://www.datacamp.com/tutorial/speculative-decoding/#step-1:-model-and-tokenizer-setup-<span)\n  * [Step 2: Autoregressive (normal) inference ](https://www.datacamp.com/tutorial/speculative-decoding/#step-2:-autoregressive-\\(normal\\)-inference\u00a0-<span)\n  * [Step 3: Speculative decoding ](https://www.datacamp.com/tutorial/speculative-decoding/#step-3:-speculative-decoding\u00a0-next,)\n  * [Step 4: Measuring latency](https://www.datacamp.com/tutorial/speculative-decoding/#step-4:-measuring-latency-after)\n\n\n  * [Testing Speculative Decoding on Five Prompts](https://www.datacamp.com/tutorial/speculative-decoding/#testing-speculative-decoding-on-five-prompts-<span)\n\n\n  * [Optimized Speculative Decoding With Quantization ](https://www.datacamp.com/tutorial/speculative-decoding/#optimized-speculative-decoding-with-quantization\u00a0\u00a0-theab)\n  * [4-bit quantization (weight compression)](https://www.datacamp.com/tutorial/speculative-decoding/#4-bit-quantization-\\(weight-compression\\)-<span)\n  * [bfloat16 for computation (efficient use of Tensor Cores)](https://www.datacamp.com/tutorial/speculative-decoding/#bfloat16-for-computation-\\(efficient-use-of-tensor-cores\\)-<span)\n  * [NF4 quantization type (optimized precision)](https://www.datacamp.com/tutorial/speculative-decoding/#nf4-quantization-type-\\(optimized-precision\\)-<span)\n  * [Double quantization disabled ](https://www.datacamp.com/tutorial/speculative-decoding/#double-quantization-disabled\u00a0-<span)\n\n\n  * [Applications of Speculative Decoding](https://www.datacamp.com/tutorial/speculative-decoding/#applications-of-speculative-decoding-<span)\n\n\n  * [Challenges of Speculative Decoding](https://www.datacamp.com/tutorial/speculative-decoding/#challenges-of-speculative-decoding-<span)\n\n\n  * [Conclusion](https://www.datacamp.com/tutorial/speculative-decoding/#conclusion-<span)\n\n\n[LLMs](https://www.datacamp.com/courses/large-language-models-llms-concepts) are so powerful, but they can often be a bit slow, and this is not ideal in scenarios where we need speed. Speculative decoding is a technique designed to speed LLMs by generating responses faster without compromising quality.\nIn essence, it\u2019s a way to \u201cguess ahead\u201d in the process of text generation, making predictions about the words that might come next while still allowing for the accuracy and depth we expect from LLMs.\nIn this blog, I\u2019ll explain what speculative decoding is, how it works, and how to implement it with Gemma 2 models.\n## What Is Speculative Decoding?\nSpeculative decoding accelerates LLMs by incorporating a smaller, faster model that generates preliminary predictions. This smaller model, often called the \u201cdraft\u201d model, generates a batch of tokens that the main, more powerful LLM can either confirm or refine. The draft model acts as a first pass, producing multiple tokens that speed up the generation process.\nInstead of the main LLM generating tokens sequentially, the draft model provides a set of likely candidates, and the main model evaluates them in parallel. This method reduces the computational load on the main LLM by offloading initial predictions, allowing it to focus only on corrections or validations.\n![Speculative Decoding pipeline](https://media.datacamp.com/cms/google/ad_4nxe9skexjvdpmgmrvqelgqhpfalhztym3paxvicwvo3znaeemswzztbayouvgg-laionqivqzlyj1aapawwbmg0vwu5uk9q-hj0sezmf9wyachjhgqbzrtyd2zgg1dmocddpkjn7rbuawnhei-vxbogguixu.png)\nThink of it like a writer with an editor. The main LLM is the writer, capable of producing high-quality text but at a slower pace. A smaller, faster \"draft\" model acts as the editor, quickly generating potential continuations of the text. The main LLM then evaluates these suggestions, incorporating the accurate ones and discarding the rest. This allows the LLM to process multiple tokens concurrently, speeding up text generation.\nLet's break down the process of speculative decoding into simple steps:\n  1. \u200bDraft generation: The smaller model (e.g., Gemma2-2B-it) generates multiple token suggestions based on the input prompt. These tokens are generated speculatively, meaning the model is not certain they are correct but provides them as \u201cdraft\u201d tokens.\n  2. Parallel \u200bverification: The larger model (e.g., Gemma2-9B-it) verifies these tokens in parallel, checking their probability against the model\u2019s learned distribution. If the tokens are deemed acceptable, they are used in the final output. If not, the larger model corrects them.\n  3. \u200bFinal output: Once the tokens are verified (or corrected), they are passed on to the user as the final output. This entire process happens much faster than traditional one-token-at-a-time decoding.\n\n\n## Traditional Decoding vs. Speculative Decoding\nTraditional decoding processes tokens one at a time, leading to high latency, but speculative decoding allows a smaller model to generate tokens in bulk, with the larger model verifying them. This can reduce response time by 30-40%, cutting latency from 25-30 seconds to as little as 15-18 seconds.\n![Traditional decoding vs speculative decoding](https://media.datacamp.com/cms/google/ad_4nxfg3vmjzp91nr8ry9x5_1y20vp_ly0osb_i5c_hrgm6ad-kwolpzc2iyq_c8ce1uwzxf_yerqqejsg-ptsocrsmb9zm2i-uc-5wp81j3-1vcf8jklmmegfgwthw_3elilwy_giitg.png)\nAdditionally, speculative decoding optimizes memory usage by shifting most of the token generation to the smaller model, reducing memory requirements from 26 GB to around 14 GB and making [on-device](https://www.datacamp.com/blog/edge-ai) inference more accessible.\nFinally, it lowers compute demands by 50%, as the larger model only verifies rather than generates tokens, enabling smoother performance on mobile devices with limited power and preventing overheating.\n## Hands-On Example: Speculative Decoding With Gemma2 Models\nTo implement a hands-on example of speculative decoding using the Gemma2 models. We\u2019ll explore how speculative decoding compares with standard inference in terms of both latency and performance.\n### Step 1: Model and Tokenizer setup\nTo get started, import the dependencies and set the seed.\nNext, check if GPU is available on the machine you are operating on. This is mainly required for big models such as Gemma 2-9B-it or LLama2-13B.\nFinally, we load both small and big model along with their tokenizers. Here, we are using the Gemma2-2b-it (instruct) model for the draft model and the Gemma2-9b-it model for verification.\nThere are a few other models that can be used alternatively as well. For example:\n  * Gemma 7B (main) & Gemma 2B (draft)\n  * Mixtral-8x7B (main) & Mistral 7B (draft)\n  * Pythia 12B (main) & Pythia 70M (draft)\n  * Llama 2 13B (main) & TinyLlama 1.1B (draft)\n\n```\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n# Set Seed\nset_seed(42)\n# Check if GPU is available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Load the smaller Gemma2 model (draft generation)\nsmall_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\")\nsmall_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n# Load the larger Gemma2 model (verification)\nbig_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\", device_map=\"auto\")\nbig_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\n### Step 2: Autoregressive (normal) inference \nFirst, we perform inference on just the large model (Gemma2-9b-it) and generate output. Start by [tokenizing](https://www.datacamp.com/tutorial/tiktoken-library-python) the input prompt and moving the tokens to the correct device (GPU if available). The `generate` method produces the model's output, generating up to `max_new_tokens`. The result is then decoded from token IDs back into human-readable text.\n```\n\ndef normal_inference(big_model, big_tokenizer, prompt, max_new_tokens=50):\n  inputs = big_tokenizer(prompt, return_tensors='pt').to(device)\n  outputs = big_model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens)\n  return big_tokenizer.decode(outputs[0], skip_special_tokens=True)\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\n### Step 3: Speculative decoding \nNext, we try the speculative decoding method, where we take the following steps:\n  1. Draft generation: The smaller model generates a draft of the text from the given prompt.\n  2. Verification: The larger model then verifies the draft by computing the log-likelihood for each token in the draft.\n  3. Log-likelihood calculation: We compute an average log-likelihood to determine how likely the large model considers the small model\u2019s draft to be correct.\n\n```\n\ndef speculative_decoding(small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens=50):\n  # Step 1: Use the small model to generate the draft\n  inputs = small_tokenizer(prompt, return_tensors='pt').to(device)\n  small_outputs = small_model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens)\n  draft = small_tokenizer.decode(small_outputs[0], skip_special_tokens=True)\n  # Step 2: Verify the draft with the big model\n  big_inputs = big_tokenizer(draft, return_tensors='pt').to(device)\n  # Step 3: Calculate log-likelihood of the draft tokens under the large model\n  with torch.no_grad():\n    outputs = big_model(big_inputs['input_ids'])\n    log_probs = torch.log_softmax(outputs.logits, dim=-1)\n  draft_token_ids = big_inputs['input_ids']\n  log_likelihood = 0\n  for i in range(draft_token_ids.size(1) - 1):\n    token_id = draft_token_ids[0, i + 1]\n    log_likelihood += log_probs[0, i, token_id].item()\n  avg_log_likelihood = log_likelihood / (draft_token_ids.size(1) - 1)\n  # Return the draft and its log-likelihood score\n  return draft, avg_log_likelihood\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\nNote: Log-likelihood is the logarithm of the probability that a model assigns to a specific sequence of tokens. Here, it reflects how likely or \"confident\" the model is that the sequence of tokens (the generated text) is valid given the previous tokens.\n### Step 4: Measuring latency\nAfter implementing both techniques, we can measure their respective latencies. For speculative decoding, we assess performance by examining the log-likelihood value. A log-likelihood value approaching zero, particularly in the negative range, indicates that the generated text is accurate.\n```\n\ndef measure_latency(small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens=50):\n  # Measure latency for normal inference (big model only)\n  start_time = time.time()\n  normal_output = normal_inference(big_model, big_tokenizer, prompt, max_new_tokens)\n  normal_inference_latency = time.time() - start_time\n  print(f\"Normal Inference Output: {normal_output}\")\n  print(f\"Normal Inference Latency: {normal_inference_latency:.4f} seconds\")\n  print(\"\\n\\n\")\n  # Measure latency for speculative decoding\n  start_time = time.time()\n  speculative_output, log_likelihood = speculative_decoding(\n    small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens\n  )\n  speculative_decoding_latency = time.time() - start_time\n  print(f\"Speculative Decoding Output: {speculative_output}\")\n  print(f\"Speculative Decoding Latency: {speculative_decoding_latency:.4f} seconds\")\n  print(f\"Log Likelihood (Verification Score): {log_likelihood:.4f}\")\n  return normal_inference_latency, speculative_decoding_latency\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\nThis returns:\n  * Log-likelihood (verification score): -0.5242\n  * Normal inference latency: 17.8899 seconds\n  * Speculative decoding latency: 10.5291 seconds (about 70% faster)\n\n\nThe lower latency is thanks to less time taken by the smaller model for text generation and less time taken by the larger model for just verifying the generated text.\n## Testing Speculative Decoding on Five Prompts\nLet's compare speculative decoding with autoregressive inference by using five prompts:\n```\n\n# List of 5 prompts\nprompts = [\n  \"The future of artificial intelligence is \",\n  \"Machine learning is transforming the world by \",\n  \"Natural language processing enables computers to understand \",\n  \"Generative models like GPT-3 can create \",\n  \"AI ethics and fairness are important considerations for \"\n]\n# Inference settings\nmax_new_tokens = 200\n# Initialize accumulators for latency, memory, and tokens per second\ntotal_tokens_per_sec_normal = 0\ntotal_tokens_per_sec_speculative = 0\ntotal_normal_latency = 0\ntotal_speculative_latency = 0\n# Perform inference on each prompt and accumulate the results\nfor prompt in prompts:\n  normal_latency, speculative_latency, _, _, tokens_per_sec_normal, tokens_per_sec_speculative = measure_latency_and_memory(\n    small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens\n  )\n  total_tokens_per_sec_normal += tokens_per_sec_normal\n  total_tokens_per_sec_speculative += tokens_per_sec_speculative\n  total_normal_latency += normal_latency\n  total_speculative_latency += speculative_latency\n# Calculate averages\naverage_tokens_per_sec_normal = total_tokens_per_sec_normal / len(prompts)\naverage_tokens_per_sec_speculative = total_tokens_per_sec_speculative / len(prompts)\naverage_normal_latency = total_normal_latency / len(prompts)\naverage_speculative_latency = total_speculative_latency / len(prompts)\n# Output the averages\nprint(f\"Average Normal Inference Latency: {average_normal_latency:.4f} seconds\")\nprint(f\"Average Speculative Decoding Latency: {average_speculative_latency:.4f} seconds\")\nprint(f\"Average Normal Inference Tokens per second: {average_tokens_per_sec_normal:.2f} tokens/second\")\nprint(f\"Average Speculative Decoding Tokens per second: {average_tokens_per_sec_speculative:.2f} tokens/second\")\nPowered By [](https://www.datacamp.com/datalab)\n\n```\n```\n\nAverage Normal Inference Latency: 25.0876 seconds\nAverage Speculative Decoding Latency: 15.7802 seconds\nAverage Normal Inference Tokens per second: 7.97 tokens/second\nAverage Speculative Decoding Tokens per second: 12.68 tokens/second\nPowered By [](https://www.datacamp.com/datalab)\n\n```\n\nThis shows that speculative decoding is more efficient, generating more tokens per second than normal inference. This improvement is because the smaller model handles the majority of the text generation, while the larger model's role is limited to verification, reducing the overall computational load in terms of latency and memory. \nWith these memory requirements, we can easily deploy speculative decoding techniques on edge devices and gain speed-ups on our on-device applications such as chatbots, language translators, games, and more.\n## Optimized Speculative Decoding With Quantization \nThe above approach is efficient, but there is a trade-off between latency and memory optimization for on-device inference. To address this, let\u2019s apply [quantization](https://www.datacamp.com/tutorial/quantization-for-large-language-models) to both small and big models. You can experiment and try applying quantization to only the large model since the small model already takes the least space.\nQuantification is applied to smaller and larger models using the `BitsAndBytesConfig` configuration from the Hugging Face `transformers` library. Quantization allows us to significantly reduce memory usage and, in many cases, improve inference speed by converting the model\u2019s weights to a more compact form.\nAdd the following code snippet to the above code to witness the effects of quantization.\n```\n\nbnb_config = BitsAndBytesConfig(\n  load_in_4bit=True, # Enables 4-bit quantization\n  bnb_4bit_quant_type=\"nf4\", # Specifies the quantization type (nf4)\n  bnb_4bit_compute_dtype=torch.bfloat16, # Uses bfloat16 for computation\n  bnb_4bit_use_double_quant=False, # Disables double quantization to avoid additional overhead\n)\n# Load the smaller and larger Gemma2 models with quantization applied\nsmall_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\", quantization_config=bnb_config)\nbig_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b-it\", device_map=\"auto\", quantization_config=bnb_config)\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\nHere is a quick comparison of outputs to showcase the effects of speculative decoding with and without quantization:\n![Comparisions with and without quantization](https://media.datacamp.com/cms/google/ad_4nxfup6giqvls9r9_fdmzkn_z7_1cuegd1ewdsrgbcipvva4xj5nlcgnpmtsadf1ehznithnkv4etzd6md0wk8zuipyk4nyp6wvlqgxhlpuwumx68rilmg8dq14m_punl35svrzz9t8hyuhoqx9ek2ptxzfm.png)\n### 4-bit quantization (weight compression)\nThe configuration specifies `load_in_4bit=True`, which means the model\u2019s weights are quantized from their original 32-bit or 16-bit floating-point format to 4-bit integers. This reduces the model's memory footprint. Quantization compresses the model\u2019s weights, allowing us to store and operate on them more efficiently. These are the concrete memory savings:\n  * By reducing the precision from 32-bit or 16-bit floats to 4-bit integers, each weight now takes up 1/4th or 1/8th of the original space, significantly reducing memory usage.\n  * This is reflected in the memory usage as:\n    * Normal inference memory usage: 26,458 MB\n    * Speculative decoding memory usage: 8,993 MB.\n\n\n### bfloat16 for computation (efficient use of Tensor Cores)\nThe configuration `bnb_4bit_compute_dtype=torch.bfloat16` specifies that the computation is performed in bfloat16 (BF16), a more efficient floating-point format. BF16 has a wider dynamic range than FP16 but takes up half the memory compared to FP32, making it a good balance between precision and performance.\nUsing BF16, especially on NVIDIA GPUs (like A100), uses Tensor Cores, which are optimized for BF16 operations. This results in faster matrix multiplications and other computations during inference.\nFor speculative decoding, we see improved latency:\n  * Normal inference latency: 27.65 seconds\n  * Speculative decoding latency: 15.56 seconds\n\n\nThe smaller memory footprint means faster memory access and more efficient use of GPU resources, leading to faster generation.\n### NF4 quantization type (optimized precision)\nThe `bnb_4bit_quant_type=\"nf4\"` option specifies Norm-Four Quantization (NF4), which is optimized for neural networks. NF4 quantization helps retain the precision of important parts of the model, even though the weights are represented in 4 bits. This minimizes the degradation in model performance compared to simple 4-bit quantization.\nNF4 helps achieve a balance between the compactness of 4-bit quantization and the accuracy of the model's predictions, ensuring that performance remains close to the original while drastically reducing memory usage.\n### Double quantization disabled \nDouble quantization (`bnb_4bit_use_double_quant=False`) introduces an additional layer of quantization on top of the 4-bit weights, which can further reduce memory usage but also add computation overhead. In this case, double quantization is disabled to avoid slowing down the inference.\n## Applications of Speculative Decoding\nThe potential applications of speculative decoding are vast and exciting. Here are a few examples:\n  * Chatbots and virtual assistants: To make those conversations with AI feel more natural and fluid with faster response times.\n  * Real-time translation: Speculative decoding reduces latency in real-time translation.\n  * Content generation: Speculative decoding speeds up content creation.\n  * Gaming and interactive applications: To improve the responsiveness of AI-powered characters or game elements for a more immersive experience, speculative decoding can help us attain almost real-time responses.\n\n\n![Applications of Speculative Decoding](https://media.datacamp.com/cms/google/ad_4nxdxjtdtb_3gotq7mu_ye_cb1wuva_dopcu3qjjrnkcwlbtchnxk1qssdq8oox6lgjuf0om_n86ffucxli8soawyypivtfxwltwaafyuuwdjc3uk2jsxysinerxwjsap-wb4yfjnl0v-a30-jog49fwy5xs.png)\n## Challenges of Speculative Decoding\nWhile speculative decoding holds immense promise, it's not without its challenges:\n  * Memory overhead: Maintaining multiple model states (for both the draft and main LLM) can increase memory usage, especially when larger models are used for verification.\n  * Tuning the draft model: Choosing the right draft model and tuning its parameters is crucial to strike the right balance between speed and accuracy, as an overly simplistic model may lead to frequent verification failures.\n  * Implementation complexity: Implementing speculative decoding is more complex than traditional methods, requiring careful synchronization between the small draft model and the larger verification model, as well as efficient error handling.\n  * Compatibility with decoding strategies: Speculative decoding currently supports only greedy search and sampling, limiting its use to more sophisticated decoding strategies such as beam search or diverse sampling.\n  * Verification overhead: If the smaller draft model generates tokens that frequently fail verification, the efficiency gains can be diminished, as the larger model will need to regenerate parts of the output, potentially negating the speed advantages.\n  * Limited support for batch processing: Speculative decoding typically doesn't support batched inputs, which can reduce its effectiveness in high-throughput systems requiring parallel processing of multiple requests.\n\n\n## Conclusion\nSpeculative decoding is a powerful technique that has the potential to revolutionize the way we interact with large language models. It can significantly speed up LLM inference without compromising the quality of the generated text. While there are challenges to overcome, the benefits of speculative decoding are undeniable, and we can expect to see its adoption grow in the coming years, enabling a new generation of faster, more responsive, and more efficient AI applications.\n![Aashi Dutt's photo](https://media.datacamp.com/legacy/v1725454401/aashi_pic_746310ff15.jpg?w=128)\nAuthor\n[Aashi Dutt](https://www.datacamp.com/portfolio/aashidutt3)\n[LinkedIn](https://www.linkedin.com/in/aashi-dutt)[Twitter](https://twitter.com/aashidutt?lang=en)\nI am a Google Developers Expert in ML(Gen AI), a Kaggle 3x Expert, and a Women Techmakers Ambassador with 3+ years of experience in tech. I co-founded a health-tech startup in 2020 and am pursuing a master's in computer science at Georgia Tech, specializing in machine learning.\nTopics\n[Artificial Intelligence](https://www.datacamp.com/tutorial/category/ai)[Large Language Models](https://www.datacamp.com/tutorial/category/large-language-models)\n![Aashi Dutt's photo](https://media.datacamp.com/legacy/v1725454401/aashi_pic_746310ff15.jpg?w=128)\n[Aashi Dutt](https://www.datacamp.com/portfolio/aashidutt3) Google Developers Expert in ML | Speaker | Kaggle 3x Expert \nTopics\n[Artificial Intelligence](https://www.datacamp.com/tutorial/category/ai)[Large Language Models](https://www.datacamp.com/tutorial/category/large-language-models)\n### [Gemini 2.0 Flash Thinking Experimental: A Guide With Examples](https://www.datacamp.com/blog/gemini-2-0-flash-experimental)\n### [Small Language Models: A Guide With Examples](https://www.datacamp.com/blog/small-language-models)\n### [AI Hallucination: A Guide With Examples](https://www.datacamp.com/blog/ai-hallucination)\n### [Speculative RAG Implementation With Transformers](https://www.datacamp.com/tutorial/speculative-rag)\n### [Fine Tuning Google Gemma: Enhancing LLMs with Customized Instructions](https://www.datacamp.com/tutorial/fine-tuning-google-gemma)\n### [Introduction to Large Language Models with GPT & LangChain](https://www.datacamp.com/code-along/introduction-to-large-language-models-gpt-langchain)\nLearn AI with these courses!\nTrack\n### [Developing AI Applications](https://www.datacamp.com/tracks/developing-ai-applications)\n23hrs hr\nLearn to create AI-powered applications with the latest AI developer tools, including the OpenAI API, Hugging Face, and LangChain.\n[See DetailsRight Arrow](https://www.datacamp.com/tracks/developing-ai-applications)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Ftracks%2Fdeveloping-ai-applications%2Fcontinue)\nCourse\n### [Developing AI Systems with the OpenAI API](https://www.datacamp.com/courses/developing-ai-systems-with-the-openai-api)\n3 hr\n5.2K\nLeverage the OpenAI API to get your AI applications ready for production.\n[See DetailsRight Arrow](https://www.datacamp.com/courses/developing-ai-systems-with-the-openai-api)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Fcourses%2Fdeveloping-ai-systems-with-the-openai-api%2Fcontinue)\nCourse\n### [AI Security and Risk Management](https://www.datacamp.com/courses/ai-security-and-risk-management)\n2 hr\n2.5K\nLearn the fundamentals of AI security to protect systems from threats, align security with business goals, and mitigate key risks. \n[See DetailsRight Arrow](https://www.datacamp.com/courses/ai-security-and-risk-management)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Fcourses%2Fai-security-and-risk-management%2Fcontinue)\n[See MoreRight Arrow](https://www.datacamp.com/category/artificial-intelligence)\nRelated\n[blogGemini 2.0 Flash Thinking Experimental: A Guide With Examples](https://www.datacamp.com/blog/gemini-2-0-flash-experimental)\nLearn about Gemini 2.0 Flash Thinking Experimental, including its features, benchmarks, limitations, and how it compares to other reasoning models.\n![Alex Olteanu's photo](https://media.datacamp.com/legacy/v1718983935/DSC_00941_5_square_42e8868173.jpg?w=48)\nAlex Olteanu \n8 min\n[blogSmall Language Models: A Guide With Examples](https://www.datacamp.com/blog/small-language-models)\nLearn about small language models (SLMs), their benefits and applications, and how they compare to large language models (LLMs).\n![Dr Ana Rojo-Echebur\u00faa's photo](https://media.datacamp.com/cms/ana_rojo.png?w=48)\nDr Ana Rojo-Echebur\u00faa \n8 min\n[blogAI Hallucination: A Guide With Examples](https://www.datacamp.com/blog/ai-hallucination)\nLearn about AI hallucinations, their types, why they occur, their potential negative impacts, and how to mitigate them.\n[![Tom Farnschl\u00e4der's photo](https://media.datacamp.com/legacy/v1719243659/1698334986991_adfcb9390d.jpg?w=48)](https://www.datacamp.com/portfolio/tomfarnschlaeder)\nTom Farnschl\u00e4der \n8 min\n[TutorialSpeculative RAG Implementation With Transformers](https://www.datacamp.com/tutorial/speculative-rag)\nLearn Speculative RAG, a technique that improves RAG through a two-step drafting and verification process, and apply your skills with a hands-on implementation using Hugging Face Transformers.\n![Bhavishya Pandit's photo](https://media.datacamp.com/legacy/v1725454467/Unknown_f0f804ba73.jpg?w=48)\nBhavishya Pandit \n8 min\n[TutorialFine Tuning Google Gemma: Enhancing LLMs with Customized Instructions](https://www.datacamp.com/tutorial/fine-tuning-google-gemma)\nLearn how to run inference on GPUs/TPUs and fine-tune the latest Gemma 7b-it model on a role-play dataset.\n[![Abid Ali Awan's photo](https://media.datacamp.com/legacy/v1658155691/Abid_Ali_Awan_415cc44670.jpg?w=48)](https://www.datacamp.com/portfolio/kingabzpro)\nAbid Ali Awan \n12 min\n[code-alongIntroduction to Large Language Models with GPT & LangChain](https://www.datacamp.com/code-along/introduction-to-large-language-models-gpt-langchain)\nLearn the fundamentals of working with large language models and build a bot that analyzes data.\n[![Richie Cotton's photo](https://media.datacamp.com/legacy/v1658155296/richie_new_barnet_be7b436d1f.jpg?w=48)](https://www.datacamp.com/portfolio/richie)\nRichie Cotton \n[See More](https://www.datacamp.com/tutorial/category/ai)[See More](https://www.datacamp.com/tutorial/category/ai)\n## Grow your data skills with DataCamp for Mobile\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\n[Download on the App Store](https://datacamp.onelink.me/xztQ/45dozwue?deep_link_sub1=%7B%22src_url%22%3A%22https%3A%2F%2Fwww.datacamp.com%2Ftutorial%2Fspeculative-decoding%22%7D)[Get it on Google Play](https://datacamp.onelink.me/xztQ/go2f19ij?deep_link_sub1=%7B%22src_url%22%3A%22https%3A%2F%2Fwww.datacamp.com%2Ftutorial%2Fspeculative-decoding%22%7D)\n**Learn**\n[Learn Python](https://www.datacamp.com/blog/how-to-learn-python-expert-guide)[Learn AI](https://www.datacamp.com/blog/how-to-learn-ai)[Learn Power BI](https://www.datacamp.com/learn/power-bi)[Learn Data Engineering](https://www.datacamp.com/category/data-engineering)[Assessments](https://www.datacamp.com/signal)[Career Tracks](https://www.datacamp.com/tracks/career)[Skill Tracks](https://www.datacamp.com/tracks/skill)[Courses](https://www.datacamp.com/courses-all)[Data Science Roadmap](https://www.datacamp.com/blog/data-science-roadmap)\n**Data Courses**\n[Python Courses](https://www.datacamp.com/category/python)[R Courses](https://www.datacamp.com/category/r)[SQL Courses](https://www.datacamp.com/category/sql)[Power BI Courses](https://www.datacamp.com/category/power-bi)[Tableau Courses](https://www.datacamp.com/category/tableau)[Alteryx Courses](https://www.datacamp.com/category/alteryx)[Azure Courses](https://www.datacamp.com/category/azure)[AWS Courses](https://www.datacamp.com/category/aws)[Google Sheets Courses](https://www.datacamp.com/category/google-sheets)[Excel Courses](https://www.datacamp.com/category/excel)[AI Courses](https://www.datacamp.com/category/artificial-intelligence)[Data Analysis Courses](https://www.datacamp.com/category/data-analysis)[Data Visualization Courses](https://www.datacamp.com/category/data-visualization)[Machine Learning Courses](https://www.datacamp.com/category/machine-learning)[Data Engineering Courses](https://www.datacamp.com/category/data-engineering)[Probability & Statistics Courses](https://www.datacamp.com/category/probability-and-statistics)\n**DataLab**\n[Get Started](https://www.datacamp.com/datalab)[Pricing](https://www.datacamp.com/datalab/pricing)[Security](https://www.datacamp.com/datalab/security)[Documentation](https://datalab-docs.datacamp.com)\n**Certification**\n[Certifications](https://www.datacamp.com/certification)[Data Scientist](https://www.datacamp.com/certification/data-scientist)[Data Analyst](https://www.datacamp.com/certification/data-analyst)[Data Engineer](https://www.datacamp.com/certification/data-engineer)[SQL Associate](https://www.datacamp.com/certification/sql-associate)[Power BI Data Analyst](https://www.datacamp.com/certification/data-analyst-in-power-bi)[Tableau Certified Data Analyst](https://www.datacamp.com/certification/data-analyst-in-tableau)[Azure Fundamentals](https://www.datacamp.com/certification/azure-fundamentals)[AI Fundamentals](https://www.datacamp.com/certification/ai-fundamentals)\n**Resources**\n[Resource Center](https://www.datacamp.com/resources)[Upcoming Events](https://www.datacamp.com/webinars)[Blog](https://www.datacamp.com/blog)[Code-Alongs](https://www.datacamp.com/code-along)[Tutorials](https://www.datacamp.com/tutorial)[Docs](https://www.datacamp.com/doc)[Open Source](https://www.datacamp.com/open-source)[RDocumentation](https://www.rdocumentation.org)[Book a Demo with DataCamp for Business](https://www.datacamp.com/business/demo)[Data Portfolio](https://www.datacamp.com/data-portfolio)\n**Plans**\n[Pricing](https://www.datacamp.com/pricing)[For Students](https://www.datacamp.com/pricing/student)[For Business](https://www.datacamp.com/business)[For Universities](https://www.datacamp.com/universities)[Discounts, Promos & Sales](https://www.datacamp.com/promo)[DataCamp Donates](https://www.datacamp.com/donates)\n**For Business**\n[Business Pricing](https://www.datacamp.com/business/compare-plans)[Teams Plan](https://www.datacamp.com/business/learn-teams)[Data & AI Unlimited Plan](https://www.datacamp.com/business/data-unlimited)[Customer Stories](https://www.datacamp.com/business/customer-stories)[Partner Program](https://www.datacamp.com/business/partner-program)\n**About**\n[About Us](https://www.datacamp.com/about)[Learner Stories](https://www.datacamp.com/stories)[Careers](https://www.datacamp.com/careers)[Become an Instructor](https://www.datacamp.com/learn/create)[Press](https://www.datacamp.com/press)[Leadership](https://www.datacamp.com/about/leadership)[Contact Us](https://support.datacamp.com/hc/en-us/articles/360021185634)[DataCamp Espa\u00f1ol](https://www.datacamp.com/es)[DataCamp Portugu\u00eas](https://www.datacamp.com/pt)[DataCamp Deutsch](https://www.datacamp.com/de)[DataCamp Fran\u00e7ais](https://www.datacamp.com/fr)\n**Support**\n[Help Center](https://support.datacamp.com/hc/en-us)[Become an Affiliate](https://www.datacamp.com/affiliates)\n[Facebook](https://www.facebook.com/datacampinc/)[Twitter](https://twitter.com/datacamp)[LinkedIn](https://www.linkedin.com/school/datacampinc/)[YouTube](https://www.youtube.com/channel/UC79Gv3mYp6zKiSwYemEik9A)[Instagram](https://www.instagram.com/datacamp/)\n[Privacy Policy](https://www.datacamp.com/privacy-policy)[Cookie Notice](https://www.datacamp.com/cookie-notice)[Do Not Sell My Personal Information](https://www.datacamp.com/do-not-sell-my-personal-information)[Accessibility](https://www.datacamp.com/accessibility)[Security](https://www.datacamp.com/security)[Terms of Use](https://www.datacamp.com/terms-of-use)\n\u00a9 2025 DataCamp, Inc. All Rights Reserved.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nSpeculative decoding accelerates LLMs by using a smaller draft model for preliminary token generation, reducing latency by 30-40%. It enhances efficiency in applications like chatbots and translation but faces challenges in memory overhead and implementation complexity.\n```"
  },
  {
    "instruction": "[Skip to main content](https://www.datacamp.com/blog/edge-ai/#main)\n[](https://www.datacamp.com/)[**Write for us**](https://datacamp-1.gitbook.io/community-submissions)\nEN\n[Log In](https://www.datacamp.com/users/sign_in?redirect=%2Fblog%2Fedge-ai)[Get Started](https://www.datacamp.com/users/sign_up?redirect=%2Fblog%2Fedge-ai)\nblogs\n[Blogs](https://www.datacamp.com/blog)\n[Tutorials](https://www.datacamp.com/tutorial)\n[docs](https://www.datacamp.com/doc)\n[Podcasts](https://www.datacamp.com/podcast)\n[Cheat Sheets](https://www.datacamp.com/cheat-sheet)\n[code-alongs](https://www.datacamp.com/code-along)\nCategory\nCategory\nAbout DataCamp \nLatest news about our products and team\n[Certification](https://www.datacamp.com/blog/category/certification)[DataCamp Classrooms](https://www.datacamp.com/blog/category/datacamp-classrooms)[DataCamp Donates](https://www.datacamp.com/blog/category/datacamp-donates)[For Business](https://www.datacamp.com/blog/category/for-business)[Learner Stories](https://www.datacamp.com/blog/category/learner-stories)[Life at DataCamp](https://www.datacamp.com/blog/category/life-at-datacamp)[Product News](https://www.datacamp.com/blog/category/product-news)\nCategory\nTechnologies \nDiscover content by tools and technology\n[Airflow](https://www.datacamp.com/blog/category/apache-airflow)[Alteryx](https://www.datacamp.com/blog/category/alteryx)[Artificial Intelligence](https://www.datacamp.com/blog/category/ai)[AWS](https://www.datacamp.com/blog/category/aws)[Azure](https://www.datacamp.com/blog/category/microsoft-azure)[Business Intelligence](https://www.datacamp.com/blog/category/learn-business-intelligence)[ChatGPT](https://www.datacamp.com/blog/category/chatgpt)[Databricks](https://www.datacamp.com/blog/category/databricks)[dbt](https://www.datacamp.com/blog/category/dbt)[Docker](https://www.datacamp.com/blog/category/docker)[Excel](https://www.datacamp.com/blog/category/excel)[Flink](https://www.datacamp.com/blog/category/apache-flink)[Generative AI](https://www.datacamp.com/blog/category/generative-ai)[Git](https://www.datacamp.com/blog/category/git)[Google Cloud Platform](https://www.datacamp.com/blog/category/google-cloud-platform)[Hadoop](https://www.datacamp.com/blog/category/apache-hadoop)[Java](https://www.datacamp.com/blog/category/java)[Julia](https://www.datacamp.com/blog/category/julia)[Kafka](https://www.datacamp.com/blog/category/apache-kafka)[Kubernetes](https://www.datacamp.com/blog/category/kubernetes)[Large Language Models](https://www.datacamp.com/blog/category/large-language-models)[MongoDB](https://www.datacamp.com/blog/category/mongodb)[MySQL](https://www.datacamp.com/blog/category/mysql)[NoSQL](https://www.datacamp.com/blog/category/nosql)[OpenAI](https://www.datacamp.com/blog/category/OpenAI)[Power BI](https://www.datacamp.com/blog/category/power-bi)[PySpark](https://www.datacamp.com/blog/category/pyspark)[Python](https://www.datacamp.com/blog/category/python)[R](https://www.datacamp.com/blog/category/r-programming)[Scala](https://www.datacamp.com/blog/category/scala)[Snowflake](https://www.datacamp.com/blog/category/snowflake)[Spreadsheets](https://www.datacamp.com/blog/category/spreadsheets)[SQL](https://www.datacamp.com/blog/category/sql)[Tableau](https://www.datacamp.com/blog/category/tableau)\nCategory\nTopics \nDiscover content by data science topics\n[AI for Business](https://www.datacamp.com/blog/category/ai-for-business)[Big Data](https://www.datacamp.com/blog/category/big-data)[Career Services](https://www.datacamp.com/blog/category/career-services)[Cloud](https://www.datacamp.com/blog/category/cloud)[Data Analysis](https://www.datacamp.com/blog/category/data-analysis)[Data Engineering](https://www.datacamp.com/blog/category/data-engineering)[Data Governance](https://www.datacamp.com/blog/category/data-governance)[Data Literacy](https://www.datacamp.com/blog/category/data-literacy)[Data Science](https://www.datacamp.com/blog/category/data-science)[Data Storytelling](https://www.datacamp.com/blog/category/data-storytelling)[Data Visualization](https://www.datacamp.com/blog/category/data-visualization)[DataCamp Product](https://www.datacamp.com/blog/category/datacamp-product)[DataLab](https://www.datacamp.com/blog/category/datalab)[Deep Learning](https://www.datacamp.com/blog/category/deep-learning)[Machine Learning](https://www.datacamp.com/blog/category/machine-learning)[MLOps](https://www.datacamp.com/blog/category/mlops)\n[Request a Demo](https://www.datacamp.com/business)[](https://www.datacamp.com/search-resources)\ncategory\n  1. [Home](https://www.datacamp.com)\n  2. [Blog](https://www.datacamp.com/blog)\n  3. [Artificial Intelligence](https://www.datacamp.com/blog/category/ai)\n\n\n# What Is Edge AI? How It Works, Applications, Challenges\nEdge AI runs AI algorithms directly on edge devices, like smartphones or IoT sensors, allowing real-time processing and decision-making without cloud reliance.\nList Contents\nAug 6, 2024 \u00b7 9 min read\nContents\n  * [What Is Edge AI?](https://www.datacamp.com/blog/edge-ai/#what-is-edge-ai?-<span)\n\n\n  * [How Edge AI Works](https://www.datacamp.com/blog/edge-ai/#how-edge-ai-works-<span)\n  * [Key components](https://www.datacamp.com/blog/edge-ai/#key-components-<span)\n  * [The Process](https://www.datacamp.com/blog/edge-ai/#the-process-<span)\n\n\n  * [Real-World Applications of Edge AI](https://www.datacamp.com/blog/edge-ai/#real-world-applications-of-edge-ai-<span)\n  * [Manufacturing](https://www.datacamp.com/blog/edge-ai/#manufacturing-<span)\n  * [Healthcare](https://www.datacamp.com/blog/edge-ai/#healthcare-<span)\n  * [Retail](https://www.datacamp.com/blog/edge-ai/#retail-<span)\n  * [Smart cities](https://www.datacamp.com/blog/edge-ai/#smart-cities-<span)\n\n\n  * [Edge AI: Hardware and Software Solutions](https://www.datacamp.com/blog/edge-ai/#edge-ai:-hardware-and-software-solutions-<span)\n  * [Hardware Platforms](https://www.datacamp.com/blog/edge-ai/#hardware-platforms-<span)\n  * [Software Frameworks](https://www.datacamp.com/blog/edge-ai/#software-frameworks-<span)\n\n\n  * [Challenges and Considerations](https://www.datacamp.com/blog/edge-ai/#challenges-and-considerations-<span)\n\n\n  * [Conclusion](https://www.datacamp.com/blog/edge-ai/#conclusion-<span)\n\n\nArtificial Intelligence (AI) is so powerful, but it often relies on [remote cloud servers](https://www.datacamp.com/courses/understanding-cloud-computing) for its heavy lifting. This approach can be problematic for applications needing quick responses or those on devices with limited resources.\nImagine a self-driving car waiting for a cloud server to decide which lane to switch into\u2014not ideal!\n**Edge AI solves this by bringing the AI capabilities directly to the device itself.**\n## Become an ML Scientist \nUpskill in Python to become a machine learning scientist.\n[Start Learning for Free](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)\n## What Is Edge AI?\nEdge AI is the practice of deploying AI models and algorithms directly on edge devices, which are devices located at the network's periphery, close to where data is generated and actions need to be taken.\nThese devices encompass a wide range, from powerful edge servers to resource-constrained IoT sensors, and include familiar examples like smartphones, smart home appliances, autonomous vehicles, and even industrial robots.\nRecent advancements in AI, such as the development of smaller and more efficient language models like [GPT-4o Mini](https://www.datacamp.com/blog/gpt-4o-mini), [Llama 3.1 8B](https://www.datacamp.com/blog/llama-3-1-405b-meta-ai), and [Gemma 2 2B](https://www.datacamp.com/tutorial/fine-tuning-gemma-2), are further accelerating the adoption of edge AI.\nThis shift to edge AI offers several advantages:\n  * Speed: Decisions and actions happen in real-time, right where the data is generated. This is necessary for applications like self-driving cars or medical devices that can't afford delays caused by sending data to the cloud for processing.\n  * Privacy: Sensitive data can be analyzed locally without needing to be sent to the cloud, enhancing security and privacy.\n  * Reliability: Edge AI reduces dependency on a constant internet connection, making it more reliable in areas with limited or unstable connectivity.\n  * Efficiency: Processing data locally reduces the bandwidth required to send everything to the cloud, saving both energy and costs.\n\n\n## How Edge AI Works\nTo understand how edge AI works, we need to understand its main components and the processes involved.\n### Key components\nEdge AI comprises three main components: edge devices, AI models, and communication.\n![Components of Edge AI](https://media.datacamp.com/legacy/v1723043761/image_a3d238bf5b.png)\n#### Edge devices\nEdge devices encompass a wide range of hardware, from powerful edge servers capable of handling substantial computational loads to highly resource-constrained IoT sensors designed for specific tasks.\nThese devices include smartphones, drones, autonomous vehicles, industrial robots, and smart home devices, all of which can run AI models locally to process data in real time. Some hardware manufacturers, like NVIDIA and Intel, even provide hardware support for [deploying ML models](https://www.datacamp.com/courses/mlops-deployment-and-life-cycling) on the edge.\n#### AI models\nEdge AI employs various types of AI models, including [machine learning](https://www.datacamp.com/courses/mlops-concepts), [deep learning](https://www.datacamp.com/tracks/deep-learning-in-python), and computer vision algorithms, optimized for efficient execution on edge devices.\nThese models are tailored to operate within the constraints of edge environments, ensuring that they can perform effectively despite limited processing power, memory, and storage.\nFor instance, models like GPT-4o Mini and Llama 3.1 8B are specifically designed to be lightweight and efficient, making them suitable for edge deployments.\n#### Communication\nEfficient data exchange between edge devices and the cloud is facilitated by communication protocols such as MQTT and REST APIs. These protocols enable seamless connectivity and data transfer, allowing synchronized operations between edge devices and central systems when necessary. These protocols allow the transfer of information in compressed form in an almost lossless fashion to keep the crucial information intact.\nMQTT (Message Queuing Telemetry Transport)is a lightweight messaging protocol designed for constrained devices and low-bandwidth, high-latency, or unreliable networks. It uses a publish-subscribe model, allowing devices to send (publish) and receive (subscribe) messages without needing a direct connection. This makes MQTT ideal for IoT applications where devices need to communicate efficiently and reliably.\nREST API (Representational State Transfer Application Programming Interface)is an architectural style for designing networked applications. It uses HTTP requests to access and use data. REST APIs are stateless, meaning each call from a client to a server must contain all the information the server needs to fulfill that request. This makes REST APIs scalable and suitable for various web services and applications, including those involving edge devices.\n### The Process\nThe workflow that powers edge AI involves three steps: data collection, data processing, and action.\n![Edge AI processes](https://media.datacamp.com/legacy/v1723043760/image_51de06a284.png)\n#### Data collection\nEdge devices continuously collect data from sensors, cameras, or other sources, providing a steady stream of information. This data can range from environmental metrics and health parameters to video feeds and audio recordings, forming the basis for real-time analysis.\nA great example of data collection is how your smartwatch collects the number of steps you took today.\n#### Data processing\nAI models deployed on edge devices process the collected data locally. This step involves analyzing the data to extract meaningful insights, detect patterns, and make predictions using AI models without relying on cloud resources.\nLocal processing ensures that decisions can be made quickly, such as a self-driving car determining which lane to choose in real-time.\n#### Real-Time Action\nBased on the AI model's output, edge devices can take immediate action. These actions might include triggering alarms, adjusting the path, or sending data to the cloud for further analysis. The ability to act in real-time is essential for scenarios requiring instant responses, such as security systems or medical devices.\n## Real-World Applications of Edge AI\nEdge AI is transforming various industries by enabling real-time intelligence and decision-making. Let\u2019s explore a few notable applications.\n### Manufacturing\nIn manufacturing, equipment downtime can be costly. Edge AI addresses this by monitoring equipment health and predicting potential failures before they occur. By analyzing data from sensors in real time, AI models can detect anomalies and alert maintenance teams to take preventive actions. This not only reduces downtime but also extends the lifespan of machinery. \nEnsuring product quality is paramount in manufacturing. AI-powered cameras equipped with edge AI can inspect products for defects in real time. These systems analyze visual data to identify flaws such as scratches, dents, or incorrect assembly. By automating the inspection process, manufacturers can achieve higher accuracy, consistency, and efficiency, ultimately enhancing product quality and customer satisfaction.\n### Healthcare\nThe healthcare industry is benefiting significantly from Edge AI. Portable devices equipped with edge AI can analyze medical images such as X-rays, MRIs, and CT scans, providing faster diagnoses. This capability is particularly valuable in remote or underserved areas where access to specialized radiologists may be limited. By processing images locally, edge AI reduces the time needed for diagnosis, enabling timely treatment and improving patient outcomes.\nWearable devices with edge AI are revolutionizing patient care by enabling continuous monitoring of health parameters. These devices collect data such as heart rate, blood pressure, and glucose levels, analyzing it in real-time to detect anomalies. If a critical condition is identified, the device can alert healthcare providers immediately. This proactive approach to patient monitoring helps manage chronic diseases, detect health issues early, and reduce hospital visits.\n### Retail\nEfficient inventory management is crucial for retail businesses. AI-powered cameras and sensors can track inventory levels in real time, ensuring shelves are always stocked. By analyzing data from these devices, edge AI can optimize stock replenishment, reduce waste, and prevent stockouts. This leads to improved customer satisfaction and lower inventory costs.\nUnderstanding customer behavior is key to delivering personalized shopping experiences. Edge AI analyzes data from in-store cameras and sensors to gain insights into customer preferences and behavior. Based on this analysis, it can deliver tailored recommendations and promotions to individual shoppers. Personalization enhances the shopping experience, increases customer loyalty, and boosts sales.\n### Smart cities\nManaging urban traffic is a complex task that requires real-time data analysis. Edge AI can optimize traffic flow by analyzing data from traffic cameras, sensors, and GPS devices. By detecting congestion patterns and predicting traffic conditions, it can adjust traffic signals, reroute vehicles, and provide real-time traffic updates to drivers. This improves traffic efficiency, reduces travel time, and enhances road safety.\nEnsuring public safety is a top priority for smart cities. AI-powered surveillance systems equipped with edge AI can monitor public spaces, detect anomalies, and identify potential threats. These systems analyze video feeds in real time, recognizing suspicious activities such as unauthorized access or unattended bags. By alerting authorities promptly, edge AI enhances security and enables rapid response to incidents.\n## Edge AI: Hardware and Software Solutions\nImplementing Edge AI requires the right combination of hardware and software solutions. Here are some popular options:\n![NVIDIA Jetson, Google Coral, Intel Movidius](https://media.datacamp.com/legacy/v1723043760/image_9e910d479d.png)\n### Hardware Platforms\nTo successfully implement Edge AI, you'll need suitable hardware platforms capable of running AI models efficiently at the edge. Let\u2019s explore a few options:\n  * NVIDIA Jetson:[NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/) is a powerful platform for AI at the edge, offering high-performance computing capabilities. It is widely used in applications such as robotics, smart cities, and healthcare. Jetson devices provide the processing power needed to run complex AI models locally, making them ideal for edge AI deployments.\n  * Google Coral:[Google Coral](https://coral.ai/) devices are designed for fast and efficient edge AI deployments. They provide accelerated machine learning inferencing capabilities, making them suitable for various applications, including computer vision and IoT. Coral devices are known for their ease of use and integration with popular AI frameworks.\n  * Intel Movidius: [Intel Movidius](https://www.intel.com/content/www/us/en/products/details/processors/movidius-vpu.html) is renowned for its energy-efficient performance, making it ideal for vision-based applications. It supports deep learning and computer vision workloads on edge devices, enabling real-time processing with minimal power consumption. Movidius chips are commonly used in drones, security cameras, and smart home devices.\n\n\n### Software Frameworks\nIn addition to hardware, robust software frameworks are crucial for developing and deploying Edge AI applications. Let\u2019s explore a few options:\n  * TensorFlow Lite: TensorFlow Lite is a lightweight version of [TensorFlow](https://www.datacamp.com/courses/introduction-to-tensorflow-in-python), optimized for mobile and edge devices. It allows developers to deploy machine learning models on resource-constrained devices efficiently. TensorFlow Lite supports a wide range of AI applications, from image classification to natural language processing.\n  * PyTorch Mobile: PyTorch Mobile enables the deployment of [PyTorch](https://www.datacamp.com/courses/introduction-to-deep-learning-with-pytorch) models on mobile and edge platforms. It supports a variety of AI tasks, including computer vision, speech recognition, and recommendation systems. PyTorch Mobile is known for its flexibility and ease of integration with existing PyTorch workflows.\n  * ONNX (Open Neural Network Exchange): ONNX is an open-source format for AI models, allowing models to be transferred between different frameworks. It is particularly useful for edge AI because it enables the use of pre-trained models across various platforms, including TensorFlow, PyTorch, and others. ONNX Runtime is optimized for performance on edge devices, ensuring efficient execution of models.\n  * Edge Impulse: Edge Impulse is a platform specifically designed for edge AI development. It offers tools for data collection, model training, and deployment, simplifying the process of creating and deploying edge AI solutions. Edge Impulse supports a wide range of hardware platforms and provides an intuitive interface for developers.\n\n\n## Challenges and Considerations\nWhile Edge AI offers numerous benefits, it also presents several challenges that need to be addressed:\n  * Limited resources: Running complex AI models on devices with limited processing power, memory, and storage is challenging. Developers must optimize models to balance performance and resource consumption, ensuring efficient execution on edge devices. Techniques such as [model quantization](https://www.datacamp.com/tutorial/quantization-for-large-language-models), pruning, and [knowledge distillation](https://www.datacamp.com/blog/distillation-llm) are commonly used to reduce the computational load and memory footprint of AI models.\n  * Power constraints: Edge devices, especially battery-powered ones, require energy-efficient AI algorithms and hardware. Ensuring long battery life while maintaining performance is crucial for the widespread adoption of Edge AI. Techniques such as low-power hardware design, [energy-efficient algorithms](https://www.datacamp.com/tutorial/big-o-notation-time-complexity), and dynamic power management are essential for minimizing power consumption without compromising on performance.\n  * Data security and privacy: Processing sensitive data at the edge necessitates robust security measures. Protecting data and ensuring compliance with regulations like GDPR are critical for maintaining user trust and preventing data breaches. Techniques such as encryption, secure boot, and trusted execution environments are commonly used to enhance data security and privacy in edge AI deployments.\n\n\n## Conclusion\nEdge AI represents a significant shift in how we deploy and utilize artificial intelligence.\nBy bringing computation closer to data sources, Edge AI addresses the limitations of cloud-based AI, such as latency, privacy concerns, and bandwidth costs.\nAdvancements in hardware and software, including the development of smaller, more efficient AI models, are driving the growth of Edge AI across various industries.\nEdge AI promises a future where intelligent devices and applications seamlessly integrate into our daily lives, making them faster, safer, and more responsive.\n## Earn a Top AI Certification\nDemonstrate you can effectively and responsibly use AI.\n[Get Certified, Get Hired](https://www.datacamp.com/certification/ai-fundamentals)\n![Bhavishya Pandit's photo](https://media.datacamp.com/legacy/v1725454467/Unknown_f0f804ba73.jpg?w=128)\nAuthor\nBhavishya Pandit\n[LinkedIn](https://www.linkedin.com/in/bhavishya-pandit)[Twitter](https://twitter.com/BhavishyaP9)\nSenior GenAI Engineer and Content Creator who has garnered 20 million views by sharing knowledge on GenAI and data science.\nTopics\n[Artificial Intelligence](https://www.datacamp.com/blog/category/ai)\n![Bhavishya Pandit's photo](https://media.datacamp.com/legacy/v1725454467/Unknown_f0f804ba73.jpg?w=128)\nBhavishya PanditSenior GenAI Engineer | Content Creator\nTopics\n[Artificial Intelligence](https://www.datacamp.com/blog/category/ai)\n### [AI Chips Explained: How AI Chips Work, Industry Trends, Applications](https://www.datacamp.com/blog/ai-chips)\n### [AI in Sports: Applications and Real-World Examples](https://www.datacamp.com/blog/ai-in-sports-use-cases)\n![](https://media.datacamp.com/legacy/v1706534939/datarhys_an_absurdist_oil_painting_of_a_human_looking_at_itself_1a844497_7f22_4ee7_897c_95e10dbf5bf8_997281dfd0.png?w=256)\n### [What is AI? A Quick-Start Guide For Beginners](https://www.datacamp.com/blog/what-is-ai-quick-start-guide-for-beginners)\n### [AI in Pharmaceuticals: Benefits, Challenges, and Insights](https://www.datacamp.com/blog/ai-in-pharmaceuticals)\n### [What is Narrow AI?](https://www.datacamp.com/blog/what-is-narrow-ai)\n![DataFramed 100- Daniel Situnayake.png](https://media.datacamp.com/legacy/v1660564513/Data_Framed_100_Daniel_Situnayake_620c069cf7.png?w=256)\n### [Embedded Machine Learning on Edge Devices](https://www.datacamp.com/podcast/embedded-machine-learning-on-edge-devices)\nLearn AI with these courses!\nCourse\n### [Fully Automated MLOps](https://www.datacamp.com/courses/fully-automated-mlops)\n4 hr\n2.7K\nLearn about MLOps architecture, CI/CD/CM/CT techniques, and automation patterns to deploy ML systems that can deliver value over time.\n[See DetailsRight Arrow](https://www.datacamp.com/courses/fully-automated-mlops)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Fcourses%2Ffully-automated-mlops%2Fcontinue)\nTrack\n### [Developing AI Applications](https://www.datacamp.com/tracks/developing-ai-applications)\n23hrs hr\nLearn to create AI-powered applications with the latest AI developer tools, including the OpenAI API, Hugging Face, and LangChain.\n[See DetailsRight Arrow](https://www.datacamp.com/tracks/developing-ai-applications)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Ftracks%2Fdeveloping-ai-applications%2Fcontinue)\nTrack\n### [Artificial Intelligence (AI) Leadership](https://www.datacamp.com/tracks/artificial-intelligence-ai-leadership)\n6 hours hr\nMaster AI leadership with this track: monetize AI, build trust with Responsible and Explainable AI, and secure your innovations. Lead with confidence!\n[See DetailsRight Arrow](https://www.datacamp.com/tracks/artificial-intelligence-ai-leadership)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Ftracks%2Fartificial-intelligence-ai-leadership%2Fcontinue)\n[See MoreRight Arrow](https://www.datacamp.com/category/artificial-intelligence)\nRelated\n[blogAI Chips Explained: How AI Chips Work, Industry Trends, Applications](https://www.datacamp.com/blog/ai-chips)\nAI chips are specialized processors designed to accelerate the execution of artificial intelligence tasks, typically involving large-scale matrix operations and parallel processing.\n![Bhavishya Pandit's photo](https://media.datacamp.com/legacy/v1725454467/Unknown_f0f804ba73.jpg?w=48)\nBhavishya Pandit \n7 min\n[blogAI in Sports: Applications and Real-World Examples](https://www.datacamp.com/blog/ai-in-sports-use-cases)\nThis article examines the diverse applications of artificial intelligence (AI) in sports, from optimizing athlete training and recovery to enhancing game strategies and decision-making.\n[![Tom Farnschl\u00e4der's photo](https://media.datacamp.com/legacy/v1719243659/1698334986991_adfcb9390d.jpg?w=48)](https://www.datacamp.com/portfolio/tomfarnschlaeder)\nTom Farnschl\u00e4der \n14 min\n![](https://media.datacamp.com/legacy/v1706534939/datarhys_an_absurdist_oil_painting_of_a_human_looking_at_itself_1a844497_7f22_4ee7_897c_95e10dbf5bf8_997281dfd0.png?w=750)\n[blogWhat is AI? A Quick-Start Guide For Beginners](https://www.datacamp.com/blog/what-is-ai-quick-start-guide-for-beginners)\nFind out what artificial intelligence really is with examples, expert input, and all the tools you need to learn more. \n[![Matt Crabtree's photo](https://media.datacamp.com/cms/matt_2.jpg?w=48)](https://www.datacamp.com/portfolio/mattcrabtree)\nMatt Crabtree \n11 min\n[blogAI in Pharmaceuticals: Benefits, Challenges, and Insights](https://www.datacamp.com/blog/ai-in-pharmaceuticals)\nExplore how AI transforms the pharmaceutical industry by learning about its applications, benefits, challenges, and future prospects in our comprehensive analysis!\n[![Maria Eugenia Inzaugarat's photo](https://media.datacamp.com/legacy/v1716472639/Maria_Eugenia_Inzaugarat_18288bb54f.png?w=48)](https://www.datacamp.com/portfolio/meugeinzaug)\nMaria Eugenia Inzaugarat \n21 min\n[blogWhat is Narrow AI?](https://www.datacamp.com/blog/what-is-narrow-ai)\nNarrow AI refers to artificial intelligence systems that are designed to perform specific tasks and operate under limited constraints.\n[![Abid Ali Awan's photo](https://media.datacamp.com/legacy/v1658155691/Abid_Ali_Awan_415cc44670.jpg?w=48)](https://www.datacamp.com/portfolio/kingabzpro)\nAbid Ali Awan \n7 min\n![DataFramed 100- Daniel Situnayake.png](https://media.datacamp.com/legacy/v1660564513/Data_Framed_100_Daniel_Situnayake_620c069cf7.png?w=750)\n[podcastEmbedded Machine Learning on Edge Devices](https://www.datacamp.com/podcast/embedded-machine-learning-on-edge-devices)\nDaniel Situnayake talks about his work with EdgeML, the biggest challenges in embedded machine learning, potential use cases of machine learning models in edge devices, and the best tips for aspiring machine learning engineers and data science practices.\n[![Richie Cotton's photo](https://media.datacamp.com/legacy/v1658155296/richie_new_barnet_be7b436d1f.jpg?w=48)](https://www.datacamp.com/portfolio/richie)\nRichie Cotton \n52 min\n[See More](https://www.datacamp.com/blog/category/ai)[See More](https://www.datacamp.com/blog/category/ai)\n## Grow your data skills with DataCamp for Mobile\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\n[Download on the App Store](https://datacamp.onelink.me/xztQ/45dozwue?deep_link_sub1=%7B%22src_url%22%3A%22https%3A%2F%2Fwww.datacamp.com%2Fblog%2Fedge-ai%22%7D)[Get it on Google Play](https://datacamp.onelink.me/xztQ/go2f19ij?deep_link_sub1=%7B%22src_url%22%3A%22https%3A%2F%2Fwww.datacamp.com%2Fblog%2Fedge-ai%22%7D)\n**Learn**\n[Learn Python](https://www.datacamp.com/blog/how-to-learn-python-expert-guide)[Learn AI](https://www.datacamp.com/blog/how-to-learn-ai)[Learn Power BI](https://www.datacamp.com/learn/power-bi)[Learn Data Engineering](https://www.datacamp.com/category/data-engineering)[Assessments](https://www.datacamp.com/signal)[Career Tracks](https://www.datacamp.com/tracks/career)[Skill Tracks](https://www.datacamp.com/tracks/skill)[Courses](https://www.datacamp.com/courses-all)[Data Science Roadmap](https://www.datacamp.com/blog/data-science-roadmap)\n**Data Courses**\n[Python Courses](https://www.datacamp.com/category/python)[R Courses](https://www.datacamp.com/category/r)[SQL Courses](https://www.datacamp.com/category/sql)[Power BI Courses](https://www.datacamp.com/category/power-bi)[Tableau Courses](https://www.datacamp.com/category/tableau)[Alteryx Courses](https://www.datacamp.com/category/alteryx)[Azure Courses](https://www.datacamp.com/category/azure)[AWS Courses](https://www.datacamp.com/category/aws)[Google Sheets Courses](https://www.datacamp.com/category/google-sheets)[Excel Courses](https://www.datacamp.com/category/excel)[AI Courses](https://www.datacamp.com/category/artificial-intelligence)[Data Analysis Courses](https://www.datacamp.com/category/data-analysis)[Data Visualization Courses](https://www.datacamp.com/category/data-visualization)[Machine Learning Courses](https://www.datacamp.com/category/machine-learning)[Data Engineering Courses](https://www.datacamp.com/category/data-engineering)[Probability & Statistics Courses](https://www.datacamp.com/category/probability-and-statistics)\n**DataLab**\n[Get Started](https://www.datacamp.com/datalab)[Pricing](https://www.datacamp.com/datalab/pricing)[Security](https://www.datacamp.com/datalab/security)[Documentation](https://datalab-docs.datacamp.com)\n**Certification**\n[Certifications](https://www.datacamp.com/certification)[Data Scientist](https://www.datacamp.com/certification/data-scientist)[Data Analyst](https://www.datacamp.com/certification/data-analyst)[Data Engineer](https://www.datacamp.com/certification/data-engineer)[SQL Associate](https://www.datacamp.com/certification/sql-associate)[Power BI Data Analyst](https://www.datacamp.com/certification/data-analyst-in-power-bi)[Tableau Certified Data Analyst](https://www.datacamp.com/certification/data-analyst-in-tableau)[Azure Fundamentals](https://www.datacamp.com/certification/azure-fundamentals)[AI Fundamentals](https://www.datacamp.com/certification/ai-fundamentals)\n**Resources**\n[Resource Center](https://www.datacamp.com/resources)[Upcoming Events](https://www.datacamp.com/webinars)[Blog](https://www.datacamp.com/blog)[Code-Alongs](https://www.datacamp.com/code-along)[Tutorials](https://www.datacamp.com/tutorial)[Docs](https://www.datacamp.com/doc)[Open Source](https://www.datacamp.com/open-source)[RDocumentation](https://www.rdocumentation.org)[Book a Demo with DataCamp for Business](https://www.datacamp.com/business/demo)[Data Portfolio](https://www.datacamp.com/data-portfolio)\n**Plans**\n[Pricing](https://www.datacamp.com/pricing)[For Students](https://www.datacamp.com/pricing/student)[For Business](https://www.datacamp.com/business)[For Universities](https://www.datacamp.com/universities)[Discounts, Promos & Sales](https://www.datacamp.com/promo)[DataCamp Donates](https://www.datacamp.com/donates)\n**For Business**\n[Business Pricing](https://www.datacamp.com/business/compare-plans)[Teams Plan](https://www.datacamp.com/business/learn-teams)[Data & AI Unlimited Plan](https://www.datacamp.com/business/data-unlimited)[Customer Stories](https://www.datacamp.com/business/customer-stories)[Partner Program](https://www.datacamp.com/business/partner-program)\n**About**\n[About Us](https://www.datacamp.com/about)[Learner Stories](https://www.datacamp.com/stories)[Careers](https://www.datacamp.com/careers)[Become an Instructor](https://www.datacamp.com/learn/create)[Press](https://www.datacamp.com/press)[Leadership](https://www.datacamp.com/about/leadership)[Contact Us](https://support.datacamp.com/hc/en-us/articles/360021185634)[DataCamp Espa\u00f1ol](https://www.datacamp.com/es)[DataCamp Portugu\u00eas](https://www.datacamp.com/pt)[DataCamp Deutsch](https://www.datacamp.com/de)[DataCamp Fran\u00e7ais](https://www.datacamp.com/fr)\n**Support**\n[Help Center](https://support.datacamp.com/hc/en-us)[Become an Affiliate](https://www.datacamp.com/affiliates)\n[Facebook](https://www.facebook.com/datacampinc/)[Twitter](https://twitter.com/datacamp)[LinkedIn](https://www.linkedin.com/school/datacampinc/)[YouTube](https://www.youtube.com/channel/UC79Gv3mYp6zKiSwYemEik9A)[Instagram](https://www.instagram.com/datacamp/)\n[Privacy Policy](https://www.datacamp.com/privacy-policy)[Cookie Notice](https://www.datacamp.com/cookie-notice)[Do Not Sell My Personal Information](https://www.datacamp.com/do-not-sell-my-personal-information)[Accessibility](https://www.datacamp.com/accessibility)[Security](https://www.datacamp.com/security)[Terms of Use](https://www.datacamp.com/terms-of-use)\n\u00a9 2025 DataCamp, Inc. All Rights Reserved.\n",
    "answer": "```markdown\n# TL;DR Summary of Edge AI\n\nEdge AI processes AI algorithms on devices like smartphones and IoT sensors, enabling real-time decision-making. It enhances speed, privacy, reliability, and efficiency. Key applications include manufacturing, healthcare, retail, and smart cities, but challenges like resource limitations and data security persist.\n```"
  },
  {
    "instruction": "[We raised a $75m series C to build the future of inference](https://www.baseten.co/blog/announcing-baseten-75m-series-c/)\n[](https://www.baseten.co/)\n  * [Model library](https://www.baseten.co/library/)\n  * [Solutions](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n**Use Case**\n    * [Transcription](https://www.baseten.co/solutions/transcription/)\n    * [Large language models](https://www.baseten.co/solutions/llms/)\n    * [Image generation](https://www.baseten.co/solutions/image-generation/)\n    * [Text-to-speech](https://www.baseten.co/solutions/text-to-speech/)\n    * [Compound AI](https://www.baseten.co/solutions/compound-ai/)\n    * [Embeddings](https://www.baseten.co/solutions/embeddings/)\n**Platform**\n    * [Model performance](https://www.baseten.co/platform/model-performance/)\n    * [Cloud-native infrastructure](https://www.baseten.co/platform/cloud-native-infrastructure/)\n    * [Embedded engineering](https://www.baseten.co/platform/embedded-engineering/)\n    * [Model management](https://www.baseten.co/platform/model-management/)\n**Deployment**\n    * [Baseten Cloud](https://www.baseten.co/deployments/baseten-cloud/)\n    * [Baseten Self-hosted](https://www.baseten.co/deployments/baseten-self-hosted/)\n    * [Baseten Hybrid](https://www.baseten.co/deployments/baseten-hybrid/)\n  * [Documentation](https://docs.baseten.co/)\n  * [Customers](https://www.baseten.co/customers/)\n  * [Pricing](https://www.baseten.co/pricing/)\n  * [Resources](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n    * [Blog](https://www.baseten.co/blog/)\n    * [Careers](https://www.baseten.co/careers/)\n    * [Changelog](https://www.baseten.co/changelog/)\n    * [Events](https://www.baseten.co/resources/event/)\n    * [Guides](https://www.baseten.co/resources/guide/)\n    * [Webinars](https://www.baseten.co/resources/webinar/)\n\n\n  * [Sign in](https://app.baseten.co/login/)\n  * [Sign up](https://app.baseten.co/signup/)\n\n\n[](https://www.baseten.co/)\n  * [Model library](https://www.baseten.co/library/)\n  * [Solutions](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n**Use Case**\n    * [Transcription](https://www.baseten.co/solutions/transcription/)\n    * [Large language models](https://www.baseten.co/solutions/llms/)\n    * [Image generation](https://www.baseten.co/solutions/image-generation/)\n    * [Text-to-speech](https://www.baseten.co/solutions/text-to-speech/)\n    * [Compound AI](https://www.baseten.co/solutions/compound-ai/)\n    * [Embeddings](https://www.baseten.co/solutions/embeddings/)\n**Platform**\n    * [Model performance](https://www.baseten.co/platform/model-performance/)\n    * [Cloud-native infrastructure](https://www.baseten.co/platform/cloud-native-infrastructure/)\n    * [Embedded engineering](https://www.baseten.co/platform/embedded-engineering/)\n    * [Model management](https://www.baseten.co/platform/model-management/)\n**Deployment**\n    * [Baseten Cloud](https://www.baseten.co/deployments/baseten-cloud/)\n    * [Baseten Self-hosted](https://www.baseten.co/deployments/baseten-self-hosted/)\n    * [Baseten Hybrid](https://www.baseten.co/deployments/baseten-hybrid/)\n  * [Documentation](https://docs.baseten.co/)\n  * [Customers](https://www.baseten.co/customers/)\n  * [Pricing](https://www.baseten.co/pricing/)\n  * [Resources](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n    * [Blog](https://www.baseten.co/blog/)\n    * [Careers](https://www.baseten.co/careers/)\n    * [Changelog](https://www.baseten.co/changelog/)\n    * [Events](https://www.baseten.co/resources/event/)\n    * [Guides](https://www.baseten.co/resources/guide/)\n    * [Webinars](https://www.baseten.co/resources/webinar/)\n\n\n  * [Sign in](https://app.baseten.co/login/)\n  * [Sign up](https://app.baseten.co/signup/)\n\n\n[Baseten](https://www.baseten.co/) / [Blog](https://www.baseten.co/blog/) / [Glossary](https://www.baseten.co/blog/category/glossary/)\n# Continuous vs dynamic batching for AI inference\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n[Matt Howard](https://www.baseten.co/author/matt-howard/)\n[Philip Kiely](https://www.baseten.co/author/philip-kiely/)\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n### Share\n  * [](https://twitter.com/intent/tweet?text=Continuous vs dynamic batching for AI inference&url=https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/&via=basetenco)\n  * [](https://www.linkedin.com/shareArticle?mini=true&url=https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/&title=Continuous vs dynamic batching for AI inference)\n  * [](https://www.facebook.com/sharer/sharer.php?u=https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n\n\n### TL;DR\nWhen using AI models in production, batching makes good use of GPU resources by processing multiple requests to a model simultaneously. But different methods for batching are appropriate in different cases. To maximize throughput of AI inference, use continuous batching for most LLM deployments and dynamic batching for most other models.\nBatch inference is essential for serving LLMs and other generative models in production. If you only run one request at a time through a GPU, most of its capacity is sitting idle. Running multiple inputs through the model simultaneously uses more of the GPU\u2019s resources to massively increase the throughput of your model deployment. However, it\u2019s important to choose the right strategy for batch inference to make sure your model still performs well on other important metrics like latency.\nThere are four ways inference requests can be batched on a GPU:\n  1. No batching: each request is processed one at a time.\n  2. Static batching: requests are placed in batches that are run when full.\n  3. Dynamic batching: requests are placed in batches as they\u2019re received and batches run once full or once enough time has elapsed since the first request.\n  4. Continuous batching: requests are processed token-by-token, with new requests getting processed as older requests finish and free up space on the GPU.\n\n\nBatching method depends on model architecture and modality. In production, you\u2019ll generally want continuous batching for LLMs and dynamic batching for most other generative models.\nIn this article, we\u2019ll examine the different methods for batching inference requests to AI models and the suitable uses for each approach. We\u2019ll limit our discussion to batch inference on the GPU itself (there are other [opportunities to add concurrency](https://docs.baseten.co/performance/concurrency) in an end-to-end system) with a goal of maximizing the utilization of GPU resources.\n##  Naive implementation for basic testing\nThe most naive implementation of any model server has no batching. Every request is processed individually in the order that it\u2019s received.\nIf you spin up a quick service with, say, FastAPI and PyTorch, you won\u2019t get batching out of the box. That\u2019s fine for basic development and testing, but wastes valuable GPU resources in production.\nWithout batching, most of the GPU capacity on the model serving instance is idle. Imagine a road full of cars where every car has only its driver, no passengers. Most cars can fit at least four people, so a lot of capacity is going unused. This is exactly what\u2019s happening on your GPU.\n##  Static batching for scheduled load\nStatic batching is the simplest method for batching requests. But it can increase latency substantially, limiting its use cases.\nWhen running inference on an LLM, or many other ML models, your bottleneck is the memory bandwidth used to load model weights. Model weights are much bigger than the activations (which are the \"state\" of a request mid-processing), so when loading a single layer of weights into the GPU's cache, you want to share that cost across processing many independent sets of activations. This gets you much better throughput than loading one layer of weights and computing it on one activation at a time.\nIf running each request individually is like everyone driving their own car, batching is like a bus. If the bus operates on static batching, the driver waits for the bus to fill entirely, then drives off to the destination. This ensures that the bus is full every time it goes through its route. Similarly, static batching for model inference waits until a set number of requests has been received, then runs a single batch to process the requests simultaneously.\nStatic batching is most appropriate when latency isn\u2019t an issue, like processing a huge corpus of documents on a daily cadence. Static batching pushes the complexity of orchestrating requests elsewhere in the system. Using static batching requires a well-managed queue of requests to feed the model and a method for accepting model output in large chunks.\n##  Dynamic batching for AI inference in production\nStatic batching works well for daily jobs or behind-the-scenes processing. But for latency-sensitive production deployments, like generating images in response to user input, static batching won\u2019t cut it.\nReturning to our bus analogy, imagine being the first person getting on the bus on a slow day. If you have to wait for the entire bus to fill up before you leave, you\u2019ll be waiting a long time. But what if the driver started a timer when the first passenger got on the bus, and left when the bus was full or the timer ran out, whichever happens first. This way, you\u2019re guaranteed to only wait a few minutes maximum.\nDynamic batching works the same way. You set up dynamic batching with:\n  1. A preset maximum batch size, which you hope to reach before kicking off each run.\n  2. A window to wait after receiving the first request before running a partial batch.\n\n\nIn the diagram below, we can see how dynamic batching results in shorter wait times when there is less traffic.\n\u2715\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\nDynamic batching runs batches once full or once a maximum time has elapsed, improving latency versus static batching while maintaining throughput in high-traffic periods.\nLet\u2019s say you set up your model server with a batch size of 16 requests and a window of 100 milliseconds. When the server receives its first request, it will:\n  1. Receive 15 more requests in under 100 milliseconds and immediately run a full batch, or\n  2. Receive fewer than 15 requests and run a partial batch once 100 milliseconds passes.\n\n\nDynamic batching is great for live traffic on models like [Stable Diffusion XL](https://www.baseten.co/library/stable-diffusion-xl/), where each inference request takes about the same amount of time. The right settings for your specific deployment depend on traffic patterns and latency requirements, but dynamic batching gives you flexibility across a wide range of options.\n##  Continuous batching for LLM inference in production\nWhile dynamic batching is great for modalities like image generation where each output takes about the same amount of time to create, we can do even better for LLMs with continuous batching.\nLLMs create a sequence of tokens as output. These output sequences will vary in length \u2013 the model could be answering a simple question or performing a detailed analysis with step-by-step reasoning. If you use a dynamic batching approach, each batch of requests is going to need to wait for the longest output to finish before the next batch can begin. This leaves GPU resources idle.\nContinuous batching works at the token level rather than at the request level. The bottleneck in LLM inference is loading model weights. So for continuous batching, the model server loads each layer of the model sequentially and applies it to the next token of each request. In continuous batching, the same model weights could be used to generate the fifth token of one response and the eighty-fifth token of another.\nIn the bus example, continuous batching is similar to how bus routes work in the real world. As the driver goes through the route, passengers ride the bus for different amounts of time. When one passenger reaches their destination, that opens up a seat for a new passenger.\n\u2715\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\nContinuous batching improves GPU utilization over dynamic batching by eliminating the idle time waiting for the longest response of each batch to finish.\nOne complication with continuous batching is that it takes a lot longer to generate the first token of a response than each subsequent token. This relies on a process called prefill, which is actually compute bound. But over the course of an entire request, next token prediction is the most expensive part, so it\u2019s the part we focus on optimizing with continuous batching.\nLike with dynamic batching, you need to configure continuous batching based on your anticipated traffic patterns. You need to specify:\n  * Maximum batch size: how many requests the model can process at once.\n  * Anticipated sequence shapes: how many tokens the input and output sequences are expected to contain.\n\n\nContinuous batching is implemented at the inference server layer. Model servers like TGI and VLLM offer continuous batching, while TensorRT-LLM uses \u201cin-flight batching\u201d to essentially the same effect.\nThanks to continuous batching, you can massively increase the throughput of your LLM deployments while still hitting ambitious latency targets. Read more about the tradeoffs between batch size and latency in our [guide to LLM benchmarking](https://www.baseten.co/blog/understanding-performance-benchmarks-for-llm-inference/), or see a concrete example in our [benchmarking results for Mistral 7B](https://www.baseten.co/blog/benchmarking-fast-mistral-7b-inference/).\nTable of Contents\n  * [Naive implementation for basic testing](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/#112924-naive-implementation-for-basic-testing)\n  * [Static batching for scheduled load](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/#112928-static-batching-for-scheduled-load)\n  * [Dynamic batching for AI inference in production](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/#3500572-dynamic-batching-for-ai-inference-in-production)\n  * [Continuous batching for LLM inference in production](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/#3500603-continuous-batching-for-llm-inference-in-production)\n\n\n## Subscribe to our newsletter\nStay up to date on model performance, GPUs, and more.\n\u200c\n### Related Glossary posts\n[View all **Glossary**](https://www.baseten.co/blog/category/glossary/)\n## [A quick introduction to speculative decoding](https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/)\nSpeculative decoding improves LLM inference latency by using a smaller model to generate draft tokens that the larger target model can accept during inference.\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n[Pankaj Gupta](https://www.baseten.co/author/pankaj-gupta/)\n2 others\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n## [Building high-performance compound AI applications with MongoDB Atlas and Baseten](https://www.baseten.co/blog/building-high-performance-compound-ai-applications-with-mongodb-atlas-and-baseten/)\nUsing MongoDB Atlas and Baseten\u2019s Chains framework for compound AI, you can build high-performance compound AI systems.\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n[Philip Kiely](https://www.baseten.co/author/philip-kiely/)\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n## [Compound AI systems explained](https://www.baseten.co/blog/compound-ai-systems-explained/)\nCompound AI systems combine multiple models and processing steps, and are forming the next generation of AI products.\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\n[Rachel Rapp](https://www.baseten.co/author/rachel-rapp/)\n![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)![](https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/)\nPopular models\n  * [DeepSeek-R1](https://www.baseten.co/library/deepseek-r1/)\n  * [Llama 3.3 70B Instruct](https://www.baseten.co/library/llama-3-3-70b-instruct/)\n  * [MARS6](https://www.baseten.co/library/mars6/)\n  * [Qwen 2.5 32B Coder Instruct](https://www.baseten.co/library/qwen-2-5-32b-coder-instruct/)\n  * [flux-schnell](https://www.baseten.co/library/flux-schnell/)\n\n\nProduct\n  * [Pricing](https://www.baseten.co/pricing/)\n  * [Customer stories](https://www.baseten.co/customers/)\n  * [Terms of service](https://www.baseten.co/terms-and-conditions/)\n  * [Privacy policy](https://www.baseten.co/privacy-policy/)\n  * [Security and trust](https://www.baseten.co/trust/)\n\n\nDevelopers\n  * [Changelog](https://www.baseten.co/changelog/)\n  * [Status](https://status.baseten.co/)\n  * [Docs](https://docs.baseten.co/)\n\n\nCompany\n  * [About](https://www.baseten.co/about/)\n  * [Blog](https://www.baseten.co/blog/)\n  * [Careers](https://www.baseten.co/careers/) We\u2019re hiring\n\n\n[](https://www.baseten.co/)\n[](https://github.com/basetenlabs)[](http://twitter.com/basetenco)[](https://www.linkedin.com/company/baseten)[](https://www.youtube.com/channel/UCOCLmqf7Jy3LcsO0SMBGP_Q)\n![Hipaa Compliant](https://www.baseten.co/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fhipaa.2eb9b526.png&w=256&q=75)![SOC 2 Type II Certified](https://www.baseten.co/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fsoc2-type2.0821ffd3.png&w=256&q=75)\n",
    "answer": "```markdown\n# TL;DR\n\nBaseten raised $75M in Series C funding to enhance AI inference. For optimal GPU utilization, use **continuous batching** for LLMs and **dynamic batching** for other models. This approach maximizes throughput while managing latency effectively.\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/GokuMohandas/testing-ml/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FGokuMohandas%2Ftesting-ml%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FGokuMohandas%2Ftesting-ml%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=GokuMohandas%2Ftesting-ml) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/GokuMohandas/testing-ml/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/GokuMohandas/testing-ml/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/GokuMohandas/testing-ml/) to refresh your session. Dismiss alert\n{{ message }}\n[ GokuMohandas ](https://github.com/GokuMohandas) / **[testing-ml](https://github.com/GokuMohandas/testing-ml) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2FGokuMohandas%2Ftesting-ml) You must be signed in to change notification settings\n  * [ Fork 13 ](https://github.com/login?return_to=%2FGokuMohandas%2Ftesting-ml)\n  * [ Star  86 ](https://github.com/login?return_to=%2FGokuMohandas%2Ftesting-ml)\n\n\nLearn how to create reliable ML systems by testing code, data and models. \n[ 86 stars ](https://github.com/GokuMohandas/testing-ml/stargazers) [ 13 forks ](https://github.com/GokuMohandas/testing-ml/forks) [ Branches ](https://github.com/GokuMohandas/testing-ml/branches) [ Tags ](https://github.com/GokuMohandas/testing-ml/tags) [ Activity ](https://github.com/GokuMohandas/testing-ml/activity)\n[ Star  ](https://github.com/login?return_to=%2FGokuMohandas%2Ftesting-ml)\n[ Notifications ](https://github.com/login?return_to=%2FGokuMohandas%2Ftesting-ml) You must be signed in to change notification settings\n  * [ Code ](https://github.com/GokuMohandas/testing-ml)\n  * [ Issues 0 ](https://github.com/GokuMohandas/testing-ml/issues)\n  * [ Pull requests 0 ](https://github.com/GokuMohandas/testing-ml/pulls)\n  * [ Actions ](https://github.com/GokuMohandas/testing-ml/actions)\n  * [ Projects 0 ](https://github.com/GokuMohandas/testing-ml/projects)\n  * [ Security ](https://github.com/GokuMohandas/testing-ml/security)\n  * [ Insights ](https://github.com/GokuMohandas/testing-ml/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/GokuMohandas/testing-ml)\n  * [ Issues  ](https://github.com/GokuMohandas/testing-ml/issues)\n  * [ Pull requests  ](https://github.com/GokuMohandas/testing-ml/pulls)\n  * [ Actions  ](https://github.com/GokuMohandas/testing-ml/actions)\n  * [ Projects  ](https://github.com/GokuMohandas/testing-ml/projects)\n  * [ Security  ](https://github.com/GokuMohandas/testing-ml/security)\n  * [ Insights  ](https://github.com/GokuMohandas/testing-ml/pulse)\n\n\n# GokuMohandas/testing-ml\nmain\n[Branches](https://github.com/GokuMohandas/testing-ml/branches)[Tags](https://github.com/GokuMohandas/testing-ml/tags)\n[](https://github.com/GokuMohandas/testing-ml/branches)[](https://github.com/GokuMohandas/testing-ml/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[5 Commits](https://github.com/GokuMohandas/testing-ml/commits/main/)[](https://github.com/GokuMohandas/testing-ml/commits/main/)  \n[.gitignore](https://github.com/GokuMohandas/testing-ml/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/GokuMohandas/testing-ml/blob/main/.gitignore \".gitignore\")  \n[README.md](https://github.com/GokuMohandas/testing-ml/blob/main/README.md \"README.md\")| [README.md](https://github.com/GokuMohandas/testing-ml/blob/main/README.md \"README.md\")  \n[testing.ipynb](https://github.com/GokuMohandas/testing-ml/blob/main/testing.ipynb \"testing.ipynb\")| [testing.ipynb](https://github.com/GokuMohandas/testing-ml/blob/main/testing.ipynb \"testing.ipynb\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/GokuMohandas/testing-ml/)\n\n\n# Testing ML\n[](https://github.com/GokuMohandas/testing-ml/#testing-ml)\nLearn how to create reliable ML systems by testing code, data and models.\n[![](https://camo.githubusercontent.com/1c8877d013d3bce832b494b16e92a58bb0b56d0756071d8d20da475c5ea4d95b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5375627363726962652d34304b2d627269676874677265656e)](https://madewithml.com) [![](https://camo.githubusercontent.com/a9dccbaf0395471a9de4f70c2f883806ec1b335a30ea3c43aa0c62e514e01497/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f476f6b754d6f68616e6461732f4d6164652d576974682d4d4c2e7376673f7374796c653d736f6369616c266c6162656c3d53746172)](https://github.com/GokuMohandas/Made-With-ML) [![](https://camo.githubusercontent.com/fc554d6bdc946c70ad1a2b66fd77c3dad48cb4d7e5d6049ef463738c0aaaf4da/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374796c652d2d3565626130302e7376673f6c6162656c3d4c696e6b6564496e266c6f676f3d6c696e6b6564696e267374796c653d736f6369616c)](https://www.linkedin.com/in/goku) [![](https://camo.githubusercontent.com/1928f043a6430a553dd6b3d8ecf4faea06dbeea5c40660d6de57a9c56edb909a/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f476f6b754d6f68616e6461732e7376673f6c6162656c3d466f6c6c6f77267374796c653d736f6369616c)](https://twitter.com/GokuMohandas)\n\ud83d\udc49 This repository contains the [interactive notebook](https://colab.research.google.com/github/GokuMohandas/testing-ml/blob/main/testing.ipynb) that complements the [testing lesson](https://madewithml.com/courses/mlops/testing/), which is a part of the [MLOps course](https://github.com/GokuMohandas/mlops-course). If you haven't already, be sure to check out the [lesson](https://madewithml.com/courses/mlops/testing/) because all the concepts are covered extensively and tied to software engineering best practices for building ML systems.\n[![](https://camo.githubusercontent.com/2d36366473e492bfb3cfd8515db7631777116c574d861d83eaee03d2781f3d3a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f939620526561642d6c6573736f6e2d396366)](https://madewithml.com/courses/mlops/testing/) [![](https://camo.githubusercontent.com/d09e210cec6e79b6b8b18e3ff272570ef547472abfbedfa88e1bfd7fd1b948dc/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d266d6573736167653d566965772532304f6e25323047697448756226636f6c6f723d353836303639266c6f676f3d676974687562266c6162656c436f6c6f723d326633363364)](https://github.com/GokuMohandas/testing-ml/blob/main/testing.ipynb) [![Open In Colab](https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/github/GokuMohandas/testing-ml/blob/main/testing.ipynb)\n  * [Data](https://github.com/GokuMohandas/testing-ml/#data)\n    * [Expectations](https://github.com/GokuMohandas/testing-ml/#expectations)\n    * [Production](https://github.com/GokuMohandas/testing-ml/#production)\n  * [Models](https://github.com/GokuMohandas/testing-ml/#models)\n    * [Training](https://github.com/GokuMohandas/testing-ml/#training)\n    * [Behavioral](https://github.com/GokuMohandas/testing-ml/#behavioral)\n    * [Adversarial](https://github.com/GokuMohandas/testing-ml/#adversarial)\n    * [Inference](https://github.com/GokuMohandas/testing-ml/#inference)\n\n\n## Data\n[](https://github.com/GokuMohandas/testing-ml/#data)\nTools such as [pytest](https://madewithml.com/courses/mlops/testing/#pytest) allow us to test the functions that interact with our data but not the validity of the data itself. We're going to use the [great expectations](https://github.com/great-expectations/great_expectations) library to create expectations as to what our data should look like in a standardized way.\n```\n!pip install great-expectations==0.15.15 -q\n```\n\n```\nimport great_expectations as ge\nimport json\nimport pandas as pd\nfrom urllib.request import urlopen\n```\n\n```\n# Load labeled projects\nprojects = pd.read_csv(\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/projects.csv\")\ntags = pd.read_csv(\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tags.csv\")\ndf = ge.dataset.PandasDataset(pd.merge(projects, tags, on=\"id\"))\nprint (f\"{len(df)} projects\")\ndf.head(5)\n```\n\nid | created_on | title | description | tag  \n---|---|---|---|---  \n0 | 6 | 2020-02-20 06:43:18 | Comparison between YOLO and RCNN on real world... | Bringing theory to experiment is cool. We can ... | computer-vision  \n1 | 7 | 2020-02-20 06:47:21 | Show, Infer & Tell: Contextual Inference for C... | The beauty of the work lies in the way it arch... | computer-vision  \n2 | 9 | 2020-02-24 16:24:45 | Awesome Graph Classification | A collection of important graph embedding, cla... | graph-learning  \n3 | 15 | 2020-02-28 23:55:26 | Awesome Monte Carlo Tree Search | A curated list of Monte Carlo tree search papers... | reinforcement-learning  \n4 | 19 | 2020-03-03 13:54:31 | Diffusion to Vector | Reference implementation of Diffusion2Vec (Com... | graph-learning  \n### Expectations\n[](https://github.com/GokuMohandas/testing-ml/#expectations)\nWhen it comes to creating expectations as to what our data should look like, we want to think about our entire dataset and all the features (columns) within it.\n```\n# Presence of specific features\ndf.expect_table_columns_to_match_ordered_list(\n  column_list=[\"id\", \"created_on\", \"title\", \"description\", \"tag\"]\n)\n```\n\n```\n# Unique combinations of features (detect data leaks!)\ndf.expect_compound_columns_to_be_unique(column_list=[\"title\", \"description\"])\n```\n\n```\n# Missing values\ndf.expect_column_values_to_not_be_null(column=\"tag\")\n```\n\n```\n# Unique values\ndf.expect_column_values_to_be_unique(column=\"id\")\n```\n\n```\n# Type adherence\ndf.expect_column_values_to_be_of_type(column=\"title\", type_=\"str\")\n```\n\n```\n# List (categorical) / range (continuous) of allowed values\ntags = [\"computer-vision\", \"graph-learning\", \"reinforcement-learning\",\n    \"natural-language-processing\", \"mlops\", \"time-series\"]\ndf.expect_column_values_to_be_in_set(column=\"tag\", value_set=tags)\n```\n\nThere are just a few of the different expectations that we can create. Be sure to explore all the [expectations](https://greatexpectations.io/expectations/), including [custom expectations](https://docs.greatexpectations.io/docs/guides/expectations/creating_custom_expectations/overview/). Here are some other popular expectations that don't pertain to our specific dataset but are widely applicable:\n  * feature value relationships with other feature values \u2192 `expect_column_pair_values_a_to_be_greater_than_b`\n  * row count (exact or range) of samples \u2192 `expect_table_row_count_to_be_between`\n  * value statistics (mean, std, median, max, min, sum, etc.) \u2192 `expect_column_mean_to_be_between`\n\n\n### Production\n[](https://github.com/GokuMohandas/testing-ml/#production)\nThe advantage of using a library such as great expectations, as opposed to isolated assert statements is that we can:\n  * reduce redundant efforts for creating tests across data modalities\n  * automatically create testing [checkpoints](https://madewithml.com/courses/mlops/testing#checkpoints) to execute as our dataset grows\n  * automatically generate [documentation](https://madewithml.com/courses/mlops/testing#documentation) on expectations and report on runs\n  * easily connect with backend data sources such as local file systems, S3, databases, etc.\n\n\n```\n# Run all tests on our DataFrame at once\nexpectation_suite = df.get_expectation_suite(discard_failed_expectations=False)\ndf.validate(expectation_suite=expectation_suite, only_return_failures=True)\n```\n\n```\n\"success\": true,\n\"evaluation_parameters\": {},\n\"results\": [],\n\"statistics\": {\n  \"evaluated_expectations\": 6,\n  \"successful_expectations\": 6,\n  \"unsuccessful_expectations\": 0,\n  \"success_percent\": 100.0\n}\n```\n\nMany of these expectations will be executed when the data is extracted, loaded and transformed during our [DataOps workflows](https://madewithml.com/courses/mlops/orchestration#dataops). Typically, the data will be extracted from a source ([database](https://madewithml.com/courses/mlops/data-stack#database), [API](https://madewithml.com/courses/mlops/api), etc.) and loaded into a data system (ex. [data warehouse](https://madewithml.com/courses/mlops/data-stack#data-warehouse)) before being transformed there (ex. using [dbt](https://www.getdbt.com/)) for downstream applications. Throughout these tasks, Great Expectations checkpoint validations can be run to ensure the validity of the data and the changes applied to it.\n[![ETL pipelines in production](https://camo.githubusercontent.com/801210afab0c444fbabeb6f87b8686b9fd11907c3e3bc448a9e244be19b1ec4c/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6d6c6f70732f74657374696e672f70726f64756374696f6e2e706e67)](https://camo.githubusercontent.com/801210afab0c444fbabeb6f87b8686b9fd11907c3e3bc448a9e244be19b1ec4c/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6d6c6f70732f74657374696e672f70726f64756374696f6e2e706e67)\n## Models\n[](https://github.com/GokuMohandas/testing-ml/#models)\nOnce we've tested our data, we can use it for downstream applications such as training machine learning models. It's important that we also test these model artifacts to ensure reliable behavior in our application.\n### Training\n[](https://github.com/GokuMohandas/testing-ml/#training)\nUnlike traditional software, ML models can run to completion without throwing any exceptions / errors but can produce incorrect systems. We want to catch errors quickly to save on time and compute.\n  * Check shapes and values of model output\n\n\n```\nassert model(inputs).shape == torch.Size([len(inputs), num_classes])\n```\n\n  * Check for decreasing loss after one batch of training\n\n\n```\nassert epoch_loss < prev_epoch_loss\n```\n\n  * Overfit on a batch\n\n\n```\naccuracy = train(model, inputs=batches[0])\nassert accuracy == pytest.approx(0.95, abs=0.05) # 0.95 \u00b1 0.05\n```\n\n  * Train to completion (tests early stopping, saving, etc.)\n\n\n```\ntrain(model)\nassert learning_rate >= min_learning_rate\nassert artifacts\n```\n\n  * On different devices\n\n\n```\nassert train(model, device=torch.device(\"cpu\"))\nassert train(model, device=torch.device(\"cuda\"))\n```\n\n### Behavioral\n[](https://github.com/GokuMohandas/testing-ml/#behavioral)\nBehavioral testing is the process of testing input data and expected outputs while treating the model as a black box (model agnostic evaluation). A landmark paper on this topic is [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://arxiv.org/abs/2005.04118) which breaks down behavioral testing into three types of tests:\n  * `invariance`: Changes should not affect outputs.\n\n\n```\n# INVariance via verb injection (changes should not affect outputs)\ntokens = [\"revolutionized\", \"disrupted\"]\ntexts = [f\"Transformers applied to NLP have {token} the ML field.\" for token in tokens]\npredict.predict(texts=texts, artifacts=artifacts)\n```\n\n```\n['natural-language-processing', 'natural-language-processing']\n\n```\n\n  * `directional`: Change should affect outputs.\n\n\n```\n# DIRectional expectations (changes with known outputs)\ntokens = [\"text classification\", \"image classification\"]\ntexts = [f\"ML applied to {token}.\" for token in tokens]\npredict.predict(texts=texts, artifacts=artifacts)\n```\n\n```\n['natural-language-processing', 'computer-vision']\n\n```\n\n  * `minimum functionality`: Simple combination of inputs and expected outputs.\n\n\n```\n# Minimum Functionality Tests (simple input/output pairs)\ntokens = [\"natural language processing\", \"mlops\"]\ntexts = [f\"{token} is the next big wave in machine learning.\" for token in tokens]\npredict.predict(texts=texts, artifacts=artifacts)\n```\n\n```\n['natural-language-processing', 'mlops']\n\n```\n\n### Adversarial\n[](https://github.com/GokuMohandas/testing-ml/#adversarial)\nBehavioral testing can be extended to adversarial testing where we test to see how the model would perform under edge cases, bias, noise, etc.\n```\ntexts = [\n  \"CNNs for text classification.\", # CNNs are typically seen in computer-vision projects\n  \"This should not produce any relevant topics.\" # should predict `other` label\n]\npredict.predict(texts=texts, artifacts=artifacts)\n```\n\n```\n  ['natural-language-processing', 'other']\n\n```\n\n### Inference\n[](https://github.com/GokuMohandas/testing-ml/#inference)\nWhen our model is deployed, most users will be using it for inference (directly / indirectly), so it's very important that we test all aspects of it.\n#### Loading artifacts\n[](https://github.com/GokuMohandas/testing-ml/#loading-artifacts)\nThis is the first time we're not loading our components from in-memory so we want to ensure that the required artifacts (model weights, encoders, config, etc.) are all able to be loaded.\n```\nartifacts = main.load_artifacts(run_id=run_id)\nassert isinstance(artifacts[\"label_encoder\"], data.LabelEncoder)\n...\n```\n\n#### Prediction\n[](https://github.com/GokuMohandas/testing-ml/#prediction)\nOnce we have our artifacts loaded, we're readying to test our prediction pipelines. We should test samples with just one input, as well as a batch of inputs (ex. padding can have unintended consequences sometimes).\n```\n# test our API call directly\ndata = {\n  \"texts\": [\n    {\"text\": \"Transfer learning with transformers for text classification.\"},\n    {\"text\": \"Generative adversarial networks in both PyTorch and TensorFlow.\"},\n  ]\n}\nresponse = client.post(\"/predict\", json=data)\nassert response.status_code == HTTPStatus.OK\nassert response.request.method == \"POST\"\nassert len(response.json()[\"data\"][\"predictions\"]) == len(data[\"texts\"])\n...\n```\n\n## Learn more\n[](https://github.com/GokuMohandas/testing-ml/#learn-more)\nWhile these are the foundational concepts for testing ML systems, there are a lot of software best practices for testing that we cannot show in an isolated repository. Learn a lot more about comprehensively testing code, data and models for ML systems in our [testing lesson](https://madewithml.com/courses/mlops/testing/).\n## About\nLearn how to create reliable ML systems by testing code, data and models. \n### Topics\n[ testing ](https://github.com/topics/testing \"Topic: testing\") [ machine-learning ](https://github.com/topics/machine-learning \"Topic: machine-learning\") [ pytest ](https://github.com/topics/pytest \"Topic: pytest\") [ mlops ](https://github.com/topics/mlops \"Topic: mlops\") [ great-expectations ](https://github.com/topics/great-expectations \"Topic: great-expectations\")\n### Resources\n[ Readme ](https://github.com/GokuMohandas/testing-ml/#readme-ov-file)\n[ Activity](https://github.com/GokuMohandas/testing-ml/activity)\n### Stars\n[ **86** stars](https://github.com/GokuMohandas/testing-ml/stargazers)\n### Watchers\n[ **2** watching](https://github.com/GokuMohandas/testing-ml/watchers)\n### Forks\n[ **13** forks](https://github.com/GokuMohandas/testing-ml/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FGokuMohandas%2Ftesting-ml&report=GokuMohandas+%28user%29)\n##  [Releases](https://github.com/GokuMohandas/testing-ml/releases)\nNo releases published\n##  [Packages 0](https://github.com/users/GokuMohandas/packages?repo_name=testing-ml)\nNo packages published \n## Languages\n  * [ Jupyter Notebook 100.0% ](https://github.com/GokuMohandas/testing-ml/search?l=jupyter-notebook)\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe repository **testing-ml** by GokuMohandas focuses on creating reliable ML systems through testing code, data, and models. It utilizes tools like **Great Expectations** for data validation and emphasizes best practices in **MLOps**. Key testing areas include data expectations, model training, behavioral, adversarial testing, and inference validation.\n```"
  }
]