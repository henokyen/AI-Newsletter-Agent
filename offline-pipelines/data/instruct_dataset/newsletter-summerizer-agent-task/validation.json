[
  {
    "instruction": "[Skip to content](https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada/#wp--skip-link--target)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nThe world\u2019s leading publication for data science, AI, and ML professionals.\nSign in\nSign out\n[Contributor Portal](https://contributor.insightmediagroup.io/)\n  * [Latest](https://towardsdatascience.com/latest/)\n  * [Editor\u2019s Picks](https://towardsdatascience.com/tag/editors-pick/)\n  * [Deep Dives](https://towardsdatascience.com/tag/deep-dives/)\n  * [Contribute](https://towardsdatascience.com/questions-96667b06af5/)\n  * [Newsletter](https://newsletter.towardsdatascience.com/subscription-to-the-newsletter)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\n\n\nToggle Mobile Navigation\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [X](https://x.com/TDataScience)\n\n\nToggle Search\nSearch\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\n# Fine-tune Llama 3 with ORPO\nA cheaper and faster unified fine-tuning technique \n[Maxime Labonne](https://towardsdatascience.com/author/mlabonne/)\nApr 19, 2024\n8 min read\nShare \n![Image generated with DALL-E 3 by author](https://towardsdatascience.com/wp-content/uploads/2024/04/0DSTQkcyX56nl4qYu.png)Image generated with DALL-E 3 by author\nORPO is a **new exciting fine-tuning technique** that combines the traditional supervised fine-tuning and preference alignment stages into a single process. This reduces the computational resources and time required for training. Moreover, empirical results demonstrate that ORPO outperforms other alignment methods on various model sizes and benchmarks.\nIn this article, we will fine-tune the new Llama 3 8B model using ORPO with the TRL library. The code is available on [Google Colab](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing) and in the [LLM Course](https://github.com/mlabonne/llm-course) on GitHub.\n## \u2696\ufe0f ORPO\nInstruction tuning and preference alignment are essential techniques for adapting [Large Language Models](https://towardsdatascience.com/tag/large-language-models/ \"Large Language Models\") (LLMs) to specific tasks. Traditionally, this involves a multi-stage process: 1/ **Supervised Fine-Tuning** (SFT) on instructions to adapt the model to the target domain, followed by 2/ **preference alignment methods** like Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO) to increase the likelihood of generating preferred responses over rejected ones.\n![Image by author](https://towardsdatascience.com/wp-content/uploads/2024/04/0LlRjrJYf7rWtVxGj.png)Image by author\nHowever, researchers have identified a limitation in this approach. While SFT effectively adapts the model to the desired domain, it inadvertently **increases the probability of generating undesirable answers** alongside preferred ones. This is why the preference alignment stage is necessary to widen the gap between the likelihoods of preferred and rejected outputs.\n![Note how the probability of rejected responses increases during supervised fine-tuning \\(image from the ORPO paper\\).](https://towardsdatascience.com/wp-content/uploads/2024/04/0rfs4IexRUX7T6-5y.png)Note how the probability of rejected responses increases during supervised fine-tuning (image from the ORPO paper).\nIntroduced by [Hong and Lee (2024)](https://arxiv.org/abs/2403.07691), ORPO offers an elegant solution to this problem by combining instruction tuning and preference alignment into a single, monolithic training process. ORPO modifies the standard language modeling objective, combining the negative log-likelihood loss with an odds ratio (OR) term. This OR loss weakly penalizes rejected responses while strongly rewarding preferred ones, allowing the model to simultaneously learn the target task and align with human preferences.\n![](https://towardsdatascience.com/wp-content/uploads/2024/04/1r3V1OdKtcWJJKS6cGT8chQ.png)\nORPO has been implemented in the major fine-tuning libraries, like [TRL](https://github.com/huggingface/trl), [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl), and [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory). In the next section, we will see how to use with TRL.\n## \ud83d\udcbb Fine-tuning Llama 3 with ORPO\n[Llama 3](https://github.com/meta-llama/llama3/tree/main) is the latest family of LLMs developed by Meta. The models were trained on an extensive dataset of **15 trillion tokens** (compared to 2T tokens for Llama 2). Two model sizes have been released: a 70 billion parameter model and a smaller 8 billion parameter model. The 70B model has already demonstrated impressive performance, scoring 82 on the MMLU benchmark and 81.7 on the HumanEval benchmark.\nLlama 3 models also increased the context length up to 8,192 tokens (4,096 tokens for Llama 2), and potentially scale up to 32k with RoPE. Additionally, the models use a new tokenizer with a 128K-token vocabulary, reducing the number of tokens required to encode text by 15%. This vocabulary also explains the bump from 7B to 8B parameters.\n![Samples from ORPO-DPO-mix-40k \\(image by author\\).](https://towardsdatascience.com/wp-content/uploads/2024/04/0G8pGN8e3ppGj0TCa.png)_Samples from ORPO-DPO-mix-40k (image by author)._\nORPO requires a preference dataset, including a prompt, a chosen answer, and a rejected answer. In this example, we will use `[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)`, a combination of the following high-quality DPO datasets:\n  * `[argilla/distilabel-capybara-dpo-7k-binarized](https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized)`: highly scored chosen answers >=5 (2,882 samples)\n  * `[argilla/distilabel-intel-orca-dpo-pairs](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs)`: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\n  * `[argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)`: highly scored chosen answers >=5 (22,799 samples)\n  * `[argilla/distilabel-math-preference-dpo](https://huggingface.co/datasets/argilla/distilabel-math-preference-dpo)`: highly scored chosen answers >=9 (2,181 samples)\n  * `[unalignment/toxic-dpo-v0.2](https://huggingface.co/datasets/unalignment/toxic-dpo-v0.2)` (541 samples)\n  * `[M4-ai/prm_dpo_pairs_cleaned](https://huggingface.co/datasets/M4-ai/prm_dpo_pairs_cleaned)` (7,958 samples)\n  * `[jondurbin/truthy-dpo-v0.1](https://huggingface.co/datasets/jondurbin/truthy-dpo-v0.1)` (1,016 samples)\n\n\nThanks to [argilla](https://huggingface.co/argilla), [unalignment](https://huggingface.co/unalignment), [M4-ai](https://huggingface.co/M4-ai), and [jondurbin](https://huggingface.co/jondurbin) for providing the source datasets.\nAs per usual, let\u2019s start by installing the required libraries:\n```\npip install -U transformers datasets accelerate peft trl bitsandbytes wandb\n```\n\nOnce it\u2019s installed, we can import the necessary libraries and log in to W&B (optional):\n```\nimport gc\nimport os\nimport torch\nimport wandb\nfrom datasets import load_dataset\nfrom google.colab import userdata\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (\n  AutoModelForCausalLM,\n  AutoTokenizer,\n  BitsAndBytesConfig,\n  TrainingArguments,\n  pipeline,\n)\nfrom trl import ORPOConfig, ORPOTrainer, setup_chat_format\nwb_token = userdata.get('wandb')\nwandb.login(key=wb_token)\n```\n\nIf you have a recent GPU, you should also be able to use the [Flash Attention library](https://github.com/Dao-AILab/flash-attention) to replace the default eager attention implementation with a more efficient one.\n```\nif torch.cuda.get_device_capability()[0] >= 8:\n  !pip install -qqq flash-attn\n  attn_implementation = \"flash_attention_2\"\n  torch_dtype = torch.bfloat16\nelse:\n  attn_implementation = \"eager\"\n  torch_dtype = torch.float16\n```\n\nIn the following, we will load the Llama 3 8B model in 4-bit precision thanks to [bitsandbytes](https://github.com/TimDettmers/bitsandbytes). We then set the LoRA configuration using [PEFT](https://github.com/huggingface/peft) for QLoRA. I\u2019m also using the convenient `setup_chat_format()` function to modify the model and tokenizer for [ChatML](https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use) support. It automatically applies this chat template, adds special tokens, and resizes the model\u2019s embedding layer to match the new vocabulary size.\nNote that you need to submit a request to access [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) and be logged in to your Hugging Face account. Alternatively, you can load ungated copies of the model, like [NousResearch/Meta-Llama-3-8B](https://huggingface.co/NousResearch/Meta-Llama-3-8B).\n```\n# Model\nbase_model = \"meta-llama/Meta-Llama-3-8B\"\nnew_model = \"OrpoLlama-3-8B\"\n# QLoRA config\nbnb_config = BitsAndBytesConfig(\n  load_in_4bit=True,\n  bnb_4bit_quant_type=\"nf4\",\n  bnb_4bit_compute_dtype=torch_dtype,\n  bnb_4bit_use_double_quant=True,\n)\n# LoRA config\npeft_config = LoraConfig(\n  r=16,\n  lora_alpha=32,\n  lora_dropout=0.05,\n  bias=\"none\",\n  task_type=\"CAUSAL_LM\",\n  target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n  base_model,\n  quantization_config=bnb_config,\n  device_map=\"auto\",\n  attn_implementation=attn_implementation\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = prepare_model_for_kbit_training(model)\n```\n\nNow that the model is ready for training, we can take care of the dataset. We load `[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)` and use the `apply_chat_template()` function to convert the \"chosen\" and \"rejected\" columns into the ChatML format. Note that I\u2019m only using 1,000 samples and not the entire dataset, as it would take too long to run.\n```\ndataset_name = \"mlabonne/orpo-dpo-mix-40k\"\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=42).select(range(1000))\ndef format_chat_template(row):\n  row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n  row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n  return row\ndataset = dataset.map(\n  format_chat_template,\n  num_proc= os.cpu_count(),\n)\ndataset = dataset.train_test_split(test_size=0.01)\n```\n\nFirst, we need to set a few hyperparameters:\n  * `learning_rate`: ORPO uses very low learning rates compared to traditional SFT or even DPO. This value of 8e-6 comes from the original paper, and roughly corresponds to an SFT learning rate of 1e-5 and a DPO learning rate of 5e-6. I would recommend increasing it around 1e-6 for a real fine-tune.\n  * `beta`: It is the $lambda$ parameter in the paper, with a default value of 0.1. An appendix from the original paper shows how it\u2019s been selected with an ablation study.\n  * Other parameters, like `max_length` and batch size are set to use as much VRAM as available (~20 GB in this configuration). Ideally, we would train the model for 3-5 epochs, but we\u2019ll stick to 1 here.\n\n\nFinally, we can train the model using the ORPOTrainer, which acts as a wrapper.\n```\norpo_args = ORPOConfig(\n  learning_rate=8e-6,\n  beta=0.1,\n  lr_scheduler_type=\"linear\",\n  max_length=1024,\n  max_prompt_length=512,\n  per_device_train_batch_size=2,\n  per_device_eval_batch_size=2,\n  gradient_accumulation_steps=4,\n  optim=\"paged_adamw_8bit\",\n  num_train_epochs=1,\n  evaluation_strategy=\"steps\",\n  eval_steps=0.2,\n  logging_steps=1,\n  warmup_steps=10,\n  report_to=\"wandb\",\n  output_dir=\"./results/\",\n)\ntrainer = ORPOTrainer(\n  model=model,\n  args=orpo_args,\n  train_dataset=dataset[\"train\"],\n  eval_dataset=dataset[\"test\"],\n  peft_config=peft_config,\n  tokenizer=tokenizer,\n)\ntrainer.train()\ntrainer.save_model(new_model)\n```\n\nTraining the model on these 1,000 samples took about 2 hours on an L4 GPU. Let\u2019s check the W&B plots:\n![](https://towardsdatascience.com/wp-content/uploads/2024/04/0HDi6G4O5z9rpjeEG.png)\nWhile the loss goes down, the difference between the chosen and rejects answers is not clear: the average margin and accuracy are only slightly above zero and 0.5, respectively.\nIn the original paper, the authors trained models on the `[Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)` dataset (161k samples) for 10 epochs, which is a lot longer than our quick run. They also experimented with Llama 3 and kindly [shared their logs](https://huggingface.co/orpo-explorers/hf-llama3-8b-orpo-v0.0/tensorboard) with me (thanks [Jiwoo Hong](https://twitter.com/jiwoohong98)).\nTo end this tutorial, let\u2019s merge the QLoRA adapter with the base model and push it to the Hugging Face Hub.\n```\n# Flush memory\ndel trainer, model\ngc.collect()\ntorch.cuda.empty_cache()\n# Reload tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(\n  base_model,\n  low_cpu_mem_usage=True,\n  return_dict=True,\n  torch_dtype=torch.float16,\n  device_map=\"auto\",\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n# Merge adapter with base model\nmodel = PeftModel.from_pretrained(model, new_model)\nmodel = model.merge_and_unload()\nmodel.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)\n```\n\nCongrats, we finished this quick fine-tune of Llama 3: [mlabonne/OrpoLlama-3\u20138B](https://huggingface.co/mlabonne/OrpoLlama-3-8B). You can play with it using this [Hugging Face Space](https://huggingface.co/spaces/mlabonne/OrpoLlama-3-8B) (here\u2019s a [notebook](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC?usp=sharing) to make your own). Although the model is undertrained, as highlighted by the W&B curves, I ran some evaluations on Nous\u2019 benchmark suite using [LLM AutoEval](https://github.com/mlabonne/llm-autoeval).\n![](https://towardsdatascience.com/wp-content/uploads/2024/04/1XLNStboeDllWwCD-XyCTXw.png)\nOur ORPO fine-tune is actually pretty decent and improves the base model\u2019s performance on every benchmark. This is encouraging and likely means that a fine-tune on the entire 40k samples would yield great results.\nThis is an exciting time for the open-source community, with more and more high-quality open-weight models being released. The gap between closed-source and open-weight models is slowly closing, and fine-tuning is an essential tool to get the best performance for your use cases.\n![Image by author](https://towardsdatascience.com/wp-content/uploads/2024/04/16MeN5SXi4yrnNyf2O_-5zQ.png)Image by author\n## Conclusion\nIn this article, we introduced the ORPO algorithm and explained how it unifies the SFT and preference alignment stages into a single process. Then, we used TRL to fine-tune a Llama 3 8B model on a custom preference dataset. The final model shows encouraging results and highlights ORPO\u2019s potential as a new fine-tuning paradigm.\nI hope it was useful, and I recommend running the [Colab notebook](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing) to fine-tune your own Llama 3 models. In future articles, we will see how to create high-quality datasets \u2013 a point that is often overlooked. If you liked this article, please follow me on [Hugging Face](https://huggingface.co/mlabonne/) and Twitter [@maximelabonne](https://twitter.com/maximelabonne).\n## References\n  * J. Hong, N. Lee, and J. Thorne, [ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691). 2024.\n  * L. von Werra et al., TRL: Transformer Reinforcement Learning. GitHub, 2020. [Online]. Available: <https://github.com/huggingface/trl>\n  * Bartolome, A., Martin, G., & Vila, D. (2023). Notus. In GitHub Repository. GitHub. <https://github.com/argilla-io/notus>\n  * AI at Meta, [Introducing Meta Llama 3](https://ai.meta.com/blog/meta-llama-3/), 2024.\n\n\nWritten By\nMaxime Labonne\n[See all from Maxime Labonne](https://towardsdatascience.com/author/mlabonne/)\nTopics:\n[Artificial Intelligence](https://towardsdatascience.com/tag/artificial-intelligence/), [Editors Pick](https://towardsdatascience.com/tag/editors-pick/), [Hands On Tutorials](https://towardsdatascience.com/tag/hands-on-tutorials/), [Large Language Models](https://towardsdatascience.com/tag/large-language-models/), [Machine Learning](https://towardsdatascience.com/tag/machine-learning/)\nShare this article:\n  * [ Share on Facebook  ](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-llama-3-with-orpo-56cfab2f9ada%2F&title=Fine-tune%20Llama%203%20with%20ORPO)\n  * [ Share on LinkedIn  ](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-llama-3-with-orpo-56cfab2f9ada%2F&title=Fine-tune%20Llama%203%20with%20ORPO)\n  * [ Share on X  ](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-llama-3-with-orpo-56cfab2f9ada%2F&text=Fine-tune%20Llama%203%20with%20ORPO)\n\n\n## Related Articles\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/0c09RmbCCpfjAbSMq.png)\n## [Implementing Convolutional Neural Networks in TensorFlow](https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nStep-by-step code guide to building a Convolutional Neural Network \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nAugust 20, 2024\n6 min read\n  * ## [What Do Large Language Models \u201cUnderstand\u201d?](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA deep dive on the meaning of understanding and how it applies to LLMs \n[Tarik Dzekman](https://towardsdatascience.com/author/tarikdzekman/)\nAugust 21, 2024\n31 min read\n  * ![Photo by Krista Mangulsone on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/0GyVVTbgotH-DhGPH-scaled.jpg)\n## [How to Forecast Hierarchical Time Series](https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA beginner\u2019s guide to forecast reconciliation \n[Dr. Robert K\u00fcbler](https://towardsdatascience.com/author/dr-robert-kuebler/)\nAugust 20, 2024\n13 min read\n  * ![Photo by davisuko on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/1bAABgtZtAIG5YW1oEjW3pA-scaled.jpeg)\n## [Hands-on Time Series Anomaly Detection using Autoencoders, with Python](https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nHere\u2019s how to use Autoencoders to detect signals with anomalies in a few lines of\u2026 \n[Piero Paialunga](https://towardsdatascience.com/author/piero-paialunga/)\nAugust 21, 2024\n12 min read\n  * ![Image from Canva.](https://towardsdatascience.com/wp-content/uploads/2024/08/1UAA9jQVdqMXnwzYiz8Q53Q.png)\n## [3 AI Use Cases (That Are Not a Chatbot)](https://towardsdatascience.com/3-ai-use-cases-that-are-not-a-chatbot-f4f328a2707a/)\n[ Machine Learning ](https://towardsdatascience.com/category/artificial-intelligence/machine-learning/)\nFeature engineering, structuring unstructured data, and lead scoring \n[Shaw Talebi](https://towardsdatascience.com/author/shawhin/)\nAugust 21, 2024\n7 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2023/02/1VEUgT5T4absnTqBMOEuNig.png)\n## [Back To Basics, Part Uno: Linear Regression and Cost Function](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nAn illustrated guide on essential machine learning concepts \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nFebruary 3, 2023\n6 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/1kM8tfYcdaoccB1HX71YDig.png)\n## [Must-Know in Statistics: The Bivariate Normal Projection Explained](https://towardsdatascience.com/must-know-in-statistics-the-bivariate-normal-projection-explained-ace7b2f70b5b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nDerivation and practical examples of this powerful concept \n[Luigi Battistoni](https://towardsdatascience.com/author/lu-battistoni/)\nAugust 14, 2024\n7 min read\n  * ![Photo by Alex Geerts on Unsplash](https://towardsdatascience.com/wp-content/uploads/2020/11/0BF38u2sw4WQdaMLS-scaled.jpg)\n## [Our Columns](https://towardsdatascience.com/our-columns-53501f74c86d/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nColumns on TDS are carefully curated collections of posts on a particular idea or category\u2026 \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nNovember 14, 2020\n4 min read\n  * ![Image created by authors with GPT-4o](https://towardsdatascience.com/wp-content/uploads/2024/08/1vilI3Q4nlwqsAQLq3TOzSA.jpg)\n## [Optimizing Marketing Campaigns with Budgeted Multi-Armed Bandits](https://towardsdatascience.com/optimizing-marketing-campaigns-with-budgeted-multi-armed-bandits-a65fccd61878/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nWith demos, our new solution, and a video \n[Vadim Arzamasov](https://towardsdatascience.com/author/vadim-arzamasov/)\nAugust 16, 2024\n10 min read\n\n\n  * [YouTube](https://www.youtube.com/c/TowardsDataScience)\n  * [X](https://x.com/TDataScience)\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [Threads](https://www.threads.net/@towardsdatascience)\n  * [Bluesky](https://bsky.app/profile/towardsdatascience.com)\n\n\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nYour home for data science and Al. The world\u2019s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\n\u00a9  Insight Media Group, LLC 2025 \n  * [About](https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/)\n  * [Privacy Policy](https://towardsdatascience.com/privacy-policy/)\n  * [Terms of Use](https://towardsdatascience.com/website-terms-of-use/)\n\n\n[Towards Data Science is now independent!](https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/)\nCookies Settings\n## Sign up to our newsletter\n![Company Logo](https://cdn.cookielaw.org/logos/static/ot_company_logo.png)\n## Privacy Preference Center\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer. [More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nAllow All\n###  Manage Consent Preferences\n#### Functional Cookies\nFunctional Cookies Active\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\n#### Strictly Necessary Cookies\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n#### Performance Cookies\nPerformance Cookies Active\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n#### Targeting Cookies\nTargeting Cookies Active\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nBack Button\n### Cookie List\nSearch Icon\nFilter Icon\nClear\ncheckbox label label\nApply Cancel\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nReject All Confirm My Choices\n[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-consent/)\nSome areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe article discusses ORPO, a new fine-tuning technique for Llama 3 that merges supervised fine-tuning and preference alignment, enhancing efficiency and performance. Empirical results show ORPO outperforms traditional methods, making it a promising approach for adapting LLMs.\n\n**Tags:** Generative AI, LLMs, Fine-tuning, ORPO, Llama 3\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\nTransformers documentation \nTemplates\n# Transformers\n\ud83c\udfe1 View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersDistilabelEvaluateGradioHubHub Python LibraryHugging Face Generative AI Services (HUGS)Huggingface.jsInference API (serverless)Inference Endpoints (dedicated)LeaderboardsLightevalOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jssmolagentstimm\nSearch documentation\nmainv4.50.0v4.49.0v4.48.2v4.47.1v4.46.3v4.45.2v4.44.2v4.43.4v4.42.4v4.41.2v4.40.2v4.39.3v4.38.2v4.37.2v4.36.1v4.35.2v4.34.1v4.33.3v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-builder-html ARDEENESFRHIITJAKOPTTETRZH [ ](https://github.com/huggingface/transformers)\nGet started\n[Transformers ](https://huggingface.co/docs/transformers/en/index)[Installation ](https://huggingface.co/docs/transformers/en/installation)[Quickstart ](https://huggingface.co/docs/transformers/en/quicktour)\nBase classes\nInference\nPipeline API\nLLMs\nChat with models\n[Chat basics ](https://huggingface.co/docs/transformers/en/conversations)[Templates ](https://huggingface.co/docs/transformers/en/chat_templating)[Multimodal templates ](https://huggingface.co/docs/transformers/en/chat_templating_multimodal)[Template writing ](https://huggingface.co/docs/transformers/en/chat_templating_writing)[Tools and RAG ](https://huggingface.co/docs/transformers/en/chat_extras)\nOptimization\n[Agents ](https://huggingface.co/docs/transformers/en/agents)[Tools ](https://huggingface.co/docs/transformers/en/tools)\nTraining\nQuantization\nExport to production\nResources\nContribute\nAPI\n![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\nJoin the Hugging Face community\nand get access to the augmented documentation experience \nCollaborate on models, datasets and Spaces \nFaster examples with accelerated inference \nSwitch between documentation themes \n[Sign Up](https://huggingface.co/join)\nto get started\n# [](https://huggingface.co/docs/transformers/en/chat_templating#templates) Templates\nThe [chat pipeline](https://huggingface.co/docs/transformers/en/conversations) guide introduced [TextGenerationPipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.TextGenerationPipeline) and the concept of a chat prompt or chat template for conversing with a model. Underlying this high-level pipeline is the `apply_chat_template` method. A chat template is a part of the tokenizer and it specifies how to convert conversations into a single tokenizable string in the expected model format.\nIn the example below, Mistral-7B-Instruct and Zephyr-7B are finetuned from the same base model but they\u2019re trained with different chat formats. Without chat templates, you have to manually write formatting code for each model and even minor errors can hurt performance. Chat templates offer a universal way to format chat inputs to any model.\nMistral \nZephyr \nCopied\n```\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\nchat = [\n {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]\ntokenizer.apply_chat_template(chat, tokenize=False)\n```\n\nCopied\n```\n<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\n```\n\nThis guide explores `apply_chat_template` and chat templates in more detail.\n## [](https://huggingface.co/docs/transformers/en/chat_templating#applychattemplate) apply_chat_template\nChats should be structured as a list of dictionaries with `role` and `content` keys. The `role` key specifies the speaker (usually between you and the system), and the `content` key contains your message. For the system, the `content` is a high-level description of how the model should behave and respond when you\u2019re chatting with it.\nPass your messages to `apply_chat_template` to tokenize and format them. You can set [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` to indicate the start of a message.\nCopied\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n  {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n ]\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\nprint(tokenizer.decode(tokenized_chat[0]))\n```\n\nCopied\n```\n<|system|>\nYou are a friendly chatbot who always responds in the style of a pirate</s>\n<|user|>\nHow many helicopters can a human eat in one sitting?</s>\n<|assistant|>\n```\n\nNow pass the tokenized chat to [generate()](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/text_generation#transformers.GenerationMixin.generate) to generate a response.\nCopied\n```\noutputs = model.generate(tokenized_chat, max_new_tokens=128) \nprint(tokenizer.decode(outputs[0]))\n```\n\nCopied\n```\n<|system|>\nYou are a friendly chatbot who always responds in the style of a pirate</s>\n<|user|>\nHow many helicopters can a human eat in one sitting?</s>\n<|assistant|>\nMatey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n```\n\n### [](https://huggingface.co/docs/transformers/en/chat_templating#addgenerationprompt) add_generation_prompt\nThe [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) parameter adds tokens that indicate the start of a response. This ensures the chat model generates a system response instead of continuing a users message.\nNot all models require generation prompts, and some models, like [Llama](https://huggingface.co/docs/transformers/en/model_doc/llama), don\u2019t have any special tokens before the system response. In this case, [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) has no effect.\nCopied\n```\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\ntokenized_chat\n```\n\nCopied\n```\n<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n```\n\n### [](https://huggingface.co/docs/transformers/en/chat_templating#continuefinalmessage) continue_final_message\nThe [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) parameter controls whether the final message in the chat should be continued or not instead of starting a new one. It removes end of sequence tokens so that the model continues generation from the final message.\nThis is useful for \u201cprefilling\u201d a model response. In the example below, the model generates text that continues the JSON string rather than starting a new message. It can be very useful for improving the accuracy for instruction following when you know how to start its replies.\nCopied\n```\nchat = [\n  {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n  {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n]\nformatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)\nmodel.generate(**formatted_chat)\n```\n\nYou shouldn\u2019t use [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) and [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) together. The former adds tokens that start a new message, while the latter removes end of sequence tokens. Using them together returns an error.\n[TextGenerationPipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.TextGenerationPipeline) sets [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` by default to start a new message. However, if the final message in the chat has the \u201cassistant\u201d role, it assumes the message is a prefill and switches to `continue_final_message=True`. This is because most models don\u2019t support multiple consecutive assistant messages. To override this behavior, explicitly pass the [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) to the pipeline.\n## [](https://huggingface.co/docs/transformers/en/chat_templating#multiple-templates) Multiple templates\nA model may have several different templates for different use cases. For example, a model may have a template for regular chat, tool use, and RAG.\nWhen there are multiple templates, the chat template is a dictionary. Each key corresponds to the name of a template. `apply_chat_template` handles multiple templates based on their name. It looks for a template named `default` in most cases and if it can\u2019t find one, it raises an error.\nFor a tool calling template, if a user passes a `tools` parameter and a `tool_use` template exists, the tool calling template is used instead of `default`.\nTo access templates with other names, pass the template name to the `chat_template` parameter in `apply_chat_template`. For example, if you\u2019re using a RAG template then set `chat_template=\"rag\"`.\nIt can be confusing to manage multiple templates though, so we recommend using a single template for all use cases. Use Jinja statements like `if tools is defined` and `{% macro %}` definitions to wrap multiple code paths in a single template.\n## [](https://huggingface.co/docs/transformers/en/chat_templating#template-selection) Template selection\nIt is important to set a chat template format that matches the template format a model was pretrained on, otherwise performance may suffer. Even if you\u2019re training the model further, performance is best if the chat tokens are kept constant.\nBut if you\u2019re training a model from scratch or finetuning a model for chat, you have more options to select a template. For example, [ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md) is a popular format that is flexbile enough to handle many use cases. It even includes support for [generation prompts](https://huggingface.co/docs/transformers/en/chat_templating#add_generation_prompt), but it doesn\u2019t add beginning-of-string (`BOS`) or end-of-string (`EOS`) tokens. If your model expects `BOS` and `EOS` tokens, set `add_special_tokens=True` and make sure to add them to your template.\nCopied\n```\n{%- for message in messages %}\n  {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n{%- endfor %}\n```\n\nSet the template with the following logic to support [generation prompts](https://huggingface.co/docs/transformers/en/chat_templating#add_generation_prompt). The template wraps each message with `<|im_start|>` and `<|im_end|>` tokens and writes the role as a string. This allows you to easily customize the roles you want to train with.\nCopied\n```\ntokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n```\n\nThe `user`, `system` and `assistant` roles are standard roles in chat templates. We recommend using these roles when it makes sense, especially if you\u2019re using your model with the [TextGenerationPipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.TextGenerationPipeline).\nCopied\n```\n<|im_start|>system\nYou are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n<|im_start|>user\nHow are you?<|im_end|>\n<|im_start|>assistant\nI'm doing great!<|im_end|>\n```\n\n## [](https://huggingface.co/docs/transformers/en/chat_templating#model-training) Model training\nTraining a model with a chat template is a good way to ensure a chat template matches the tokens a model is trained on. Apply the chat template as a preprocessing step to your dataset. Set `add_generation_prompt=False` because the additional tokens to prompt an assistant response aren\u2019t helpful during training.\nAn example of preprocessing a dataset with a chat template is shown below.\nCopied\n```\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\nchat1 = [\n  {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n  {\"role\": \"assistant\", \"content\": \"The sun.\"}\n]\nchat2 = [\n  {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n  {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n]\ndataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\ndataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\nprint(dataset['formatted_chat'][0])\n```\n\nCopied\n```\n<|user|>\nWhich is bigger, the moon or the sun?</s>\n<|assistant|>\nThe sun.</s>\n```\n\nAfter this step, you can continue following the [training recipe](https://huggingface.co/docs/transformers/en/tasks/language_modeling) for causal language models using the `formatted_chat` column.\nSome tokenizers add special `<bos>` and `<eos>` tokens. Chat templates should already include all the necessary special tokens, and adding additional special tokens is often incorrect or duplicated, hurting model performance. When you format text with `apply_chat_template(tokenize=False)`, make sure you set `add_special_tokens=False` as well to avoid duplicating them.\nCopied\n```\napply_chat_template(messages, tokenize=False, add_special_tokens=False)\n```\n\nThis isn\u2019t an issue if `apply_chat_template(tokenize=True)`.\n[< > Update on GitHub](https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md)\n[\u2190Chat basics](https://huggingface.co/docs/transformers/en/conversations) [Multimodal templates\u2192](https://huggingface.co/docs/transformers/en/chat_templating_multimodal)\n[Templates](https://huggingface.co/docs/transformers/en/chat_templating#templates) [apply_chat_template](https://huggingface.co/docs/transformers/en/chat_templating#applychattemplate) [add_generation_prompt](https://huggingface.co/docs/transformers/en/chat_templating#addgenerationprompt) [continue_final_message](https://huggingface.co/docs/transformers/en/chat_templating#continuefinalmessage) [Multiple templates](https://huggingface.co/docs/transformers/en/chat_templating#multiple-templates) [Template selection](https://huggingface.co/docs/transformers/en/chat_templating#template-selection) [Model training](https://huggingface.co/docs/transformers/en/chat_templating#model-training)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe Hugging Face Transformers documentation details the use of chat templates for LLMs, emphasizing the `apply_chat_template` method for formatting conversations. It covers template selection, training models with chat templates, and optimizing chat interactions. Proper template usage enhances model performance and response accuracy.\n```"
  },
  {
    "instruction": "[Skip to main content](https://learn.microsoft.com/en-us/collections/o1njfe825p602p/#main)\n## AI Skills Fest\nApril 8 \u2013 May 28, 2025\nDeepen your AI skills over 50 days of learning with deep dives, hackathons, and more. \n[ Register now ](https://aka.ms/AISkillsFest_LearnPromoBanner)\nDismiss alert\nThis browser is no longer supported.\nUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\n[Download Microsoft Edge](https://go.microsoft.com/fwlink/p/?LinkID=2092881 ) [More info about Internet Explorer and Microsoft Edge](https://learn.microsoft.com/en-us/lifecycle/faq/internet-explorer-microsoft-edge)\n[ Learn ](https://learn.microsoft.com/en-us/) [ ](https://www.microsoft.com)\nSuggestions will filter as you type \n[ Sign in ](https://learn.microsoft.com/en-us/collections/o1njfe825p602p/)\n  * [ Profile ](https://learn.microsoft.com/en-us/users/me/activity/)\n  * [ Settings ](https://learn.microsoft.com/en-us/users/me/settings/)\n\n\n[ Sign out ](https://learn.microsoft.com/en-us/collections/o1njfe825p602p/)\n[ ](https://www.microsoft.com) [ Learn ](https://learn.microsoft.com/en-us/)\n  * Discover\n    * [ Documentation ](https://learn.microsoft.com/en-us/docs/)\nIn-depth articles on Microsoft developer tools and technologies\n    * [ Training ](https://learn.microsoft.com/en-us/training/)\nPersonalized learning paths and courses\n    * [ Credentials ](https://learn.microsoft.com/en-us/credentials/)\nGlobally recognized, industry-endorsed credentials\n    * [ Q&A ](https://learn.microsoft.com/en-us/answers/)\nTechnical questions and answers moderated by Microsoft\n    * [ Code Samples ](https://learn.microsoft.com/en-us/samples/)\nCode sample library for Microsoft developer tools and technologies\n    * [ Assessments ](https://learn.microsoft.com/en-us/assessments/)\nInteractive, curated guidance and recommendations\n    * [ Shows ](https://learn.microsoft.com/en-us/shows/)\nThousands of hours of original programming from Microsoft experts\nMicrosoft AI Skills Fest\n[ Register Now! ](https://aka.ms/AISkillsFest_MSLearn)\nApril 8 - May 28, 2025\n  * Product documentation\n    * [ ASP.NET ](https://learn.microsoft.com/en-us/aspnet/)\n    * [ Azure ](https://learn.microsoft.com/en-us/azure/)\n    * [ Dynamics 365 ](https://learn.microsoft.com/en-us/dynamics365/)\n    * [ Microsoft 365 ](https://learn.microsoft.com/en-us/microsoft-365/)\n    * [ Microsoft Copilot ](https://learn.microsoft.com/en-us/copilot/)\n    * [ Microsoft Edge ](https://learn.microsoft.com/en-us/microsoft-edge/)\n    * [ Microsoft Entra ](https://learn.microsoft.com/en-us/entra/)\n    * [ Microsoft Graph ](https://learn.microsoft.com/en-us/graph/)\n    * [ Microsoft Intune ](https://learn.microsoft.com/en-us/mem/)\n    * [ Microsoft Purview ](https://learn.microsoft.com/en-us/purview/)\n    * [ Microsoft Teams ](https://learn.microsoft.com/en-us/microsoftteams/)\n    * [ .NET ](https://learn.microsoft.com/en-us/dotnet/)\n    * [ Power Apps ](https://learn.microsoft.com/en-us/power-apps/)\n    * [ Power BI ](https://learn.microsoft.com/en-us/power-bi/)\n    * [ Power Platform ](https://learn.microsoft.com/en-us/power-platform/)\n    * [ PowerShell ](https://learn.microsoft.com/en-us/powershell/)\n    * [ SQL ](https://learn.microsoft.com/en-us/sql/)\n    * [ Sysinternals ](https://learn.microsoft.com/en-us/sysinternals/)\n    * [ Visual Studio ](https://learn.microsoft.com/en-us/visualstudio/)\n    * [ Windows ](https://learn.microsoft.com/en-us/windows/)\n    * [ Windows Server ](https://learn.microsoft.com/en-us/windows-server/)\n[ View all products ](https://learn.microsoft.com/en-us/docs/)\nMicrosoft AI Skills Fest\n[ Register Now! ](https://aka.ms/AISkillsFest_MSLearn)\nApril 8 - May 28, 2025\n  * Development languages\n    * [ C++ ](https://learn.microsoft.com/en-us/cpp/)\n    * [ C# ](https://learn.microsoft.com/en-us/dotnet/csharp/)\n    * [ DAX ](https://learn.microsoft.com/en-us/dax/)\n    * [ Java ](https://learn.microsoft.com/en-us/java/)\n    * [ OData ](https://learn.microsoft.com/en-us/odata/)\n    * [ OpenAPI ](https://learn.microsoft.com/en-us/openapi/)\n    * [ Power Query M ](https://learn.microsoft.com/en-us/powerquery-m/)\n    * [ VBA ](https://learn.microsoft.com/en-us/office/vba/)\nMicrosoft AI Skills Fest\n[ Register Now! ](https://aka.ms/AISkillsFest_MSLearn)\nApril 8 - May 28, 2025\n  * Topics\n    * [ Learn for Organizations ](https://learn.microsoft.com/en-us/training/organizations/)\n    * [ Artificial intelligence ](https://learn.microsoft.com/en-us/ai/)\n    * [ Compliance ](https://learn.microsoft.com/en-us/compliance/)\n    * [ DevOps ](https://learn.microsoft.com/en-us/devops/)\n    * [ Platform engineering ](https://learn.microsoft.com/en-us/platform-engineering/)\n    * [ Security ](https://learn.microsoft.com/en-us/security/)\nMicrosoft AI Skills Fest\n[ Register Now! ](https://aka.ms/AISkillsFest_MSLearn)\nApril 8 - May 28, 2025\n\n\nSuggestions will filter as you type \n[ Sign in ](https://learn.microsoft.com/en-us/collections/o1njfe825p602p/)\n  * [ Profile ](https://learn.microsoft.com/en-us/users/me/activity/)\n  * [ Settings ](https://learn.microsoft.com/en-us/users/me/settings/)\n\n\n[ Sign out ](https://learn.microsoft.com/en-us/collections/o1njfe825p602p/)\nTable of contents Exit focus mode\nTable of contents\n[ English (United States)](https://learn.microsoft.com/en-us/locale?target=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fcollections%2Fo1njfe825p602p%2F)\n[ Your Privacy Choices ](https://aka.ms/yourcaliforniaprivacychoices)\nTheme\n  * Light\n  * Dark\n  * High contrast\n\n\n  * [Previous Versions](https://learn.microsoft.com/en-us/previous-versions/)\n  * [Blog](https://techcommunity.microsoft.com/t5/microsoft-learn-blog/bg-p/MicrosoftLearnBlog)\n  * [Contribute](https://learn.microsoft.com/en-us/contribute/)\n  * [Privacy](https://go.microsoft.com/fwlink/?LinkId=521839)\n  * [Terms of Use](https://learn.microsoft.com/en-us/legal/termsofuse)\n  * [Trademarks](https://www.microsoft.com/legal/intellectualproperty/Trademarks/)\n  * \u00a9 Microsoft 2025\n\n\n[ English (United States)](https://learn.microsoft.com/en-us/locale?target=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fcollections%2Fo1njfe825p602p%2F)\n[ Your Privacy Choices ](https://aka.ms/yourcaliforniaprivacychoices)\nTheme\n  * Light\n  * Dark\n  * High contrast\n\n\n  * [Previous Versions](https://learn.microsoft.com/en-us/previous-versions/)\n  * [Blog](https://techcommunity.microsoft.com/t5/microsoft-learn-blog/bg-p/MicrosoftLearnBlog)\n  * [Contribute](https://learn.microsoft.com/en-us/contribute/)\n  * [Privacy](https://go.microsoft.com/fwlink/?LinkId=521839)\n  * [Terms of Use](https://learn.microsoft.com/en-us/legal/termsofuse)\n  * [Trademarks](https://www.microsoft.com/legal/intellectualproperty/Trademarks/)\n  * \u00a9 Microsoft 2025\n\n\n",
    "answer": "```markdown\n## TL;DR Summary\n\n**Event:** Microsoft AI Skills Fest  \n**Dates:** April 8 \u2013 May 28, 2025  \n**Overview:** A 50-day learning event featuring deep dives, hackathons, and training focused on AI skills.  \n**Registration:** [Register now](https://aka.ms/AISkillsFest_LearnPromoBanner)  \n**Topics Covered:** AI, DevOps, Security, Compliance, and more.\n\n**Tags:** #GenerativeAI #AI #LLMs #Microsoft\n```"
  },
  {
    "instruction": "[ Skip to content ](https://madewithml.com/courses/mlops/testing/#intuition)\n\u26a1\ufe0f Checkout our new [End-to-end LLM Workflows Guide](https://www.anyscale.com/blog/end-to-end-llm-workflows-guide)\n[ ![logo](https://madewithml.com/static/images/logo.png) ](https://madewithml.com/ \"Made With ML by Anyscale\")\nMade With ML by Anyscale \nTesting Machine Learning Systems: Code, Data and Models \nInitializing search \n[ GokuMohandas/MadeWithML \n  * 38.4k\n  * 6.1k\n\n](https://github.com/GokuMohandas/Made-With-ML \"Go to repository\")\n  * [ Home ](https://madewithml.com/)\n  * [ About ](https://madewithml.com/about/)\n  * [ Course ](https://madewithml.com/#course)\n  * [ Foundations ](https://madewithml.com/courses/foundations/)\n  * [ Subscribe ](https://madewithml.com/misc/newsletter/)\n  * [ Community ](https://discord.com/channels/1078171187609337896/1078171189169635472)\n\n\n[ ![logo](https://madewithml.com/static/images/logo.png) ](https://madewithml.com/ \"Made With ML by Anyscale\") Made With ML by Anyscale \n[ GokuMohandas/MadeWithML \n  * 38.4k\n  * 6.1k\n\n](https://github.com/GokuMohandas/Made-With-ML \"Go to repository\")\n  * [ Home ](https://madewithml.com/)\n  * [ About ](https://madewithml.com/about/)\n  * Course  Course \n    * [ Lessons ](https://madewithml.com/#course)\n    * \ud83c\udfa8 Design  \ud83c\udfa8 Design \n      * [ Setup ](https://madewithml.com/courses/mlops/setup/)\n      * [ Product ](https://madewithml.com/courses/mlops/product-design/)\n      * [ Systems ](https://madewithml.com/courses/mlops/systems-design/)\n    * \ud83d\udd22 Data  \ud83d\udd22 Data \n      * [ Preparation ](https://madewithml.com/courses/mlops/preparation/)\n      * [ Exploration ](https://madewithml.com/courses/mlops/exploratory-data-analysis/)\n      * [ Preprocessing ](https://madewithml.com/courses/mlops/preprocessing/)\n      * [ Distributed ](https://madewithml.com/courses/mlops/distributed-data/)\n    * \ud83d\udcc8 Model  \ud83d\udcc8 Model \n      * [ Training ](https://madewithml.com/courses/mlops/training/)\n      * [ Tracking ](https://madewithml.com/courses/mlops/experiment-tracking/)\n      * [ Tuning ](https://madewithml.com/courses/mlops/tuning/)\n      * [ Evaluation ](https://madewithml.com/courses/mlops/evaluation/)\n      * [ Serving ](https://madewithml.com/courses/mlops/serving/)\n    * \ud83d\udcbb Developing  \ud83d\udcbb Developing \n      * [ Scripting ](https://madewithml.com/courses/mlops/scripting/)\n      * [ CLI ](https://madewithml.com/courses/mlops/cli/)\n    * \ud83d\udce6 Utilities  \ud83d\udce6 Utilities \n      * [ Logging ](https://madewithml.com/courses/mlops/logging/)\n      * [ Documentation ](https://madewithml.com/courses/mlops/documentation/)\n      * [ Styling ](https://madewithml.com/courses/mlops/styling/)\n      * [ Pre-commit ](https://madewithml.com/courses/mlops/pre-commit/)\n    * \u2705 Testing  \u2705 Testing \n      * Code  [ Code ](https://madewithml.com/courses/mlops/testing/) Table of contents \n        * [ Intuition ](https://madewithml.com/courses/mlops/testing/#intuition)\n          * [ Types of tests ](https://madewithml.com/courses/mlops/testing/#types-of-tests)\n          * [ How should we test? ](https://madewithml.com/courses/mlops/testing/#how-should-we-test)\n          * [ What should we test? ](https://madewithml.com/courses/mlops/testing/#what-should-we-test)\n        * [ Best practices ](https://madewithml.com/courses/mlops/testing/#best-practices)\n        * [ Implementation ](https://madewithml.com/courses/mlops/testing/#implementation)\n        * [ \ud83d\udcbb Code ](https://madewithml.com/courses/mlops/testing/#code)\n          * [ Configuration ](https://madewithml.com/courses/mlops/testing/#configuration)\n          * [ Assertions ](https://madewithml.com/courses/mlops/testing/#assertions)\n          * [ Execution ](https://madewithml.com/courses/mlops/testing/#execution)\n          * [ Classes ](https://madewithml.com/courses/mlops/testing/#classes)\n          * [ Parametrize ](https://madewithml.com/courses/mlops/testing/#parametrize)\n          * [ Fixtures ](https://madewithml.com/courses/mlops/testing/#fixtures)\n          * [ Markers ](https://madewithml.com/courses/mlops/testing/#markers)\n          * [ Coverage ](https://madewithml.com/courses/mlops/testing/#coverage)\n          * [ Exclusions ](https://madewithml.com/courses/mlops/testing/#exclusions)\n        * [ \ud83d\udd22 Data ](https://madewithml.com/courses/mlops/testing/#data)\n          * [ Expectations ](https://madewithml.com/courses/mlops/testing/#expectations)\n          * [ Suite ](https://madewithml.com/courses/mlops/testing/#suite)\n          * [ Production ](https://madewithml.com/courses/mlops/testing/#production)\n        * [ \ud83e\udd16 Models ](https://madewithml.com/courses/mlops/testing/#models)\n          * [ Training ](https://madewithml.com/courses/mlops/testing/#training)\n          * [ Behavioral testing ](https://madewithml.com/courses/mlops/testing/#behavioral-testing)\n        * [ Testing vs. monitoring ](https://madewithml.com/courses/mlops/testing/#testing-vs-monitoring)\n        * [ Resources ](https://madewithml.com/courses/mlops/testing/#resources)\n      * [ Data ](https://madewithml.com/courses/mlops/testing/#data)\n      * [ Models ](https://madewithml.com/courses/mlops/testing/#models)\n    * \u267b\ufe0f Reproducibility  \u267b\ufe0f Reproducibility \n      * [ Versioning ](https://madewithml.com/courses/mlops/versioning/)\n    * \ud83d\ude80 Production  \ud83d\ude80 Production \n      * [ Jobs & Services ](https://madewithml.com/courses/mlops/jobs-and-services/)\n      * [ CI/CD workflows ](https://madewithml.com/courses/mlops/cicd/)\n      * [ Monitoring ](https://madewithml.com/courses/mlops/monitoring/)\n      * [ Data engineering ](https://madewithml.com/courses/mlops/data-engineering/)\n  * Foundations  Foundations \n    * [ Lessons ](https://madewithml.com/courses/foundations/)\n    * \ud83d\udee0 Toolkit  \ud83d\udee0 Toolkit \n      * [ Notebooks ](https://madewithml.com/courses/foundations/notebooks/)\n      * [ Python ](https://madewithml.com/courses/foundations/python/)\n      * [ NumPy ](https://madewithml.com/courses/foundations/numpy/)\n      * [ Pandas ](https://madewithml.com/courses/foundations/pandas/)\n      * [ PyTorch ](https://madewithml.com/courses/foundations/pytorch/)\n    * \ud83d\udd25 Machine Learning  \ud83d\udd25 Machine Learning \n      * [ Linear regression ](https://madewithml.com/courses/foundations/linear-regression/)\n      * [ Logistic regression ](https://madewithml.com/courses/foundations/logistic-regression/)\n      * [ Neural networks ](https://madewithml.com/courses/foundations/neural-networks/)\n      * [ Data quality ](https://madewithml.com/courses/foundations/data-quality/)\n      * [ Utilities ](https://madewithml.com/courses/foundations/utilities/)\n    * \ud83e\udd16 Deep Learning  \ud83e\udd16 Deep Learning \n      * [ CNNs ](https://madewithml.com/courses/foundations/convolutional-neural-networks/)\n      * [ Embeddings ](https://madewithml.com/courses/foundations/embeddings/)\n      * [ RNNs ](https://madewithml.com/courses/foundations/recurrent-neural-networks/)\n      * [ Attention ](https://madewithml.com/courses/foundations/attention/)\n      * [ Transformers ](https://madewithml.com/courses/foundations/transformers/)\n  * [ Subscribe ](https://madewithml.com/misc/newsletter/)\n  * [ Community ](https://discord.com/channels/1078171187609337896/1078171189169635472)\n\n\nTable of contents \n  * [ Intuition ](https://madewithml.com/courses/mlops/testing/#intuition)\n    * [ Types of tests ](https://madewithml.com/courses/mlops/testing/#types-of-tests)\n    * [ How should we test? ](https://madewithml.com/courses/mlops/testing/#how-should-we-test)\n    * [ What should we test? ](https://madewithml.com/courses/mlops/testing/#what-should-we-test)\n  * [ Best practices ](https://madewithml.com/courses/mlops/testing/#best-practices)\n  * [ Implementation ](https://madewithml.com/courses/mlops/testing/#implementation)\n  * [ \ud83d\udcbb Code ](https://madewithml.com/courses/mlops/testing/#code)\n    * [ Configuration ](https://madewithml.com/courses/mlops/testing/#configuration)\n    * [ Assertions ](https://madewithml.com/courses/mlops/testing/#assertions)\n    * [ Execution ](https://madewithml.com/courses/mlops/testing/#execution)\n    * [ Classes ](https://madewithml.com/courses/mlops/testing/#classes)\n    * [ Parametrize ](https://madewithml.com/courses/mlops/testing/#parametrize)\n    * [ Fixtures ](https://madewithml.com/courses/mlops/testing/#fixtures)\n    * [ Markers ](https://madewithml.com/courses/mlops/testing/#markers)\n    * [ Coverage ](https://madewithml.com/courses/mlops/testing/#coverage)\n    * [ Exclusions ](https://madewithml.com/courses/mlops/testing/#exclusions)\n  * [ \ud83d\udd22 Data ](https://madewithml.com/courses/mlops/testing/#data)\n    * [ Expectations ](https://madewithml.com/courses/mlops/testing/#expectations)\n    * [ Suite ](https://madewithml.com/courses/mlops/testing/#suite)\n    * [ Production ](https://madewithml.com/courses/mlops/testing/#production)\n  * [ \ud83e\udd16 Models ](https://madewithml.com/courses/mlops/testing/#models)\n    * [ Training ](https://madewithml.com/courses/mlops/testing/#training)\n    * [ Behavioral testing ](https://madewithml.com/courses/mlops/testing/#behavioral-testing)\n  * [ Testing vs. monitoring ](https://madewithml.com/courses/mlops/testing/#testing-vs-monitoring)\n  * [ Resources ](https://madewithml.com/courses/mlops/testing/#resources)\n\n\n# Testing Machine Learning Systems: Code, Data and Models\n[ View all lessons](https://madewithml.com/courses/mlops)\nLearn how to test ML artifacts (code, data and models) to ensure a reliable ML system.\n![Goku Mohandas](https://madewithml.com/static/images/goku_circle.png)\nGoku Mohandas\n[ ](https://twitter.com/GokuMohandas) \u00b7 [ ](https://linkedin.com/in/goku) \u00b7 [ ](https://github.com/gokumohandas) \u00b7 [ ](https://www.youtube.com/c/MadeWithML)\n[Repository](https://github.com/GokuMohandas/Made-With-ML) \u00b7  [Notebook](https://github.com/GokuMohandas/testing-ml/blob/main/testing.ipynb)\n\u00d7\nSubscribe to our newsletter\n\ud83d\udcec Receive new lessons straight to your inbox (once a month) and join **40K+** developers in learning how to responsibly deliver value with ML.\nSubscribe\n## Intuition\nIn this lesson, we'll learn how to test code, data and machine learning models to construct a machine learning system that we can reliably iterate on. Tests are a way for us to ensure that something works as intended. We're incentivized to implement tests and discover sources of error as early in the development cycle as possible so that we can decrease [downstream costs](https://assets.deepsource.io/39ed384/images/blog/cost-of-fixing-bugs/chart.jpg) and wasted time. Once we've designed our tests, we can automatically execute them every time we change or add to our codebase.\nTip\nWe highly recommend that you explore this lesson _after_ completing the previous lessons since the topics (and code) are iteratively developed. We did, however, create the [testing-ml](https://github.com/GokuMohandas/testing-ml) repository for a quick overview with an interactive notebook.\n### Types of tests\nThere are four majors types of tests which are utilized at different points in the development cycle:\n  1. `Unittests`: tests on individual components that each have a [single responsibility](https://en.wikipedia.org/wiki/Single-responsibility_principle) (ex. function that filters a list).\n  2. `Integrationtests`: tests on the combined functionality of individual components (ex. data processing).\n  3. `Systemtests`: tests on the design of a system for expected outputs given inputs (ex. training, inference, etc.).\n  4. `Acceptancetests`: tests to verify that requirements have been met, usually referred to as User Acceptance Testing (UAT).\n  5. `Regressiontests`: tests based on errors we've seen before to ensure new changes don't reintroduce them.\n\n\nWhile ML systems are probabilistic in nature, they are composed of many deterministic components that can be tested in a similar manner as traditional software systems. The distinction between testing ML systems begins when we move from testing code to testing the [data](https://madewithml.com/courses/mlops/testing/#data) and [models](https://madewithml.com/courses/mlops/testing/#models).\n![types of tests](https://madewithml.com/static/images/mlops/testing/tests.png)\n> There are many other types of functional and non-functional tests as well, such as smoke tests (quick health checks), performance tests (load, stress), security tests, etc. but we can generalize all of these under the system tests above.\n### How should we test?\nThe framework to use when composing tests is the [Arrange Act Assert](http://wiki.c2.com/?ArrangeActAssert) methodology.\n  * `Arrange`: set up the different inputs to test on.\n  * `Act`: apply the inputs on the component we want to test.\n  * `Assert`: confirm that we received the expected output.\n\n\n> `Cleaning` is an unofficial fourth step to this methodology because it's important to not leave remnants of a previous test which may affect subsequent tests. We can use packages such as [pytest-randomly](https://github.com/pytest-dev/pytest-randomly) to test against state dependency by executing tests randomly.\nIn Python, there are many tools, such as [unittest](https://docs.python.org/3/library/unittest.html), [pytest](https://docs.pytest.org/en/stable/), etc. that allow us to easily implement our tests while adhering to the _Arrange Act Assert_ framework. These tools come with powerful built-in functionality such as parametrization, filters, and more, to test many conditions at scale.\n### What should we test?\nWhen _arranging_ our inputs and _asserting_ our expected outputs, what are some aspects of our inputs and outputs that we should be testing for?\n  * **inputs** : data types, format, length, edge cases (min/max, small/large, etc.)\n  * **outputs** : data types, formats, exceptions, intermediary and final outputs\n\n\n> \ud83d\udc49 We'll cover specific details pertaining to what to test for regarding our [data](https://madewithml.com/courses/mlops/testing/#data) and [models](https://madewithml.com/courses/mlops/testing/#models) below.\n## Best practices\nRegardless of the framework we use, it's important to strongly tie testing into the development process.\n  * `atomic`: when creating functions and classes, we need to ensure that they have a [single responsibility](https://en.wikipedia.org/wiki/Single-responsibility_principle) so that we can easily test them. If not, we'll need to split them into more granular components.\n  * `compose`: when we create new components, we want to compose tests to validate their functionality. It's a great way to ensure reliability and catch errors early on.\n  * `reuse`: we should maintain central repositories where core functionality is tested at the source and reused across many projects. This significantly reduces testing efforts for each new project's code base.\n  * `regression`: we want to account for new errors we come across with a regression test so we can ensure we don't reintroduce the same errors in the future.\n  * `coverage`: we want to ensure [100% coverage](https://madewithml.com/courses/mlops/testing/#coverage) for our codebase. This doesn't mean writing a test for every single line of code but rather accounting for every single line.\n  * `automate`: in the event we forget to run our tests before committing to a repository, we want to auto run tests when we make changes to our codebase. We'll learn how to do this locally using [pre-commit hooks](https://madewithml.com/courses/mlops/pre-commit/) and remotely via [GitHub actions](https://madewithml.com/courses/mlops/cicd/#github-actions) in subsequent lessons.\n\n\n## Implementation\nIn our [codebase](https://github.com/GokuMohandas/Made-With-ML), we'll be testing the [code](https://madewithml.com/courses/mlops/testing/#\ud83d\udcbbnbsp-code), [data](https://madewithml.com/courses/mlops/testing/#\ud83d\udd22nbsp-data) and [models](https://madewithml.com/courses/mlops/testing/#\ud83e\udd16nbsp-models).\n```\ntests/\n\u251c\u2500\u2500code/\n\u2502\u251c\u2500\u2500conftest.py\n\u2502\u251c\u2500\u2500test_data.py\n\u2502\u251c\u2500\u2500test_predict.py\n\u2502\u251c\u2500\u2500test_train.py\n\u2502\u251c\u2500\u2500test_tune.py\n\u2502\u251c\u2500\u2500test_utils.py\n\u2502\u2514\u2500\u2500utils.py\n\u251c\u2500\u2500data/\n\u2502\u251c\u2500\u2500conftest.py\n\u2502\u2514\u2500\u2500test_dataset.py\n\u2514\u2500\u2500models/\n\u2502\u251c\u2500\u2500conftest.py\n\u2502\u2514\u2500\u2500test_behavioral.py\n\n```\n\n> Note that we aren't testing `evaluate.py` and `serve.py` because it involves complicated testing that's based on the data and models. We'll be testing these components as part of our integration tests when we test our system end-to-end.\n## \ud83d\udcbb Code\nWe'll start by testing our code and we'll use [pytest](https://docs.pytest.org/en/stable/) as our testing framework for it's powerful builtin features such as [parametrization](https://madewithml.com/courses/mlops/testing/#parametrize), [fixtures](https://madewithml.com/courses/mlops/testing/#fixtures), [markers](https://madewithml.com/courses/mlops/testing/#markers) and more.\n### Configuration\nPytest expects tests to be organized under a `tests` directory by default. However, we can also add to our existing `pyproject.toml` file to configure any other test directories as well. Once in the directory, pytest looks for python scripts starting with `tests_*.py` but we can configure it to read any other file patterns as well.\n```\n1\n2\n3\n4\n```\n| ```\n# Pytest\n[tool.pytest.ini_options]\ntestpaths=[\"tests\"]\npython_files=\"test_*.py\"\n\n```\n  \n---|---  \n### Assertions\nLet's see what a sample test and it's results look like. Assume we have a simple function that decodes a list of indices into their respective classes using a dictionary mapping.\n```\n1\n2\n3\n```\n| ```\n# madewithml/predict.py\ndefdecode(indices: Iterable[Any], index_to_class: Dict) -> List:\n  return [index_to_class[index] for index in indices]\n\n```\n  \n---|---  \nTo test this function, we can use [assert statements](https://docs.pytest.org/en/stable/assert.html) to map inputs with expected outputs. The statement following the word `assert` must return `True`.\n```\n1\n2\n3\n4\n5\n6\n```\n| ```\n# tests/code/test_predict.py\ndeftest_decode():\ndecoded = predict.decode(\n  indices=[0, 1, 1],\n  index_to_class={0: \"x\", 1: \"y\"})\nassert decoded == [\"x\", \"y\", \"y\"]\n\n```\n  \n---|---  \n> We can also have assertions about [exceptions](https://docs.pytest.org/en/stable/assert.html#assertions-about-expected-exceptions) like we do in lines 6-8 where all the operations under the with statement are expected to raise the specified exception.\n### Execution\nWe can execute our tests above using several different levels of granularity:\n```\npython3-mpytest# all tests\npython3-mpytesttests/code# tests under a directory\npython3-mpytesttests/code/test_predict.py# tests for a single file\npython3-mpytesttests/code/test_predict.py::test_decode# tests for a single function\n\n```\n\nRunning our specific test above would produce the following output: \n```\npython3-mpytesttests/code/test_predict.py::test_decode--verbose--disable-warnings\n\n```\n\n```\ntests/code/test_predict.py::test_decode PASSED      [100%]\n\n```\n\nHad any of our assertions in this test failed, we would see the failed assertions, along with the expected and actual output from our function.\n```\ntests/code/test_predict.py::test_decode FAILED             [100%]\n  def test_decode():\n    decoded = predict.decode(\n      indices=[0, 1, 1],\n      index_to_class={0: \"x\", 1: \"y\"})\n>    assert decoded == [\"x\", \"x\", \"y\"]\nE    AssertionError: assert ['x', 'y', 'y'] == ['x', 'x', 'y']\nE    At index 1 diff: 'y' != 'x'\n\n```\n\nTip\nIt's important to test for the variety of inputs and expected outputs that we outlined [above](https://madewithml.com/courses/mlops/testing/#how-should-we-test) and to **never assume that a test is trivial**. In our example above, it's important that we test for both \"apple\" and \"Apple\" in the event that our function didn't account for casing!\n### Classes\nWe can also test classes and their respective functions.\n```\n1\n2\n3\n4\n5\n```\n| ```\ndeftest_fit_transform():\n  preprocessor = data.CustomPreprocessor()\n  ds = data.load_data(dataset_loc=\"...\")\n  preprocessor.fit_transform(ds)\n  assert len(preprocessor.class_to_index) == 4\n\n```\n  \n---|---  \n> There are also more [xunit-style testing](https://docs.pytest.org/en/stable/xunit_setup.html) options available as well for more involved testing with classes.\n### Parametrize\nSo far, in our tests, we've had to create individual assert statements to validate different combinations of inputs and expected outputs. However, there's a bit of redundancy here because the inputs always feed into our functions as arguments and the outputs are compared with our expected outputs. To remove this redundancy, pytest has the [`@pytest.mark.parametrize`](https://docs.pytest.org/en/stable/parametrize.html) decorator which allows us to represent our inputs and outputs as parameters.\n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n```\n| ```\n@pytest.mark.parametrize(\n  \"text, sw, clean_text\",\n  [\n    (\"hi\", [], \"hi\"),\n    (\"hi you\", [\"you\"], \"hi\"),\n    (\"hi yous\", [\"you\"], \"hi yous\"),\n  ],\n)\ndeftest_clean_text(text, sw, clean_text):\n  assert data.clean_text(text=text, stopwords=sw) == clean_text\n\n```\n  \n---|---  \n  1. `[Line2]`: define the names of the parameters under the decorator, ex. \"fruit, crisp\" (note that this is one string).\n  2. `[Lines3-7]`: provide a list of combinations of values for the parameters from Step 1.\n  3. `[Line9]`: pass in parameter names to the test function.\n  4. `[Line10]`: include necessary assert statements which will be executed for each of the combinations in the list from Step 2.\n\n\nSimilarly, we could pass in an exception as the expected result as well:\n```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n```\n| ```\n@pytest.mark.parametrize(\n  \"x, exception\",\n  [\n    (1, ValueError),\n  ],\n)\ndeftest_foo(x, exception):\n  with pytest.raises(exception):\n    foo(x=x)\n\n```\n  \n---|---  \n### Fixtures\nParametrization allows us to reduce redundancy inside test functions but what about reducing redundancy across different test functions? For example, suppose that different test functions all have a common component (ex. preprocessor). Here, we can use pytest's builtin [fixture](https://docs.pytest.org/en/stable/fixture.html), which is a function that is executed before the test function. Let's rewrite our `test_fit_transform` function from above using a fixture:\n```\n1\n2\n3\n4\n```\n| ```\ndeftest_fit_transform(dataset_loc, preprocessor):\n  ds = data.load_data(dataset_loc=dataset_loc)\n  preprocessor.fit_transform(ds)\n  assert len(preprocessor.class_to_index) == 4\n\n```\n  \n---|---  \nwhere `dataset_loc` and `preprocessor` are fixtures defined in our `tests/code/conftest.py` script:\n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n```\n| ```\n# tests/code/conftest.py\nimportpytest\nfrommadewithml.dataimport CustomPreprocessor\n@pytest.fixture\ndefdataset_loc():\n  return \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\n@pytest.fixture\ndefpreprocessor():\n  return CustomPreprocessor()\n\n```\n  \n---|---  \nAll of our test scripts know to look inside a `conftest.py` script in the same directory for any fixtures. Note that the name of the fixture and the input argument to our function have to be the same.\nFixture scopes\nFixtures can have different scopes depending on how we want to use them. For example our `df` fixture has the module scope because we don't want to keep recreating it after every test but, instead, we want to create it just once for all the tests in our module (`tests/test_data.py`).\n  * `function`: fixture is destroyed after every test. `[default]`\n  * `class`: fixture is destroyed after the last test in the class.\n  * `module`: fixture is destroyed after the last test in the module (script).\n  * `package`: fixture is destroyed after the last test in the package.\n  * `session`: fixture is destroyed after the last test of the session.\n\n\n### Markers\nWe've been able to execute our tests at various levels of granularity (all tests, script, function, etc.) but we can create custom granularity by using [markers](https://docs.pytest.org/en/stable/mark.html). We've already used one type of marker (parametrize) but there are several other [builtin markers](https://docs.pytest.org/en/stable/mark.html#mark) as well. For example, the [`skipif`](https://docs.pytest.org/en/stable/skipping.html#id1) marker allows us to skip execution of a test if a condition is met. For example, supposed we only wanted to test training our model if a GPU is available:\n```\n1\n2\n3\n4\n5\n6\n```\n| ```\n@pytest.mark.skipif(\n  not torch.cuda.is_available(),\n  reason=\"Full training tests require a GPU.\"\n)\ndeftest_training():\n  pass\n\n```\n  \n---|---  \nWe can also create our own custom markers with the exception of a few [reserved](https://docs.pytest.org/en/stable/reference.html#marks) marker names.\n```\n1\n2\n3\n```\n| ```\n@pytest.mark.training\ndeftest_train_model(dataset_loc):\n  pass\n\n```\n  \n---|---  \nWe can execute them by using the `-m` flag which requires a (case-sensitive) marker expression like below:\n```\npytest-m\"training\"# runs all tests marked with `training`\npytest-m\"not training\"# runs all tests besides those marked with `training`\n\n```\n\nTip\nThe proper way to use markers is to explicitly list the ones we've created in our [pyproject.toml](https://github.com/GokuMohandas/Made-With-ML/blob/main/pyproject.toml) file. Here we can specify that all markers must be defined in this file with the `--strict-markers` flag and then declare our markers (with some info about them) in our `markers` list:\n```\n1\n2\n3\n```\n| ```\n@pytest.mark.training\ndeftest_train_model():\n  assert ...\n\n```\n  \n---|---  \n```\n1\n2\n3\n4\n5\n6\n7\n8\n```\n| ```\n# Pytest\n[tool.pytest.ini_options]\ntestpaths=[\"tests\"]\npython_files=\"test_*.py\"\naddopts=\"--strict-markers --disable-pytest-warnings\"\nmarkers=[\n\"training: tests that involve training\",\n]\n\n```\n  \n---|---  \nOnce we do this, we can view all of our existing list of markers by executing `pytest--markers` and we'll receive an error when we're trying to use a new marker that's not defined here. \n### Coverage\nAs we're developing tests for our application's components, it's important to know how well we're covering our code base and to know if we've missed anything. We can use the [Coverage](https://coverage.readthedocs.io/) library to track and visualize how much of our codebase our tests account for. With pytest, it's even easier to use this package thanks to the [pytest-cov](https://pytest-cov.readthedocs.io/) plugin.\n```\npython3-mpytesttests/code--covmadewithml--cov-reporthtml--disable-warnings\n\n```\n\n![pytest](https://madewithml.com/static/images/mlops/testing/pytest.png)\nHere we're asking to run all tests under `tests/code` and to check for coverage for all the code in our `madewithml` directory. When we run this, we'll see the tests from our tests directory executing while the coverage plugin is keeping tracking of which lines in our application are being executed. Once our tests are done, we can view the generated report either through the terminal:\n```\ncoveragereport-m\n\n```\n\n```\nName          Stmts  Miss Cover  Missing\n-----------------------------------------------------\nmadewithml/config.py    16   0  100%\nmadewithml/data.py     51   0  100%\nmadewithml/models.py    2   0  100%\nmadewithml/predict.py   23   0  100%\nmadewithml/train.py    45   0  100%\nmadewithml/tune.py     51   0  100%\nmadewithml/utils.py    39   0  100%\n-----------------------------------------------------\nTOTAL           227   0  100%\n\n```\n\nbut a more interactive way is to view it through the `htmlcov/index.html` on a browser. Here we can click on individual files to see which parts were not covered by any tests.\n![test coverage](https://madewithml.com/static/images/mlops/testing/coverage.png)\nWarning\nThough we have 100% coverage, this does not mean that our application is perfect. Coverage only indicates that a piece of code executed in a test, not necessarily that every part of it was tested, let alone thoroughly tested. Therefore, coverage should **never** be used as a representation of correctness. However, it is very useful to maintain coverage at 100% so we can know when new functionality has yet to be tested. In our CI/CD lesson, we'll see how to use GitHub actions to make 100% coverage a requirement when pushing to specific branches.\n### Exclusions\nSometimes it doesn't make sense to write tests to cover every single line in our application yet we still want to account for these lines so we can maintain 100% coverage. We have two levels of purview when applying exclusions:\n  1. Excusing lines by adding this comment `# pragma: no cover, <MESSAGE>`\n```\n1\n2\n```\n| ```\nif results_fp: # pragma: no cover, saving results\n  utils.save_dict(d, results_fp)\n\n```\n  \n---|---  \n  2. Excluding files by specifying them in our `pyproject.toml` configuration:\n\n\n```\n1\n2\n3\n```\n| ```\n# Pytest cov\n[tool.coverage.run]\nomit=[\"madewithml/evaluate.py\",\"madewithml/serve.py\"]\n\n```\n  \n---|---  \n> The main point is that we were able to add justification to these exclusions through comments so our team can follow our reasoning.\nNow that we have a foundation for testing traditional software, let's dive into testing our data and models in the context of machine learning systems.\n## \ud83d\udd22 Data\nSo far, we've used unit and integration tests to test the functions that interact with our data but we haven't tested the validity of the data itself. We're going to use the [great expectations](https://github.com/great-expectations/great_expectations) library to test what our data is expected to look like. It's a library that allows us to create expectations as to what our data should look like in a standardized way. It also provides modules to seamlessly connect with backend data sources such as local file systems, S3, databases, etc. Let's explore the library by implementing the expectations we'll need for our application.\n> \ud83d\udc49 Follow along interactive notebook in the [**testing-ml**](https://github.com/GokuMohandas/testing-ml) repository as we implement the concepts below.\nFirst we'll load the data we'd like to apply our expectations on. We can load our data from a variety of [sources](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/connect_to_data_overview) (filesystem, database, cloud etc.) which we can then wrap around a [Dataset module](https://legacy.docs.greatexpectations.io/en/latest/autoapi/great_expectations/dataset/index.html) (Pandas / Spark DataFrame, SQLAlchemy). Since multiple data tests may want access to this data, we'll create a fixture for it.\n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n```\n| ```\n# tests/data/conftest.py\nimportgreat_expectationsasge\nimportpandasaspd\nimportpytest\n@pytest.fixture(scope=\"module\")\ndefdf(request):\n  dataset_loc = request.config.getoption(\"--dataset-loc\")\n  df = ge.dataset.PandasDataset(pd.read_csv(dataset_loc))\n  return df\n\n```\n  \n---|---  \n### Expectations\nWhen it comes to creating expectations as to what our data should look like, we want to think about our entire dataset and all the features (columns) within it.\n```\ncolumn_list = [\"id\", \"created_on\", \"title\", \"description\", \"tag\"]\ndf.expect_table_columns_to_match_ordered_list(column_list=column_list) # schema adherence\ntags = [\"computer-vision\", \"natural-language-processing\", \"mlops\", \"other\"]\ndf.expect_column_values_to_be_in_set(column=\"tag\", value_set=tags) # expected labels\ndf.expect_compound_columns_to_be_unique(column_list=[\"title\", \"description\"]) # data leaks\ndf.expect_column_values_to_not_be_null(column=\"tag\") # missing values\ndf.expect_column_values_to_be_unique(column=\"id\") # unique values\ndf.expect_column_values_to_be_of_type(column=\"title\", type_=\"str\") # type adherence\n\n```\n\nEach of these expectations will create an output with details about success or failure, expected and observed values, expectations raised, etc. For example, the expectation `df.expect_column_values_to_be_of_type(column=\"title\", type_=\"str\")` would produce the following if successful:\n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n```\n| ```\n{\n\"exception_info\":{\n\"raised_exception\":false,\n\"exception_traceback\":null,\n\"exception_message\":null\n},\n\"success\":true,\n\"meta\":{},\n\"expectation_config\":{\n\"kwargs\":{\n\"column\":\"title\",\n\"type_\":\"str\",\n\"result_format\":\"BASIC\"\n},\n\"meta\":{},\n\"expectation_type\":\"_expect_column_values_to_be_of_type__map\"\n},\n\"result\":{\n\"element_count\":955,\n\"missing_count\":0,\n\"missing_percent\":0.0,\n\"unexpected_count\":0,\n\"unexpected_percent\":0.0,\n\"unexpected_percent_nonmissing\":0.0,\n\"partial_unexpected_list\":[]\n}\n}\n\n```\n  \n---|---  \nand if we have a failed expectation (ex. `df.expect_column_values_to_be_of_type(column=\"title\", type_=\"int\")`), we'd receive this output(notice the counts and examples for what caused the failure): \n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n```\n| ```\n{\n\"success\":false,\n\"exception_info\":{\n\"raised_exception\":false,\n\"exception_traceback\":null,\n\"exception_message\":null\n},\n\"expectation_config\":{\n\"meta\":{},\n\"kwargs\":{\n\"column\":\"title\",\n\"type_\":\"int\",\n\"result_format\":\"BASIC\"\n},\n\"expectation_type\":\"_expect_column_values_to_be_of_type__map\"\n},\n\"result\":{\n\"element_count\":955,\n\"missing_count\":0,\n\"missing_percent\":0.0,\n\"unexpected_count\":955,\n\"unexpected_percent\":100.0,\n\"unexpected_percent_nonmissing\":100.0,\n\"partial_unexpected_list\":[\n\"How to Deal with Files in Google Colab: What You Need to Know\",\n\"Machine Learning Methods Explained (+ Examples)\",\n\"OpenMMLab Computer Vision\",\n\"...\",\n]\n},\n\"meta\":{}\n}\n\n```\n  \n---|---  \nThere are just a few of the different expectations that we can create. Be sure to explore all the [expectations](https://greatexpectations.io/expectations/), including [custom expectations](https://docs.greatexpectations.io/docs/guides/expectations/creating_custom_expectations/overview/). Here are some other popular expectations that don't pertain to our specific dataset but are widely applicable:\n  * feature value relationships with other feature values \u2192 `expect_column_pair_values_a_to_be_greater_than_b`\n  * value statistics (mean, std, median, max, min, sum, etc.) \u2192 `expect_column_mean_to_be_between`\n\n\n### Suite\nInstead of running each of these individually, we can combine them all into an expectation suite.\n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n```\n| ```\n# tests/data/test_dataset.py\ndeftest_dataset(df):\n\"\"\"Test dataset quality and integrity.\"\"\"\n  column_list = [\"id\", \"created_on\", \"title\", \"description\", \"tag\"]\n  df.expect_table_columns_to_match_ordered_list(column_list=column_list) # schema adherence\n  tags = [\"computer-vision\", \"natural-language-processing\", \"mlops\", \"other\"]\n  df.expect_column_values_to_be_in_set(column=\"tag\", value_set=tags) # expected labels\n  df.expect_compound_columns_to_be_unique(column_list=[\"title\", \"description\"]) # data leaks\n  df.expect_column_values_to_not_be_null(column=\"tag\") # missing values\n  df.expect_column_values_to_be_unique(column=\"id\") # unique values\n  df.expect_column_values_to_be_of_type(column=\"title\", type_=\"str\") # type adherence\n  # Expectation suite\n  expectation_suite = df.get_expectation_suite(discard_failed_expectations=False)\n  results = df.validate(expectation_suite=expectation_suite, only_return_failures=True).to_json_dict()\n  assert results[\"success\"]\n\n```\n  \n---|---  \nWe can now execute these data tests just like a code test.\n```\nexportDATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\npytest--dataset-loc=$DATASET_LOCtests/data--verbose--disable-warnings\n\n```\n\nNote\nWe've added a `--dataset-loc` flag to pytest by specifying in our `tests/data/conftest.py` script. This allows us to pass in the dataset location as an argument to our tests.\n```\n1\n2\n3\n```\n| ```\n# tests/data/conftest.py\ndefpytest_addoption(parser):\n  parser.addoption(\"--dataset-loc\", action=\"store\", default=None, help=\"Dataset location.\")\n\n```\n  \n---|---  \n> We're keeping things simple by using our expectations with pytest but Great expectations also has a lot more functionality around [connecting](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/connect_to_data_lp) to data sources, [Checkpoints](https://docs.greatexpectations.io/docs/terms/checkpoint/) to execute suites across various parts of the pipeline, [data docs](https://docs.greatexpectations.io/docs/terms/data_docs/) to generate reports, etc.\n### Production\nWhile we're validating our datasets inside our machine learning applications, in most production scenarios, the data validation happens much further upstream. Our dataset may not be used just for our specific application and may actually be feeding into many other downstream application (ML and otherwise). Therefore, it's a great idea to execute these data validation tests as up stream as possible so that downstream applications can reliably use the data.\n![ELT pipelines in production](https://madewithml.com/static/images/mlops/testing/production.png)\n> Learn more about different data systems in our [data engineering lesson](https://madewithml.com/courses/mlops/data-engineering/) if you're not familiar with them.\n## \ud83e\udd16 Models\nThe final aspect of testing ML systems involves how to test machine learning models during training, evaluation, inference and deployment.\n### Training\nWe want to write tests iteratively while we're developing our training pipelines so we can catch errors quickly. This is especially important because, unlike traditional software, ML systems can run to completion without throwing any exceptions / errors but can produce incorrect systems. We also want to catch errors quickly to save on time and compute.\n  * Check shapes and values of model output \n```\n1\n```\n| ```\nassert model(inputs).shape == torch.Size([len(inputs), num_classes])\n\n```\n  \n---|---  \n  * Check for decreasing loss after one batch of training \n```\n1\n```\n| ```\nassert epoch_loss < prev_epoch_loss\n\n```\n  \n---|---  \n  * Overfit on a batch \n```\n1\n2\n```\n| ```\naccuracy = train(model, inputs=batches[0])\nassert accuracy == pytest.approx(0.95, abs=0.05) # 0.95 \u00b1 0.05\n\n```\n  \n---|---  \n  * Train to completion (tests early stopping, saving, etc.) \n```\n1\n2\n3\n```\n| ```\ntrain(model)\nassert learning_rate >= min_learning_rate\nassert artifacts\n\n```\n  \n---|---  \n  * On different devices \n```\n1\n2\n```\n| ```\nassert train(model, device=torch.device(\"cpu\"))\nassert train(model, device=torch.device(\"cuda\"))\n\n```\n  \n---|---  \n\n\nNote\nYou can mark the compute intensive tests with a pytest marker and only execute them when there is a change being made to system affecting the model. \n```\n1\n2\n3\n```\n| ```\n@pytest.mark.training\ndeftest_train_model():\n  ...\n\n```\n  \n---|---  \n### Behavioral testing\nBehavioral testing is the process of testing input data and expected outputs while treating the model as a black box (model agnostic evaluation). They don't necessarily have to be adversarial in nature but more along the types of perturbations we may expect to see in the real world once our model is deployed. A landmark paper on this topic is [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://arxiv.org/abs/2005.04118) which breaks down behavioral testing into three types of tests:\n  * `invariance`: Changes should not affect outputs. \n```\n1\n2\n3\n```\n| ```\n# INVariance via verb injection (changes should not affect outputs)\nget_label(text=\"Transformers applied to NLP have revolutionized machine learning.\", predictor=predictor)\nget_label(text=\"Transformers applied to NLP have disrupted machine learning.\", predictor=predictor)\n\n```\n  \n---|---  \n\n```\n'natural-language-processing'\n'natural-language-processing'\n\n```\n\n  * `directional`: Change should affect outputs. \n```\n1\n2\n3\n4\n```\n| ```\n# DIRectional expectations (changes with known outputs)\nget_label(text=\"ML applied to text classification.\", predictor=predictor)\nget_label(text=\"ML applied to image classification.\", predictor=predictor)\nget_label(text=\"CNNs for text classification.\", predictor=predictor)\n\n```\n  \n---|---  \n\n```\n'natural-language-processing'\n'computer-vision'\n'natural-language-processing'\n\n```\n\n  * `minimumfunctionality`: Simple combination of inputs and expected outputs. \n```\n1\n2\n3\n4\n```\n| ```\n# Minimum Functionality Tests (simple input/output pairs)\nget_label(text=\"Natural language processing is the next big wave in machine learning.\", predictor=predictor)\nget_label(text=\"MLOps is the next big wave in machine learning.\", predictor=predictor)\nget_label(text=\"This is about graph neural networks.\", predictor=predictor)\n\n```\n  \n---|---  \n\n```\n'natural-language-processing'\n'mlops'\n'other'\n\n```\n\nAnd we can convert these tests into proper parameterized tests by first defining from fixtures in our `tests/model/conftest.py` and our `tests/model/utils.py` scripts:\n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n```\n| ```\n# tests/model/conftest.py\nimportpytest\nfromray.train.torch.torch_predictorimport TorchPredictor\nfrommadewithmlimport predict\ndefpytest_addoption(parser):\n  parser.addoption(\"--run-id\", action=\"store\", default=None, help=\"Run ID of model to use.\")\n\n@pytest.fixture(scope=\"module\")\ndefrun_id(request):\n  return request.config.getoption(\"--run-id\")\n\n@pytest.fixture(scope=\"module\")\ndefpredictor(run_id):\n  best_checkpoint = predict.get_best_checkpoint(run_id=run_id)\n  predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n  return predictor\n\n```\n  \n---|---  \n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n```\n| ```\n# tests/model/utils.py\nimportnumpyasnp\nimportpandasaspd\nfrommadewithmlimport predict\ndefget_label(text, predictor):\n  df = pd.DataFrame({\"title\": [text], \"description\": \"\", \"tag\": \"other\"})\n  z = predictor.predict(data=df)[\"predictions\"]\n  preprocessor = predictor.get_preprocessor()\n  label = predict.decode(np.stack(z).argmax(1), preprocessor.index_to_class)[0]\n  return label\n\n```\n  \n---|---  \nAnd now, we can use these components to create our behavioral tests:\n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n```\n| ```\n# tests/model/test_behavioral.py\n@pytest.mark.parametrize(\n  \"input_a, input_b, label\",\n  [\n    (\n      \"Transformers applied to NLP have revolutionized machine learning.\",\n      \"Transformers applied to NLP have disrupted machine learning.\",\n      \"natural-language-processing\",\n    ),\n  ],\n)\ndeftest_invariance(input_a, input_b, label, predictor):\n\"\"\"INVariance via verb injection (changes should not affect outputs).\"\"\"\n  label_a = utils.get_label(text=input_a, predictor=predictor)\n  label_b = utils.get_label(text=input_b, predictor=predictor)\n  assert label_a == label_b == label\n\n```\n  \n---|---  \n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n```\n| ```\n# tests/model/test_behavioral.py\n@pytest.mark.parametrize(\n  \"input, label\",\n  [\n    (\n      \"ML applied to text classification.\",\n      \"natural-language-processing\",\n    ),\n    (\n      \"ML applied to image classification.\",\n      \"computer-vision\",\n    ),\n    (\n      \"CNNs for text classification.\",\n      \"natural-language-processing\",\n    ),\n  ],\n)\ndeftest_directional(input, label, predictor):\n\"\"\"DIRectional expectations (changes with known outputs).\"\"\"\n  prediction = utils.get_label(text=input, predictor=predictor)\n  assert label == prediction\n\n```\n  \n---|---  \n```\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n```\n| ```\n# tests/model/test_behavioral.py\n@pytest.mark.parametrize(\n  \"input, label\",\n  [\n    (\n      \"Natural language processing is the next big wave in machine learning.\",\n      \"natural-language-processing\",\n    ),\n    (\n      \"MLOps is the next big wave in machine learning.\",\n      \"mlops\",\n    ),\n    (\n      \"This is about graph neural networks.\",\n      \"other\",\n    ),\n  ],\n)\ndeftest_mft(input, label, predictor):\n\"\"\"Minimum Functionality Tests (simple input/output pairs).\"\"\"\n  prediction = utils.get_label(text=input, predictor=predictor)\n  assert label == prediction\n\n```\n  \n---|---  \nAnd we can execute them just like any other test:\n```\n# Model tests\nexportEXPERIMENT_NAME=\"llm\"\nexportRUN_ID=$(pythonmadewithml/predict.pyget-best-run-id--experiment-name$EXPERIMENT_NAME--metricval_loss--modeASC)\npytest--run-id=$RUN_IDtests/model--verbose--disable-warnings\n\n```\n\n## Testing vs. monitoring\nWe'll conclude by talking about the similarities and distinctions between testing and [monitoring](https://madewithml.com/courses/mlops/monitoring/). They're both integral parts of the ML development pipeline and depend on each other for iteration. Testing is assuring that our system (code, data and models) passes the expectations that we've established offline. Whereas, monitoring involves that these expectations continue to pass online on live production data while also ensuring that their data distributions are [comparable](https://madewithml.com/courses/mlops/monitoring/#measuring-drift) to the reference window (typically subset of training data) through tn. When these conditions no longer hold true, we need to inspect more closely (retraining may not always fix our root problem).\nWith [monitoring](https://madewithml.com/courses/mlops/monitoring/), there are quite a few distinct concerns that we didn't have to consider during testing since it involves (live) data we have yet to see.\n  * features and prediction distributions (drift), typing, schema mismatches, etc.\n  * determining model performance (rolling and window metrics on overall and slices of data) using indirect signals (since labels may not be readily available).\n  * in situations with large data, we need to know which data points to label and upsample for training.\n  * identifying anomalies and outliers.\n\n\n> We'll cover all of these concepts in much more depth (and code) in our [monitoring](https://madewithml.com/courses/mlops/monitoring/) lesson.\n## Resources\n  * [The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf)\n  * [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://arxiv.org/abs/2005.04118)\n  * [Robustness Gym: Unifying the NLP Evaluation Landscape](https://arxiv.org/abs/2101.04840)\n\n\nUpcoming live cohorts\nSign up for our upcoming live cohort, where we'll provide **live lessons + QA** , **compute (GPUs)** and **community** to learn everything in one day. \nLearn more \nTo cite this content, please use:\n```\n1\n2\n3\n4\n5\n6\n```\n| ```\n@article{madewithml,\nauthor={Goku Mohandas},\ntitle={ Code - Made With ML },\nhowpublished={\\url{https://madewithml.com/}},\nyear={2023}\n}\n\n```\n  \n---|---  \n[![](https://madewithml.com/static/images/anyscale-white-text.svg)](https://www.anyscale.com?utm_source=madewithmml&utm_medium=website&utm_campaign=footer) \u00a9 2025 Anyscale, Inc. [Anyscale Privacy Policy](https://www.anyscale.com/privacy-policy)\nMade with [ Material for MkDocs ](https://squidfunk.github.io/mkdocs-material/)\n[ ](https://twitter.com/GokuMohandas \"twitter.com\") [ ](https://linkedin.com/in/goku \"linkedin.com\") [ ](https://github.com/GokuMohandas \"github.com\") [ ](https://www.youtube.com/madewithml?sub_confirmation=1 \"www.youtube.com\")\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis document outlines best practices for testing machine learning systems, including code, data, and models. Key insights include the importance of unit, integration, system, and acceptance tests, as well as using frameworks like pytest for automation. It emphasizes data validation with libraries like Great Expectations and the distinction between testing and monitoring in production environments.\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/coleam00/ai-agents-masterclass/tree/main/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fcoleam00%2Fai-agents-masterclass%2Ftree%2Fmain%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fcoleam00%2Fai-agents-masterclass%2Ftree%2Fmain%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=coleam00%2Fai-agents-masterclass) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/coleam00/ai-agents-masterclass/tree/main/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/coleam00/ai-agents-masterclass/tree/main/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/coleam00/ai-agents-masterclass/tree/main/) to refresh your session. Dismiss alert\n{{ message }}\n[ coleam00 ](https://github.com/coleam00) / **[ai-agents-masterclass](https://github.com/coleam00/ai-agents-masterclass) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Fcoleam00%2Fai-agents-masterclass) You must be signed in to change notification settings\n  * [ Fork 919 ](https://github.com/login?return_to=%2Fcoleam00%2Fai-agents-masterclass)\n  * [ Star  1.9k ](https://github.com/login?return_to=%2Fcoleam00%2Fai-agents-masterclass)\n\n\nFollow along with my AI Agents Masterclass videos! All of the code I create and use in this series on YouTube will be here for you to use and even build on top of! \n### License\n[ MIT license ](https://github.com/coleam00/ai-agents-masterclass/blob/main/LICENSE)\n[ 1.9k stars ](https://github.com/coleam00/ai-agents-masterclass/stargazers) [ 919 forks ](https://github.com/coleam00/ai-agents-masterclass/forks) [ Branches ](https://github.com/coleam00/ai-agents-masterclass/branches) [ Tags ](https://github.com/coleam00/ai-agents-masterclass/tags) [ Activity ](https://github.com/coleam00/ai-agents-masterclass/activity)\n[ Star  ](https://github.com/login?return_to=%2Fcoleam00%2Fai-agents-masterclass)\n[ Notifications ](https://github.com/login?return_to=%2Fcoleam00%2Fai-agents-masterclass) You must be signed in to change notification settings\n  * [ Code ](https://github.com/coleam00/ai-agents-masterclass)\n  * [ Issues 28 ](https://github.com/coleam00/ai-agents-masterclass/issues)\n  * [ Pull requests 9 ](https://github.com/coleam00/ai-agents-masterclass/pulls)\n  * [ Actions ](https://github.com/coleam00/ai-agents-masterclass/actions)\n  * [ Projects 0 ](https://github.com/coleam00/ai-agents-masterclass/projects)\n  * [ Security ](https://github.com/coleam00/ai-agents-masterclass/security)\n  * [ Insights ](https://github.com/coleam00/ai-agents-masterclass/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/coleam00/ai-agents-masterclass)\n  * [ Issues  ](https://github.com/coleam00/ai-agents-masterclass/issues)\n  * [ Pull requests  ](https://github.com/coleam00/ai-agents-masterclass/pulls)\n  * [ Actions  ](https://github.com/coleam00/ai-agents-masterclass/actions)\n  * [ Projects  ](https://github.com/coleam00/ai-agents-masterclass/projects)\n  * [ Security  ](https://github.com/coleam00/ai-agents-masterclass/security)\n  * [ Insights  ](https://github.com/coleam00/ai-agents-masterclass/pulse)\n\n\n# coleam00/ai-agents-masterclass\nmain\n[**1** Branch](https://github.com/coleam00/ai-agents-masterclass/branches)[**0** Tags](https://github.com/coleam00/ai-agents-masterclass/tags)\n[](https://github.com/coleam00/ai-agents-masterclass/branches)[](https://github.com/coleam00/ai-agents-masterclass/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[42 Commits](https://github.com/coleam00/ai-agents-masterclass/commits/main/)[](https://github.com/coleam00/ai-agents-masterclass/commits/main/)  \n[1-first-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/1-first-agent \"1-first-agent\")| [1-first-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/1-first-agent \"1-first-agent\")  \n[10-deploy-ai-agent-langserve](https://github.com/coleam00/ai-agents-masterclass/tree/main/10-deploy-ai-agent-langserve \"10-deploy-ai-agent-langserve\")| [10-deploy-ai-agent-langserve](https://github.com/coleam00/ai-agents-masterclass/tree/main/10-deploy-ai-agent-langserve \"10-deploy-ai-agent-langserve\")  \n[2-langchain-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/2-langchain-agent \"2-langchain-agent\")| [2-langchain-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/2-langchain-agent \"2-langchain-agent\")  \n[3-agent-ui](https://github.com/coleam00/ai-agents-masterclass/tree/main/3-agent-ui \"3-agent-ui\")| [3-agent-ui](https://github.com/coleam00/ai-agents-masterclass/tree/main/3-agent-ui \"3-agent-ui\")  \n[4-task-management-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/4-task-management-agent \"4-task-management-agent\")| [4-task-management-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/4-task-management-agent \"4-task-management-agent\")  \n[5-rag-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/5-rag-agent \"5-rag-agent\")| [5-rag-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/5-rag-agent \"5-rag-agent\")  \n[6-rag-task-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/6-rag-task-agent \"6-rag-task-agent\")| [6-rag-task-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/6-rag-task-agent \"6-rag-task-agent\")  \n[7-langgraph-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/7-langgraph-agent \"7-langgraph-agent\")| [7-langgraph-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/7-langgraph-agent \"7-langgraph-agent\")  \n[8-n8n-asana-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/8-n8n-asana-agent \"8-n8n-asana-agent\")| [8-n8n-asana-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/8-n8n-asana-agent \"8-n8n-asana-agent\")  \n[9-n8n-rag-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/9-n8n-rag-agent \"9-n8n-rag-agent\")| [9-n8n-rag-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/9-n8n-rag-agent \"9-n8n-rag-agent\")  \n[cost-saving-ai-router](https://github.com/coleam00/ai-agents-masterclass/tree/main/cost-saving-ai-router \"cost-saving-ai-router\")| [cost-saving-ai-router](https://github.com/coleam00/ai-agents-masterclass/tree/main/cost-saving-ai-router \"cost-saving-ai-router\")  \n[gohighlevel-saas-backend](https://github.com/coleam00/ai-agents-masterclass/tree/main/gohighlevel-saas-backend \"gohighlevel-saas-backend\")| [gohighlevel-saas-backend](https://github.com/coleam00/ai-agents-masterclass/tree/main/gohighlevel-saas-backend \"gohighlevel-saas-backend\")  \n[korvus-simple-rag](https://github.com/coleam00/ai-agents-masterclass/tree/main/korvus-simple-rag \"korvus-simple-rag\")| [korvus-simple-rag](https://github.com/coleam00/ai-agents-masterclass/tree/main/korvus-simple-rag \"korvus-simple-rag\")  \n[llama3-function-calling-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/llama3-function-calling-agent \"llama3-function-calling-agent\")| [llama3-function-calling-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/llama3-function-calling-agent \"llama3-function-calling-agent\")  \n[llm-agent-evaluation-framework](https://github.com/coleam00/ai-agents-masterclass/tree/main/llm-agent-evaluation-framework \"llm-agent-evaluation-framework\")| [llm-agent-evaluation-framework](https://github.com/coleam00/ai-agents-masterclass/tree/main/llm-agent-evaluation-framework \"llm-agent-evaluation-framework\")  \n[local-ai-packaged](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-ai-packaged \"local-ai-packaged\")| [local-ai-packaged](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-ai-packaged \"local-ai-packaged\")  \n[local-llm-tool-calling](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-llm-tool-calling \"local-llm-tool-calling\")| [local-llm-tool-calling](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-llm-tool-calling \"local-llm-tool-calling\")  \n[local-or-managed-llm-rag](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-or-managed-llm-rag \"local-or-managed-llm-rag\")| [local-or-managed-llm-rag](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-or-managed-llm-rag \"local-or-managed-llm-rag\")  \n[local-swarm-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-swarm-agent \"local-swarm-agent\")| [local-swarm-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/local-swarm-agent \"local-swarm-agent\")  \n[madlibs](https://github.com/coleam00/ai-agents-masterclass/tree/main/madlibs \"madlibs\")| [madlibs](https://github.com/coleam00/ai-agents-masterclass/tree/main/madlibs \"madlibs\")  \n[n8n-langchain-agent-advanced](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-langchain-agent-advanced \"n8n-langchain-agent-advanced\")| [n8n-langchain-agent-advanced](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-langchain-agent-advanced \"n8n-langchain-agent-advanced\")  \n[n8n-langchain-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-langchain-agent \"n8n-langchain-agent\")| [n8n-langchain-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-langchain-agent \"n8n-langchain-agent\")  \n[n8n-rag-pdfs-excel](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-rag-pdfs-excel \"n8n-rag-pdfs-excel\")| [n8n-rag-pdfs-excel](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-rag-pdfs-excel \"n8n-rag-pdfs-excel\")  \n[n8n-streamlit-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-streamlit-agent \"n8n-streamlit-agent\")| [n8n-streamlit-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/n8n-streamlit-agent \"n8n-streamlit-agent\")  \n[o1-ai-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/o1-ai-agent \"o1-ai-agent\")| [o1-ai-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/o1-ai-agent \"o1-ai-agent\")  \n[pydantic-ai](https://github.com/coleam00/ai-agents-masterclass/tree/main/pydantic-ai \"pydantic-ai\")| [pydantic-ai](https://github.com/coleam00/ai-agents-masterclass/tree/main/pydantic-ai \"pydantic-ai\")  \n[sql-ai-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/sql-ai-agent \"sql-ai-agent\")| [sql-ai-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/sql-ai-agent \"sql-ai-agent\")  \n[streamlit-chatbot](https://github.com/coleam00/ai-agents-masterclass/tree/main/streamlit-chatbot \"streamlit-chatbot\")| [streamlit-chatbot](https://github.com/coleam00/ai-agents-masterclass/tree/main/streamlit-chatbot \"streamlit-chatbot\")  \n[supabase-n8n-rag-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/supabase-n8n-rag-agent \"supabase-n8n-rag-agent\")| [supabase-n8n-rag-agent](https://github.com/coleam00/ai-agents-masterclass/tree/main/supabase-n8n-rag-agent \"supabase-n8n-rag-agent\")  \n[v0-agent-frontend](https://github.com/coleam00/ai-agents-masterclass/tree/main/v0-agent-frontend \"v0-agent-frontend\")| [v0-agent-frontend](https://github.com/coleam00/ai-agents-masterclass/tree/main/v0-agent-frontend \"v0-agent-frontend\")  \n[.gitattributes](https://github.com/coleam00/ai-agents-masterclass/blob/main/.gitattributes \".gitattributes\")| [.gitattributes](https://github.com/coleam00/ai-agents-masterclass/blob/main/.gitattributes \".gitattributes\")  \n[.gitignore](https://github.com/coleam00/ai-agents-masterclass/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/coleam00/ai-agents-masterclass/blob/main/.gitignore \".gitignore\")  \n[LICENSE](https://github.com/coleam00/ai-agents-masterclass/blob/main/LICENSE \"LICENSE\")| [LICENSE](https://github.com/coleam00/ai-agents-masterclass/blob/main/LICENSE \"LICENSE\")  \n[README.md](https://github.com/coleam00/ai-agents-masterclass/blob/main/README.md \"README.md\")| [README.md](https://github.com/coleam00/ai-agents-masterclass/blob/main/README.md \"README.md\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/coleam00/ai-agents-masterclass/tree/main/)\n  * [MIT license](https://github.com/coleam00/ai-agents-masterclass/tree/main/)\n\n\n[ ![AI Agents Masterclass](https://camo.githubusercontent.com/c6245efe5a6c4510b164c6e49f55ef6dcddf8f4fbfcd0999b73897d05a8497a2/68747470733a2f2f692e696d6775722e636f6d2f384772327042412e706e67) ](https://www.youtube.com/channel/UCMwVTLZIRRUyyVrkjDpn4pA)\nAI Agents Masterclass[](https://github.com/coleam00/ai-agents-masterclass/tree/main/#ai-agents-masterclass)\nArtificial Intelligence is the #1 thing for all developers to spend their time on now. The problem is, most developers aren't focusing on AI agents, which is the real way to unleash the full power of AI. This is why I'm creating this AI Agents Masterclass - so I can show YOU how to use AI agents to transform businesses and create incredibly powerful software like I've already done many times! Click the image or link above to go to the masterclass on YouTube. \n[**What are AI Agents?**](https://github.com/coleam00/ai-agents-masterclass/tree/main/#what-are-ai-agents) \u00b7 [**How this Repo Works**](https://github.com/coleam00/ai-agents-masterclass/tree/main/#how-this-repo-works) \u00b7 [**Instructions to Follow Along**](https://github.com/coleam00/ai-agents-masterclass/tree/main/#instructions-to-follow-along)\n## What are AI Agents?\n[](https://github.com/coleam00/ai-agents-masterclass/tree/main/#what-are-ai-agents)\nAI agents are simply Large Language Models that have been given the ability to interact with the outside world. They can do things like draft emails, book appointments in your CRM, create tasks in your task management software, and really anything you can dream of! I hope that everything I show here can really help you dream big and create incredible things with AI!\nAI agents can be very powerful without having to create a lot of code. That doesn't mean there isn't room though to create more complex applications to tie together many different agents to accomplish truly incredible things! That's where we'll be heading with this masterclass and I really look forward to it!\nBelow is a very basic diagram just to get an idea of what an AI agent looks like:\n[![Trainers Ally LangGraph graph](https://camo.githubusercontent.com/9fa1c160b31faee0870076572e7282e143586daa9497aec03adff8951b6d275a/68747470733a2f2f692e696d6775722e636f6d2f4368526f5638572e706e67)](https://camo.githubusercontent.com/9fa1c160b31faee0870076572e7282e143586daa9497aec03adff8951b6d275a/68747470733a2f2f692e696d6775722e636f6d2f4368526f5638572e706e67)\n## How this Repo Works\n[](https://github.com/coleam00/ai-agents-masterclass/tree/main/#how-this-repo-works)\nEach week there will be a new video for my AI Agents Masterclass! Each video will have its own folder in this repo, starting with [/1-first-agent/](https://github.com/coleam00/ai-agents-masterclass/blob/main/1-first-agent) for the first video in the masterclass where I create our very first AI agent!\nAny folder that starts with a number is for a masterclass video. The other folders are for other content on my YouTube channel. The other content goes very well with the masterclass series (think of it as supplemental material) which is why it is here too!\nThe code in each folder will be exactly what I used/created in the accompanying masterclass video.\n## Instructions to Follow Along\n[](https://github.com/coleam00/ai-agents-masterclass/tree/main/#instructions-to-follow-along)\nThe below instructions assume you already have Git, Python, and Pip installed. If you do not, you can install [Python + Pip from here](https://www.python.org/downloads/) and [Git from here](https://git-scm.com/).\nTo follow along with any of my videos, first clone this GitHub repository, open up a terminal, and change your directory to the folder for the current video you are watching (example: 1st video is [/1-first-agent/](https://github.com/coleam00/ai-agents-masterclass/blob/main/1-first-agent)).\nThe below instructions work on any OS - Windows, Linux, or Mac!\nYou will need to use the environment variables defined in the .env.example file in the folder (example for the first video: [`1-first-agent/.env.example`](https://github.com/coleam00/ai-agents-masterclass/blob/main/1-first-agent/.env.example)) to set up your API keys and other configuration. Turn the .env.example file into a `.env` file, and supply the necessary environment variables.\nAfter setting up the .env file, run the below commands to create a Python virtual environment and install the necessary Python packages to run the code from the masterclass. Creating a virtual environment is optional but recommended! Creating a virtual environment for the entire masterclass is a one time thing. Make sure to run the pip install for each video though!\n```\npython -m venv ai-agents-masterclass\n# On Windows:\n.\\ai-agents-masterclass\\Scripts\\activate\n# On MacOS/Linux: \nsource ai-agents-masterclass/bin/activate\ncd 1-first-agent (or whichever folder)\npip install -r requirements.txt\n```\n\nThen, you can execute the code in the folder with:\n```\npython [script name].py\n```\n\n## About\nFollow along with my AI Agents Masterclass videos! All of the code I create and use in this series on YouTube will be here for you to use and even build on top of! \n### Resources\n[ Readme ](https://github.com/coleam00/ai-agents-masterclass/tree/main/#readme-ov-file)\n### License\n[ MIT license ](https://github.com/coleam00/ai-agents-masterclass/tree/main/#MIT-1-ov-file)\n[ Activity](https://github.com/coleam00/ai-agents-masterclass/activity)\n### Stars\n[ **1.9k** stars](https://github.com/coleam00/ai-agents-masterclass/stargazers)\n### Watchers\n[ **80** watching](https://github.com/coleam00/ai-agents-masterclass/watchers)\n### Forks\n[ **919** forks](https://github.com/coleam00/ai-agents-masterclass/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fcoleam00%2Fai-agents-masterclass&report=coleam00+%28user%29)\n##  [Releases](https://github.com/coleam00/ai-agents-masterclass/releases)\nNo releases published\n##  [Packages 0](https://github.com/users/coleam00/packages?repo_name=ai-agents-masterclass)\nNo packages published \n##  [Contributors 2](https://github.com/coleam00/ai-agents-masterclass/graphs/contributors)\n  * [ ![@coleam00](https://avatars.githubusercontent.com/u/47287758?s=64&v=4) ](https://github.com/coleam00) [ **coleam00** Cole Medin ](https://github.com/coleam00)\n  * [ ![@jeffwiegner](https://avatars.githubusercontent.com/u/56127674?s=64&v=4) ](https://github.com/jeffwiegner) [ **jeffwiegner** ](https://github.com/jeffwiegner)\n\n\n## Languages\n  * [ Python 73.2% ](https://github.com/coleam00/ai-agents-masterclass/search?l=python)\n  * [ JavaScript 21.9% ](https://github.com/coleam00/ai-agents-masterclass/search?l=javascript)\n  * [ TypeScript 4.8% ](https://github.com/coleam00/ai-agents-masterclass/search?l=typescript)\n  * [ CSS 0.1% ](https://github.com/coleam00/ai-agents-masterclass/search?l=css)\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe **AI Agents Masterclass** by Cole Medin teaches developers to utilize AI agents\u2014Large Language Models (LLMs) that interact with external systems\u2014to enhance productivity. The repository includes code from YouTube videos, facilitating hands-on learning. Follow instructions to set up the environment and explore various AI agent implementations.\n\n**Key Tags**: Generative AI, LLMs, AI Agents, Python, GitHub\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/scrapy/scrapy/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=scrapy%2Fscrapy) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/scrapy/scrapy/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/scrapy/scrapy/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/scrapy/scrapy/) to refresh your session. Dismiss alert\n{{ message }}\n[ scrapy ](https://github.com/scrapy) / **[scrapy](https://github.com/scrapy/scrapy) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy) You must be signed in to change notification settings\n  * [ Fork 10.7k ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy)\n  * [ Star  54.7k ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy)\n\n\nScrapy, a fast high-level web crawling & scraping framework for Python. \n[scrapy.org](https://scrapy.org \"https://scrapy.org\")\n### License\n[ BSD-3-Clause license ](https://github.com/scrapy/scrapy/blob/master/LICENSE)\n[ 54.7k stars ](https://github.com/scrapy/scrapy/stargazers) [ 10.7k forks ](https://github.com/scrapy/scrapy/forks) [ Branches ](https://github.com/scrapy/scrapy/branches) [ Tags ](https://github.com/scrapy/scrapy/tags) [ Activity ](https://github.com/scrapy/scrapy/activity)\n[ Star  ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy)\n[ Notifications ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy) You must be signed in to change notification settings\n  * [ Code ](https://github.com/scrapy/scrapy)\n  * [ Issues 437 ](https://github.com/scrapy/scrapy/issues)\n  * [ Pull requests 185 ](https://github.com/scrapy/scrapy/pulls)\n  * [ Discussions ](https://github.com/scrapy/scrapy/discussions)\n  * [ Actions ](https://github.com/scrapy/scrapy/actions)\n  * [ Projects 0 ](https://github.com/scrapy/scrapy/projects)\n  * [ Wiki ](https://github.com/scrapy/scrapy/wiki)\n  * [ Security 10 ](https://github.com/scrapy/scrapy/security)\n  * [ Insights ](https://github.com/scrapy/scrapy/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/scrapy/scrapy)\n  * [ Issues  ](https://github.com/scrapy/scrapy/issues)\n  * [ Pull requests  ](https://github.com/scrapy/scrapy/pulls)\n  * [ Discussions  ](https://github.com/scrapy/scrapy/discussions)\n  * [ Actions  ](https://github.com/scrapy/scrapy/actions)\n  * [ Projects  ](https://github.com/scrapy/scrapy/projects)\n  * [ Wiki  ](https://github.com/scrapy/scrapy/wiki)\n  * [ Security  ](https://github.com/scrapy/scrapy/security)\n  * [ Insights  ](https://github.com/scrapy/scrapy/pulse)\n\n\n# scrapy/scrapy\nmaster\n[**46** Branches](https://github.com/scrapy/scrapy/branches)[**119** Tags](https://github.com/scrapy/scrapy/tags)\n[](https://github.com/scrapy/scrapy/branches)[](https://github.com/scrapy/scrapy/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n[![wRAR](https://avatars.githubusercontent.com/u/241039?v=4&size=40)](https://github.com/wRAR)[wRAR](https://github.com/scrapy/scrapy/commits?author=wRAR)[Convert test_downloadermiddleware_robotstxt.py from callbacks to awai\u2026](https://github.com/scrapy/scrapy/commit/9f99da8f865efff11f6c8736567b8fbd4413091c)Mar 24, 2025[9f99da8](https://github.com/scrapy/scrapy/commit/9f99da8f865efff11f6c8736567b8fbd4413091c) \u00b7 Mar 24, 2025\n## History\n[10,758 Commits](https://github.com/scrapy/scrapy/commits/master/)[](https://github.com/scrapy/scrapy/commits/master/)  \n[.github](https://github.com/scrapy/scrapy/tree/master/.github \".github\")| [.github](https://github.com/scrapy/scrapy/tree/master/.github \".github\")| [Codecov: Add test analytics (](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")[#6741](https://github.com/scrapy/scrapy/pull/6741)[)](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")| Mar 21, 2025  \n[artwork](https://github.com/scrapy/scrapy/tree/master/artwork \"artwork\")| [artwork](https://github.com/scrapy/scrapy/tree/master/artwork \"artwork\")| [artwork/README.rst: add missing articles (](https://github.com/scrapy/scrapy/commit/874a87976830ed519ee49742c5b427618fa362e8 \"artwork/README.rst: add missing articles \\(#5827\\)\")[#5827](https://github.com/scrapy/scrapy/pull/5827)[)](https://github.com/scrapy/scrapy/commit/874a87976830ed519ee49742c5b427618fa362e8 \"artwork/README.rst: add missing articles \\(#5827\\)\")| Feb 14, 2023  \n[docs](https://github.com/scrapy/scrapy/tree/master/docs \"docs\")| [docs](https://github.com/scrapy/scrapy/tree/master/docs \"docs\")| [Syntax Error Fixed (](https://github.com/scrapy/scrapy/commit/3ca882fba86750199c2f41ef24b5495a4afa7988 \"Syntax Error Fixed \\(#6738\\)\n* Syntax error fix issue #6731\n* test case added\n* extra logic removed\n* mock spider fixture\n* Update scrapy/utils/misc.py\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\n* settings.rst updated\n* settings.rst updated\n* settings.rst updated\n---------\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\")[#6738](https://github.com/scrapy/scrapy/pull/6738)[)](https://github.com/scrapy/scrapy/commit/3ca882fba86750199c2f41ef24b5495a4afa7988 \"Syntax Error Fixed \\(#6738\\)\n* Syntax error fix issue #6731\n* test case added\n* extra logic removed\n* mock spider fixture\n* Update scrapy/utils/misc.py\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\n* settings.rst updated\n* settings.rst updated\n* settings.rst updated\n---------\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\")| Mar 20, 2025  \n[extras](https://github.com/scrapy/scrapy/tree/master/extras \"extras\")| [extras](https://github.com/scrapy/scrapy/tree/master/extras \"extras\")| [Run and fix linkcheck. (](https://github.com/scrapy/scrapy/commit/ce5a132f12341a4118edb7c8ae3b7c2a27306057 \"Run and fix linkcheck. \\(#6524\\)\")[#6524](https://github.com/scrapy/scrapy/pull/6524)[)](https://github.com/scrapy/scrapy/commit/ce5a132f12341a4118edb7c8ae3b7c2a27306057 \"Run and fix linkcheck. \\(#6524\\)\")| Nov 4, 2024  \n[scrapy](https://github.com/scrapy/scrapy/tree/master/scrapy \"scrapy\")| [scrapy](https://github.com/scrapy/scrapy/tree/master/scrapy \"scrapy\")| [Syntax Error Fixed (](https://github.com/scrapy/scrapy/commit/3ca882fba86750199c2f41ef24b5495a4afa7988 \"Syntax Error Fixed \\(#6738\\)\n* Syntax error fix issue #6731\n* test case added\n* extra logic removed\n* mock spider fixture\n* Update scrapy/utils/misc.py\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\n* settings.rst updated\n* settings.rst updated\n* settings.rst updated\n---------\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\")[#6738](https://github.com/scrapy/scrapy/pull/6738)[)](https://github.com/scrapy/scrapy/commit/3ca882fba86750199c2f41ef24b5495a4afa7988 \"Syntax Error Fixed \\(#6738\\)\n* Syntax error fix issue #6731\n* test case added\n* extra logic removed\n* mock spider fixture\n* Update scrapy/utils/misc.py\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\n* settings.rst updated\n* settings.rst updated\n* settings.rst updated\n---------\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\")| Mar 20, 2025  \n[sep](https://github.com/scrapy/scrapy/tree/master/sep \"sep\")| [sep](https://github.com/scrapy/scrapy/tree/master/sep \"sep\")| [Remove trailing whitespace](https://github.com/scrapy/scrapy/commit/bee74fb753afa373ecdb1dbf272af9a108b14b22 \"Remove trailing whitespace\")| Mar 11, 2025  \n[tests](https://github.com/scrapy/scrapy/tree/master/tests \"tests\")| [tests](https://github.com/scrapy/scrapy/tree/master/tests \"tests\")| [Convert test_downloadermiddleware_robotstxt.py from callbacks to awai\u2026](https://github.com/scrapy/scrapy/commit/9f99da8f865efff11f6c8736567b8fbd4413091c \"Convert test_downloadermiddleware_robotstxt.py from callbacks to awaits. \\(#6743\\)\")| Mar 24, 2025  \n[tests_typing](https://github.com/scrapy/scrapy/tree/master/tests_typing \"tests_typing\")| [tests_typing](https://github.com/scrapy/scrapy/tree/master/tests_typing \"tests_typing\")| [Drop Python 3.8 Support (](https://github.com/scrapy/scrapy/commit/53916630723a86277836053e1c54fe50655f0bd5 \"Drop Python 3.8 Support \\(#6472\\)\")[#6472](https://github.com/scrapy/scrapy/pull/6472)[)](https://github.com/scrapy/scrapy/commit/53916630723a86277836053e1c54fe50655f0bd5 \"Drop Python 3.8 Support \\(#6472\\)\")| Oct 16, 2024  \n[.git-blame-ignore-revs](https://github.com/scrapy/scrapy/blob/master/.git-blame-ignore-revs \".git-blame-ignore-revs\")| [.git-blame-ignore-revs](https://github.com/scrapy/scrapy/blob/master/.git-blame-ignore-revs \".git-blame-ignore-revs\")| [chore: fix some typos in comments (](https://github.com/scrapy/scrapy/commit/5f67c01d1d2cc8a0104361b0323d755e22ed93dc \"chore: fix some typos in comments \\(#6317\\)\nSigned-off-by: TechVest <techdashen@qq.com>\")[#6317](https://github.com/scrapy/scrapy/pull/6317)[)](https://github.com/scrapy/scrapy/commit/5f67c01d1d2cc8a0104361b0323d755e22ed93dc \"chore: fix some typos in comments \\(#6317\\)\nSigned-off-by: TechVest <techdashen@qq.com>\")| Apr 17, 2024  \n[.gitattributes](https://github.com/scrapy/scrapy/blob/master/.gitattributes \".gitattributes\")| [.gitattributes](https://github.com/scrapy/scrapy/blob/master/.gitattributes \".gitattributes\")| [Maybe the problem is not in the code after all](https://github.com/scrapy/scrapy/commit/24ba5a71aca9b368eee21bd7d9043a7d26dba403 \"Maybe the problem is not in the code after all\")| Aug 12, 2020  \n[.gitignore](https://github.com/scrapy/scrapy/blob/master/.gitignore \".gitignore\")| [.gitignore](https://github.com/scrapy/scrapy/blob/master/.gitignore \".gitignore\")| [Codecov: Add test analytics (](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")[#6741](https://github.com/scrapy/scrapy/pull/6741)[)](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")| Mar 21, 2025  \n[.pre-commit-config.yaml](https://github.com/scrapy/scrapy/blob/master/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/scrapy/scrapy/blob/master/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [Remove trailing whitespace](https://github.com/scrapy/scrapy/commit/bee74fb753afa373ecdb1dbf272af9a108b14b22 \"Remove trailing whitespace\")| Mar 11, 2025  \n[.readthedocs.yml](https://github.com/scrapy/scrapy/blob/master/.readthedocs.yml \".readthedocs.yml\")| [.readthedocs.yml](https://github.com/scrapy/scrapy/blob/master/.readthedocs.yml \".readthedocs.yml\")| [chore(docs): refactor config (](https://github.com/scrapy/scrapy/commit/e0c828b7f665d8c82e17787996634bc072e416ae \"chore\\(docs\\): refactor config \\(#6623\\)\")[#6623](https://github.com/scrapy/scrapy/pull/6623)[)](https://github.com/scrapy/scrapy/commit/e0c828b7f665d8c82e17787996634bc072e416ae \"chore\\(docs\\): refactor config \\(#6623\\)\")| Jan 20, 2025  \n[AUTHORS](https://github.com/scrapy/scrapy/blob/master/AUTHORS \"AUTHORS\")| [AUTHORS](https://github.com/scrapy/scrapy/blob/master/AUTHORS \"AUTHORS\")| [Scrapinghub \u2192 Zyte](https://github.com/scrapy/scrapy/commit/f30f53b3cc958770406628411db3d93c925db59e \"Scrapinghub \u2192 Zyte\")| Feb 2, 2021  \n[CODE_OF_CONDUCT.md](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| [CODE_OF_CONDUCT.md](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| [Update Code of Conduct to Contributor Covenant v2.1](https://github.com/scrapy/scrapy/commit/9f45be439de8a3b9a6d201c33e98b408a73c02bb \"Update Code of Conduct to Contributor Covenant v2.1\")| Oct 27, 2022  \n[CONTRIBUTING.md](https://github.com/scrapy/scrapy/blob/master/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/scrapy/scrapy/blob/master/CONTRIBUTING.md \"CONTRIBUTING.md\")| [Be consistent with domain used for links to documentation website](https://github.com/scrapy/scrapy/commit/65d631329a1434ec013f24341e4b8520241aec70 \"Be consistent with domain used for links to documentation website\")| Jan 30, 2019  \n[INSTALL.md](https://github.com/scrapy/scrapy/blob/master/INSTALL.md \"INSTALL.md\")| [INSTALL.md](https://github.com/scrapy/scrapy/blob/master/INSTALL.md \"INSTALL.md\")| [Update and rename INSTALL to INSTALL.md](https://github.com/scrapy/scrapy/commit/e60e8224a23ddd4fd98c54318d128f6744f40a6a \"Update and rename INSTALL to INSTALL.md\")| Oct 6, 2022  \n[LICENSE](https://github.com/scrapy/scrapy/blob/master/LICENSE \"LICENSE\")| [LICENSE](https://github.com/scrapy/scrapy/blob/master/LICENSE \"LICENSE\")| [added oxford commas to LICENSE](https://github.com/scrapy/scrapy/commit/9bd5444a42e82a05d2791fd10a205a9dd99303a6 \"added oxford commas to LICENSE\")| Jun 1, 2018  \n[MANIFEST.in](https://github.com/scrapy/scrapy/blob/master/MANIFEST.in \"MANIFEST.in\")| [MANIFEST.in](https://github.com/scrapy/scrapy/blob/master/MANIFEST.in \"MANIFEST.in\")| [Integrating configs into pyproject.toml (](https://github.com/scrapy/scrapy/commit/8c23da943c5e892515f4fa2eb57229839802010a \"Integrating configs into pyproject.toml \\(#6547\\)\")[#6547](https://github.com/scrapy/scrapy/pull/6547)[)](https://github.com/scrapy/scrapy/commit/8c23da943c5e892515f4fa2eb57229839802010a \"Integrating configs into pyproject.toml \\(#6547\\)\")| Nov 19, 2024  \n[NEWS](https://github.com/scrapy/scrapy/blob/master/NEWS \"NEWS\")| [NEWS](https://github.com/scrapy/scrapy/blob/master/NEWS \"NEWS\")| [added NEWS file pointing to docs/news.rst](https://github.com/scrapy/scrapy/commit/f5b87dbef8f3c657e31eeb026d8a77b0ccc77c29 \"added NEWS file pointing to docs/news.rst\")| Apr 28, 2012  \n[README.rst](https://github.com/scrapy/scrapy/blob/master/README.rst \"README.rst\")| [README.rst](https://github.com/scrapy/scrapy/blob/master/README.rst \"README.rst\")| [Remove trailing whitespace](https://github.com/scrapy/scrapy/commit/bee74fb753afa373ecdb1dbf272af9a108b14b22 \"Remove trailing whitespace\")| Mar 11, 2025  \n[SECURITY.md](https://github.com/scrapy/scrapy/blob/master/SECURITY.md \"SECURITY.md\")| [SECURITY.md](https://github.com/scrapy/scrapy/blob/master/SECURITY.md \"SECURITY.md\")| [Bump version: 2.11.2 \u2192 2.12.0](https://github.com/scrapy/scrapy/commit/b1f9e56693cd2000ddcea922306f726f3e9339af \"Bump version: 2.11.2 \u2192 2.12.0\")| Nov 18, 2024  \n[codecov.yml](https://github.com/scrapy/scrapy/blob/master/codecov.yml \"codecov.yml\")| [codecov.yml](https://github.com/scrapy/scrapy/blob/master/codecov.yml \"codecov.yml\")| [codecov config: disable project check, tweak PR comments](https://github.com/scrapy/scrapy/commit/9ce03d096d7ed569071a1951d9e327d7294e1a83 \"codecov config: disable project check, tweak PR comments\")| May 18, 2017  \n[conftest.py](https://github.com/scrapy/scrapy/blob/master/conftest.py \"conftest.py\")| [conftest.py](https://github.com/scrapy/scrapy/blob/master/conftest.py \"conftest.py\")| [Fix running simple tests with --reactor=default. (](https://github.com/scrapy/scrapy/commit/872924721344502cca92a72ce68d5f78a9b9e5bc \"Fix running simple tests with --reactor=default. \\(#6735\\)\")[#6735](https://github.com/scrapy/scrapy/pull/6735)[)](https://github.com/scrapy/scrapy/commit/872924721344502cca92a72ce68d5f78a9b9e5bc \"Fix running simple tests with --reactor=default. \\(#6735\\)\")| Mar 15, 2025  \n[pyproject.toml](https://github.com/scrapy/scrapy/blob/master/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/scrapy/scrapy/blob/master/pyproject.toml \"pyproject.toml\")| [Convert remaining unittest assert* calls, use the tmp_path fixture. (](https://github.com/scrapy/scrapy/commit/eb654aa1a8d2ef6433957fcc1361420b6141094e \"Convert remaining unittest assert* calls, use the tmp_path fixture. \\(#6725\\)\")[#\u2026](https://github.com/scrapy/scrapy/pull/6725)| Mar 11, 2025  \n[tox.ini](https://github.com/scrapy/scrapy/blob/master/tox.ini \"tox.ini\")| [tox.ini](https://github.com/scrapy/scrapy/blob/master/tox.ini \"tox.ini\")| [Codecov: Add test analytics (](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")[#6741](https://github.com/scrapy/scrapy/pull/6741)[)](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")| Mar 21, 2025  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/scrapy/scrapy/)\n  * [Code of conduct](https://github.com/scrapy/scrapy/)\n  * [BSD-3-Clause license](https://github.com/scrapy/scrapy/)\n  * [Security](https://github.com/scrapy/scrapy/)\n\n\n[![https://scrapy.org/img/scrapylogo.png](https://camo.githubusercontent.com/664eb5eae435b24275fd1740cb050d5079478554958dc25f44a063b40ee65551/68747470733a2f2f7363726170792e6f72672f696d672f7363726170796c6f676f2e706e67)](https://scrapy.org/)\n## Scrapy\n[](https://github.com/scrapy/scrapy/#scrapy)\n[![PyPI Version](https://camo.githubusercontent.com/7f23a6a9b0298efa3e2944ff9502a91459d70dd0d5244cb444fc342e1533c8f6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f5363726170792e737667) ](https://pypi.org/pypi/Scrapy) [![Supported Python Versions](https://camo.githubusercontent.com/6bfde535940031459b168bf7591e43cac9d5e0b3d1209f282ca1624cd63e4d14/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f5363726170792e737667) ](https://pypi.org/pypi/Scrapy) [![Ubuntu](https://github.com/scrapy/scrapy/workflows/Ubuntu/badge.svg) ](https://github.com/scrapy/scrapy/actions?query=workflow%3AUbuntu) [![Windows](https://github.com/scrapy/scrapy/workflows/Windows/badge.svg) ](https://github.com/scrapy/scrapy/actions?query=workflow%3AWindows) [![Wheel Status](https://camo.githubusercontent.com/55f433059ee431d5b2ffd26fdea757a2b88923121950c6840987f5563900e40f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f776865656c2d7965732d627269676874677265656e2e737667) ](https://pypi.org/pypi/Scrapy) [![Coverage report](https://camo.githubusercontent.com/cbe200ed5f89e6f0076d98a4d7f2deac81dcf7db4bad2401a83337a7d136ed4e/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f7363726170792f7363726170792f6d61737465722e737667) ](https://codecov.io/github/scrapy/scrapy?branch=master) [![Conda Version](https://camo.githubusercontent.com/32de2f70318d9be0205e95ca2ff5c2946a26b7e3519890747e4d37c0aa709526/68747470733a2f2f616e61636f6e64612e6f72672f636f6e64612d666f7267652f7363726170792f6261646765732f76657273696f6e2e737667) ](https://anaconda.org/conda-forge/scrapy)\n### Overview\n[](https://github.com/scrapy/scrapy/#overview)\nScrapy is a BSD-licensed fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.\nScrapy is maintained by [Zyte](https://www.zyte.com/) (formerly Scrapinghub) and [many other contributors](https://github.com/scrapy/scrapy/graphs/contributors).\nCheck the Scrapy homepage at <https://scrapy.org> for more information, including a list of features.\n### Requirements\n[](https://github.com/scrapy/scrapy/#requirements)\n  * Python 3.9+\n  * Works on Linux, Windows, macOS, BSD\n\n\n### Install\n[](https://github.com/scrapy/scrapy/#install)\nThe quick way:\n```\npip install scrapy\n```\n\nSee the install section in the documentation at <https://docs.scrapy.org/en/latest/intro/install.html> for more details.\n### Documentation\n[](https://github.com/scrapy/scrapy/#documentation)\nDocumentation is available online at <https://docs.scrapy.org/> and in the `docs` directory.\n### Releases\n[](https://github.com/scrapy/scrapy/#releases)\nYou can check <https://docs.scrapy.org/en/latest/news.html> for the release notes.\n### Community (blog, twitter, mail list, IRC)\n[](https://github.com/scrapy/scrapy/#community-blog-twitter-mail-list-irc)\nSee <https://scrapy.org/community/> for details.\n### Contributing\n[](https://github.com/scrapy/scrapy/#contributing)\nSee <https://docs.scrapy.org/en/master/contributing.html> for details.\n#### Code of Conduct\n[](https://github.com/scrapy/scrapy/#code-of-conduct)\nPlease note that this project is released with a Contributor [Code of Conduct](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md).\nBy participating in this project you agree to abide by its terms. Please report unacceptable behavior to opensource@zyte.com.\n### Companies using Scrapy\n[](https://github.com/scrapy/scrapy/#companies-using-scrapy)\nSee <https://scrapy.org/companies/> for a list.\n### Commercial Support\n[](https://github.com/scrapy/scrapy/#commercial-support)\nSee <https://scrapy.org/support/> for details.\n## About\nScrapy, a fast high-level web crawling & scraping framework for Python. \n[scrapy.org](https://scrapy.org \"https://scrapy.org\")\n### Topics\n[ python ](https://github.com/topics/python \"Topic: python\") [ crawler ](https://github.com/topics/crawler \"Topic: crawler\") [ framework ](https://github.com/topics/framework \"Topic: framework\") [ scraping ](https://github.com/topics/scraping \"Topic: scraping\") [ crawling ](https://github.com/topics/crawling \"Topic: crawling\") [ web-scraping ](https://github.com/topics/web-scraping \"Topic: web-scraping\") [ hacktoberfest ](https://github.com/topics/hacktoberfest \"Topic: hacktoberfest\") [ web-scraping-python ](https://github.com/topics/web-scraping-python \"Topic: web-scraping-python\")\n### Resources\n[ Readme ](https://github.com/scrapy/scrapy/#readme-ov-file)\n### License\n[ BSD-3-Clause license ](https://github.com/scrapy/scrapy/#BSD-3-Clause-1-ov-file)\n### Code of conduct\n[ Code of conduct ](https://github.com/scrapy/scrapy/#coc-ov-file)\n### Security policy\n[ Security policy ](https://github.com/scrapy/scrapy/#security-ov-file)\n[ Activity](https://github.com/scrapy/scrapy/activity)\n[ Custom properties](https://github.com/scrapy/scrapy/custom-properties)\n### Stars\n[ **54.7k** stars](https://github.com/scrapy/scrapy/stargazers)\n### Watchers\n[ **1.8k** watching](https://github.com/scrapy/scrapy/watchers)\n### Forks\n[ **10.7k** forks](https://github.com/scrapy/scrapy/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy&report=scrapy+%28user%29)\n##  [Releases 39](https://github.com/scrapy/scrapy/releases)\n[ 2.12.0 Latest  Nov 18, 2024 ](https://github.com/scrapy/scrapy/releases/tag/2.12.0)\n[+ 38 releases](https://github.com/scrapy/scrapy/releases)\n##  [Packages 0](https://github.com/orgs/scrapy/packages?repo_name=scrapy)\nNo packages published \n##  [Used by 52.2k](https://github.com/scrapy/scrapy/network/dependents)\n[\n  * ![@PHFernandes9](https://avatars.githubusercontent.com/u/194980440?s=64&v=4)\n  * ![@Victor-Genius](https://avatars.githubusercontent.com/u/200241745?s=64&v=4)\n  * ![@Ayaaayaaa](https://avatars.githubusercontent.com/u/146444257?s=64&v=4)\n  * ![@maichi98](https://avatars.githubusercontent.com/u/58106854?s=64&v=4)\n  * ![@hiteshsurya17](https://avatars.githubusercontent.com/u/83537863?s=64&v=4)\n  * ![@ximengailun](https://avatars.githubusercontent.com/u/145632297?s=64&v=4)\n  * ![@moizez256](https://avatars.githubusercontent.com/u/28323489?s=64&v=4)\n  * ![@dkffkrsm](https://avatars.githubusercontent.com/u/47800954?s=64&v=4)\n\n+ 52,146  ](https://github.com/scrapy/scrapy/network/dependents)\n##  [Contributors 577](https://github.com/scrapy/scrapy/graphs/contributors)\n  * [ ![@pablohoffman](https://avatars.githubusercontent.com/u/185212?s=64&v=4) ](https://github.com/pablohoffman)\n  * [ ![@dangra](https://avatars.githubusercontent.com/u/37369?s=64&v=4) ](https://github.com/dangra)\n  * [ ![@wRAR](https://avatars.githubusercontent.com/u/241039?s=64&v=4) ](https://github.com/wRAR)\n  * [ ![@kmike](https://avatars.githubusercontent.com/u/107893?s=64&v=4) ](https://github.com/kmike)\n  * [ ![@Gallaecio](https://avatars.githubusercontent.com/u/705211?s=64&v=4) ](https://github.com/Gallaecio)\n  * [ ![@elacuesta](https://avatars.githubusercontent.com/u/1731933?s=64&v=4) ](https://github.com/elacuesta)\n  * [ ![@redapple](https://avatars.githubusercontent.com/u/886296?s=64&v=4) ](https://github.com/redapple)\n  * [ ![@void](https://avatars.githubusercontent.com/u/25639?s=64&v=4) ](https://github.com/void)\n  * [ ![@curita](https://avatars.githubusercontent.com/u/213781?s=64&v=4) ](https://github.com/curita)\n  * [ ![@eliasdorneles](https://avatars.githubusercontent.com/u/37565?s=64&v=4) ](https://github.com/eliasdorneles)\n  * [ ![@lopuhin](https://avatars.githubusercontent.com/u/424613?s=64&v=4) ](https://github.com/lopuhin)\n  * [ ![@Laerte](https://avatars.githubusercontent.com/u/5853172?s=64&v=4) ](https://github.com/Laerte)\n  * [ ![@whalebot-helmsman](https://avatars.githubusercontent.com/u/1234272?s=64&v=4) ](https://github.com/whalebot-helmsman)\n  * [ ![@adityaa30](https://avatars.githubusercontent.com/u/37837542?s=64&v=4) ](https://github.com/adityaa30)\n\n\n[+ 563 contributors](https://github.com/scrapy/scrapy/graphs/contributors)\n## Languages\n  * [ Python 99.8% ](https://github.com/scrapy/scrapy/search?l=python)\n  * Other 0.2%\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n## TL;DR Summary\n\n**Scrapy** is a high-level web crawling and scraping framework for Python, maintained by **Zyte**. It supports Python 3.9+ and is used for data mining, monitoring, and automated testing. Installation is simple via `pip install scrapy`. More info at [scrapy.org](https://scrapy.org).\n```"
  },
  {
    "instruction": "[![VectorHub by Superlinked](https://superlinked.com/vectorhub/_next/static/media/vectorhub-logo.d71e47bd.svg)](https://superlinked.com/vectorhub/)\n[Building Blocks](https://superlinked.com/vectorhub/building-blocks)[Articles](https://superlinked.com/vectorhub/all-articles)[Contributing](https://superlinked.com/vectorhub/contributing/contributing)[VDB Comparison](https://superlinked.com/vector-db-comparison/)\n[---](https://github.com/superlinked/superlinked)\nSearch\nSubscribe\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nMenu\nImprove your vector search[![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow-primary.2ff1db1a.svg)Try Superlinked](https://github.com/superlinked/superlinked)\nHome\nManifesto\nMost Recent\nCompare Vector DBs\nContributeSubscribe\nSearch\nTable of Contents\nRAPTOR's basic steps\nBuilding a RAPTOR tree\nCreating the Chunks\nHierarchical clustering and dimensionality reduction - setup\nHierarchical clustering\nRAPTOR tree\nOur query example\nRAPTOR query methods\nImplementing collapsed tree retrieval\nSetting up vector database and RAG\nHow did our RAPTOR RAG do?\nYour turn\nContributors\n  1. [Articles](https://superlinked.com/vectorhub/all-articles)\n/\n  2. [Improve rag with raptor](https://superlinked.com/vectorhub/articles/improve-rag-with-raptor/)\n\n\nPublication Date: October 29, 2024|\n#RAG\n|Update on Github\n# Improving RAG with RAPTOR\n## Takeaways\n[Notebook Code](https://colab.research.google.com/github/superlinked/VectorHub/blob/main/docs/assets/use_cases/improve-rag-with-raptor/raptor_with_rag.ipynb)\n  * RAPTOR solves standard RAG's chunking issues through hierarchical clustering\n\n\n  * Tree construction process: chunk docs -> embed chunks -> cluster -> summarize recursively\n\n\n  * Collapsed tree method works better for factual queries vs tree traversal\n\n\n  * Implementation uses LanceDB + embeddings via sentence-transformers\n\n\n  * RAPTOR outperforms vanilla RAG by preserving document relationships\n\n\n  * Enables both hierarchical and flat retrieval through single vector store\n\n\nTraditional [RAG](https://superlinked.com/vectorhub/articles/retrieval-augmented-generation) setups commonly split documents into fixed-size chunks. But this creates problems. If key concepts span multiple chunks, the embeddings can lose the semantic coherence of the original text. LLM queries that retrieve single chunks frequently _miss_ their relationship to crucial pieces of information buried inside other chunks. This leads to incomplete or misleading responses. **Because its chunk embeddings lack any weighting or hierarchical structure, traditional RAG's flat retrieval returns results based only on similarity or relevance scores. Key insights are often lost.**\nSo, **is there a way of getting our embeddings to preserve the relationships and hierarchical structure that exists within source documents, so that our retrieval can surface key insights, and do it efficiently**?\nYes! It's a form of semantic chunking called **hierarchical-cluster-embedding-based chunking**. Below, we'll look closely at a recent and innovative implementation of hierarchical clustering called [RAPTOR](https://arxiv.org/pdf/2401.18059) (Recursive Abstractive Processing for Tree Organized Retrieval). We'll walk you step-by-step through setting up RAPTOR RAG. Then we'll run an example query on a financial news document to evaluate **how RAPTOR performs against vanilla RAG**.\nLet's get started!\n## RAPTOR's basic steps\nRAPTOR organizes information in a tree-like structure, progressively narrowing as it moves upwards, representing original documents at higher and higher levels of abstraction, so it can retain the original meaning in full. Let's break this into steps.\n![High level overview of RAPTOR](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/improve-rag-with-raptor/raptor-7.png)\n_Above: \"Figure 1. Tree construction process\" from the[RAPTOR paper](https://arxiv.org/pdf/2401.18059)_\n**RAPTOR tree construction - basic steps:**\n  * **starting documents (leaves)** : RAPTOR first splits each raw document into chunks\n  * **embedding and clustering** : we embed these chunks ([RAPTOR authors](https://arxiv.org/pdf/2401.18059) use the UMAP dimensionality reduction algorithm to minimize chunk embedding size), and, based on their vector embeddings, cluster the chunks (using Gaussian Mixture Models (GMM) to generate distributions that consider the entire training space; each point can belong to more than one distribution) to ensure effective grouping of high-dimensional vector embeddings\n  * **summarizing clusters** : we summarize the clusters of similar chunks into a node of higher-level abstractions\n  * **recursive iteration** : we treat these nodes in turn as chunks, clustering them, summarizing these clusters, iteratively, building a tree-like structure that encapsulates the relationships and hierarchical structures inherent in the raw documents we started from\n\n\n![Raptor workflow](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/improve-rag-with-raptor/raptor-2.png)\nIn short, RAPTOR chunks raw documents, embeds the chunks, groups embeddings into clusters, summarizes each cluster, then (recursively) chunks these summaries, embeds, summarizes again, and so on, moving upwards to a comprehensive and efficient data representation, preserving nuances and key points.\n## Building a RAPTOR tree\nNow that we\u2019ve laid out RAPTOR's basic steps, let\u2019s walk through how to _implement_ a RAPTOR tree.\nWe start with our basic setup and imports.\n```\n\npip install lancedb scikit-learn openai torch sentence_transformers tiktoken umap-learn PyPDF2\n\n```\n```\n\nimport os\nimport uuid\nimport tiktoken\nimport re\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\nimport umap.umap_ as umap\nimport matplotlib.pyplot as plt\nfrom openai import OpenAI\nfrom typing import List, Tuple, Optional, Dict\nfrom sklearn.mixture import GaussianMixture\nfrom sentence_transformers import SentenceTransformer\n\nopenai_api_key =\"sk-XXXXXXXXXXXXXXX\"client = OpenAI(api_key=openai_api_key)\n\n```\n\nNote: Don't forget to input your open_ai_key above (we'll be using gpt-4 below).\n### Creating the Chunks\nFirst, create a directory_path to your source documents. To simplify things, our use case example will use just [one financial document](https://www.nttdata.com/global/en/-/media/nttdataglobal/1_files/investors/financial-results/2021/fy2021_fs_3q.pdf) as a source. You can of course load any source documents (and alter the query below) in your own experiments!\n(Note: make sure you use an unencrypted version of this file to avoid errors when PyPDF2 tries to read the file.)\n```\n\ndirectory_path ='/content/data'ifnot os.path.exists(directory_path):  os.makedirs(directory_path)print(f\"Directory '{directory_path}' created successfully.\")else:print(f\"Directory '{directory_path}' already exists.\")\n\n```\n\nNow, let's break our text documents down into smaller chunks.\n```\n\nimport PyPDF2\n\n# Function to extract text from a PDF filedefextract_pdf_text(file_path):withopen(file_path,'rb')asfile:    reader = PyPDF2.PdfReader(file)    text =\"\"for page_num inrange(len(reader.pages)):      page = reader.pages[page_num]      text += page.extract_text()return text\n\n# Function to split text into chunks with overlapdefsplit_text(text, chunk_size=1000, chunk_overlap=50):  chunks =[]  start =0while start <len(text):    end =min(start + chunk_size,len(text))    chunks.append(text[start:end])    start += chunk_size - chunk_overlap\nreturn chunks\n\n# Function to process all PDFs in a directorydefprocess_directory(directory_path, chunk_size=1000, chunk_overlap=50):  all_chunks =[]# Iterate over all PDF files in the directoryfor filename in os.listdir(directory_path):if filename.endswith(\".pdf\"):      file_path = os.path.join(directory_path, filename)print(f\"Processing file: {file_path}\")      \n# Step 1: Extract text from the PDF      pdf_text = extract_pdf_text(file_path)      \n# Step 2: Split the extracted text into chunks      chunks = split_text(pdf_text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)      \n# Append chunks from this file to the overall list      all_chunks.extend(chunks)  \nreturn all_chunks\n\ndirectory_path = os.path.join(os.getcwd(),\"data\")# Specify your directory pathchunk_size =1000chunk_overlap =50\n# Process all PDF files in the directory and get the chunkschunks = process_directory(directory_path, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n# Optional: Print the number of chunks and preview some of the chunksprint(f\"Total number of chunks: {len(chunks)}\")for i, chunk inenumerate(chunks[:5]):# Show first 5 chunks as a previewprint(f\"Chunk {i+1}:\\n{chunk}\\n\")\n\n```\n\nNow that we have our chunks, let's move on and set up our embedding process, first by initializing our embedding model. We'll use `all-MiniLM-L6-v2` (Sentence Transformers), but you can choose any embedding model that fits your use case.\n```\n\nembedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n```\n\n### Hierarchical clustering and dimensionality reduction - setup\nTo preserve the essential structures and meaning of our data, we need to cluster it so we can easily access both its broad categories and its intricate details. One efficient way of doing this is with a **hierarchical clustering structure**. This is where UMAP comes in. Traditional clustering methods like Gaussian Mixture Models (GMM) often struggle with clustering high-dimensional data chunks. We can make things easier for our GMM by first using UMAP (Uniform Manifold Approximation and Projection) to simplify our data but preserve its essential structures.\nUMAP's **`n_neighbors`**parameter dictates how much of the data's neighborhood UMAP considers during dimensionality reduction. Higher`n_neighbors` tells UMAP to look at many neighbors, maintaining the data's global structure and producing larger, more general clusters - **global clusters**. Lower `n_neighbors`, conversely, prompts UMAP to focus on close relationships, preserving local structure and forming smaller, more detailed, **local clusters**.\n```\n\ndefdimensionality_reduction(  embeddings: np.ndarray,  target_dim:int,  clustering_type:str,  metric:str=\"cosine\",)-> np.ndarray:if clustering_type ==\"local\":    n_neighbors =max(2,min(10,len(embeddings)-1))    min_dist =0.01elif clustering_type ==\"global\":    n_neighbors =max(2,min(int((len(embeddings)-1)**0.5),len(embeddings)//10,len(embeddings)-1))    min_dist =0.1else:raise ValueError(\"clustering_type must be either 'local' or 'global'\")\n  umap_model = umap.UMAP(    n_neighbors=n_neighbors,    min_dist=min_dist,    n_components=target_dim,    metric=metric,)return umap_model.fit_transform(embeddings)\n\n```\n\nTo determine the number of clusters that optimally captures our data's relationships and structure, we'll use the Elbow Method and the Bayesian Information Criterion (BIC). The Elbow Method plots explain variance against the number of clusters, helping us know when adding more clusters results in diminishing returns, while BIC evaluates models based on their fit to the data while penalizing complexity.\nOnce we've determined our `optimal_cluster_number`, we'll fit a GMM with it, predict the probability of each embedding belonging to each cluster, and assign labels to our embeddings.\n```\n\ndefcompute_inertia(embeddings: np.ndarray, labels: np.ndarray, centroids: np.ndarray)->float:return np.sum(np.min(np.sum((embeddings[:, np.newaxis]- centroids)**2, axis=2), axis=1))\ndefoptimal_cluster_number(  embeddings: np.ndarray,  max_clusters:int=50,  random_state:int= SEED\n)->int:  max_clusters =min(max_clusters,len(embeddings))  number_of_clusters = np.arange(1, max_clusters +1)  inertias =[]  bic_scores =[]  \nfor n in number_of_clusters:    gmm = GaussianMixture(n_components=n, random_state=random_state)    labels = gmm.fit_predict(embeddings)    centroids = gmm.means_\n    inertia = compute_inertia(embeddings, labels, centroids)    inertias.append(inertia)    bic_scores.append(gmm.bic(embeddings))  \n  inertia_changes = np.diff(inertias)  elbow_optimal = number_of_clusters[np.argmin(inertia_changes)+1]  bic_optimal = number_of_clusters[np.argmin(bic_scores)]  \nreturnmax(elbow_optimal, bic_optimal)\ndefgmm_clustering(  embeddings: np.ndarray,  threshold:float,  random_state:int= SEED\n)-> Tuple[List[np.ndarray],int]:  n_clusters = optimal_cluster_number(embeddings, random_state=random_state)  gm = GaussianMixture(n_components=n_clusters, random_state=random_state, n_init=2)  gm.fit(embeddings)  probs = gm.predict_proba(embeddings)  labels =[np.where(prob > threshold)[0]for prob in probs]return labels, n_clusters \n\n```\n\nSo far, we've chunked our documents and defined functions - for embedding our chunks, reducing their dimensions using UMAP, and clustering them. Now let's set up the recursive hierarchical clustering that is RAPTOR's defining feature.\n## Hierarchical clustering\nTo set up our hierarchical clustering, we define several functions:\n  * a clustering_algorithm function to manage both global and local clustering using GMMs - first globally, then locally within each global cluster (start with a broad overview, and then zoom in for more detailed clusters before summarizing)\n  * a generate_summary function to generate text summaries using a language model\n  * an embed clusters function to manage the embedding and initial clustering\n  * an embed cluster summaries function for summarizing each cluster\n  * a recursive_embedding_with_cluster_summarization function that leverages these methods to create a multi-level hierarchical clustering and summarization structure\n\n```\n\n#define our clustering algorithmdefclustering_algorithm(  embeddings: np.ndarray,  target_dim:int,  threshold:float,  random_state:int= SEED\n)-> Tuple[List[np.ndarray],int]:iflen(embeddings)<= target_dim +1:return[np.array([0])for _ inrange(len(embeddings))],1  \n# Global clustering  reduced_global_embeddings = dimensionality_reduction(embeddings, target_dim,\"global\")  global_clusters, n_global_clusters = gmm_clustering(reduced_global_embeddings, threshold, random_state=random_state)\n  all_local_clusters =[np.array([])for _ inrange(len(embeddings))]  total_clusters =0\n# Local clustering within each global clusterfor i inrange(n_global_clusters):    global_cluster_mask = np.array([i in gc for gc in global_clusters])    global_cluster_embeddings = embeddings[global_cluster_mask]\niflen(global_cluster_embeddings)<= target_dim +1:# Assign all points in this global cluster to a single local clusterfor idx in np.where(global_cluster_mask)[0]:        all_local_clusters[idx]= np.append(all_local_clusters[idx], total_clusters)      total_clusters +=1continue\ntry:      reduced_local_embeddings = dimensionality_reduction(global_cluster_embeddings, target_dim,\"local\")      local_clusters, n_local_clusters = gmm_clustering(reduced_local_embeddings, threshold, random_state=random_state)\n# Assign local cluster IDsfor j inrange(n_local_clusters):        local_cluster_mask = np.array([j in lc for lc in local_clusters])        global_indices = np.where(global_cluster_mask)[0]        local_indices = global_indices[local_cluster_mask]for idx in local_indices:          all_local_clusters[idx]= np.append(all_local_clusters[idx], j + total_clusters)\n      total_clusters += n_local_clusters\nexcept Exception as e:print(f\"Error in local clustering for global cluster {i}: {str(e)}\")# Assign all points in this global cluster to a single local clusterfor idx in np.where(global_cluster_mask)[0]:        all_local_clusters[idx]= np.append(all_local_clusters[idx], total_clusters)      total_clusters +=1\nreturn all_local_clusters, total_clusters\n  \ndefgenerate_summary(context:str)->str:  prompt =f\"\"\"\n  Provide the Summary for the given context. Here are some additional instructions for you:\n  Instructions:\n  1. Don't make things up, Just use the contexts and generate the relevant summary.\n  2. Don't mix the numbers, Just use the numbers in the context.\n  3. Don't try to use fancy words, stick to the basics of the language that is being used in the context.\n  Context: {context}  \"\"\"  response = client.chat.completions.create(    model=\"gpt-4\",    messages=[{\"role\":\"system\",\"content\":\"You are a helpful assistant that summarizes text.\"},{\"role\":\"user\",\"content\": prompt}],    max_tokens=200,    n=1,    stop=None,    temperature=0.7)  summary = response.choices[0].message.content.strip()return summary\n\ndefembed_clusters(  texts: List[str],  target_dim:int=10,  threshold:float=0.1)-> pd.DataFrame:  textual_embeddings = np.array(embedding_model.encode(texts))  clusters, number_of_clusters = clustering_algorithm(textual_embeddings, target_dim, threshold)print(f\"Number of clusters: {number_of_clusters}\")return pd.DataFrame({\"texts\": texts,\"embedding\":list(textual_embeddings),\"clusters\": clusters\n})\ndefembed_cluster_summaries(  texts: List[str],  level:int,  target_dim:int=10,  threshold:float=0.1)-> Tuple[pd.DataFrame, pd.DataFrame]:  df_clusters = embed_clusters(texts, target_dim, threshold)  main_list =[]  \nfor _, row in df_clusters.iterrows():for cluster in row[\"clusters\"]:      main_list.append({\"text\": row[\"texts\"],\"embedding\": row[\"embedding\"],\"clusters\": cluster\n})  \n  main_df = pd.DataFrame(main_list)  unique_clusters = main_df[\"clusters\"].unique()iflen(unique_clusters)==0:return df_clusters, pd.DataFrame(columns=[\"summaries\",\"level\",\"clusters\"])\nprint(f\"--Generated {len(unique_clusters)} clusters--\")\n  summaries =[]for cluster in unique_clusters:    text_in_df = main_df[main_df[\"clusters\"]== cluster]    unique_texts = text_in_df[\"text\"].tolist()    text =\"------\\n------\".join(unique_texts)    summary = generate_summary(text)    summaries.append(summary)\n  df_summaries = pd.DataFrame({\"summaries\": summaries,\"level\":[level]*len(summaries),\"clusters\": unique_clusters\n})\nreturn df_clusters, df_summaries\n\ndefrecursive_embedding_with_cluster_summarization(  texts: List[str],  number_of_levels:int=3,  level:int=1,  target_dim:int=10,  threshold:float=0.1)-> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:if level > number_of_levels:return{}  \n  results ={}  df_clusters, df_summaries = embed_cluster_summaries(texts, level, target_dim, threshold)  results[level]=(df_clusters, df_summaries)  \nif df_summaries.empty orlen(df_summaries['clusters'].unique())==1:print(f\"No more unique clusters found at level {level}. Stopping recursion.\")return results\n  \nif level < number_of_levels:    next_level_texts = df_summaries['summaries'].tolist()    next_level_results = recursive_embedding_with_cluster_summarization(      next_level_texts,      number_of_levels,      level +1,      target_dim,      threshold\n)    results.update(next_level_results)  \nreturn results\n\n```\n\nNow we define a process_text_hierarchy function that takes a hierarchical text dataset, processes it to generate embeddings and cluster summaries, and returns the processed results organized by hierarchical level.\n```\n\ndefprocess_text_hierarchy(  texts: List[str],  number_of_levels:int=3,  target_dim:int=10,  threshold:float=0.1)-> Dict[str, pd.DataFrame]:  hierarchy_results = recursive_embedding_with_cluster_summarization(    texts, number_of_levels, target_dim=target_dim, threshold=threshold\n)  \n  processed_results ={}for level,(df_clusters, df_summaries)in hierarchy_results.items():if df_clusters.empty or df_summaries.empty:print(f\"No data for level {level}. Skipping.\")continue    processed_results[f\"level_{level}_clusters\"]= df_clusters\n    processed_results[f\"level_{level}_summaries\"]= df_summaries\n  \nreturn processed_results\n\nresults = process_text_hierarchy(chunks, number_of_levels=3)\n\n```\n\nAfter several rounds of clustering and summarization, we have our RAPTOR tree.\n### RAPTOR tree\n  * **leaf nodes (base level)** - at the tree's base, we have our original, unprocessed text chunks - the raw data inputs that RAPTOR RAG uses for analysis and retrieval\n  * **intermediate summary nodes** - moving up the tree, we cluster our chunks, and summarize the cluster, creating summary nodes - each summary node captures the meaning of the cluster it represents, facilitating efficient retrieval and accurate contextual understanding\n  * **hierarchical embeddings (multi-level summarization)** - each level above the base (leaf) has summary nodes capturing the meaning of lower level summary nodes, continuing until we reach the top (or root node) of the tree\n\n\nWith our RAPTOR tree built, starting from leaf nodes at the bottom and summarized nodes in between, it\u2019s time to query the RAG.\n### Our query example\nFor our use case we'll use a straightforward example - a query about financial news, aimed at understanding an increase in NTT DATA's net income attributable to shareholders:\n```\n\nquery = \"NTT DATA's net income attributable to shareholders increased from \u00a569,227 million in Q3 FY2020 to \u00a5110,191 million in Q3 FY2021. How does this growth align with their acquisition strategy, particularly considering their stated reasons for acquiring Nexient, LLC and the provisional goodwill recorded in this transaction?\"\n\n```\n\nTo keep things simple in our implementation, our data source will be [this pdf](https://www.nttdata.com/global/en/-/media/nttdataglobal/1_files/investors/financial-results/2021/fy2021_fs_3q.pdf).\nNow that we have our query and raw source document, let's choose the right RAPTOR method for querying our RAPTOR tree.\n### RAPTOR query methods\nRAPTOR has two distinct strategies for querying the RAPTOR tree: tree traversal and collapsed tree.\n  1. **Tree traversal** method - traverses the tree layer by layer, first selecting the most relevant (top-k in query vector cosine similarity) root nodes, then (in the next layer) navigating each selected root node's children to again find the top-k most relevant nodes, continuing on until it reaches the leaf nodes (original text chunks). Then, it concatenates the text from all the selected nodes, returning it as the query result.\n  2. **Collapsed tree** method - flattens the tree into a single layer, directly comparing the query embedding to the vector embeddings of all the leaf nodes (the original text chunks) and summary nodes. For factual, keyword-based queries where you need specific details, collapsed tree returns more relevant and accurate results (with lower latency) than the tree traversal method.\n\n\n![How the retrieval happens](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/improve-rag-with-raptor/raptor-4.png) _Above: \"Figure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms\" from the[RAPTOR paper](https://arxiv.org/pdf/2401.1805)_\nIf our query demanded complex multi-level reasoning, and a contextually rich and precise result, it would make sense to use tree traversal. But for specific queries requiring specific factual information - like our financial news query, we want to be able to directly compare our query embedding with the vector embeddings of all nodes (both leaf and summary), efficiently bypassing RAPTOR's hierarchical structure and going straight to the most relevant data points.\nBut even though the collapsed tree method's retrieval bypasses the RAPTOR tree's hierarchy, it still capitalizes on the RAPTOR tree's hierarchical encapsulation of meaning to retrieve context. Because the collapsed tree method treats summarized nodes from higher levels simply as additional (same level) chunks, we can pull in higher-level summaries (the global perspective) alongside granular details in just one pass. We want our retrieval to get both an overall perspective and pinpoint very specific details of a particular company's financial quarter.\nFor our purposes, the collapsed tree method is a better fit than tree traversal.\nSo how do we implement it?\n## Implementing collapsed tree retrieval\nCollapsed tree retrieval retrieves top-k nodes (highest cosine similarity to the query vector). To do this, we gather our original root document text chunks and summarized nodes at each level for all the clusters into one big list. Next, we embed all these data points, storing them in a [vector database](https://superlinked.com/vector-db-comparison), which we'll query with our RAG.\nFirst, the embeddings.\n```\n\nraptor_texts =[]for level, row in results.items():if level.endswith(\"clusters\"):    raptor_texts.extend(row[\"texts\"])else:    raptor_texts.extend(row[\"summaries\"])    \nraptor_embeddings = embedding_model.encode(raptor_texts)len(raptor_embeddings)\n\n```\n\n### Setting up vector database and RAG\nNow, we set up a LanceDB vector database.\n```\n\nraptor_embeddings = embedding_model.encode(raptor_texts)\nraptor_dict ={\"texts\":[],\"embeddings\":[]}for texts, embeddings inzip(raptor_texts, raptor_embeddings):  raptor_dict[\"texts\"].append(texts)  raptor_dict[\"embeddings\"].append(embeddings.tolist())\n\n```\n```\n\nimport lancedb\nimport pyarrow as pa\nfrom lancedb.pydantic import Vector, LanceModel\n\nuri =\"lancedb_database\"db = lancedb.connect(uri)\nclassRAG(LanceModel):  texts :str  embeddings : Vector(384)\ntable_name =\"rag_with_raptor\"raptor_table = db.create_table(table_name, schema = RAG, mode=\"overwrite\")raptor_table.add(rag_raptor_df)raptor_table.create_fts_index(\"texts\", replace=True)\n\n```\n\nNext, we define our results generation function...\n```\n\ndefgenerate_results(  query :str,  context_text :str)->str:\n  prompt =f\"\"\"\n  Based on the context provided, use it to answer the query. \n  query : {query}  Instructions:\n  1. Don't make things up, Just use the contexts and generate the relevant answer.\n  2. Don't mix the numbers, Just use the numbers in the context.\n  3. Don't try to use fancy words, stick to the basics of the language that is being used in the context.\n{context_text}  \"\"\"  response = client.chat.completions.create(    model=\"gpt-4\",    messages=[{\"role\":\"system\",\"content\":\"You are a helpful assistant that answers query and give the answers.\"},{\"role\":\"user\",\"content\": prompt}],    max_tokens=200,    n=1,    stop=None,    temperature=0.7)  answer = response.choices[0].message.content.strip()return answer\n\n```\n\n...and, finally, input our query.\n```\n\nquery =\"NTT DATA's net income attributable to shareholders increased from \u00a569,227 million in Q3 FY2020 to \u00a5110,191 million in Q3 FY2021. How does this growth align with their acquisition strategy, particularly considering their stated reasons for acquiring Nexient, LLC and the provisional goodwill recorded in this transaction?\"\n\n```\n\nWe want our retrieval to address our query's key points. First, we expect our retrieval to take account of the provided factual data regarding NTT DATA's increased shareholder income. Second, we need our retrieval to answer how this increase fits in with NTT Data's acquisition strategy (in particular the Nexient acquisition rationale). Let's keep these in mind when we examine our RAPTOR RAG results. For comparison purposes, we'll also create a vanilla (i.e., normal) RAG:\n```\n\nnormal_embeddings = embedding_model.encode(chunks)# default chunks from our data\nnormal_dict ={\"texts\":[],\"embeddings\":[]}for texts, embeddings inzip(chunks, normal_embeddings):  normal_dict[\"texts\"].append(texts)  normal_dict[\"embeddings\"].append(embeddings.tolist())  \nrag_normal_df = pd.DataFrame(normal_dict)\ntable_name =\"rag_without_raptor\"normal_table = db.create_table(table_name, schema = RAG, mode=\"overwrite\")normal_table.add(rag_normal_df)normal_table.create_fts_index(\"texts\", replace=True)\n\n```\n\nBecause we now have cluster-level summary nodes in addition to our original text chunks, our RAPTOR RAG has more chunks than our vanilla / normal RAG.\n![Number of chunks in RAPTOR](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/improve-rag-with-raptor/raptor-5.png)\n### How did our RAPTOR RAG do?\nLet's generate and then compare and evaluate our results.\n```\n\nraptor_contexts = raptor_table.search(query).limit(5).select([\"texts\"]).to_list()raptor_context_text =\"------\\n\\n\".join([context[\"texts\"]for context in raptor_contexts])raptor_context_text =\"------\\n\\n\"+ raptor_context_text\n\nnormal_contexts = normal_table.search(query).limit(5).select([\"texts\"]).to_list()normal_context_text =\"------\\n\\n\".join([context[\"texts\"]for context in normal_contexts])normal_context_text =\"------\\n\\n\"+ normal_context_text\n\nraptor_answer = generate_results(query, raptor_context_text)normal_answer = generate_results(query, normal_context_text)\n\n```\n\n![Comparison for a Query](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/improve-rag-with-raptor/raptor-6.png)\nRAPTOR RAG performed **better than vanilla RAG** at handling retrieval on our hierarchically chunked and embedded source document. RAPTOR retrieved specific details about NTT's financial growth for the specified quarter, and connected this growth to the broader acquisition strategy, pulling relevant context from our example [source document](https://www.nttdata.com/global/en/-/media/nttdataglobal/1_files/investors/financial-results/2021/fy2021_fs_3q.pdf). (Note: collapsed tree retrieval will _work well_ in this kind of use case - _a factual, keyword-based query requiring specific details_ - whether you have multiple source chunks or even many source documents.) Whereas, our vanilla RAG, while correctly identifying the specific details, failed to elucidate either NTT's strategy or a relationship between their quarterly growth, broader strategy, and the particular (Nexient) acquisition in question.\n## Your turn\nNow it's your turn to try out RAPTOR RAG! Here's the Google [colab](https://colab.research.google.com/github/superlinked/VectorHub/blob/main/docs/assets/use_cases/improve-rag-with-raptor/raptor_with_rag.ipynb).\nTo learn more about the intricacies of RAPTOR, check out their official [GitHub repository](https://github.com/parthsarthi03/raptor/tree/master). For an even deeper dive, we highly recommend the official [paper](https://arxiv.org/pdf/2401.18059)!\n## Contributors\n  * [Vipul Maheshwari, author](https://www.linkedin.com/in/vipulmaheshwarii/)\n  * [Robert Turner, editor](https://www.linkedin.com/in/robertdhayanturner/)\n\n\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nStay updated with VectorHub\nSubscribe\n![arrow](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nContinue Reading\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/research_agent_3a22caf3b8.png)\n#Superlinked\nSimplifying Complex Research with AI\nLearn how to build an AI research assistant that finds, summarizes, and answers questions about scie...\nMarch 23, 2025\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/rag_feedback_878ba93f75.png)\n#RAG\nYour RAG application is a communication system\nEffective RAG can take the best features of language models and moderate their shortcomings. In this...\nJuly 28, 2024\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/RAG_Evaluation_with_Arize_Phoenix_2x_1_ea16819113.png)\n#RAG\n#Quality And Evaluation\nRAG Evaluation with Arize Phoenix\nThis article covers a step by step implementation of RAG evaluation using a framework tool called Ar...\nJune 13, 2024\n[![Superlinked](https://superlinked.com/vectorhub/_next/static/media/superlinked-logo.9ad3911a.svg)](https://superlinked.com/)\nSubscribe\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nBy subscribing, you agree to our [Terms and Conditions](https://www.superlinked.com/policies/terms-and-conditions).\nAbout\n[Company](https://superlinked.com/)[Careers](https://superlinked.notion.site/Work-at-Superlinked-541f2d74ce714c889a1c6dba6dd0dbf1)[Index](https://superlinked.com/vectorhub/siteindex)\nSupport\nContact Us[Terms of Use](https://superlinked.com/policies/terms-and-conditions)[Privacy Policy](https://superlinked.com/policies/privacy-policy)[Cookie Policy](https://superlinked.com/policies/cookie-policy)\nSocial\n[Github](https://github.com/superlinked/superlinked)[X (Twitter)](https://twitter.com/superlinked)[LinkedIn](https://www.linkedin.com/company/superlinked/)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nRAPTOR enhances Retrieval-Augmented Generation (RAG) by addressing chunking issues through hierarchical clustering, enabling better semantic coherence. It outperforms traditional RAG by preserving document relationships, improving retrieval accuracy for factual queries.\n```"
  },
  {
    "instruction": "[ Tell 120+K peers about your AI research \u2192 Learn more \ud83d\udca1 ![](https://neptune.ai/wp-content/themes/neptune/img/icon-cancel.svg) ](https://neptune.ai/neurips-2024)\n[ ![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg) ](https://neptune.ai \"neptune.ai\")\n  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Walkthrough [2 min]](https://neptune.ai/walkthrough)\n      * [Play with public sandbox](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n      * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Neptune vs WandB](https://neptune.ai/vs/wandb)\n      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n      * [Other comparisons](https://neptune.ai/vs)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune Scale projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [AI Researcher](https://neptune.ai/product/ai-researcher)\n      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n      * [Academia & Kagglers](https://neptune.ai/research)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Monitor training](https://neptune.ai/product/monitor-training)\n      * [Compare experiments](https://neptune.ai/product/compare-experiments)\n      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)\n      * [Reports](https://neptune.ai/product/reports)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/deepsense-ai)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2025/01/Artera-color-signet.png?fit=129%2C97&ssl=1)Case studyHow Neptune Helps Artera Bring AI Solutions to Market Faster ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/artera)\n[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](https://neptune.ai/customers)\n  * [Documentation](https://docs.neptune.ai/)\n  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Menu Item](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Blog](https://neptune.ai/blog)\n      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Grici\u016bnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)\n  * [Pricing](https://neptune.ai/pricing)\n  * [Enterprise](https://neptune.ai/product/enterprise)\n  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Menu Item](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [About us](https://neptune.ai/about-us)\n      * [Customers](https://neptune.ai/customers)\n      * [Careers](https://neptune.ai/jobs)\n      * [Security](https://security.neptune.ai/)\n      * [Contact us](https://neptune.ai/contact-us)\n\n\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find? \nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/) > [Blog](https://neptune.ai/blog) > [LLMOps](https://neptune.ai/blog/category/llmops)\n  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Categories](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n      * [MLOps](https://neptune.ai/blog/category/mlops)\n      * [LLMOps](https://neptune.ai/blog/category/llmops)\n      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)\n    * [Categories](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)\n      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)\n      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)\n\n\n[ LLMOps Learning Hub ](https://neptune.ai/llmops-learning-hub)\nSearch in Blog... \n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg) ![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog... \n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Categories](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n      * [MLOps](https://neptune.ai/blog/category/mlops)\n      * [LLMOps](https://neptune.ai/blog/category/llmops)\n      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)\n    * [Categories](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)\n      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)\n      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)\n\n\n[ LLMOps Learning Hub ](https://neptune.ai/llmops-learning-hub)\n  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Walkthrough [2 min]](https://neptune.ai/walkthrough)\n      * [Play with public sandbox](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n      * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Neptune vs WandB](https://neptune.ai/vs/wandb)\n      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n      * [Other comparisons](https://neptune.ai/vs)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune Scale projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [AI Researcher](https://neptune.ai/product/ai-researcher)\n      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n      * [Academia & Kagglers](https://neptune.ai/research)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Monitor training](https://neptune.ai/product/monitor-training)\n      * [Compare experiments](https://neptune.ai/product/compare-experiments)\n      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)\n      * [Reports](https://neptune.ai/product/reports)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/deepsense-ai)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2025/01/Artera-color-signet.png?fit=129%2C97&ssl=1)Case studyHow Neptune Helps Artera Bring AI Solutions to Market Faster ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/artera)\n[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](https://neptune.ai/customers)\n  * [Documentation](https://docs.neptune.ai/)\n  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Menu Item](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [Blog](https://neptune.ai/blog)\n      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Grici\u016bnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)\n  * [Pricing](https://neptune.ai/pricing)\n  * [Enterprise](https://neptune.ai/product/enterprise)\n  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Menu Item](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n      * [About us](https://neptune.ai/about-us)\n      * [Customers](https://neptune.ai/customers)\n      * [Careers](https://neptune.ai/jobs)\n      * [Security](https://security.neptune.ai/)\n      * [Contact us](https://neptune.ai/contact-us)\n\n\n[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)\n[Neptune Blog](https://neptune.ai/blog)\n#  LLM Fine-Tuning and Model Selection Using Neptune and Transformers \n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/Pedro-Gabriel-Gengo-Lourenco.jpeg?fit=722%2C722&ssl=1)\n[ Pedro Gabriel Gengo Louren\u00e7o  ](https://neptune.ai/blog/author/pedro-gabriel-gengo-lourenco)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg) 13 min \n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg) 13th December, 2024 \n[LLMOps](https://neptune.ai/blog/category/llmops)[Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 3 2'%3E%3C/svg%3E)\nImagine you\u2019re facing the following challenge: you want to develop a Large Language Model (LLM) that can proficiently respond to inquiries in Portuguese. You have a valuable dataset and can choose from various base models. But here\u2019s the catch \u2014 you\u2019re working with limited computational resources and can\u2019t rely on expensive, high-power machines for fine-tuning. How do you decide on the right model to use in this scenario?\nThis post explores these questions, offering insights and strategies for selecting the best model and conducting efficient fine-tuning, even when resources are constrained. We\u2019ll look at ways to reduce a model\u2019s memory footprint, speed up training, and best practices for monitoring.\n![LLM fine-tuning and model selection, implemented workflow](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-4.png?resize=1800%2C942&ssl=1) The workflow we\u2019ll implement. We will fine-tune different foundation LLM models on a dataset, evaluate them, and select the best model.\n## Large language models\n[Large Language Models (LLMs)](https://arxiv.org/pdf/2303.18223.pdf) are huge deep-learning models pre-trained on vast data. These models are usually based on [an architecture called transformers](https://arxiv.org/pdf/1706.03762.pdf). Unlike the earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. Initially, the transformer architecture was designed for translation tasks. But nowadays, it is used for various tasks, ranging from language modeling to computer vision and generative AI.\nBelow, you can see a basic transformer architecture consisting of an encoder (left) and a decoder (right). The encoder receives the inputs and generates a contextualized interpretation of the inputs, called embeddings. The decoder uses the information in the embeddings to generate the model\u2019s output, one token at a time.\n[![Large Language Models \\(LLMs\\) are huge deep-learning models pre-trained on vast data. These models are usually based on an architecture called transformers.](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers.png?resize=1800%2C1884&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers.png?ssl=1)Transformers architecture. On the left side, we can see the encoder part, which is composed of a stack of multi-head attention and fully connected layers. On the right side, we can see the decoder, which is also composed of a stack of multi-head attention, cross-attention to leverage the information from the encoder, and fully connected layers.\n## Hands-on: fine-tuning and selecting an LLM for Brazilian Portuguese\nIn this project, we\u2019re taking on the challenge of fine-tuning three LLMs: [GPT-2](https://huggingface.co/gpt2), [GPT2-medium](https://huggingface.co/gpt2-medium), [GPT2-large](https://huggingface.co/gpt2-large), and [OPT 125M](https://huggingface.co/facebook/opt-125m). The models have 137 million, 380 million, 812 million, and 125 million parameters, respectively. The largest one, GPT2-large, takes up over 3GB when stored on disk. All these models were trained to generate English-language text.\nOur goal is to optimize these models for enhanced performance in Portuguese question answering, addressing the growing demand for AI capabilities in diverse languages. To accomplish this, we\u2019ll need to have a dataset with inputs and labels and use it to \u201cteach\u201d the LLM. Taking a pre-trained model and specializing it to solve new tasks is called fine-tuning. The main advantage of this technique is you can leverage the knowledge the model has to use as a starting point.\n### Setting up\nI have designed this project to be accessible and reproducible, with a setup that can be replicated on a Colab environment using T4 GPUs. I encourage you to follow along and experiment with the fine-tuning process yourself.\nNote that I used a V100 GPU to produce the examples below, which is available if you have a Colab Pro subscription. You can see that I\u2019ve already made a first trade-off between time and money spent here. Colab does not reveal detailed prices, but a [T4 costs $0.35/hour on the underlying Google Cloud Platform, while a V100 costs $2.48/hour.](https://cloud.google.com/compute/gpus-pricing) According to [this benchmark](https://www.dell.com/support/kbdoc/en-us/000132094/deep-learning-performance-on-t4-gpus-with-mlperf-benchmarks), a V100 is three times faster than a T4. Thus, by spending seven times more, we save two-thirds of our time.\nYou can find all the code in two Colab notebooks:\n  * [Fine-tuning](https://colab.research.google.com/drive/1wOg944mbbjLqbW0BvQiu8Qze3-5BPsUH?usp=sharing)\n  * [Model selection](https://colab.research.google.com/drive/1TK9vvFpiStxb1veAIBtKXWJPOgzPRFr0?usp=sharing)\n\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  How to Version and Organize ML Experiments That You Run in Google Colab  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/google-colab-version-organize-ml-experiments)\nWe will use Python 3.10 in our codes. Before we begin, we\u2019ll install all the libraries we will need. Don\u2019t worry if you\u2019re not familiar with them yet. We\u2019ll go into their purpose in detail when we first use them:\n```\npip install transformers==4.35.2 bitsandbytes==0.41.3 peft==0.7.0\naccelerate==0.25.0 datasets==2.16.1 neptune==1.8.6 evaluate==0.4.1 -qq\n```\n\nCopy the JavaScript snippet!\n### Loading and pre-processing the dataset\nWe\u2019ll use the FaQuAD dataset to fine-tune our models. It\u2019s a Portuguese question-answering dataset available in the Hugging Face dataset collection.\nFirst, we\u2019ll look at [the dataset card](https://huggingface.co/datasets/eraldoluis/faquad) to understand how the dataset is structured. We have about 1,000 samples, each consisting of a context, a question, and an answer. Our model\u2019s task is to answer the question based on the context. (The dataset also contains a title and an ID column, but we won\u2019t use them to fine-tune our model.)\n![Fine-tunning the models using FaQuAD dataset](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-fine-tuning-and-model-selection-using-Neptune-and-transformers-3.png?resize=978%2C384&ssl=1)Each sample in the FaQuAD dataset consists of a context, a question, and the corresponding answer. | [Source](https://huggingface.co/datasets/eraldoluis/faquad/viewer/plain_text/train)\nWe can conveniently load the dataset using the Hugging Face datasets library:\n```\nfrom datasets import load_dataset\ndataset = load_dataset(\"eraldoluis/faquad\")\n```\n\nCopy the JavaScript snippet!\nOur next step is to convert the dataset into a format our models can process. For our question-answering task, that\u2019s a sequence-to-sequence format: The model receives a sequence of tokens as the input and produces a sequence of tokens as the output. The input contains the context and the question, and the output contains the answer.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  Tokenization in NLP: Types, Challenges, Examples, Tools  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/tokenization-in-nlp)\nFor training, we\u2019ll create a so-called prompt that contains not only the question and the context but also the answer. Using a small helper function, we concatenate the context, question, and answer, divided by section headings (Later, we\u2019ll leave out the answer and ask the model to fill in the \u201cResposta\u201d section on its own).\nWe\u2019ll also prepare a helper function that wraps the tokenizer. The tokenizer is what turns the text into a sequence of integer tokens. It is specific to each model, so we\u2019ll have to load and use a different tokenizer for each. The helper function makes that process more manageable, allowing us to process the entire dataset at once using _map_. Last, we\u2019ll shuffle the dataset to ensure the model sees it in randomized order.\nHere\u2019s the complete code:\n```\ndef generate_prompt(data_point):\n  out = f\"\"\"Dado o contexto abaixo, responda a quest\u00e3o\n### Contexto:\n{data_point[\"context\"]}\n### Quest\u00e3o:\n{data_point[\"question\"]}\n### Resposta:\n\"\"\"\n  if data_point.get(\"answers\"):\n   out += data_point[\"answers\"][\"text\"][0]\n  return out\nCUTOFF_LEN = 1024\ndef tokenize(prompt, tokenizer):\n  result = tokenizer(\n    prompt,\n    truncation=True,\n    max_length=CUTOFF_LEN + 1,\n    padding=\"max_length\",\n  )\n  return {\n    \"input_ids\": result[\"input_ids\"][:-1],\n    \"attention_mask\": result[\"attention_mask\"][:-1],\n  }\n```\n\nCopy the JavaScript snippet!\n### Loading and preparing the models\nNext, we load and prepare the models that we\u2019ll fine-tune. LLMs are huge models. Without any kind of optimization, for the GPT2-large model in full precision (float32), we have around 800 million parameters, and we need 2.9 GB of memory to load the model and 11.5 GB during the training to handle the gradients. That just about fits in the 16 GB of memory that the T4 in the free tier offers. But we would only be able to compute tiny batches, making training painfully slow.\nFaced with these memory and compute resource constraints, we\u2019ll not use the models as-is but use quantization and a method called LoRA to reduce their number of trainable parameters and memory footprint.\n### Quantization\nQuantization is a technique used to reduce a model\u2019s size in memory by using fewer bits to represent its parameters. For example, instead of using 32 bits to represent a floating point number, we\u2019ll use only 16 or even as little as 4 bits.\nThis approach can significantly decrease the memory footprint of a model, which is especially important when deploying large models on devices with limited memory or processing power. By reducing the precision of the parameters, quantization can lead to a faster inference time and lower power consumption. However, it\u2019s essential to balance the level of quantization with the potential loss in the model\u2019s task performance, as excessive quantization can degrade accuracy or effectiveness.\nThe Hugging Face transformers library has [built-in support for quantization](https://huggingface.co/docs/transformers/v4.36.1/en/quantization#bitsandbytes) through the [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) library. You can pass\n```\n`load_in_8bit=True`\n```\n\nCopy the JavaScript snippet!\nor \n```\n`load_in_4bit=True` to the `from_pretrained()`\n```\n\nCopy the JavaScript snippet!\nmodel loading methods to load a model with 8-bit or 4-bit precision, respectively.\nAfter loading the model, we call the wrapper function prepare_model_for_kbit_training from the peft library. It prepares the model for training in a way that saves memory. It does this by freezing the model parameters, making sure all parts use the same type of data format, and using a special technique called [gradient checkpointing](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing) if the model can handle it. This helps in training large AI models, even on computers with little memory.\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import prepare_model_for_kbit_training\nfrom peft import get_peft_model, LoraConfig\n\nmodel_name = 'gpt2-large'\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                      device_map = \"auto\",\n                      load_in_8bit=True,\n                      trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\nmodel = prepare_model_for_kbit_training(model)\n```\n\nCopy the JavaScript snippet!\nAfter quantizing the model to 8 bits, it takes only a fourth of the memory to load and train the model, respectively. For GPT2-large, instead of needing 2.9 GB to load, it now takes only 734 MB.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  Deploying Large NLP Models: Infrastructure Cost Optimization  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/nlp-models-infrastructure-cost-optimization)\n### LoRA\nAs we know, Large Language Models have a lot of parameters. When we want to fine-tune one of these models, we usually update all the model\u2019s weights. That means we need to save all the gradient states in memory during fine-tuning, which requires almost twice the model size of memory. Sometimes, when updating all parameters, we can mess up with what the model already learned, leading to worse results in terms of generalization.\nGiven this context, a team of researchers proposed a new technique called [Low-Rank Adaptation (LoRA)](https://arxiv.org/pdf/2106.09685.pdf). This reparametrization method aims to reduce the number of trainable parameters through low-rank decomposition.\nLow-rank decomposition approximates a large matrix into a product of two smaller matrices, such that multiplying a vector by the two smaller matrices yields approximately the same results as multiplying a vector by the original matrix. For example, we could decompose a 3\u00d73 matrix into the product of a 3\u00d71 and a 1\u00d73 matrix so that instead of having nine parameters, we have only six.\n![Low-Rank Adaptation \\(LoRA\\)](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-1.png?resize=1800%2C1800&ssl=1)Low-rank decomposition is a method to split a large matrix M into a product of two smaller matrices, L and R, that approximates it.\nWhen fine-tuning a model, we want to slightly change its weights to adapt it to the new task. More formally, we\u2019re looking for new weights derived from the original weights: _W_ _new_ _=__W_ _old_ _+__W_. Looking at this equation, you can see that we keep the original weights in their original shape and just learn _W_ as LoRA matrices.\nIn other words, you can freeze your original weights and train just the two LoRA matrices with substantially fewer parameters in total. Or, even more simply, you create a set of new weights in parallel with the original weights and only train the new ones. During the inference, you pass your input to both sets of weights and sum them at the end.\n![Fine-tuning using low-rank decomposition](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-2.png?resize=1800%2C1800&ssl=1)Fine-tuning using low-rank decomposition. In blue, we can see the original set of weights of the pre-trained model. Those will be frozen during the fine-tuning. In orange, we can see the low-rank matrices A and B, which will have their weights updated during the fine-tuning.\nWith our base model loaded, we now want to add the LoRA layers in parallel with the original model weights for fine-tuning. To do this, we need to define a LoraConfig.\nInside the LoraConfig, we can define the rank of the LoRA matrices (parameter r), the dimension of the vector space generated by the matrix columns. We can also look at the rank as a measure of how much compression we are applying to our matrices, i.e., how small the bottleneck between A and B in the figure above will be.\nWhen choosing the rank, keeping in mind the trade-off between the rank of your LoRA matrix and the learning process is essential. Smaller ranks mean less room to learn, i.e., as you have fewer parameters to update, it can be harder to achieve significant improvements. On the other hand, higher ranks provide more parameters, allowing for greater flexibility and adaptability during training. However, this increased capacity comes at the cost of additional computational resources and potentially longer training times. Thus, finding the optimal rank for your LoRA matrix that balances these factors well is crucial, and the best way to find this is by experimenting! A good approach is to start with lower ranks (8 or 16), as you will have fewer parameters to update, so it will be faster, and increase it if you see the model is not learning as much as you want.\nYou also need to define which modules inside the model you want to apply the LoRA technique to. You can think of a module as a set of layers (or a building block) inside the model. If you want to know more, I\u2019ve prepared a deep dive, but feel free to skip it.\n![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n##  Deep dive: which modules can and should you apply LoRA to? \nWithin the LoraConfig, you need to specify which modules to apply LoRA to. You can apply LoRA for most of a model\u2019s modules, but you need to specify the module names that the original developers assigned at model creation. Which modules exist, and their names are different for each model.\nThe LoRA paper reports that adding LoRA layers only to the keys and values linear projections is a good tradeoff compared to adding LoRA layers to all linear projections in attention blocks. In our case, for the GPT2 model, we will apply LoRA on the c_attn layers, as we don\u2019t have the query, value, and keys weights split, and for the OPT model, we will apply LoRA on the q_proj and v_proj.\nIf you use other models, you can print the modules\u2019 names and choose the ones you want:\n```\nlist(model.named_modules())\n```\n\nCopy the JavaScript snippet!\nIn addition to specifying the rank and modules, you must also set up a hyperparameter called alpha, which scales the LoRA matrix:\n```\nscaling = alpha / r\nweight += (lora_B @ lora_A) * scaling \n\n```\n\nCopy the JavaScript snippet!\nAs a rule of thumb (as discussed in [this article by Sebastian Raschka](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)), you can start setting this to be two times the rank r. If your results are not good, you can try lower values.\nHere\u2019s the complete LoRA configuration for our experiments:\n```\nconfig = LoraConfig(\n  r=8,\n  lora_alpha=16,\n  target_modules=[\"c_attn\"], # for gpt2 models\n  # target_modules=[\"q_proj\", \"v_proj\"], # for opt models\n  lora_dropout=0.1,\n  bias=\"none\",\n  task_type=\"CAUSAL_LM\",\n)\n\n```\n\nCopy the JavaScript snippet!\nWe can apply this configuration to our model by calling\n```\nmodel = get_peft_model(model, config)\n\n```\n\nCopy the JavaScript snippet!\nNow, just to show how many parameters we are saving, let\u2019s print the trainable parameters of GPT2-large:\n```\nmodel.print_trainable_parameters()\n>> trainable params: 2,949,120 || all params: 776,979,200 || trainable%: 0.3795622842928099\n\n```\n\nCopy the JavaScript snippet!\nWe can see that we are updating less than 1% of the parameters! What an efficiency gain!\n### Fine-tuning the models\nWith the dataset and models prepared, it\u2019s time to move on to fine-tuning. Before we start our experiments, let\u2019s take a step back and consider our approach. We\u2019ll be training four different models with different modifications and using different training parameters. We\u2019re not only interested in the model\u2019s performance but also have to work with constrained resources.\nThus, it will be crucial that we keep track of what we\u2019re doing and progress as systematically as possible. At any point in time, we want to ensure that we\u2019re moving in the right direction and spending our time and money wisely.\n[ ![](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/07/blog_feature_image_030500_9_2_5_1.jpg?fit=200%2C105&ssl=1) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  ML Experiment Tracking: What It Is, Why It Matters, and How to Implement It  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/ml-experiment-tracking)\n### What is essential to log and monitor during the fine-tuning process?\nAside from monitoring standard metrics like training and validation loss and training parameters such as the learning rate, in our case, we also want to be able to log and monitor other aspects of the fine-tuning:\n  1. **Resource Utilization:** Since you\u2019re operating with limited computational resources, it\u2019s vital to keep a close eye on GPU and CPU usage, memory consumption, and disk usage. This ensures you\u2019re not overtaxing your system and can help troubleshoot performance issues.\n  2. **Model Parameters and Hyperparameters:** To ensure that others can replicate your experiment, storing all the details about the model setup and the training script is crucial. This includes the architecture of the model, such as the sizes of the layers and the dropout rates, as well as the hyperparameters, like the batch size and the number of epochs. Keeping a record of these elements is key to understanding how they affect the model\u2019s performance and allowing others to recreate your experiment accurately.\n  3. **Epoch Duration and Training Time:** Record the duration of each training epoch and the total training time. This data helps assess the time efficiency of your training process and plan future resource allocation.\n\n\n### Set up logging with neptune.ai\n[neptune.ai](https://neptune.ai/product) is the most scalable experiment tracker for teams that train foundation models. It lets you monitor months-long model training, track massive amounts of data, and compare thousands of metrics in the blink of an eye. Neptune is integrated with the transformers library\u2019s Trainer module, allowing you to log and monitor your model training seamlessly. This integration was contributed by Neptune\u2019s developers, who maintain it to this day.\nTo use Neptune, you\u2019ll have to sign up for an account first (don\u2019t worry, it\u2019s free for personal use) and create a project in your workspace. Have a look at [the Quickstart guide in Neptune\u2019s documentation](https://docs.neptune.ai/usage/quickstart/). There, you\u2019ll also find up-to-date instructions for obtaining the project and token IDs you\u2019ll need to connect your Colab environment to Neptune.\nWe\u2019ll set these as environment variables:\n```\nimport os\nos.environ[\"NEPTUNE_PROJECT\"] = \"your-project-ID-goes-here\"\nos.environ[\"NEPTUNE_API_TOKEN\"] = \"your-API-token-goes-here\"\n```\n\nCopy the JavaScript snippet!\nThere are two options for logging information from transformer training to Neptune: You can either setreport_to=\u201dneptune\u201d in the TrainingArguments or pass an instance of NeptuneCallback to the Trainer\u2019s callbacks parameter. I prefer the second option because it gives me more control over what I log. Note that if you pass a logging callback, you should set report_to=\u201dnone\u201d in the TrainingArgumentsto avoid duplicate data being reported.\nBelow, you can see how I typically instantiate the NeptuneCallback. I specified a name for my experiment run and asked Neptune to log all parameters used and the hardware metrics. Setting log_checkpoints=\u201dlast\u201d ensures that the last model checkpoint will also be saved on Neptune.\n```\nfrom transformers.integrations import NeptuneCallback\n\nneptune_callback = NeptuneCallback(\n                 name=f\"fine-tuning-{model_name}\",\n                 log_parameters=True,\n                 log_checkpoints=\"last\",\n                 capture_hardware_metrics=True\n                 )\n\n```\n\nCopy the JavaScript snippet!\n### Training a model\nAs the last step before configuring the Trainer, it\u2019s time to tokenize the dataset with the model\u2019s tokenizer. Since we\u2019ve loaded the tokenizer together with the model, we can now put the helper function we prepared earlier into action:\n```\ntokenized_datasets = dataset.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))\n```\n\nCopy the JavaScript snippet!\nThe training is managed by a Trainer object. The Trainer uses a DataCollatorForLanguageModeling, which prepares the data in a way suitable for language model training.\nHere\u2019s the full setup of the Trainer:\n```\nfrom transformers import (\n             Trainer,\n             TrainingArguments,\n             GenerationConfig,\n             DataCollatorForLanguageModeling,\n             set_seed\n            )\n\nset_seed(42)\n\nEPOCHS = 20\nGRADIENT_ACCUMULATION_STEPS = 8\nMICRO_BATCH_SIZE = 8\nLEARNING_RATE = 2e-3\nWARMUP_STEPS = 100\nLOGGING_STEPS = 20\n\ntrainer = Trainer(\n  model=model,\n  train_dataset=tokenized_datasets[\"train\"],\n  args=TrainingArguments(\n    per_device_train_batch_size=MICRO_BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    warmup_steps=WARMUP_STEPS,\n    num_train_epochs=EPOCHS,\n    learning_rate=LEARNING_RATE,\n    output_dir=\"lora-faquad\",\n    logging_steps=LOGGING_STEPS,\n    save_strategy=\"epoch\",\n    gradient_checkpointing=True,\n    report_to=\"none\"\n  ),\n  callbacks=[neptune_callback],\n  data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\nmodel.config.use_cache = False\n\n```\n\nCopy the JavaScript snippet!\nThat\u2019s a lot of code, so let\u2019s go through it in detail:\n  * The training process is defined to run for 20 epochs (EPOCHS = 20). You\u2019ll likely find that training for even more epochs will lead to better results.\n\n\n  * We\u2019re using a technique called [gradient accumulation](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-accumulation), set here to 8 steps (GRADIENT_ACCUMULATION_STEPS = 8), which helps handle larger batch sizes effectively, especially when memory resources are limited. In simple terms, gradient accumulation is a technique to handle large batches. Instead of having a batch of 64 samples and updating the weights for every step, we can have a batch size of 8 samples and perform eight steps, just updating the weights in the last step. It generates the same result as a batch of 64 but saves memory.\n  * The MICRO_BATCH_SIZE is set to 8, indicating the number of samples processed each step. It is extremely important to find an amount of samples that can fit in your GPU memory during the training to avoid out-of-memory issues (Have a look at [the transformers documentation](https://huggingface.co/docs/transformers/v4.18.0/en/performance#batch-sizes) to learn more about this).\n  * The learning rate, a crucial hyperparameter in training neural networks, is set to 0.002 (LEARNING_RATE = 2e-3), determining the step size at each iteration when moving toward a minimum of the loss function. To facilitate a smoother and more effective training process, the model will gradually increase its learning rate for the first 100 steps (WARMUP_STEPS = 100), helping to stabilize early training phases.\n  * The trainer is set not to use the model\u2019s cache (model.config.use_cache = False) to manage memory more efficiently.\n\n\nWith all of that in place, we can launch the training:\n```\ntrainer_output = trainer.train(resume_from_checkpoint=False)\n\n```\n\nCopy the JavaScript snippet!\nWhile training is running, head over to Neptune, navigate to your project, and click on the experiment that is running. There, click on Charts to see how your training progresses (loss and learning rate). To see resource utilization, click the Monitoring tab and follow how GPU and CPU usage and memory utilization change over time. When the training finishes, you can see other information like training samples per second, training steps per second, and more.\nAt the end of the training, we capture the output of this process in trainer_output, which typically includes details about the training performance and metrics that we will later use to save the model on the model registry.\nBut first, we\u2019ll have to check whether our training was successful.\n![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n##  **Aside**\nTrack months-long model training with more confidence. Use neptune.ai forking feature to iterate faster and optimize the usage of GPU resources. \nWith Neptune, users can visualize forked training out of the box. This means you can:\n  * Test multiple configs at the same time. Stop the runs that don\u2019t improve accuracy. And continue from the most accurate last step. \n  * Restart failed training sessions from any previous step. The training history is inherited, and the entire experiment is visible on a single chart. \n\n\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/app-screenshot/bar-buttons.svg)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button--test-tube.svg) See in app  ](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&dash=charts&query=\\(\\(%60sys%2Ftags%60%3AstringSet%20CONTAINS%20%22forks%22\\)\\)%20AND%20\\(\\(%60sys%2Fgroup_tags%60%3AstringSet%20CONTAINS%20%22experiments_CGB5CSFI%22\\)\\)&lbViewUnpacked=true&sortBy=%5B%22sys%2Fcreation_time%22%5D&sortFieldType=%5B%22datetime%22%5D&sortFieldAggregationMode=%5B%22auto%22%5D&sortDirection=%5B%22descending%22%5D&experimentsOnly=false&runsLineage=FULL&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/icon-zoom.svg) Full screen preview \n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nCheck the [documentation](https://docs-beta.neptune.ai/fork_experiment/)\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nPlay with an[ interactive example project](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Get in touch](https://neptune.ai/contact-us) to go through a custom demo with our engineering team\n\n\n### Evaluating the fine-tuned LLMs\nModel evaluation in AI, particularly for language models, is a complex and multifaceted task. It involves navigating a series of trade-offs among cost, data applicability, and alignment with human preferences. This process is critical in ensuring that the developed models are not only technically proficient but also practical and user-centric.\n### LLM evaluation approaches\n![LLM evaluation approaches](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-5.png?resize=1800%2C942&ssl=1)Diagram of different evaluation strategies organized by evaluation metrics and data | Modified based on [source](https://www.youtube.com/watch?v=2CIIQ5KZWUM)\nThe chart above shows that the least expensive (and most commonly used) approach is to use public benchmarks. On the one hand, this approach is highly cost-effective and easy to test. However, on the other hand, it is less likely to resemble production data. Another option, slightly more costly than benchmarks, is AutoEval, where other language models are used to evaluate the target model. For those with a higher budget, user testing, where the model is made accessible to users, or human evaluation, which involves a dedicated team of humans focused on assessing the model, is an option.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--resource.svg) Related post  How Elevatus Uses Neptune to Check Experiment Results in 1 Minute  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/customers/elevatus)\n### Evaluating question-answering models with F1 scores and the exact match metric\nIn our project, considering the need to balance cost-effectiveness with maintaining evaluation standards for the dataset, we will employ two specific metrics: exact match and F1 score. We\u2019ll use the validation set provided along with the FaQuAD dataset. Hence, our evaluation strategy falls into the `Public Benchmarks category, as it relies on a well-known dataset to evaluate PTBR models.\nThe exact match metric determines if the response given by the model precisely aligns with the target answer. This is a straightforward and effective way to assess the model\u2019s accuracy in replicating expected responses. We\u2019ll also calculate the F1 score, which combines precision and recall, of the returned tokens. This will give us a more nuanced evaluation of the model\u2019s performance. By adopting these metrics, we aim to assess our model\u2019s capabilities reliably without incurring significant expenses.\nAs we said previously, there are various ways to evaluate an LLM, and we choose this way, using standard metrics, because it is fast and cheap. However, there are some trade-offs when choosing \u201chard\u201d metrics to evaluate results that can be correct, even when the metrics say it is not good.\nOne example is: imagine the target answer for some question is \u201cThe rat found the cheese and ate it.\u201d and the model\u2019s prediction is \u201cThe mouse discovered the cheese and consumed it.\u201d Both examples have almost the same meaning, but the words chosen differ. For metrics like exact match and F1, the scores will be really low. A better \u2013 but more costly \u2013 evaluation approach would be to have humans annotate or use another LLM to verify if both sentences have the same meaning.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  The Ultimate Guide to Evaluation and Selection of Models in Machine Learning  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/ml-model-evaluation-and-selection)\n### Implementing the evaluation functions\nLet\u2019s return to our code. I\u2019ve decided to create my own evaluation functions instead of using the Trainer\u2019s built-in capabilities to perform the evaluation. On the one hand, this gives us more control. On the other hand, I frequently encountered out-of-memory (OOM) errors while doing evaluations directly with the Trainer.\nFor our evaluation, we\u2019ll need two functions:\n  * get_logits_and_labels: Processes a sample, generates a prompt from it, passes this prompt through a model, and returns the model\u2019s logits (scores) along with the token IDs of the target answer.\n  * compute_metrics: Evaluates a model on a dataset, calculating exact match (EM) and F1 scores. It iterates through the dataset, using the _get_logits_and_labels_ function to generate model predictions and corresponding labels. Predictions are determined by selecting the most likely token indices from the logits. For the EM score, it decodes these predictions and labels into text and computes the EM score. For the F1 score, it maintains the original token IDs and calculates the score for each sample, averaging them at the end.\n\n\nHere\u2019s the complete code:\n```\nimport evaluate\nimport torch\nfrom tqdm.auto import tqdm\nimport numpy as np\n\ndef get_logits_and_labels(sample_, max_new_tokens):\n  sample = sample_.copy()\n  del sample[\"answers\"]\n  prompt = generate_prompt(sample)\n  inputs = tokenizer(prompt, return_tensors=\"pt\")\n  input_ids = inputs[\"input_ids\"].cuda()\n  attention_mask = inputs[\"attention_mask\"].cuda()\n  generation_output = model.generate(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    return_dict_in_generate=True,\n    output_scores=True,\n    max_new_tokens=max_new_tokens,\n    num_beams=1,\n    do_sample=False\n  )\n\ntarget_ids = tokenizer(sample_[\"answers\"][\"text\"][0], return_tensors=\"pt\")\n  scores = torch.concat(generation_output[\"scores\"])\n  return scores.cpu(), target_ids[\"input_ids\"]\n\ndef compute_metrics(dataset, max_new_tokens):\n metric1 = evaluate.load(\"exact_match\")\n metric2 = evaluate.load(\"f1\")\n\n em_preds = []\n em_refs = []\n f1_preds = []\n f1_refs = []\n for s in tqdm(dataset):\n  logits, labels = get_logits_and_labels(s, max_new_tokens)\n  predictions = np.argmax(logits, axis=-1)[:len(labels[0])]\n  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n  labels = labels[0, :len(predictions)]\n  f1_preds.append(predictions)\n  f1_refs.append(labels)\n\n  em_pred = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n  em_ref = tokenizer.batch_decode(labels, skip_special_tokens=True)\n  em_preds.append(\"\".join(em_pred))\n  em_refs.append(\"\".join(em_ref))\n\nem=metric1.compute(predictions=em_preds, references=em_refs)[\"exact_match\"]\n\n f1_result = 0\n for pred, ref in zip(f1_preds, f1_refs):\n  f1_result += metric2.compute(predictions=pred, references=ref, average=\"macro\")[\"f1\"]\n return em, f1_result / len(f1_preds)\n\n```\n\nCopy the JavaScript snippet!\nBefore assessing our model, we must switch it to evaluation mode, which deactivates dropout. Additionally, we should re-enable the model\u2019s cache to conserve memory during prediction.\n```\nmodel.eval()\nmodel.config.use_cache = True # We need this to avoid OOM issues\n```\n\nCopy the JavaScript snippet!\nFollowing this setup, simply execute the compute_metrics function on the evaluation dataset and specify the desired number of generated tokens to use (Note that using more tokens will increase processing time).\n```\nem, f1 = compute_metrics(tokenized_datasets[\"validation\"], max_new_tokens=5)\n\n```\n\nCopy the JavaScript snippet!\n### Storing the models and evaluation results\nNow that we\u2019ve finished fine-tuning and evaluating a model, we should save it and move on to the next model. To this end, we\u2019ll create a model_version to store in Neptune\u2019s model registry.\nIn detail, we\u2019ll save the latest model checkpoint along with the loss, the F1 score, and the exact match metric. These metrics will later allow us to select the optimal model. To create a model and a model version, you will need to define the model key, which is the model identifier and must be uppercase and unique within the project. After defining the model key, to use this model to create a model version, you need to concatenate it with the project identifier that you can find on Neptune under \u201cAll projects\u201d \u2013 \u201cEdit project information\u201d \u2013 \u201cProject key\u201d.\n```\nimport neptune\n\ntry:\n neptune_model = neptune.init_model(\n   key=\"QAPTBR\", # must be uppercase and unique within the project\n   name=\"ptbr qa model\", # optional\n )\nexcept neptune.exceptions.NeptuneModelKeyAlreadyExistsError:\n print(\"Model already exists in this project. Reusing it.\")\n\n\nmodel_version = neptune.init_model_version(\n  model=\"LLMFIN-QAPTBR\", ## Project id + key\n)\nmodel_version[f\"model/artifacts\"].upload_files(\"/content/lora-faquad/checkpoint-260\")\nmodel_version[\"model/model-name\"] = model_name\nmodel_version[\"model/loss\"] = trainer_output.training_loss\nmodel_version[\"model/exact-match\"] = em\nmodel_version[\"model/f1\"] = f1\n\n```\n\nCopy the JavaScript snippet!\n### Model selection\nOnce we\u2019re done with all our model training and experiments, it\u2019s time to jointly evaluate them. This is possible because we monitored the training and stored all the information on Neptune. Now, we\u2019ll use the platform to compare different runs and models to choose the best one for our use case.\nAfter completing all your runs, you can click Compare runs at the top of the project\u2019s page and enable the \u201csmall eye\u201d for the runs you want to compare. Then, you can go to the Charts tab, and you will find a joint plot of the losses for all the experiments. [Here\u2019s how it looks in my project.](https://app.neptune.ai/pedro.gengo/llm-finetuning/runs/compare?viewId=standard-view&detailsTab=charts&shortId=LLMFIN-61&dash=charts&type=run&compare=KwBgNMYEzUA) In purple, we can see the loss for the gpt2-large model. As we trained for fewer epochs, we can see that we have a shorter curve, which nevertheless achieved a better loss.\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/app-screenshot/bar-buttons.svg)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button--test-tube.svg) See in the app  ](https://app.neptune.ai/pedro.gengo/llm-finetuning/runs/compare?viewId=standard-view&detailsTab=charts&shortId=LLMFIN-61&dash=charts&type=run&compare=KwBgNMYEzUA) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/icon-zoom.svg) Full screen preview \nComparison of the loss across different experiments. Purple: gpt2-large. Yellow: opt-125m. Red: gpt-medium. Gray: gpt2. \nThe loss function is not yet saturated, indicating that our models still have room for growth and could likely achieve higher levels of performance with additional training time.\nGo to the Models page and click on the model you created. You will see an overview of all the versions you trained and uploaded. You can also see the metrics reported and the model name.\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/note/list-arrow.svg)\n[Here\u2019s a link to my Neptune project](https://app.neptune.ai/pedro.gengo/llm-finetuning/models?shortId=LLMFIN-QAPTBR&type=model)\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/app-screenshot/bar-buttons.svg)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button--test-tube.svg) See in the app  ](https://app.neptune.ai/pedro.gengo/llm-finetuning/models?shortId=LLMFIN-QAPTBR&type=model) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/icon-zoom.svg) Full screen preview \nModel versions saved on Neptune\u2019s model registry. Listed are the model version\u2019s ID, the time of creation, the owner, and the metrics stored with the model version. \nYou\u2019ll notice that none of the model versions have been assigned to a \u201cStage\u201d yet. Neptune allows you to [assign models to different stages](https://docs.neptune.ai/model_registry/managing_stage/), namely \u201cStaging,\u201d \u201cProduction,\u201d and \u201cArchived.\u201d\nWhile we can promote a model through the UI, we\u2019ll return to our code and automatically identify the best model. For this, we first fetch all model versions\u2019 metadata, sort by the exact match and f1 scores, and promote the best model according to these metrics to production:\n```\nimport neptune\n\nmodel = neptune.init_model(with_id=\"LLMFIN-QAPTBR\")\n\nmodel_versions_df = model.fetch_model_versions_table().to_pandas()\n\ndf_sorted = model_versions_df.sort_values([\"model/exact-match\", \"model/f1\"], ascending=False)\nmodel_version = df_sorted.iloc[0][\"sys/id\"]\nmodel_name = df_sorted.iloc[0][\"model/model-name\"]\n\nmodel_version = neptune.init_model_version(\n  with_id=model_version,\n)\nmodel_version.change_stage(\"production\")\n\n```\n\nCopy the JavaScript snippet!\nAfter executing this, we can see, as expected, that gpt2-large (our largest model) was the best model and was chosen to go to production:\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/app-screenshot/bar-buttons.svg)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button--test-tube.svg) See in the app  ](https://app.neptune.ai/pedro.gengo/llm-finetuning/models?shortId=LLMFIN-QAPTBR&type=model) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/icon-zoom.svg) Full screen preview \nThe gpt2-large model achieved the best metric scores and was promoted to the \u201cProduction\u201d stage. \nOnce more, we\u2019ll return to our code and finally use our best model to answer questions in Brazilian Portuguese:\n```\nimport neptune\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = neptune.init_model(with_id=\"LLMFIN-QAPTBR\")\nmodel_versions_df = model.fetch_model_versions_table().to_pandas()\n\ndf_prod_model = model_versions_df[model_versions_df[\"sys/stage\"] == \"production\"]\nmodel_version = df_prod_model.iloc[0][\"sys/id\"]\nmodel_name = df_prod_model.iloc[0][\"model/model-name\"]\n\nmodel_version = neptune.init_model_version(\n  with_id=model_version,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                      device_map = \"auto\",\n                      load_in_8bit=True,\n                      trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmodel_version[\"model/artifacts\"].download()\n\n!unzip artifacts\n\nmodel = PeftModel.from_pretrained(model, \"/content/lora-faquad/checkpoint-260\", local_files_only=True)\n```\n\nCopy the JavaScript snippet!\n[![LLM inference before and after fine-tuning](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-6.png?resize=1920%2C4480&ssl=1)](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/01/LLM-Fine-Tuning-and-Model-Selection-Using-Neptune-and-Transformers-6.png?ssl=1)Model inference before and after fine-tuning. The text shows a small piece of information about the rules to pass a course and asks: \u201cWhat does passing the course depend on?\u201d Before fine-tuning, the model only repeats the question. After fine-tuning, the model can answer the question correctly.\nLet\u2019s compare the prediction without fine-tuning and the prediction after fine-tuning. As demonstrated, before fine-tuning, the model didn\u2019t know how to handle Brazilian Portuguese at all and answered by repeating some part of the input or returning special characters like \u201c##########.\u201d However, after fine-tuning, it becomes evident that the model handles the input much better, answering the question correctly (it only added a \u201c?\u201d at the end, but the rest is exactly the answer we\u2019d expect).\nWe can also look at the metrics before and after fine-tuning and verify how much it improved:\nExact Match  |  F1   \n---|---  \n**Before fine-tuning** |  0  |  0.007  \n**After fine-tuning** |  0.143 |  0.157  \nGiven the metrics and the prediction example, we can conclude that the fine-tuning was in the right direction, even though we have room for improvement.\n![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n##  **Editor\u2019s note**\nDo you feel like experimenting with neptune.ai?\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Create a free account](https://neptune.ai/register) right away and give it a go\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Try it out first](https://docs.neptune.ai/usage/quickstart/) and learn how it works (zero setup, no registration)\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[See the docs](https://docs.neptune.ai/) or watch a short [product demo (2 min)](https://neptune.ai/walkthrough)\n\n\n## How to improve the solution?\nIn this article, we\u2019ve detailed a simple and efficient technique for fine-tuning LLMs.\nOf course, we still have some way to go to achieve good performance and consistency. There are various additional, more advanced strategies you can employ, such as:\n  * **More Data:** Add more high-quality, diverse, and relevant data to the training set to improve the model\u2019s learning and generalization.\n  * **Tokenizer Merging:** Combine tokenizers for better input processing, especially for multilingual models.\n  * **Model-Weight Tuning:** Directly adjust the pre-trained model weights to fit the new data better, which can be more effective than tuning adapter weights.\n  * **Reinforcement Learning with Human Feedback:** Employ human raters to provide feedback on the model\u2019s outputs, which is used to fine-tune the model through reinforcement learning, aligning it more closely with complex objectives.\n  * **More Training Steps:** Increasing the number of training steps can further enhance the model\u2019s understanding and adaptation to the data.\n\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  How to Improve ML Model Performance [Best Practices From Ex-Amazon AI Researcher]  Read also  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/improving-ml-model-performance)\n## Conclusion\nWe engaged in four distinct trials throughout our experiments, each employing a different model. We\u2019ve used quantization and LoRA to reduce the memory and compute resource requirements. Throughout the training and evaluation, we\u2019ve used Neptune to log metrics and store and manage the different model versions.\nI hope this article inspired you to explore the possibilities of LLMs further. In particular, if you\u2019re a native speaker of a language that\u2019s not English, I\u2019d like to encourage you to explore fine-tuning LLMs in your native tongue.\n##  Was the article useful? \n![yes](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--yes.svg) Yes  ![no](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--no.svg) No \n![](https://neptune.ai/wp-content/themes/neptune/img/icon-bulb.svg) Suggest changes \nYour email Your message (optional)\nThis site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply. \nI am familiar with the [Privacy Policy](https://neptune.staginglab.eu/privacy-policy)*\nSubmit\n\u0394\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/i-box/header-icon.svg)\n### **More about** LLM Fine-Tuning and Model Selection Using Neptune and Transformers \n####  Check out our  **product resources** and  **related articles** below: \n[ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  LLMOps: What It Is, Why It Matters, and How to Implement It  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/llmops) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  ML Experiment Tracking: What It Is, Why It Matters, and How to Implement It  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/ml-experiment-tracking) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-bulb.svg) Product resource  How Veo Eliminated Work Loss With Neptune  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/customers/veo) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  Building LLM Applications With Vector Databases  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/building-llm-applications-with-vector-databases)\n###  Explore more content topics: \n[ Computer Vision ](https://neptune.ai/blog/category/computer-vision) [ General ](https://neptune.ai/blog/category/general) [ LLMOps ](https://neptune.ai/blog/category/llmops) [ ML Model Development ](https://neptune.ai/blog/category/machine-learning-model-development) [ ML Tools ](https://neptune.ai/blog/category/machine-learning-tools) [ MLOps ](https://neptune.ai/blog/category/mlops) [ Natural Language Processing ](https://neptune.ai/blog/category/natural-language-processing) [ Paper Reflections ](https://neptune.ai/blog/category/paper-reflections) [ Product Updates ](https://neptune.ai/blog/category/product-updates) [ Reinforcement Learning ](https://neptune.ai/blog/category/reinforcement-learning) [ Tabular Data ](https://neptune.ai/blog/category/tabular-data) [ Time Series ](https://neptune.ai/blog/category/time-series-forecasting)\nAbout neptune.ai ![chevron](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1920 1080'%3E%3C/svg%3E) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-play.svg)\nNeptune is the experiment tracker purpose-built for foundation model training. \nIt lets you monitor and visualize thousands of per-layer metrics\u2014losses, gradients, and activations\u2014at any scale. Drill down into logs and debug training issues fast. Keep your model training stable while reducing wasted GPU cycles. \n[ Play with a live project  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://demo.neptune.ai/) [ See Docs  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://docs.neptune.ai/)\nTable of contents\n![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-small.svg)\n  1. Large language models\n  2. Hands-on: fine-tuning and selecting an LLM for Brazilian Portuguese\n     * Setting up\n     * Loading and pre-processing the dataset\n     * Loading and preparing the models\n     * Fine-tuning the models\n     * Evaluating the fine-tuned LLMs\n     * Storing the models and evaluation results\n     * Model selection\n  3. How to improve the solution?\n  4. Conclusion\n\n\n![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-cancel.svg) ![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-more.svg)\nCheck also: [Deploying Conversational AI Products to Production With Jason Flaks](https://neptune.ai/blog/deploying-conversational-ai-products-with-jason-flaks)\n##  **Monitor your model training at scale**\nJoin 60,000+ researchers and practitioners who use Neptune to debug training failures, spot anomalies, and compare experiments.\n[ Try for free  ](https://app.neptune.ai/register) [ Get early access to Neptune Scale  ](https://neptune.ai/early-access)\n[ Take an interactive product tour  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)\n![](https://neptune.ai/wp-content/themes/neptune/img/footer-post-illustration.svg)\n#### Newsletter\nTop articles, case studies, events (and more) in your inbox every month.\nGet Newsletter\n  * [Product](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Walkthrough](https://neptune.ai/walkthrough)\n    * [Documentation](https://docs.neptune.ai/)\n    * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [Pricing](https://neptune.ai/pricing)\n    * [Service status](https://status.neptune.ai/)\n\n\n  * [Solutions](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [AI Researcher](https://neptune.ai/product/ai-researcher)\n    * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n    * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n    * [Enterprise](https://neptune.ai/product/enterprise)\n    * [Academic Research](https://neptune.ai/research)\n\n\n  * [Compare](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Neptune vs Weights & Biases](https://neptune.ai/vs/wandb)\n    * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n    * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n    * [Other comparisons](https://neptune.ai/vs)\n    * [ML experiment tracking tools](https://neptune.ai/blog/best-ml-experiment-tracking-tools)\n\n\n  * [Community](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Blog](https://neptune.ai/blog)\n    * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n    * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n    * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n    * [How to Build an Experiment Tracker](https://neptune.ai/blog/build-experiment-tracking-tool)\n\n\n  * [Company](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/)\n    * [Customers](https://neptune.ai/customers)\n    * [About us](https://neptune.ai/about-us)\n    * [Careers](https://neptune.ai/jobs)\n    * [Security portal and SOC 2](https://security.neptune.ai/)\n    * [Contact us](https://neptune.ai/contact-us)\n\n\n[ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-linked-in.svg) ](https://www.linkedin.com/company/neptuneai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-twitter.svg) ](https://twitter.com/neptune_ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-github.svg) ](https://github.com/neptune-ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-facebook.svg) ](https://www.facebook.com/neptuneAI) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-youtube.svg) ](https://www.youtube.com/channel/UCvOJU-ubyUqxGSDRN7xK4Ng) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-spotify.svg) ](https://open.spotify.com/show/4kGi82i4wTYgHbWmVMri5x) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-apple-podcast.svg) ](https://podcasts.apple.com/us/podcast/mlops-live/id1634179447)\n[ Terms of Service ](https://neptune.ai/terms-of-service) [ Privacy Policy ](https://neptune.ai/privacy-policy) [ SLA ](https://neptune.ai/service-level-agreement)\nCopyright \u00a9 2025 Neptune Labs. All rights reserved.\n![](https://neptune.ai/wp-content/themes/neptune/img/footer-stars--414w.png) ![](https://pixel.wp.com/g.gif?v=ext&blog=211928962&post=34430&tz=0&srv=neptune.ai&hp=atomic&ac=2&amp=0&j=1%3A14.5-a.9&host=neptune.ai&ref=&fcp=1916&rand=0.08928365801976357)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis document discusses fine-tuning Large Language Models (LLMs) for Brazilian Portuguese using Neptune. It covers model selection, quantization, and LoRA techniques to optimize performance with limited resources. The evaluation metrics include exact match and F1 scores, demonstrating significant improvements post-fine-tuning.\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/kelvins/awesome-mlops/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fkelvins%2Fawesome-mlops%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fkelvins%2Fawesome-mlops%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=kelvins%2Fawesome-mlops) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/kelvins/awesome-mlops/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/kelvins/awesome-mlops/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/kelvins/awesome-mlops/) to refresh your session. Dismiss alert\n{{ message }}\n[ kelvins ](https://github.com/kelvins) / **[awesome-mlops](https://github.com/kelvins/awesome-mlops) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Fkelvins%2Fawesome-mlops) You must be signed in to change notification settings\n  * [ Fork 604 ](https://github.com/login?return_to=%2Fkelvins%2Fawesome-mlops)\n  * [ Star  4.4k ](https://github.com/login?return_to=%2Fkelvins%2Fawesome-mlops)\n\n\n\ud83d\ude0e A curated list of awesome MLOps tools \n[ 4.4k stars ](https://github.com/kelvins/awesome-mlops/stargazers) [ 604 forks ](https://github.com/kelvins/awesome-mlops/forks) [ Branches ](https://github.com/kelvins/awesome-mlops/branches) [ Tags ](https://github.com/kelvins/awesome-mlops/tags) [ Activity ](https://github.com/kelvins/awesome-mlops/activity)\n[ Star  ](https://github.com/login?return_to=%2Fkelvins%2Fawesome-mlops)\n[ Notifications ](https://github.com/login?return_to=%2Fkelvins%2Fawesome-mlops) You must be signed in to change notification settings\n  * [ Code ](https://github.com/kelvins/awesome-mlops)\n  * [ Issues 4 ](https://github.com/kelvins/awesome-mlops/issues)\n  * [ Pull requests 2 ](https://github.com/kelvins/awesome-mlops/pulls)\n  * [ Actions ](https://github.com/kelvins/awesome-mlops/actions)\n  * [ Security ](https://github.com/kelvins/awesome-mlops/security)\n  * [ Insights ](https://github.com/kelvins/awesome-mlops/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/kelvins/awesome-mlops)\n  * [ Issues  ](https://github.com/kelvins/awesome-mlops/issues)\n  * [ Pull requests  ](https://github.com/kelvins/awesome-mlops/pulls)\n  * [ Actions  ](https://github.com/kelvins/awesome-mlops/actions)\n  * [ Security  ](https://github.com/kelvins/awesome-mlops/security)\n  * [ Insights  ](https://github.com/kelvins/awesome-mlops/pulse)\n\n\n# kelvins/awesome-mlops\nmain\n[Branches](https://github.com/kelvins/awesome-mlops/branches)[Tags](https://github.com/kelvins/awesome-mlops/tags)\n[](https://github.com/kelvins/awesome-mlops/branches)[](https://github.com/kelvins/awesome-mlops/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[430 Commits](https://github.com/kelvins/awesome-mlops/commits/main/)[](https://github.com/kelvins/awesome-mlops/commits/main/)  \n[.github](https://github.com/kelvins/awesome-mlops/tree/main/.github \".github\")| [.github](https://github.com/kelvins/awesome-mlops/tree/main/.github \".github\")  \n[CODE_OF_CONDUCT.md](https://github.com/kelvins/awesome-mlops/blob/main/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| [CODE_OF_CONDUCT.md](https://github.com/kelvins/awesome-mlops/blob/main/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")  \n[CONTRIBUTING.md](https://github.com/kelvins/awesome-mlops/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/kelvins/awesome-mlops/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")  \n[README.md](https://github.com/kelvins/awesome-mlops/blob/main/README.md \"README.md\")| [README.md](https://github.com/kelvins/awesome-mlops/blob/main/README.md \"README.md\")  \n[check_order.py](https://github.com/kelvins/awesome-mlops/blob/main/check_order.py \"check_order.py\")| [check_order.py](https://github.com/kelvins/awesome-mlops/blob/main/check_order.py \"check_order.py\")  \n[mlc_config.json](https://github.com/kelvins/awesome-mlops/blob/main/mlc_config.json \"mlc_config.json\")| [mlc_config.json](https://github.com/kelvins/awesome-mlops/blob/main/mlc_config.json \"mlc_config.json\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/kelvins/awesome-mlops/)\n  * [Code of conduct](https://github.com/kelvins/awesome-mlops/)\n\n\n# Awesome MLOps [![Awesome](https://camo.githubusercontent.com/8693bde04030b1670d5097703441005eba34240c32d1df1eb82a5f0d6716518e/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667)](https://github.com/sindresorhus/awesome)\n[](https://github.com/kelvins/awesome-mlops/#awesome-mlops-)\nA curated list of awesome MLOps tools.\nInspired by [awesome-python](https://github.com/vinta/awesome-python).\n  * [Awesome MLOps](https://github.com/kelvins/awesome-mlops/#awesome-mlops)\n    * [AutoML](https://github.com/kelvins/awesome-mlops/#automl)\n    * [CI/CD for Machine Learning](https://github.com/kelvins/awesome-mlops/#cicd-for-machine-learning)\n    * [Cron Job Monitoring](https://github.com/kelvins/awesome-mlops/#cron-job-monitoring)\n    * [Data Catalog](https://github.com/kelvins/awesome-mlops/#data-catalog)\n    * [Data Enrichment](https://github.com/kelvins/awesome-mlops/#data-enrichment)\n    * [Data Exploration](https://github.com/kelvins/awesome-mlops/#data-exploration)\n    * [Data Management](https://github.com/kelvins/awesome-mlops/#data-management)\n    * [Data Processing](https://github.com/kelvins/awesome-mlops/#data-processing)\n    * [Data Validation](https://github.com/kelvins/awesome-mlops/#data-validation)\n    * [Data Visualization](https://github.com/kelvins/awesome-mlops/#data-visualization)\n    * [Drift Detection](https://github.com/kelvins/awesome-mlops/#drift-detection)\n    * [Feature Engineering](https://github.com/kelvins/awesome-mlops/#feature-engineering)\n    * [Feature Store](https://github.com/kelvins/awesome-mlops/#feature-store)\n    * [Hyperparameter Tuning](https://github.com/kelvins/awesome-mlops/#hyperparameter-tuning)\n    * [Knowledge Sharing](https://github.com/kelvins/awesome-mlops/#knowledge-sharing)\n    * [Machine Learning Platform](https://github.com/kelvins/awesome-mlops/#machine-learning-platform)\n    * [Model Fairness and Privacy](https://github.com/kelvins/awesome-mlops/#model-fairness-and-privacy)\n    * [Model Interpretability](https://github.com/kelvins/awesome-mlops/#model-interpretability)\n    * [Model Lifecycle](https://github.com/kelvins/awesome-mlops/#model-lifecycle)\n    * [Model Serving](https://github.com/kelvins/awesome-mlops/#model-serving)\n    * [Model Testing & Validation](https://github.com/kelvins/awesome-mlops/#model-testing--validation)\n    * [Optimization Tools](https://github.com/kelvins/awesome-mlops/#optimization-tools)\n    * [Simplification Tools](https://github.com/kelvins/awesome-mlops/#simplification-tools)\n    * [Visual Analysis and Debugging](https://github.com/kelvins/awesome-mlops/#visual-analysis-and-debugging)\n    * [Workflow Tools](https://github.com/kelvins/awesome-mlops/#workflow-tools)\n  * [Resources](https://github.com/kelvins/awesome-mlops/#resources)\n    * [Articles](https://github.com/kelvins/awesome-mlops/#articles)\n    * [Books](https://github.com/kelvins/awesome-mlops/#books)\n    * [Events](https://github.com/kelvins/awesome-mlops/#events)\n    * [Other Lists](https://github.com/kelvins/awesome-mlops/#other-lists)\n    * [Podcasts](https://github.com/kelvins/awesome-mlops/#podcasts)\n    * [Slack](https://github.com/kelvins/awesome-mlops/#slack)\n    * [Websites](https://github.com/kelvins/awesome-mlops/#websites)\n  * [Contributing](https://github.com/kelvins/awesome-mlops/#contributing)\n\n\n## AutoML\n[](https://github.com/kelvins/awesome-mlops/#automl)\n_Tools for performing AutoML._\n  * [AutoGluon](https://github.com/awslabs/autogluon) - Automated machine learning for image, text, tabular, time-series, and multi-modal data.\n  * [AutoKeras](https://github.com/keras-team/autokeras) - AutoKeras goal is to make machine learning accessible for everyone.\n  * [AutoPyTorch](https://github.com/automl/Auto-PyTorch) - Automatic architecture search and hyperparameter optimization for PyTorch.\n  * [AutoSKLearn](https://github.com/automl/auto-sklearn) - Automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator.\n  * [EvalML](https://github.com/alteryx/evalml) - A library that builds, optimizes, and evaluates ML pipelines using domain-specific functions.\n  * [FLAML](https://github.com/microsoft/FLAML) - Finds accurate ML models automatically, efficiently and economically.\n  * [H2O AutoML](https://h2o.ai/platform/h2o-automl) - Automates ML workflow, which includes automatic training and tuning of models.\n  * [MindsDB](https://github.com/mindsdb/mindsdb) - AI layer for databases that allows you to effortlessly develop, train and deploy ML models.\n  * [MLBox](https://github.com/AxeldeRomblay/MLBox) - MLBox is a powerful Automated Machine Learning python library.\n  * [Model Search](https://github.com/google/model_search) - Framework that implements AutoML algorithms for model architecture search at scale.\n  * [NNI](https://github.com/microsoft/nni) - An open source AutoML toolkit for automate machine learning lifecycle.\n\n\n## CI/CD for Machine Learning\n[](https://github.com/kelvins/awesome-mlops/#cicd-for-machine-learning)\n_Tools for performing CI/CD for Machine Learning._\n  * [ClearML](https://github.com/allegroai/clearml) - Auto-Magical CI/CD to streamline your ML workflow.\n  * [CML](https://github.com/iterative/cml) - Open-source library for implementing CI/CD in machine learning projects.\n  * [KitOps](https://github.com/jozu-ai/kitops) \u2013 Open source MLOps project that eases model handoffs between data scientist and DevOps.\n\n\n## Cron Job Monitoring\n[](https://github.com/kelvins/awesome-mlops/#cron-job-monitoring)\n_Tools for monitoring cron jobs (recurring jobs)._\n  * [Cronitor](https://cronitor.io/cron-job-monitoring) - Monitor any cron job or scheduled task.\n  * [HealthchecksIO](https://healthchecks.io/) - Simple and effective cron job monitoring.\n\n\n## Data Catalog\n[](https://github.com/kelvins/awesome-mlops/#data-catalog)\n_Tools for data cataloging._\n  * [Amundsen](https://www.amundsen.io/) - Data discovery and metadata engine for improving the productivity when interacting with data.\n  * [Apache Atlas](https://atlas.apache.org) - Provides open metadata management and governance capabilities to build a data catalog.\n  * [CKAN](https://github.com/ckan/ckan) - Open-source DMS (data management system) for powering data hubs and data portals.\n  * [DataHub](https://github.com/linkedin/datahub) - LinkedIn's generalized metadata search & discovery tool.\n  * [Magda](https://github.com/magda-io/magda) - A federated, open-source data catalog for all your big data and small data.\n  * [Metacat](https://github.com/Netflix/metacat) - Unified metadata exploration API service for Hive, RDS, Teradata, Redshift, S3 and Cassandra.\n  * [OpenMetadata](https://open-metadata.org/) - A Single place to discover, collaborate and get your data right.\n\n\n## Data Enrichment\n[](https://github.com/kelvins/awesome-mlops/#data-enrichment)\n_Tools and libraries for data enrichment._\n  * [Snorkel](https://github.com/snorkel-team/snorkel) - A system for quickly generating training data with weak supervision.\n  * [Upgini](https://github.com/upgini/upgini) - Enriches training datasets with features from public and community shared data sources.\n\n\n## Data Exploration\n[](https://github.com/kelvins/awesome-mlops/#data-exploration)\n_Tools for performing data exploration._\n  * [Apache Zeppelin](https://zeppelin.apache.org/) - Enables data-driven, interactive data analytics and collaborative documents.\n  * [BambooLib](https://github.com/tkrabel/bamboolib) - An intuitive GUI for Pandas DataFrames.\n  * [DataPrep](https://github.com/sfu-db/dataprep) - Collect, clean and visualize your data in Python.\n  * [Google Colab](https://colab.research.google.com) - Hosted Jupyter notebook service that requires no setup to use.\n  * [Jupyter Notebook](https://jupyter.org/) - Web-based notebook environment for interactive computing.\n  * [JupyterLab](https://jupyterlab.readthedocs.io) - The next-generation user interface for Project Jupyter.\n  * [Jupytext](https://github.com/mwouts/jupytext) - Jupyter Notebooks as Markdown Documents, Julia, Python or R scripts.\n  * [Pandas Profiling](https://github.com/ydataai/pandas-profiling) - Create HTML profiling reports from pandas DataFrame objects.\n  * [Polynote](https://polynote.org/) - The polyglot notebook with first-class Scala support.\n\n\n## Data Management\n[](https://github.com/kelvins/awesome-mlops/#data-management)\n_Tools for performing data management._\n  * [Arrikto](https://www.arrikto.com/) - Dead simple, ultra fast storage for the hybrid Kubernetes world.\n  * [BlazingSQL](https://github.com/BlazingDB/blazingsql) - A lightweight, GPU accelerated, SQL engine for Python. Built on RAPIDS cuDF.\n  * [Delta Lake](https://github.com/delta-io/delta) - Storage layer that brings scalable, ACID transactions to Apache Spark and other engines.\n  * [Dolt](https://github.com/dolthub/dolt) - SQL database that you can fork, clone, branch, merge, push and pull just like a git repository.\n  * [Dud](https://github.com/kevin-hanselman/dud) - A lightweight CLI tool for versioning data alongside source code and building data pipelines.\n  * [DVC](https://dvc.org/) - Management and versioning of datasets and machine learning models.\n  * [Git LFS](https://git-lfs.github.com) - An open source Git extension for versioning large files.\n  * [Hub](https://github.com/activeloopai/Hub) - A dataset format for creating, storing, and collaborating on AI datasets of any size.\n  * [Intake](https://github.com/intake/intake) - A lightweight set of tools for loading and sharing data in data science projects.\n  * [lakeFS](https://github.com/treeverse/lakeFS) - Repeatable, atomic and versioned data lake on top of object storage.\n  * [Marquez](https://github.com/MarquezProject/marquez) - Collect, aggregate, and visualize a data ecosystem's metadata.\n  * [Milvus](https://github.com/milvus-io/milvus/) - An open source embedding vector similarity search engine powered by Faiss, NMSLIB and Annoy.\n  * [Pinecone](https://www.pinecone.io) - Managed and distributed vector similarity search used with a lightweight SDK.\n  * [Qdrant](https://github.com/qdrant/qdrant) - An open source vector similarity search engine with extended filtering support.\n  * [Quilt](https://github.com/quiltdata/quilt) - A self-organizing data hub with S3 support.\n\n\n## Data Processing\n[](https://github.com/kelvins/awesome-mlops/#data-processing)\n_Tools related to data processing and data pipelines._\n  * [Airflow](https://airflow.apache.org/) - Platform to programmatically author, schedule, and monitor workflows.\n  * [Azkaban](https://github.com/azkaban/azkaban) - Batch workflow job scheduler created at LinkedIn to run Hadoop jobs.\n  * [Dagster](https://github.com/dagster-io/dagster) - A data orchestrator for machine learning, analytics, and ETL.\n  * [Hadoop](https://hadoop.apache.org/) - Framework that allows for the distributed processing of large data sets across clusters.\n  * [OpenRefine](https://github.com/OpenRefine/OpenRefine) - Power tool for working with messy data and improving it.\n  * [Spark](https://spark.apache.org/) - Unified analytics engine for large-scale data processing.\n\n\n## Data Validation\n[](https://github.com/kelvins/awesome-mlops/#data-validation)\n_Tools related to data validation._\n  * [Cerberus](https://github.com/pyeve/cerberus) - Lightweight, extensible data validation library for Python.\n  * [Cleanlab](https://github.com/cleanlab/cleanlab) - Python library for data-centric AI and machine learning with messy, real-world data and labels.\n  * [Great Expectations](https://greatexpectations.io) - A Python data validation framework that allows to test your data against datasets.\n  * [JSON Schema](https://json-schema.org/) - A vocabulary that allows you to annotate and validate JSON documents.\n  * [TFDV](https://github.com/tensorflow/data-validation) - An library for exploring and validating machine learning data.\n\n\n## Data Visualization\n[](https://github.com/kelvins/awesome-mlops/#data-visualization)\n_Tools for data visualization, reports and dashboards._\n  * [Count](https://count.co) - SQL/drag-and-drop querying and visualisation tool based on notebooks.\n  * [Dash](https://github.com/plotly/dash) - Analytical Web Apps for Python, R, Julia, and Jupyter.\n  * [Data Studio](https://datastudio.google.com) - Reporting solution for power users who want to go beyond the data and dashboards of GA.\n  * [Facets](https://github.com/PAIR-code/facets) - Visualizations for understanding and analyzing machine learning datasets.\n  * [Grafana](https://grafana.com/grafana/) - Multi-platform open source analytics and interactive visualization web application.\n  * [Lux](https://github.com/lux-org/lux) - Fast and easy data exploration by automating the visualization and data analysis process.\n  * [Metabase](https://www.metabase.com/) - The simplest, fastest way to get business intelligence and analytics to everyone.\n  * [Redash](https://redash.io/) - Connect to any data source, easily visualize, dashboard and share your data.\n  * [SolidUI](https://github.com/CloudOrc/SolidUI) - AI-generated visualization prototyping and editing platform, support 2D and 3D models.\n  * [Superset](https://superset.incubator.apache.org/) - Modern, enterprise-ready business intelligence web application.\n  * [Tableau](https://www.tableau.com) - Powerful and fastest growing data visualization tool used in the business intelligence industry.\n\n\n## Drift Detection\n[](https://github.com/kelvins/awesome-mlops/#drift-detection)\n_Tools and libraries related to drift detection._\n  * [Alibi Detect](https://github.com/SeldonIO/alibi-detect) - An open source Python library focused on outlier, adversarial and drift detection.\n  * [Frouros](https://github.com/IFCA/frouros) - An open source Python library for drift detection in machine learning systems.\n  * [TorchDrift](https://github.com/torchdrift/torchdrift/) - A data and concept drift library for PyTorch.\n\n\n## Feature Engineering\n[](https://github.com/kelvins/awesome-mlops/#feature-engineering)\n_Tools and libraries related to feature engineering._\n  * [Feature Engine](https://github.com/feature-engine/feature_engine) - Feature engineering package with SKlearn like functionality.\n  * [Featuretools](https://github.com/alteryx/featuretools) - Python library for automated feature engineering.\n  * [TSFresh](https://github.com/blue-yonder/tsfresh) - Python library for automatic extraction of relevant features from time series.\n\n\n## Feature Store\n[](https://github.com/kelvins/awesome-mlops/#feature-store)\n_Feature store tools for data serving._\n  * [Butterfree](https://github.com/quintoandar/butterfree) - A tool for building feature stores. Transform your raw data into beautiful features.\n  * [ByteHub](https://github.com/bytehub-ai/bytehub) - An easy-to-use feature store. Optimized for time-series data.\n  * [Feast](https://feast.dev/) - End-to-end open source feature store for machine learning.\n  * [Feathr](https://github.com/linkedin/feathr) - An enterprise-grade, high performance feature store.\n  * [Featureform](https://github.com/featureform/featureform) - A Virtual Feature Store. Turn your existing data infrastructure into a feature store.\n  * [Tecton](https://www.tecton.ai/) - A fully-managed feature platform built to orchestrate the complete lifecycle of features.\n\n\n## Hyperparameter Tuning\n[](https://github.com/kelvins/awesome-mlops/#hyperparameter-tuning)\n_Tools and libraries to perform hyperparameter tuning._\n  * [Advisor](https://github.com/tobegit3hub/advisor) - Open-source implementation of Google Vizier for hyper parameters tuning.\n  * [Hyperas](https://github.com/maxpumperla/hyperas) - A very simple wrapper for convenient hyperparameter optimization.\n  * [Hyperopt](https://github.com/hyperopt/hyperopt) - Distributed Asynchronous Hyperparameter Optimization in Python.\n  * [Katib](https://github.com/kubeflow/katib) - Kubernetes-based system for hyperparameter tuning and neural architecture search.\n  * [KerasTuner](https://github.com/keras-team/keras-tuner) - Easy-to-use, scalable hyperparameter optimization framework.\n  * [Optuna](https://optuna.org/) - Open source hyperparameter optimization framework to automate hyperparameter search.\n  * [Scikit Optimize](https://github.com/scikit-optimize/scikit-optimize) - Simple and efficient library to minimize expensive and noisy black-box functions.\n  * [Talos](https://github.com/autonomio/talos) - Hyperparameter Optimization for TensorFlow, Keras and PyTorch.\n  * [Tune](https://docs.ray.io/en/latest/tune.html) - Python library for experiment execution and hyperparameter tuning at any scale.\n\n\n## Knowledge Sharing\n[](https://github.com/kelvins/awesome-mlops/#knowledge-sharing)\n_Tools for sharing knowledge to the entire team/company._\n  * [Knowledge Repo](https://github.com/airbnb/knowledge-repo) - Knowledge sharing platform for data scientists and other technical professions.\n  * [Kyso](https://kyso.io/) - One place for data insights so your entire team can learn from your data.\n\n\n## Machine Learning Platform\n[](https://github.com/kelvins/awesome-mlops/#machine-learning-platform)\n_Complete machine learning platform solutions._\n  * [aiWARE](https://www.veritone.com/aiware/aiware-os/) - aiWARE helps MLOps teams evaluate, deploy, integrate, scale & monitor ML models.\n  * [Algorithmia](https://algorithmia.com/) - Securely govern your machine learning operations with a healthy ML lifecycle.\n  * [Allegro AI](https://allegro.ai/) - Transform ML/DL research into products. Faster.\n  * [Bodywork](https://bodywork.readthedocs.io/en/latest/) - Deploys machine learning projects developed in Python, to Kubernetes.\n  * [CNVRG](https://cnvrg.io/) - An end-to-end machine learning platform to build and deploy AI models at scale.\n  * [DAGsHub](https://dagshub.com/) - A platform built on open source tools for data, model and pipeline management.\n  * [Dataiku](https://www.dataiku.com/) - Platform democratizing access to data and enabling enterprises to build their own path to AI.\n  * [DataRobot](https://www.datarobot.com/) - AI platform that democratizes data science and automates the end-to-end ML at scale.\n  * [Domino](https://www.dominodatalab.com/) - One place for your data science tools, apps, results, models, and knowledge.\n  * [Edge Impulse](https://edgeimpulse.com/) - Platform for creating, optimizing, and deploying AI/ML algorithms for edge devices.\n  * [envd](https://github.com/tensorchord/envd) - Machine learning development environment for data science and AI/ML engineering teams.\n  * [FedML](https://fedml.ai/) - Simplifies the workflow of federated learning anywhere at any scale.\n  * [Gradient](https://gradient.paperspace.com/) - Multicloud CI/CD and MLOps platform for machine learning teams.\n  * [H2O](https://www.h2o.ai/) - Open source leader in AI with a mission to democratize AI for everyone.\n  * [Hopsworks](https://www.hopsworks.ai/) - Open-source platform for developing and operating machine learning models at scale.\n  * [Iguazio](https://www.iguazio.com/) - Data science platform that automates MLOps with end-to-end machine learning pipelines.\n  * [Katonic](https://katonic.ai/) - Automate your cycle of intelligence with Katonic MLOps Platform.\n  * [Knime](https://www.knime.com/) - Create and productionize data science using one easy and intuitive environment.\n  * [Kubeflow](https://www.kubeflow.org/) - Making deployments of ML workflows on Kubernetes simple, portable and scalable.\n  * [LynxKite](https://lynxkite.com/) - A complete graph data science platform for very large graphs and other datasets.\n  * [ML Workspace](https://github.com/ml-tooling/ml-workspace) - All-in-one web-based IDE specialized for machine learning and data science.\n  * [MLReef](https://github.com/MLReef/mlreef) - Open source MLOps platform that helps you collaborate, reproduce and share your ML work.\n  * [Modzy](https://www.modzy.com/) - Deploy, connect, run, and monitor machine learning (ML) models in the enterprise and at the edge.\n  * [Neu.ro](https://neu.ro) - MLOps platform that integrates open-source and proprietary tools into client-oriented systems.\n  * [Omnimizer](https://www.omniml.ai) - Simplifies and accelerates MLOps by bridging the gap between ML models and edge hardware.\n  * [Pachyderm](https://www.pachyderm.com/) - Combines data lineage with end-to-end pipelines on Kubernetes, engineered for the enterprise.\n  * [Polyaxon](https://www.github.com/polyaxon/polyaxon/) - A platform for reproducible and scalable machine learning and deep learning on kubernetes.\n  * [Sagemaker](https://aws.amazon.com/sagemaker/) - Fully managed service that provides the ability to build, train, and deploy ML models quickly.\n  * [SAS Viya](https://www.sas.com/en_us/software/viya.html) - Cloud native AI, analytic and data management platform that supports the analytics life cycle.\n  * [Sematic](https://sematic.dev) - An open-source end-to-end pipelining tool to go from laptop prototype to cloud in no time.\n  * [SigOpt](https://sigopt.com/) - A platform that makes it easy to track runs, visualize training, and scale hyperparameter tuning.\n  * [TrueFoundry](https://www.truefoundry.com) - A Cloud-native MLOps Platform over Kubernetes to simplify training and serving of ML Models.\n  * [Valohai](https://valohai.com/) - Takes you from POC to production while managing the whole model lifecycle.\n\n\n## Model Fairness and Privacy\n[](https://github.com/kelvins/awesome-mlops/#model-fairness-and-privacy)\n_Tools for performing model fairness and privacy in production._\n  * [AIF360](https://github.com/Trusted-AI/AIF360) - A comprehensive set of fairness metrics for datasets and machine learning models.\n  * [Fairlearn](https://github.com/fairlearn/fairlearn) - A Python package to assess and improve fairness of machine learning models.\n  * [Opacus](https://github.com/pytorch/opacus) - A library that enables training PyTorch models with differential privacy.\n  * [TensorFlow Privacy](https://github.com/tensorflow/privacy) - Library for training machine learning models with privacy for training data.\n\n\n## Model Interpretability\n[](https://github.com/kelvins/awesome-mlops/#model-interpretability)\n_Tools for performing model interpretability/explainability._\n  * [Alibi](https://github.com/SeldonIO/alibi) - Open-source Python library enabling ML model inspection and interpretation.\n  * [Captum](https://github.com/pytorch/captum) - Model interpretability and understanding library for PyTorch.\n  * [ELI5](https://github.com/eli5-org/eli5) - Python package which helps to debug machine learning classifiers and explain their predictions.\n  * [InterpretML](https://github.com/interpretml/interpret) - A toolkit to help understand models and enable responsible machine learning.\n  * [LIME](https://github.com/marcotcr/lime) - Explaining the predictions of any machine learning classifier.\n  * [Lucid](https://github.com/tensorflow/lucid) - Collection of infrastructure and tools for research in neural network interpretability.\n  * [SAGE](https://github.com/iancovert/sage) - For calculating global feature importance using Shapley values.\n  * [SHAP](https://github.com/slundberg/shap) - A game theoretic approach to explain the output of any machine learning model.\n\n\n## Model Lifecycle\n[](https://github.com/kelvins/awesome-mlops/#model-lifecycle)\n_Tools for managing model lifecycle (tracking experiments, parameters and metrics)._\n  * [Aeromancy](https://github.com/quant-aq/aeromancy) - A framework for performing reproducible AI and ML for Weights and Biases.\n  * [Aim](https://github.com/aimhubio/aim) - A super-easy way to record, search and compare 1000s of ML training runs.\n  * [Cascade](https://github.com/Oxid15/cascade) - Library of ML-Engineering tools for rapid prototyping and experiment management.\n  * [Comet](https://github.com/comet-ml/comet-examples) - Track your datasets, code changes, experimentation history, and models.\n  * [Guild AI](https://guild.ai/) - Open source experiment tracking, pipeline automation, and hyperparameter tuning.\n  * [Keepsake](https://github.com/replicate/keepsake) - Version control for machine learning with support to Amazon S3 and Google Cloud Storage.\n  * [Losswise](https://losswise.com) - Makes it easy to track the progress of a machine learning project.\n  * [MLflow](https://mlflow.org/) - Open source platform for the machine learning lifecycle.\n  * [ModelDB](https://github.com/VertaAI/modeldb/) - Open source ML model versioning, metadata, and experiment management.\n  * [Neptune AI](https://neptune.ai/) - The most lightweight experiment management tool that fits any workflow.\n  * [Sacred](https://github.com/IDSIA/sacred) - A tool to help you configure, organize, log and reproduce experiments.\n  * [Weights and Biases](https://github.com/wandb/client) - A tool for visualizing and tracking your machine learning experiments.\n\n\n## Model Serving\n[](https://github.com/kelvins/awesome-mlops/#model-serving)\n_Tools for serving models in production._\n  * [Banana](https://banana.dev) - Host your ML inference code on serverless GPUs and integrate it into your app with one line of code.\n  * [Beam](https://beam.cloud) - Develop on serverless GPUs, deploy highly performant APIs, and rapidly prototype ML models.\n  * [BentoML](https://github.com/bentoml/BentoML) - Open-source platform for high-performance ML model serving.\n  * [BudgetML](https://github.com/ebhy/budgetml) - Deploy a ML inference service on a budget in less than 10 lines of code.\n  * [Cog](https://github.com/replicate/cog) - Open-source tool that lets you package ML models in a standard, production-ready container.\n  * [Cortex](https://www.cortex.dev/) - Machine learning model serving infrastructure.\n  * [Geniusrise](https://docs.geniusrise.ai) - Host inference APIs, bulk inference and fine tune text, vision, audio and multi-modal models.\n  * [Gradio](https://github.com/gradio-app/gradio) - Create customizable UI components around your models.\n  * [GraphPipe](https://oracle.github.io/graphpipe) - Machine learning model deployment made simple.\n  * [Hydrosphere](https://github.com/Hydrospheredata/hydro-serving) - Platform for deploying your Machine Learning to production.\n  * [KFServing](https://github.com/kubeflow/kfserving) - Kubernetes custom resource definition for serving ML models on arbitrary frameworks.\n  * [LocalAI](https://github.com/mudler/LocalAI) - Drop-in replacement REST API that\u2019s compatible with OpenAI API specifications for inferencing.\n  * [Merlin](https://github.com/gojek/merlin) - A platform for deploying and serving machine learning models.\n  * [MLEM](https://github.com/iterative/mlem) - Version and deploy your ML models following GitOps principles.\n  * [Opyrator](https://github.com/ml-tooling/opyrator) - Turns your ML code into microservices with web API, interactive GUI, and more.\n  * [PredictionIO](https://github.com/apache/predictionio) - Event collection, deployment of algorithms, evaluation, querying predictive results via APIs.\n  * [Quix](https://quix.io) - Serverless platform for processing data streams in real-time with machine learning models.\n  * [Rune](https://github.com/hotg-ai/rune) - Provides containers to encapsulate and deploy EdgeML pipelines and applications.\n  * [Seldon](https://www.seldon.io/) - Take your ML projects from POC to production with maximum efficiency and minimal risk.\n  * [Streamlit](https://github.com/streamlit/streamlit) - Lets you create apps for your ML projects with deceptively simple Python scripts.\n  * [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) - Flexible, high-performance serving system for ML models, designed for production.\n  * [TorchServe](https://github.com/pytorch/serve) - A flexible and easy to use tool for serving PyTorch models.\n  * [Triton Inference Server](https://github.com/triton-inference-server/server) - Provides an optimized cloud and edge inferencing solution.\n  * [Vespa](https://github.com/vespa-engine/vespa) - Store, search, organize and make machine-learned inferences over big data at serving time.\n  * [Wallaroo.AI](https://wallaroo.ai/) - A platform for deploying, serving, and optimizing ML models in both cloud and edge environments.\n\n\n## Model Testing & Validation\n[](https://github.com/kelvins/awesome-mlops/#model-testing--validation)\n_Tools for testing and validating models._\n  * [Deepchecks](https://github.com/deepchecks/deepchecks) - Open-source package for validating ML models & data, with various checks and suites.\n  * [Starwhale](https://github.com/star-whale/starwhale) - An MLOps/LLMOps platform for model building, evaluation, and fine-tuning.\n  * [Trubrics](https://github.com/trubrics/trubrics-sdk) - Validate machine learning with data science and domain expert feedback.\n\n\n## Optimization Tools\n[](https://github.com/kelvins/awesome-mlops/#optimization-tools)\n_Optimization tools related to model scalability in production._\n  * [Accelerate](https://github.com/huggingface/accelerate) - A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision.\n  * [Dask](https://dask.org/) - Provides advanced parallelism for analytics, enabling performance at scale for the tools you love.\n  * [DeepSpeed](https://github.com/microsoft/DeepSpeed) - Deep learning optimization library that makes distributed training easy, efficient, and effective.\n  * [Fiber](https://uber.github.io/fiber/) - Python distributed computing library for modern computer clusters.\n  * [Horovod](https://github.com/horovod/horovod) - Distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.\n  * [Mahout](https://mahout.apache.org/) - Distributed linear algebra framework and mathematically expressive Scala DSL.\n  * [MLlib](https://spark.apache.org/mllib/) - Apache Spark's scalable machine learning library.\n  * [Modin](https://github.com/modin-project/modin) - Speed up your Pandas workflows by changing a single line of code.\n  * [Nebullvm](https://github.com/nebuly-ai/nebullvm) - Easy-to-use library to boost AI inference.\n  * [Nos](https://github.com/nebuly-ai/nos) - Open-source module for running AI workloads on Kubernetes in an optimized way.\n  * [Petastorm](https://github.com/uber/petastorm) - Enables single machine or distributed training and evaluation of deep learning models.\n  * [Rapids](https://rapids.ai/index.html) - Gives the ability to execute end-to-end data science and analytics pipelines entirely on GPUs.\n  * [Ray](https://github.com/ray-project/ray) - Fast and simple framework for building and running distributed applications.\n  * [Singa](http://singa.apache.org/en/index.html) - Apache top level project, focusing on distributed training of DL and ML models.\n  * [Tpot](https://github.com/EpistasisLab/tpot) - Automated ML tool that optimizes machine learning pipelines using genetic programming.\n\n\n## Simplification Tools\n[](https://github.com/kelvins/awesome-mlops/#simplification-tools)\n_Tools related to machine learning simplification and standardization._\n  * [Chassis](https://chassisml.io) - Turns models into ML-friendly containers that run just about anywhere.\n  * [Hermione](https://github.com/a3data/hermione) - Help Data Scientists on setting up more organized codes, in a quicker and simpler way.\n  * [Hydra](https://github.com/facebookresearch/hydra) - A framework for elegantly configuring complex applications.\n  * [Koalas](https://github.com/databricks/koalas) - Pandas API on Apache Spark. Makes data scientists more productive when interacting with big data.\n  * [Ludwig](https://github.com/uber/ludwig) - Allows users to train and test deep learning models without the need to write code.\n  * [MLNotify](https://github.com/aporia-ai/mlnotify) - No need to keep checking your training, just one import line and you'll know the second it's done.\n  * [PyCaret](https://pycaret.org/) - Open source, low-code machine learning library in Python.\n  * [Sagify](https://github.com/Kenza-AI/sagify) - A CLI utility to train and deploy ML/DL models on AWS SageMaker.\n  * [Soopervisor](https://github.com/ploomber/soopervisor) - Export ML projects to Kubernetes (Argo workflows), Airflow, AWS Batch, and SLURM.\n  * [Soorgeon](https://github.com/ploomber/soorgeon) - Convert monolithic Jupyter notebooks into maintainable pipelines.\n  * [TrainGenerator](https://github.com/jrieke/traingenerator) - A web app to generate template code for machine learning.\n  * [Turi Create](https://github.com/apple/turicreate) - Simplifies the development of custom machine learning models.\n\n\n## Visual Analysis and Debugging\n[](https://github.com/kelvins/awesome-mlops/#visual-analysis-and-debugging)\n_Tools for performing visual analysis and debugging of ML/DL models._\n  * [Aporia](https://www.aporia.com/) - Observability with customized monitoring and explainability for ML models.\n  * [Arize](https://www.arize.com/) - A free end-to-end ML observability and model monitoring platform.\n  * [Evidently](https://github.com/evidentlyai/evidently) - Interactive reports to analyze ML models during validation or production monitoring.\n  * [Fiddler](https://www.fiddler.ai/) - Monitor, explain, and analyze your AI in production.\n  * [Manifold](https://github.com/uber/manifold) - A model-agnostic visual debugging tool for machine learning.\n  * [NannyML](https://github.com/NannyML/nannyml) - Algorithm capable of fully capturing the impact of data drift on performance.\n  * [Netron](https://github.com/lutzroeder/netron) - Visualizer for neural network, deep learning, and machine learning models.\n  * [Opik](https://github.com/comet-ml/opik) - Evaluate, test, and ship LLM applications with a suite of observability tools.\n  * [Phoenix](https://phoenix.arize.com) - MLOps in a Notebook for troubleshooting and fine-tuning generative LLM, CV, and tabular models.\n  * [Radicalbit](https://github.com/radicalbit/radicalbit-ai-monitoring/) - The open source solution for monitoring your AI models in production.\n  * [Superwise](https://www.superwise.ai) - Fully automated, enterprise-grade model observability in a self-service SaaS platform.\n  * [Whylogs](https://github.com/whylabs/whylogs) - The open source standard for data logging. Enables ML monitoring and observability.\n  * [Yellowbrick](https://github.com/DistrictDataLabs/yellowbrick) - Visual analysis and diagnostic tools to facilitate machine learning model selection.\n\n\n## Workflow Tools\n[](https://github.com/kelvins/awesome-mlops/#workflow-tools)\n_Tools and frameworks to create workflows or pipelines in the machine learning context._\n  * [Argo](https://github.com/argoproj/argo) - Open source container-native workflow engine for orchestrating parallel jobs on Kubernetes.\n  * [Automate Studio](https://www.veritone.com/applications/automate-studio/) - Rapidly build & deploy AI-powered workflows.\n  * [Couler](https://github.com/couler-proj/couler) - Unified interface for constructing and managing workflows on different workflow engines.\n  * [dstack](https://github.com/dstackai/dstack) - An open-core tool to automate data and training workflows.\n  * [Flyte](https://flyte.org/) - Easy to create concurrent, scalable, and maintainable workflows for machine learning.\n  * [Hamilton](https://github.com/dagworks-inc/hamilton) - A scalable general purpose micro-framework for defining dataflows.\n  * [Kale](https://github.com/kubeflow-kale/kale) - Aims at simplifying the Data Science experience of deploying Kubeflow Pipelines workflows.\n  * [Kedro](https://github.com/quantumblacklabs/kedro) - Library that implements software engineering best-practice for data and ML pipelines.\n  * [Luigi](https://github.com/spotify/luigi) - Python module that helps you build complex pipelines of batch jobs.\n  * [Metaflow](https://metaflow.org/) - Human-friendly lib that helps scientists and engineers build and manage data science projects.\n  * [MLRun](https://github.com/mlrun/mlrun) - Generic mechanism for data scientists to build, run, and monitor ML tasks and pipelines.\n  * [Orchest](https://github.com/orchest/orchest/) - Visual pipeline editor and workflow orchestrator with an easy to use UI and based on Kubernetes.\n  * [Ploomber](https://github.com/ploomber/ploomber) - Write maintainable, production-ready pipelines. Develop locally, deploy to the cloud.\n  * [Prefect](https://docs.prefect.io/) - A workflow management system, designed for modern infrastructure.\n  * [VDP](https://github.com/instill-ai/vdp) - An open-source tool to seamlessly integrate AI for unstructured data into the modern data stack.\n  * [Wordware](https://www.wordware.ai) - A web-hosted IDE where non-technical domain experts can build task-specific AI agents.\n  * [ZenML](https://github.com/maiot-io/zenml) - An extensible open-source MLOps framework to create reproducible pipelines.\n\n\n# Resources\n[](https://github.com/kelvins/awesome-mlops/#resources)\nWhere to discover new tools and discuss about existing ones.\n## Articles\n[](https://github.com/kelvins/awesome-mlops/#articles)\n  * [A Tour of End-to-End Machine Learning Platforms](https://databaseline.tech/a-tour-of-end-to-end-ml-platforms/) (Databaseline)\n  * [Continuous Delivery for Machine Learning](https://martinfowler.com/articles/cd4ml.html) (Martin Fowler)\n  * [Machine Learning Operations (MLOps): Overview, Definition, and Architecture](https://arxiv.org/abs/2205.02302) (arXiv)\n  * [MLOps Roadmap: A Complete MLOps Career Guide](https://www.scaler.com/blog/mlops-roadmap/) (Scaler Blogs)\n  * [MLOps: Continuous delivery and automation pipelines in machine learning](https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning) (Google)\n  * [MLOps: Machine Learning as an Engineering Discipline](https://towardsdatascience.com/ml-ops-machine-learning-as-an-engineering-discipline-b86ca4874a3f) (Medium)\n  * [Rules of Machine Learning: Best Practices for ML Engineering](https://developers.google.com/machine-learning/guides/rules-of-ml) (Google)\n  * [The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf) (Google)\n  * [What Is MLOps?](https://blogs.nvidia.com/blog/2020/09/03/what-is-mlops/) (NVIDIA)\n\n\n## Books\n[](https://github.com/kelvins/awesome-mlops/#books)\n  * [Beginning MLOps with MLFlow](https://www.amazon.com/Beginning-MLOps-MLFlow-SageMaker-Microsoft/dp/1484265483) (Apress)\n  * [Building Machine Learning Pipelines](https://www.oreilly.com/library/view/building-machine-learning/9781492053187) (O'Reilly)\n  * [Building Machine Learning Powered Applications](https://www.oreilly.com/library/view/building-machine-learning/9781492045106) (O'Reilly)\n  * [Deep Learning in Production](https://www.amazon.com/gp/product/6180033773) (AI Summer)\n  * [Designing Machine Learning Systems](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956) (O'Reilly)\n  * [Engineering MLOps](https://www.packtpub.com/product/engineering-mlops/9781800562882) (Packt)\n  * [Implementing MLOps in the Enterprise](https://www.oreilly.com/library/view/implementing-mlops-in/9781098136574) (O'Reilly)\n  * [Introducing MLOps](https://www.oreilly.com/library/view/introducing-mlops/9781492083283) (O'Reilly)\n  * [Kubeflow for Machine Learning](https://www.oreilly.com/library/view/kubeflow-for-machine/9781492050117) (O'Reilly)\n  * [Kubeflow Operations Guide](https://www.oreilly.com/library/view/kubeflow-operations-guide/9781492053262) (O'Reilly)\n  * [Machine Learning Design Patterns](https://www.oreilly.com/library/view/machine-learning-design/9781098115777) (O'Reilly)\n  * [Machine Learning Engineering in Action](https://www.manning.com/books/machine-learning-engineering-in-action) (Manning)\n  * [ML Ops: Operationalizing Data Science](https://www.oreilly.com/library/view/ml-ops-operationalizing/9781492074663) (O'Reilly)\n  * [MLOps Engineering at Scale](https://www.manning.com/books/mlops-engineering-at-scale) (Manning)\n  * [MLOps Lifecycle Toolkit](https://link.springer.com/book/10.1007/978-1-4842-9642-4) (Apress)\n  * [Practical Deep Learning at Scale with MLflow](https://www.packtpub.com/product/practical-deep-learning-at-scale-with-mlflow/9781803241333) (Packt)\n  * [Practical MLOps](https://www.oreilly.com/library/view/practical-mlops/9781098103002) (O'Reilly)\n  * [Production-Ready Applied Deep Learning](https://www.packtpub.com/product/production-ready-applied-deep-learning/9781803243665) (Packt)\n  * [Reliable Machine Learning](https://www.oreilly.com/library/view/reliable-machine-learning/9781098106218) (O'Reilly)\n  * [The Machine Learning Solutions Architect Handbook](https://www.packtpub.com/product/the-machine-learning-solutions-architect-handbook/9781801072168) (Packt)\n\n\n## Events\n[](https://github.com/kelvins/awesome-mlops/#events)\n  * [apply() - The ML data engineering conference](https://www.applyconf.com/)\n  * [MLOps Conference - Keynotes and Panels](https://www.youtube.com/playlist?list=PLH8M0UOY0uy6d_n3vEQe6J_gRBUrISF9m)\n  * [MLOps World: Machine Learning in Production Conference](https://mlopsworld.com/)\n  * [NormConf - The Normcore Tech Conference](https://normconf.com/)\n  * [Stanford MLSys Seminar Series](https://mlsys.stanford.edu/)\n\n\n## Other Lists\n[](https://github.com/kelvins/awesome-mlops/#other-lists)\n  * [Applied ML](https://github.com/eugeneyan/applied-ml)\n  * [Awesome AutoML Papers](https://github.com/hibayesian/awesome-automl-papers)\n  * [Awesome AutoML](https://github.com/windmaple/awesome-AutoML)\n  * [Awesome Data Science](https://github.com/academic/awesome-datascience)\n  * [Awesome DataOps](https://github.com/kelvins/awesome-dataops)\n  * [Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning)\n  * [Awesome Game Datasets](https://github.com/leomaurodesenv/game-datasets) (includes AI content)\n  * [Awesome Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)\n  * [Awesome MLOps](https://github.com/visenger/awesome-mlops)\n  * [Awesome Production Machine Learning](https://github.com/EthicalML/awesome-production-machine-learning)\n  * [Awesome Python](https://github.com/vinta/awesome-python)\n  * [Deep Learning in Production](https://github.com/ahkarami/Deep-Learning-in-Production)\n\n\n## Podcasts\n[](https://github.com/kelvins/awesome-mlops/#podcasts)\n  * [Kubernetes Podcast from Google](https://kubernetespodcast.com/)\n  * [Machine Learning \u2013 Software Engineering Daily](https://podcasts.google.com/?feed=aHR0cHM6Ly9zb2Z0d2FyZWVuZ2luZWVyaW5nZGFpbHkuY29tL2NhdGVnb3J5L21hY2hpbmUtbGVhcm5pbmcvZmVlZC8)\n  * [MLOps.community](https://podcasts.google.com/?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8xNzRjYjFiOC9wb2RjYXN0L3Jzcw)\n  * [Pipeline Conversation](https://podcast.zenml.io/)\n  * [Practical AI: Machine Learning, Data Science](https://changelog.com/practicalai)\n  * [This Week in Machine Learning & AI](https://twimlai.com/)\n  * [True ML Talks](https://www.youtube.com/playlist?list=PL4-eEhdXDO5F9Myvh41EeUh7oCgzqFRGk)\n\n\n## Slack\n[](https://github.com/kelvins/awesome-mlops/#slack)\n  * [Kubeflow Workspace](https://kubeflow.slack.com/#/)\n  * [MLOps Community Wokspace](https://mlops-community.slack.com)\n\n\n## Websites\n[](https://github.com/kelvins/awesome-mlops/#websites)\n  * [Feature Stores for ML](http://featurestore.org/)\n  * [Made with ML](https://github.com/GokuMohandas/Made-With-ML)\n  * [ML-Ops](https://ml-ops.org/)\n  * [MLOps Community](https://mlops.community/)\n  * [MLOps Guide](https://mlops-guide.github.io/)\n  * [MLOps Now](https://mlopsnow.com)\n\n\n# Contributing\n[](https://github.com/kelvins/awesome-mlops/#contributing)\nAll contributions are welcome! Please take a look at the [contribution guidelines](https://github.com/kelvins/awesome-mlops/blob/main/CONTRIBUTING.md) first.\n## About\n\ud83d\ude0e A curated list of awesome MLOps tools \n### Topics\n[ data-science ](https://github.com/topics/data-science \"Topic: data-science\") [ machine-learning ](https://github.com/topics/machine-learning \"Topic: machine-learning\") [ awesome ](https://github.com/topics/awesome \"Topic: awesome\") [ ai ](https://github.com/topics/ai \"Topic: ai\") [ ml ](https://github.com/topics/ml \"Topic: ml\") [ machine-learning-engineering ](https://github.com/topics/machine-learning-engineering \"Topic: machine-learning-engineering\") [ mle ](https://github.com/topics/mle \"Topic: mle\") [ mlops ](https://github.com/topics/mlops \"Topic: mlops\")\n### Resources\n[ Readme ](https://github.com/kelvins/awesome-mlops/#readme-ov-file)\n### Code of conduct\n[ Code of conduct ](https://github.com/kelvins/awesome-mlops/#coc-ov-file)\n[ Activity](https://github.com/kelvins/awesome-mlops/activity)\n### Stars\n[ **4.4k** stars](https://github.com/kelvins/awesome-mlops/stargazers)\n### Watchers\n[ **91** watching](https://github.com/kelvins/awesome-mlops/watchers)\n### Forks\n[ **604** forks](https://github.com/kelvins/awesome-mlops/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fkelvins%2Fawesome-mlops&report=kelvins+%28user%29)\n##  [Contributors 67](https://github.com/kelvins/awesome-mlops/graphs/contributors)\n  * [ ![@kelvins](https://avatars.githubusercontent.com/u/1009397?s=64&v=4) ](https://github.com/kelvins)\n  * [ ![@mheriyanto](https://avatars.githubusercontent.com/u/22278148?s=64&v=4) ](https://github.com/mheriyanto)\n  * [ ![@jwmueller](https://avatars.githubusercontent.com/u/1390638?s=64&v=4) ](https://github.com/jwmueller)\n  * [ ![@gleydson404](https://avatars.githubusercontent.com/u/9268819?s=64&v=4) ](https://github.com/gleydson404)\n  * [ ![@emmanuelraj7](https://avatars.githubusercontent.com/u/17778830?s=64&v=4) ](https://github.com/emmanuelraj7)\n  * [ ![@Oxid15](https://avatars.githubusercontent.com/u/31771351?s=64&v=4) ](https://github.com/Oxid15)\n  * [ ![@akashg3627](https://avatars.githubusercontent.com/u/67226124?s=64&v=4) ](https://github.com/akashg3627)\n  * [ ![@romaup](https://avatars.githubusercontent.com/u/95645411?s=64&v=4) ](https://github.com/romaup)\n  * [ ![@anayden](https://avatars.githubusercontent.com/u/17383?s=64&v=4) ](https://github.com/anayden)\n  * [ ![@idomic](https://avatars.githubusercontent.com/u/11596985?s=64&v=4) ](https://github.com/idomic)\n  * [ ![@safoinme](https://avatars.githubusercontent.com/u/34200873?s=64&v=4) ](https://github.com/safoinme)\n  * [ ![@anmorgan24](https://avatars.githubusercontent.com/u/86856445?s=64&v=4) ](https://github.com/anmorgan24)\n  * [ ![@Paras-96](https://avatars.githubusercontent.com/u/169983983?s=64&v=4) ](https://github.com/Paras-96)\n  * [ ![@ixaxaar](https://avatars.githubusercontent.com/u/144122?s=64&v=4) ](https://github.com/ixaxaar)\n\n\n[+ 53 contributors](https://github.com/kelvins/awesome-mlops/graphs/contributors)\n## Languages\n  * [ Python 100.0% ](https://github.com/kelvins/awesome-mlops/search?l=python)\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary\n\nA curated list of **awesome MLOps tools** categorized into areas like AutoML, CI/CD, Data Management, Model Serving, and more. It includes resources for articles, books, events, and podcasts to enhance knowledge in MLOps. Contributions are welcome!\n\n**Tags**: #MLOps #MachineLearning #AI #DataScience\n```"
  },
  {
    "instruction": "[Qwak is Now Officially Part of JFrog! \ud83d\udc9a Learn More -->](https://www.jfrog.com/jfrog-ml/)\nx\n[](https://www.qwak.com/)\n[Platform](https://jfrog.com/jfrog-ml/)\nResources\n[Blog](https://www.qwak.com/blog)[Events & Webinars](https://www.qwak.com/events)[Customer Stories](https://www.qwak.com/case-studies)[Documentation](https://docs.qwak.com/docs)\n[![State of AI and LLMs by Qwak](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ab/66ab89d732723793e7d1ff9d_state%20of%20ai%20%26llms.png)](https://www.qwak.com/state-of-ai)\nNew Report\n[Discover the latest AI trends, insights, and innovations in 2024. Access the Report ->](https://www.qwak.com/state-of-ai)\n[Pricing](https://www.qwak.com/old-pricing)\n[Schedule a Demo](https://www.qwak.com/demo)\n[Start for Free](https://app.qwak.ai)\n# The latest from JFrog ML\nSubscribe to our newsletter and never miss out on the latest ML updates, news, and events.\n[![Bridging the Gap: How MLOps and DevOps Work Together for AI Adoption in 2025](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/67558c238ffe48f18f8b6403_stateofai-blog.webp)Bridging the Gap: How MLOps and DevOps Work Together for AI Adoption in 2025Discover how DevOps and MLOps are teaming up to make AI work smarter, not harder\u2014breaking down barriers and turning tech challenges into game-changing opportunities.![Bridging the Gap: How MLOps and DevOps Work Together for AI Adoption in 2025](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/673b2597b8c150ffe6776c2c_Guy%20Eshet.png)Guy EshetDecember 8, 2024](https://www.qwak.com/post/ai-adoption-in-2025)\nThank you! Your submission has been received!\nOops! Something went wrong while submitting the form.\n[All](https://www.qwak.com/blog/#w-tabs-0-data-w-pane-0)[MLOps](https://www.qwak.com/blog/#w-tabs-0-data-w-pane-1)[LLMOps](https://www.qwak.com/blog/#w-tabs-0-data-w-pane-2)[Feature Store](https://www.qwak.com/blog/#w-tabs-0-data-w-pane-3)[Announcements](https://www.qwak.com/blog/#announcements)\n[![Uncovering the Power of Real-Time Feature Engineering in Machine Learning](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/66e84e71f825fc090c4d54b9_real%20time%20Feature%20engineering%20Sharing%20image.webp)Feature StoreUncovering the Power of Real-Time Feature Engineering in Machine Learning![Uncovering the Power of Real-Time Feature Engineering in Machine Learning](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaSeptember 16, 2024](https://www.qwak.com/post/real-time-feature-engineering)\n[![Introducing JFrog ML: Unifying DevOps, DevSecOps and MLOps for a Secure Software Supply Chain Platform](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/66e02a93111d106f3f24ba7f_1200x630%20-%20introducing%20jfrog%20ml%20-%20blog.png)AnnouncementsIntroducing JFrog ML: Unifying DevOps, DevSecOps and MLOps for a Secure Software Supply Chain Platform![Introducing JFrog ML: Unifying DevOps, DevSecOps and MLOps for a Secure Software Supply Chain Platform](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/673b2597b8c150ffe6776c2c_Guy%20Eshet.png)Guy EshetSeptember 10, 2024](https://www.qwak.com/post/jfrog-ml)\n[![Simplifying Machine Learning with the Qwak Feature Store](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/66b351d098d31ae6b411754a_new%20Feature%20Store%20Webinar%20Sharing%20image.webp)Feature StoreSimplifying Machine Learning with the Qwak Feature Store![Simplifying Machine Learning with the Qwak Feature Store](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d4a5b13ae20858614e0fcd_Hudson.png)Hudson BuzbyAugust 6, 2024](https://www.qwak.com/post/leverage-feature-store)\n[![Data Engineering vs Feature Engineering: What Every Data Scientist Should Know](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/669e515a6b873540e27c7eb1_Feature%20engineering%20vs%20Sharing%20image.webp)Feature StoreData Engineering vs Feature Engineering: What Every Data Scientist Should Know![Data Engineering vs Feature Engineering: What Every Data Scientist Should Know](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d4a5b13ae20858614e0fcd_Hudson.png)Hudson BuzbyJuly 23, 2024](https://www.qwak.com/post/data-engineering-vs-feature-engineering)\n[![Building Robust Feature Engineering Pipelines: From Experimentation to Production](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/668e4df8515ffe2cd1a6dd6f___Building%20Robust%20Feature%20Engineering%20Pipelines.webp)Feature StoreBuilding Robust Feature Engineering Pipelines: From Experimentation to Production![Building Robust Feature Engineering Pipelines: From Experimentation to Production](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaJuly 10, 2024](https://www.qwak.com/post/feature-engineering-pipeline)\n[![JFrog & Qwak: Accelerating Models Into Production \u2013 The DevOps Way](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6686b5141b04553fff8b7162_jfrog_qwak_banner.png)AnnouncementsJFrog & Qwak: Accelerating Models Into Production \u2013 The DevOps Way![JFrog & Qwak: Accelerating Models Into Production \u2013 The DevOps Way](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6592d4e7b4985a7582bceec7_Yuval.png)Yuval FernbachJuly 3, 2024](https://www.qwak.com/post/jfrog-qwak-accelerating-models-into-production)\n[![Mastering LLM Gateway: A Developer\u2019s Guide to AI Model Interfacing](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/667a8b5114958797799e730a_Mastering%20LLM%20Gateway_%20A%20Developer%E2%80%99s%20Guide%20to%20AI%20Model%20Interfacing.webp)LLMopsMastering LLM Gateway: A Developer\u2019s Guide to AI Model Interfacing![Mastering LLM Gateway: A Developer\u2019s Guide to AI Model Interfacing](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaJune 27, 2024](https://www.qwak.com/post/llm-gateway)\n[![Breaking Down the Cost of Large Language Models](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/667a7ac93d4e456261f7f5ef_breaking%20down%20the%20costs%20of%20llms.webp)LLMopsBreaking Down the Cost of Large Language Models![Breaking Down the Cost of Large Language Models](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d4a5b13ae20858614e0fcd_Hudson.png)Hudson BuzbyJune 25, 2024](https://www.qwak.com/post/llm-cost)\n[![Building LLM Applications for Production - Key Strategies & Best Practices](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/66793b9702cd433542c83809_Building%20LLM%20Applications%20for%20Production%20-%20Key%20Strategies%20%26%20Best%20Practices.webp)LLMopsBuilding LLM Applications for Production - Key Strategies & Best Practices![Building LLM Applications for Production - Key Strategies & Best Practices](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65756649b900dfa46cf52d98_Pavel%20Klushin.webp)Pavel KlushinJune 24, 2024](https://www.qwak.com/post/building-llm-applications-for-production)\n[Next](https://www.qwak.com/blog/?b38d322f_page=2)\n[![Bridging the Gap: How MLOps and DevOps Work Together for AI Adoption in 2025](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/67558c238ffe48f18f8b6403_stateofai-blog.webp)MLOpsBridging the Gap: How MLOps and DevOps Work Together for AI Adoption in 2025![Guy Eshet](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/673b2597b8c150ffe6776c2c_Guy%20Eshet.png)Guy EshetDecember 8, 2024](https://www.qwak.com/post/ai-adoption-in-2025)\n[![Model Deployment: Strategies, Best Practices, and Use Cases](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/662a1ee037944f4cf83ae8b1_Model%20Deployment_%20Strategies%2C%20Best%20Practices%2C%20and%20Use%20Cases.jpg)MLOpsModel Deployment: Strategies, Best Practices, and Use Cases![Hudson Buzby](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d4a5b13ae20858614e0fcd_Hudson.png)Hudson BuzbyApril 24, 2024](https://www.qwak.com/post/model-deployment)\n[![How to Build An End-to-End Machine Learning Pipeline in 2024](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6568678f94f291294481c076_How%20to%20Build%20An%20End-to-End%20Machine%20Learning%20Pipeline%20in%202024.webp)MLOpsHow to Build An End-to-End Machine Learning Pipeline in 2024![Alon Lev](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65756616f22229e90015e9b7_Alon%20Lev.webp)Alon LevMarch 27, 2024](https://www.qwak.com/post/end-to-end-machine-learning-pipeline)\n[![Top End-to-End MLOps Platforms and Tools in 2024](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65bb793d8fad640c4dcdcb1d_Top%20End%20to%20End%20MLOps%20Platforms.webp)MLOpsTop End-to-End MLOps Platforms and Tools in 2024![Grig Duta](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaFebruary 15, 2024](https://www.qwak.com/post/top-mlops-end-to-end)\n[![What is Model Registry in Machine Learning? The Ultimate Guide in 2024](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65c0b2a287e4c83badb95873_What%20is%20Model%20Registry%20in%20Machine%20Learning_%20The%20Ultimate%20Guide%20in%202025.webp)MLOpsWhat is Model Registry in Machine Learning? The Ultimate Guide in 2024![Hudson Buzby](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d4a5b13ae20858614e0fcd_Hudson.png)Hudson BuzbyFebruary 6, 2024](https://www.qwak.com/post/what-is-model-registry)\n[![A Recap of Key Strategies for Mastering MLOps and ML Engineering in 2024](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d491bfebef83c3f5830f25_webinarrecapbanner.webp)MLOpsA Recap of Key Strategies for Mastering MLOps and ML Engineering in 2024![Yuval Fernbach](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6592d4e7b4985a7582bceec7_Yuval.png)Yuval FernbachFebruary 1, 2024](https://www.qwak.com/post/mastermlops-webinar-recap)\n[![How to Deploy Machine Learning Models in Production](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6568888b99b1e2bced86ccec_what%20does%20it%20take%20to%20deploy%20ML%20models%20in%20production.webp)MLOpsHow to Deploy Machine Learning Models in Production![Pavel Klushin](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65756649b900dfa46cf52d98_Pavel%20Klushin.webp)Pavel KlushinJanuary 29, 2024](https://www.qwak.com/post/what-does-it-take-to-deploy-ml-models-in-production)\n[![Mastering MLOps in 2024: A Comprehensive Guide to MLOps, Its Integration, and Qwak's Unified Platform](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/656889468422eda7e036c061_Mastering%20MLOps%20in%202024.webp)MLOpsMastering MLOps in 2024: A Comprehensive Guide to MLOps, Its Integration, and Qwak's Unified Platform![Grig Duta](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaJanuary 22, 2024](https://www.qwak.com/post/the-ultimate-guide-to-mlops-tools-in-2024)\n[![5 Best Open Source Tools to Build End-to-End MLOps Pipeline in 2024](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/656866a7aabad96c260a3ac3_Building%20an%20End-to-End%20MLOps%20Pipeline%20with%20Open-Source%20Tools.webp)MLOps5 Best Open Source Tools to Build End-to-End MLOps Pipeline in 2024![Grig Duta](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaJanuary 15, 2024](https://www.qwak.com/post/mlops-pipeline-with-open-source-tools)\n[Next](https://www.qwak.com/blog/?f0e42b3e_page=2)\n[![Mastering LLM Gateway: A Developer\u2019s Guide to AI Model Interfacing](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/667a8b5114958797799e730a_Mastering%20LLM%20Gateway_%20A%20Developer%E2%80%99s%20Guide%20to%20AI%20Model%20Interfacing.webp)LLMopsMastering LLM Gateway: A Developer\u2019s Guide to AI Model Interfacing![Grig Duta](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaJune 27, 2024](https://www.qwak.com/post/llm-gateway)\n[![Breaking Down the Cost of Large Language Models](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/667a7ac93d4e456261f7f5ef_breaking%20down%20the%20costs%20of%20llms.webp)LLMopsBreaking Down the Cost of Large Language Models![Hudson Buzby](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d4a5b13ae20858614e0fcd_Hudson.png)Hudson BuzbyJune 25, 2024](https://www.qwak.com/post/llm-cost)\n[![Building LLM Applications for Production - Key Strategies & Best Practices](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/66793b9702cd433542c83809_Building%20LLM%20Applications%20for%20Production%20-%20Key%20Strategies%20%26%20Best%20Practices.webp)LLMopsBuilding LLM Applications for Production - Key Strategies & Best Practices![Pavel Klushin](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65756649b900dfa46cf52d98_Pavel%20Klushin.webp)Pavel KlushinJune 24, 2024](https://www.qwak.com/post/building-llm-applications-for-production)\n[![Mastering LLM Monitoring and Observability: A Comprehensive Guide for 2024](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/665382467b7a962274133b11_Mastering%20LLM%20Monitoring%20and%20Observability_%20A%20Comprehensive%20Guide%20for%202025.png)LLMopsMastering LLM Monitoring and Observability: A Comprehensive Guide for 2024![Hudson Buzby](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d4a5b13ae20858614e0fcd_Hudson.png)Hudson BuzbyMay 27, 2024](https://www.qwak.com/post/llm-monitoring-and-observability)\n[![Generative AI at Scale with Enhanced Data Capabilities](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6648a3ddba35c520931e09ea_GenAI%20Qwak%20Snowflake%20data%20sources.png)LLMopsGenerative AI at Scale with Enhanced Data Capabilities![Guy Eshet](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/673b2597b8c150ffe6776c2c_Guy%20Eshet.png)Guy EshetMay 18, 2024](https://www.qwak.com/post/generative-ai-with-enhanced-data)\n[![What is Prompt Management for LLM Applications? Tools, Techniques and Best Practices](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6649b62bd901dbd0eea015d0_prompt%20management.png)LLMopsWhat is Prompt Management for LLM Applications? Tools, Techniques and Best Practices![Grig Duta](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaMay 16, 2024](https://www.qwak.com/post/prompt-management)\n[![Integrating Vector Databases with LLMs: A Hands-On Guide](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/656889607dc7d69766809ced_LLM%E2%80%99s%20%E2%9D%A4%EF%B8%8F%20Vector%20Databases%20_%20A%20Hands-On%20Guide.webp)LLMopsIntegrating Vector Databases with LLMs: A Hands-On Guide![Ran Romano](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6592d4eeaf078b1674bb1f3d_Ran.png)Ran RomanoFebruary 20, 2024](https://www.qwak.com/post/utilizing-llms-with-embedding-stores)\n[![How to Fine-Tune LLMs on Custom Datasets](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6568897a6e0348ed1f770b16_How%20to%20Fine-Tune%20LLMs%20on%20Custom%20Datasets.webp)LLMopsHow to Fine-Tune LLMs on Custom Datasets![Pavel Klushin](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65756649b900dfa46cf52d98_Pavel%20Klushin.webp)Pavel KlushinFebruary 15, 2024](https://www.qwak.com/post/fine-tune-llms-on-your-data)\n[![What is Vector Search? Building a Production-Grade Vector Search System](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65688abdbabca220e6a1b99f_Building%20Production-Grade%20Vector%20Search.webp)LLMopsWhat is Vector Search? Building a Production-Grade Vector Search System![Hudson Buzby](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d4a5b13ae20858614e0fcd_Hudson.png)Hudson BuzbyFebruary 5, 2024](https://www.qwak.com/post/production-grade-vector-search)\n[Next](https://www.qwak.com/blog/?3de74e16_page=2)\n[![Uncovering the Power of Real-Time Feature Engineering in Machine Learning](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/66e84e71f825fc090c4d54b9_real%20time%20Feature%20engineering%20Sharing%20image.webp)Feature StoreUncovering the Power of Real-Time Feature Engineering in Machine Learning![Grig Duta](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaSeptember 16, 2024](https://www.qwak.com/post/real-time-feature-engineering)\n[![Simplifying Machine Learning with the Qwak Feature Store](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/66b351d098d31ae6b411754a_new%20Feature%20Store%20Webinar%20Sharing%20image.webp)Feature StoreSimplifying Machine Learning with the Qwak Feature Store![Hudson Buzby](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d4a5b13ae20858614e0fcd_Hudson.png)Hudson BuzbyAugust 6, 2024](https://www.qwak.com/post/leverage-feature-store)\n[![Data Engineering vs Feature Engineering: What Every Data Scientist Should Know](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/669e515a6b873540e27c7eb1_Feature%20engineering%20vs%20Sharing%20image.webp)Feature StoreData Engineering vs Feature Engineering: What Every Data Scientist Should Know![Hudson Buzby](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d4a5b13ae20858614e0fcd_Hudson.png)Hudson BuzbyJuly 23, 2024](https://www.qwak.com/post/data-engineering-vs-feature-engineering)\n[![Building Robust Feature Engineering Pipelines: From Experimentation to Production](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/668e4df8515ffe2cd1a6dd6f___Building%20Robust%20Feature%20Engineering%20Pipelines.webp)Feature StoreBuilding Robust Feature Engineering Pipelines: From Experimentation to Production![Grig Duta](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaJuly 10, 2024](https://www.qwak.com/post/feature-engineering-pipeline)\n[![What is a Feature Store in ML, and Do I Need One?](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/657157057b1fbe482bfb7a06_feature_store.webp)Feature StoreWhat is a Feature Store in ML, and Do I Need One?![Grig Duta](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaJune 2, 2024](https://www.qwak.com/post/what-is-a-feature-store-in-ml)\n[![Building a Streaming Aggregation Data Pipeline for ML - Challenges & Solutions](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/66585a845d1b2fd689232a11_Building%20a%20streaming%20aggregation%20data%20pipeline%20for%20ML%20-%20Challenges%20%26%20Solutions.png)Feature StoreBuilding a Streaming Aggregation Data Pipeline for ML - Challenges & Solutions![Tal Gabay](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6658924c13c8a2c177b80328_Tal_color.png)Tal GabayMay 30, 2024](https://www.qwak.com/post/building-streaming-data-pipeline)\n[![Feature Store Benefits: The Advantages of Feature Store in Machine Learning](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65942e35a24b8eb7c3f4fa67_Feature%20Store%20Benefits_%20The%20Advantages%20of%20Feature%20Store%20in%20Machine%20Learning.webp)Feature StoreFeature Store Benefits: The Advantages of Feature Store in Machine Learning![Pavel Klushin](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65756649b900dfa46cf52d98_Pavel%20Klushin.webp)Pavel KlushinJanuary 1, 2024](https://www.qwak.com/post/feature-store-benefits)\n[![Feature Store Architecture and How to Build One](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/656866ba89f9bf514dfbe750_Feature%20Store%20Architecture%20and%20How%20to%20Build%20One.webp)Feature StoreFeature Store Architecture and How to Build One![Grig Duta](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaNovember 1, 2023](https://www.qwak.com/post/feature-store-architecture)\n[![Leveraging Snowflake Data for Machine Learning with Qwak's Feature Store](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6568896cf3321239c6368c31_Leveraging%20Snowflake%20Data%20for%20Machine%20Learning%20with%20Qwak%27s%20Feature%20Store.webp)Feature StoreLeveraging Snowflake Data for Machine Learning with Qwak's Feature Store![Grig Duta](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d7531a85bff915b6aaf4b9_Grig.png)Grig DutaSeptember 5, 2023](https://www.qwak.com/post/ml-features-on-snowflake)\n[Next](https://www.qwak.com/blog/?7d77d0b6_page=2)\n[![Introducing JFrog ML: Unifying DevOps, DevSecOps and MLOps for a Secure Software Supply Chain Platform](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/66e02a93111d106f3f24ba7f_1200x630%20-%20introducing%20jfrog%20ml%20-%20blog.png)AnnouncementsIntroducing JFrog ML: Unifying DevOps, DevSecOps and MLOps for a Secure Software Supply Chain Platform![Guy Eshet](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/673b2597b8c150ffe6776c2c_Guy%20Eshet.png)Guy EshetSeptember 10, 2024](https://www.qwak.com/post/jfrog-ml)\n[![JFrog & Qwak: Accelerating Models Into Production \u2013 The DevOps Way](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6686b5141b04553fff8b7162_jfrog_qwak_banner.png)AnnouncementsJFrog & Qwak: Accelerating Models Into Production \u2013 The DevOps Way![Yuval Fernbach](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6592d4e7b4985a7582bceec7_Yuval.png)Yuval FernbachJuly 3, 2024](https://www.qwak.com/post/jfrog-qwak-accelerating-models-into-production)\n[![Introducing Prompt Management](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/666811f25c3d03266e4d361d_prompts_sharing.jpg)AnnouncementsIntroducing Prompt Management![Guy Eshet](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/673b2597b8c150ffe6776c2c_Guy%20Eshet.png)Guy EshetJune 17, 2024](https://www.qwak.com/post/introducing-prompts)\n[![Qwak Announces Native GCP Support with Seamless Multi-Cloud Capabilities](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/660e7629e2dbdf782397e95f_partner%20google.png)AnnouncementsQwak Announces Native GCP Support with Seamless Multi-Cloud Capabilities![Amit Ashbel](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6575e142c741f24f58e438bb_Amit%20Ashbel.webp)Amit AshbelApril 4, 2024](https://www.qwak.com/post/qwak-gcp-support-seamless-multi-cloud)\n[![Revolutionizing ML Deployment: The JFrog-Qwak Partnership](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65df500c688ddcd8094bc735_JFROG.png)AnnouncementsRevolutionizing ML Deployment: The JFrog-Qwak Partnership![Amit Ashbel](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6575e142c741f24f58e438bb_Amit%20Ashbel.webp)Amit AshbelFebruary 28, 2024](https://www.qwak.com/post/revolutionizing-ml-deployment-the-jfrog-qwak-partnership)\n[![Qwak's Feature Store & Analytics Lake are now fully compatible with Snowflake](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65d73f0357145f1a7b941c40_Snowflake%20feature%20store%20announcement.webp)AnnouncementsQwak's Feature Store & Analytics Lake are now fully compatible with Snowflake![Ran Romano](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6592d4eeaf078b1674bb1f3d_Ran.png)Ran RomanoFebruary 22, 2024](https://www.qwak.com/post/qwak-feature-store-analytics-lake-snowflake)\n[![Model Build API, Athena Data Source Support & more - Product Updates 01/2024](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/659e6a42101ffbe8ab41ab69_Qwak%20product%20updates%20q1%202024.webp)AnnouncementsModel Build API, Athena Data Source Support & more - Product Updates 01/2024![Guy Eshet](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/673b2597b8c150ffe6776c2c_Guy%20Eshet.png)Guy EshetJanuary 16, 2024](https://www.qwak.com/post/athena-data-source-support)\n[![Simplify Vector Storage, Retrieval, & Embedding Operations](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/65688aaf452b44dc1ed98122_Simplify%20Vector%20Storage%2C%20Retrieval%2C%20%26%20Embedding%20Operations.webp)AnnouncementsSimplify Vector Storage, Retrieval, & Embedding Operations![Guy Eshet](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/673b2597b8c150ffe6776c2c_Guy%20Eshet.png)Guy EshetSeptember 20, 2023](https://www.qwak.com/post/qwak-vector-store)\n[![Qwak Named One of 20 Rising Stars in Forbes' Cloud 100](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/656889119621435f85a59212_Qwak%20Named%20One%20of%2020%20Rising%20Stars%20in%20Forbes%27%20Cloud%20100.webp)AnnouncementsQwak Named One of 20 Rising Stars in Forbes' Cloud 100![Amit Ashbel](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ac/6575e142c741f24f58e438bb_Amit%20Ashbel.webp)Amit AshbelAugust 8, 2023](https://www.qwak.com/post/qwak-forbes-cloud100)\n[Next](https://www.qwak.com/blog/?3c68c4e6_page=2)\n## Get started with JFrog ML today.\nAccess JFrog ML to see how the best ML engineering and data science teams deploy models in production. Say goodbye to complex MLOps and start delivering today.\n[Book Demo](https://www.qwak.com/demo)[Learn More](https://www.qwak.com/blog)\n[![Qwak Logo Footer](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ab/6720b199b1e9016806e44919_JFrogML_Oct25.svg)](https://www.qwak.com/)\nYour AI Platform. Built to Scale.\n\u00a9 2024 JFrog ML. All rights reserved.\n[Terms & Conditions](https://www.qwak.com/terms-of-use)[Privacy Notice](https://jfrog.com/privacy-policy/)[Cookie Notice](https://jfrog.com/jfrog-cookies-policy/)[Cookies Settings](https://www.qwak.com/blog/)![Qwak MLOps platform has SOC2 compliance](https://cdn.prod.website-files.com/64b3ee21cac9398c75e5d3ab/655d250e320f966cc4d6d595_qwak-soc2.svg)\nPlatform\n[MLOps](https://www.qwak.com/products/mlops)[LLMOps](https://www.qwak.com/products/llmops)[Feature Store](https://www.qwak.com/products/feature-store)\nResources\n[Blog](https://www.qwak.com/blog)[Case Studies](https://www.qwak.com/case-studies)[Documentation](https://docs.qwak.com/docs)\nStay updated\nFollow us\n[](https://www.linkedin.com/company/qwakai/)[](https://twitter.com/Qwak_ai)[](https://github.com/qwak-ai/qwak-examples/tree/main)[](https://mlengineering.medium.com/)\n![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=5eb86fc8-89ac-40d4-a2ac-bce6252424bb&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=b93745ea-10f9-41f8-a563-d94b5cd971e1&tw_document_href=https%3A%2F%2Fwww.qwak.com%2Fblog&tw_iframe_status=0&txn_id=oe76x&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=5eb86fc8-89ac-40d4-a2ac-bce6252424bb&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=b93745ea-10f9-41f8-a563-d94b5cd971e1&tw_document_href=https%3A%2F%2Fwww.qwak.com%2Fblog&tw_iframe_status=0&txn_id=oe76x&type=javascript&version=2.3.31)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nQwak has joined JFrog, enhancing its ML platform. Key insights include the integration of DevOps, MLOps, and security for streamlined AI adoption. The latest reports cover AI trends, feature engineering, and LLM applications, emphasizing real-time capabilities and cost management in ML.\n\n**Tags:** Generative AI, LLMs, MLOps, DevOps\n```"
  },
  {
    "instruction": "[ ![dot](https://redis.io/wp-content/uploads/2022/12/Ellipse-47.svg) Stop testing, start deploying your AI apps. See how with MIT Technology Review\u2019s latest research. Download now ](https://redis.io/resources/mit-report-genai/)\n[ ![White Redis Logo](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120) ](https://redis.io/) [Back](javascript:void\\(0\\);)\n  * [Redis for AI](https://redis.io/redis-for-ai/)\n  * Products\n    * Products\n      * [ Redis CloudFully managed and integrated with Google Cloud, Azure, and AWS.](https://redis.io/cloud/)\n      * [ Redis for AIBuild the fastest, most reliable GenAI apps with our advanced vector database.](https://redis.io/redis-for-ai)\n      * [ Redis SoftwareSelf-managed software with enterprise-grade compliance and reliability.](https://redis.io/software/)\n      * [ Redis Data Integration (RDI)Synchronize data in near-real time to make data fast\u2014without writing code.](https://redis.io/data-integration/)\n      * [ Community EditionIn-memory database for caching & streaming.](https://redis.io/community-edition/)\n    * Tools\n      * [ Redis Insight](https://redis.io/insight/)\n      * [ Clients and connectors](https://redis.io/clients/)\n    * Get Redis\n      * [ Downloads](https://redis.io/downloads/)\n  * Resources\n    * CONNECT\n      * [ Customers Stories](https://redis.io/customers/)\n      * [ Partners](https://redis.io/partners/)\n      * [ Support](https://redis.io/support/)\n      * [ Community](https://redis.io/community/)\n      * [ Events & Webinars](https://redis.io/events/)\n      * [ Professional Services](https://redis.io/services/professional-services/)\n    * Learn\n      * [ Docs](https://redis.io/docs/)\n      * [ Commands](https://redis.io/commands/)\n      * [ Quick starts](https://redis.io/docs/get-started/)\n      * [ Tutorials](https://redis.io/learn/)\n      * [ University](https://redis.io/university/)\n      * [ Knowledge base](https://redis.io/kb/)\n      * [ Resources](https://redis.io/resources/)\n      * [ Blog](https://redis.io/blog/)\n    * LATEST\n      * [ Releases](https://redis.io/release/)\n      * [ News & updates](https://redis.io/company/news/)\n    * See how it works\n      * [ Visit Demo Center](https://redis.io/demo-center/)\n  * [Docs](https://redis.io/docs/)\n  * [Pricing](https://redis.io/pricing/)\n\n\n[Try Redis](https://redis.io/try-free/)\n[ Book a meeting ](https://redis.io/meeting/)\n[Login](https://app.redislabs.com/)\n[ ![search](https://redis.io/wp-content/themes/redislabs-glide/assets/src/images/search-icon.svg) ](javascript:void\\(0\\);)\n[Login](https://cloud.redis.io)\n[ Book a meeting ](https://redis.io/meeting/)\n[Try Redis](https://redis.io/try-free/)\n[ ](https://redis.io/blog/benchmarking-results-for-vector-databases/)\n![close](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/close-icon.svg)\n![](https://redis.io/wp-content/uploads/2023/11/UI-Flexible.svg?auto=webp&quality=85,75&width=80)\nSearch\nSearch: ![](https://redis.io/wp-content/themes/redislabs-glide/assets/src/images/search-icon.svg)\n# Benchmarking results for vector databases\nJune 20, 2024 \n[![LinkedIn-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/LinkedIn.svg)](https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fredis.io%2Fblog%2Fbenchmarking-results-for-vector-databases%2F) [![facebook-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/facebook.svg)](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fredis.io%2Fblog%2Fbenchmarking-results-for-vector-databases%2F) [![X-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/X.svg)](https://twitter.com/intent/tweet?text=Benchmarking results for vector databases&url=https://redis.io/blog/benchmarking-results-for-vector-databases/)\n![share-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/share-icon.svg)share\nShare \n![close-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/close-icon.svg)\n[![LinkedIn-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/LinkedIn.svg)Share to LinkedIn](https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fredis.io%2Fblog%2Fbenchmarking-results-for-vector-databases%2F)\n[![facebook-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/facebook.svg)Share to Facebook](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fredis.io%2Fblog%2Fbenchmarking-results-for-vector-databases%2F)\n[![X-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/X.svg)Share to X](https://twitter.com/intent/tweet?text=Benchmarking results for vector databases&url=https://redis.io/blog/benchmarking-results-for-vector-databases/)\n![Filipe Olviera, Performance Engineer, Redis Labs.](https://redis.io/wp-content/uploads/2020/06/Filipe-Olviera-1-100x100.jpg?auto=webp&quality=85,75&width=80)\n[Filipe Oliveira](https://redis.io/blog/author/filipeolviera)\n![adriano headshot](https://redis.io/wp-content/uploads/2024/06/adriano-100x100.jpg?auto=webp&quality=85,75&width=80)\n[Adriano Amaral](https://redis.io/blog/author/aamaral)\n![](https://redis.io/wp-content/uploads/2024/06/dvirdukhan-100x100.jpeg?auto=webp&quality=85,75&width=80)\n[Dvir Dukhan](https://redis.io/blog/author/dvirvolk)\n![](https://redis.io/wp-content/uploads/2024/05/jimallenwallace-100x100.jpeg)\n[Jim Allen Wallace](https://redis.io/blog/author/jim-allenwallaceredis-com)\nAt Redis, we\u2019re fast. To show how fast we are, we benchmarked the top providers in the market for vector databases using our new Redis Query Engine, now GA for Redis Software. This feature enhances our engine enabling concurrent access to the index, improving throughput for Redis queries, search, and vector database workloads. This blog post shows our benchmark results, explains the challenges with increasing query throughput, and how we overcame those with our new Redis Query Engine. This blog has three main parts:\n  1. See a detailed comparison of benchmark results across vector databases \n  2. Delves into the architectural design of the new Redis Query Engine\n  3. Read an in-depth look of our testing methodology\n\n\nLet\u2019s start with what matters most, how fast Redis is.\n## **Redis is the fastest on competitive vector benchmarks.**\nNext to ingestion and index creation time, we benchmarked two key metrics: throughput and latency (see below the details about the metrics and principles) among 7 vector database players. Throughput indicates a system\u2019s capability to process numerous queries or large datasets in a short amount of time, while latency measures how fast individual similarity searches return results. \nTo ensure we cover both, we\u2019ve proceeded with two benchmarks, one multi-client benchmark focusing on throughput, and another single-client and under load (multi-client) benchmark focusing on latency. All the results can be filtered in the graphs below and all the details of how we conduct the tests can be explored in the blog. For Redis, prioritizing throughput and latency aligns with our core philosophy of delivering exceptional speed and reliability.Our tests show that Redis is faster for vector database workloads compared to any other vector database we tested, at recall >= 0.98. Redis has 62% more throughput than the second-ranked database for lower-dimensional datasets (deep-image-96-angular) and has 21% more throughput for high-dimensional datasets (dbpedia-openai-1M-angular). \n![](https://redis.io/wp-content/uploads/2024/06/Redis_Blog_ScalableSearchAnnouncement_InfographicIllustration_1-1-1.png?format=pjpg&auto=webp&quality=1&width=2042)\nCaveat: MongoDB tests only provided results when using a smaller recall level for the gist-960-euclidean dataset. The results for this dataset considered a median of the results for recall between 0.82 and 0.98. For all other datasets we\u2019re considering recall >=0.98\n### **Our results show we\u2019re faster than any pure vector database providers (Qdrant, Milvus, Weviate).**\nRedis outperformed other pure vector database providers in querying throughput and latency times. \n**Querying:** Redis achieved up to 3.4 times higher queries per second (QPS) than Qdrant, 3.3 times higher QPS than Milvus, and 1.7 times higher QPS than Weaviate for the same recall levels. On latency, considered here as an average response for the multi-client test, Redis achieved up to 4 times less latency than Qdrant, 4.67 times than Milvus, and 1.71 times faster than Weaviate for the same recall levels. On latency, considered here as an average response under load (multi-client), Redis achieved up to 4 times less latency than Qdrant, 4.67 times than Milvus, and 1.71 times faster than Weaviate.\n**Ingestion and indexing:** Qdrant is the fastest due to its multiple segments index design, but Redis excels in fast querying. Redis showed up to 2.8 times lower indexing time than Milvus and up to 3.2 times lower indexing time than Weaviate.\nThis section provides a comprehensive comparison between Redis 7.4 and other industry providers that exclusively have vector capabilities such as Milvus 2.4.1, Qdrant 1.7.4, and Weaviate 1.25.1.\nIn the graph below you can analyze all the results across RPS (request per second), latency (both single-client and multi-client, under load), P95 latency, and index time. All are measured across the different selected datasets.\n## Pure vector database providers\nDataset: glove-100-angulargist-960-euclideandeep-image-96-angulardbpedia-openai-1M-1536-angular\nClient count: 1001\nPlot values:  RPS  |  Avg. Latency (s)  |  p95 Latency (s)  |  Index time (s) \nEngine| setup_name| Dataset| RPS| parallel| Precision| P50 (s)| P95 (s)| P99 (s)| Avg. Latency (s)| Upload Time (s)| Upload + Index Time (s)| engine_params  \n---|---|---|---|---|---|---|---|---|---|---|---|---  \nRedis 7.4| redis-m-64-ef_construction-512| glove-100-angular| 3186.63| 100| 0.82| 0.027| 0.053| 0.059| 0.029| 671.90| 671.90| {\"search_params\":{\"ef\":64}}  \nWeaviate 1.25.1| weaviate-m-64-ef_construction-512| glove-100-angular| 2179.092| 100| 0.82| n/a| 0.063| 0.14| 0.044| 1372.011| 1372.011| {}  \nQdrant 1.7.4| qdrant-m-32-ef_construction-256| glove-100-angular| 1638.61| 100| 0.83| 0.061| 0.068| 0.079| 0.057| 36.81| 448.18| {\"search_params\":{\"hnsw_ef\":64}}  \nMilvus 2.4.1| milvus-m-64-ef_construction-256| glove-100-angular| 1316.23| 100| 0.93| n/a| 0.079| 0.083| 0.071| 634.34| 1207.20| {\"params\":{\"ef\":256}}  \nThere were some cases in single client benchmarks in which the performance of Redis and the competitors was at the same level. Weaviate and Milvus showcased operational problems on the cloud setups; findings are fully described in the appendix.\n### **Redis is faster across all data sizes than general-purpose databases (PostgreSQL, MongoDB, OpenSearch).**\nIn the benchmarks for querying performance in general-purpose databases with vector similarity support, Redis significantly outperformed competitors. \n**Querying:** Redis achieved up to 9.5 times higher queries per second (QPS) and up to 9.7 times lower latencies than Amazon Aurora PostgreSQL v16.1 with pgvector 0.5.1 for the same recall. Against MongoDB Atlas v7.0.8 with Atlas Search, Redis demonstrated up to 11 times higher QPS and up to 14.2 times lower latencies. Against Amazon OpenSearch, Redis demonstrated up to 53 times higher QPS and up to 53 times lower latencies. \n**Ingestion and indexing:** Redis showed a substantial advantage over Amazon Aurora PostgreSQL v16.1 with pgvector 0.5.1, with indexing times ranging from 5.5 to 19 times lower.\nThis section is a comprehensive comparison between Redis 7.4 and Amazon Aurora PostgreSQL v16.1 with pgvector 0.5.1, as well as MongoDB Atlas v7.0.8 with Atlas Search, and Amazon OpenSearch 2.11, offering valuable insights into the performance of vector similarity searches in general-purpose DB cloud environments.\nIn the graph below you can analyze all the results across RPS (request per second), latency (both single-client and multi-client, under load) and P95 latency, and index time. All are measured across the different selected datasets.\n## General purpose benchmarks\nDataset: glove-100-angulargist-960-euclideandeep-image-96-angulardbpedia-openai-1M-1536-angular\nClient count: 1001\nPlot values:  RPS  |  Avg. Latency (s)  |  p95 Latency (s)  |  Index time (s) \nEngine| setup_name| Dataset| RPS| parallel| Precision| P50 (s)| P95 (s)| P99 (s)| Avg. Latency (s)| Upload Time (s)| Upload + Index Time (s)| engine_params  \n---|---|---|---|---|---|---|---|---|---|---|---|---  \nRedis 7.4| redis-m-32-ef_construction-256| glove-100-angular| 3512.86| 100| 0.76| 0.021| 0.033| 0.040| 0.020| 250.070| 250.070| {\"search_params\":{\"ef\":64}}  \nAmazon Aurora (PostgreSQL v16.1 and pgvector 0.5.1)| pgvector-m-32-ef_construction-128| glove-100-angular| 1211.88| 100| 0.82| 0.076| 0.16| 0.20| 0.081| 2050.81| 2050.81| {\"search_params\":{\"hnsw_ef\":128}}  \nAmazon OpenSearch 2.11 (lucene 9.7.0)| opensearch-m-16-ef_construction-128| glove-100-angular| 108.80| 100| 0.90| n/a| 0.99| 1.17| 0.91| 425.19| 429.33| {}  \nApart from the performance advantages showcased above, some general-purpose databases presented vector search limitations related to lack of precision and the possibility of index configuration, fully described in the appendix.\n### **Redis is faster across all data sizes vs. cloud service providers (MemoryDB and MemoryStore).**\nCompared to other Redis imitators, such as Amazon MemoryDB and Google Cloud MemoryStore for Redis, Redis demonstrates a significant performance advantage. This indicates that Redis and its enterprise implementations are optimized for performance, outpacing other providers that copied Redis. \n**Querying:** Against Amazon MemoryDB, Redis achieved up to 3.9 times higher queries per second (QPS) and up to 4.1 times lower latencies for the same recall. Compared to GCP MemoryStore for Redis v7.2, Redis demonstrated up to 2.5 times higher QPS and up to 4.8 times lower latencies. \n**Ingestion and indexing:** Redis had an advantage over Amazon MemoryDB with indexing times ranging from 1.39 to 3.89 times lower. Against GCP MemoryStore for Redis v7.2, Redis showed an even greater indexing advantage, with times ranging from 4.9 to 10.34 times lower.\nIn the graph below you can analyze all the results across: RPS (request per second), latency (both single-client and multi-client under load), P95 latency, and index time. All are measured across the different selected datasets.\n## Cloud service providers\nDataset: glove-100-angulargist-960-euclideandeep-image-96-angulardbpedia-openai-1M-1536-angular\nClient count: 1001\nPlot values:  RPS  |  Avg. Latency (s)  |  p95 Latency (s)  |  Index time (s) \nEngine| setup_name| Dataset| RPS| parallel| Precision| P50 (s)| P95 (s)| P99 (s)| Avg. Latency (s)| Upload Time (s)| Upload + Index Time (s)| engine_params  \n---|---|---|---|---|---|---|---|---|---|---|---|---  \nRedis 7.4| redis-m-64-ef_construction-512| glove-100-angular| 3186.63| 100| 0.82| 0.027| 0.053| 0.059| 0.029| 671.90| 671.90| {\"search_params\":{\"ef\":64}}  \nGCP MemoryStore for Redis v7.2| redis-m-64-ef_construction-512| glove-100-angular| 2660.59| 100| 0.82| 0.032| 0.074| 0.10| 0.035| 5930.26| 5930.26| {\"search_params\":{\"ef\":64}}  \nAmazon MemoryDB for Redis v7.1| redis-m-32-ef_construction-128| glove-100-angular| 669.96| 100| 0.84| 0.13| 0.39| 0.45| 0.14| 386.88| 387.97| {\"search_params\":{\"ef\":256}}  \n## **The fastest vector database available leverages the enhanced Redis Query Engine.**\nTo achieve the showcased results, we introduce a new enhancement to enable queries to concurrently access the index. This section elaborates on the enhancements and how our engineering team overcame these challenges.\n### **Single-threading is not optimal for all searches.**\nRedis has [a proven single-thread](https://redis.io/blog/redis-architecture-13-years-later/) architecture and we continually seek improvements to take Redis even further. To do so, we needed to overcome a few constraints of the existing design. First, Redis\u2019 way of scaling, both read and write commands (Redis operations), is based on the assumption that most of the commands are short, usually O(1) time complexity for each command, and are independent of each other. Thus, sharding the data with Redis Cluster into independent Redis shards (or scaling out) causes fanning out multiple requests to multiple shards, so we get immediate load balancing and evenly distribute commands between the shards. That isn\u2019t true for all Redis queries with high expressivity (multiple query terms and conditions) or with any data manipulation (sorting, grouping, transformations). Second, long-running queries on a single thread cause congestion, increase the overall command latency, and drop the Redis server\u2019s overall throughput. Searching in data, leveraging in an inverted index, is one such long-running query, for instance.\nSearch is not a O(1) time complexity command. Searches usually combine multiple scans of indexes to comply with the several query predicates. Those scans are usually done in logarithmic time complexity O(log(n)) where _n_ is the amount of data points mapped by the index. Having multiple of those, combining their results and aggregating them, is considerably heavier with respect to computing, compared to typical Redis operations for GET, SET, HSET, etc. This is counter to our assumption that commands are simple and have a short runtime.\n![](https://redis.io/wp-content/uploads/2024/06/Redis_Blog_ScalableSearchEngineering_TechnicalIllustration_1.png?format=pjpg&auto=webp&quality=1&width=2000)\n### **Horizontal scaling isn\u2019t always sufficient to scale throughput.**\nScaling out with sublinear time complexity doesn\u2019t help reduce the single client latency which is required to increase the throughput in a single-threaded architecture. Even though sharding can distribute data across multiple Redis processes (and nodes), the total query time doesn\u2019t significantly decrease as more shards, and hence more Redis processes, are added. We have two reasons for that. \n  1. The time complexity of index access is often O(log(n)). So for example, splitting the dataset into two shards, and halving the data set into two shards doesn\u2019t reduce the latency in half. Mathematically, O(log(n/2)) is the equivalent of O(log(n)). \n  2. We also have distribution costs, including the overhead of communicating across shards and post-processing sharded results. This scatter and gather cost becomes highly evident when combined with higher concurrency (at high throughput) in a multi-client scenario.\n\n\n![](https://redis.io/wp-content/uploads/2024/11/Redis_Blog_ScalableSearchEngineering_TechnicalIllustration_2_20241031.png?format=pjpg&auto=webp&quality=1&width=2000)\nFor example, searching for a similar vector today, even with the state-of-art algorithms is compute-heavy since it compares the queried vector to each of the candidate vectors the index provides. This comparison is not a O(1) comparison but is O(d) where d is the vector dimension. In simple terms, each computation is comparing the entire two vectors. This is computer-heavy. When running on the main thread, this holds the Redis\u2019 main thread longer than regular Redis workloads and even other search cases.\n### **Multi-threading improves search speed at scale.**\nScaling search efficiently requires combining the distribution of data loads horizontally (going out) and multi-threading vertically, enabling concurrency on accessing the index (going up). The image below illustrates the architecture of a single shard.\n![](https://redis.io/wp-content/uploads/2024/06/Redis_Blog_ScalableSearchEngineering_TechnicalIllustration_3.png?format=pjpg&auto=webp&quality=1&width=2000)\nMultiple queries are being executed, each on a separate thread. We incorporated the simple but famous [producer-consumer pattern](https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem). 1. The query context (planning) is prepared on the main thread and queued on a shared queue. 2. From here, threads consume the queue and execute the query pipeline, concurrently to other threads. This allows us to execute multiple concurrent queries while keeping the main thread alive to handle more incoming requests, such as other Redis commands, or prepare and queue additional queries. 3. Once finished, the query results are sent back to the main thread.\n### **Redis Query Engine scales 16X higher than before.**\nTo benchmark our Redis Query Engine, we\u2019ve tested Redis with both full-text and vector search queries. Specifically for the vector search we\u2019ve picked the gist-960 dataset with 1M vectors, with each vector having 960 dimensions, and compared an old setup with a single shard to a new setup using a thread configuration of 6, 12, and 24, while varying the M (maximum number of connections per node) and EF (the size of the dynamic candidate list during search) configurations of the HNSW index.\nThe performance improvements were consistent, demonstrating that for every 2X increase in query throughput, a 3X increase in threads was required, indicating efficient resource utilization across all configurations. We tested the [FLAT](https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/vectors/#flat) (Brute-Force) algorithm and the [HNSW](https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/vectors/#hnsw) scaling, and our tests confirmed that the vertical scaling speedup ratio (how effectively the added resources increased the achievable throughput) for each scaling factor applied to both algorithms. The graph below illustrates the scalable factor for the vector workload.\n![](https://redis.io/wp-content/uploads/2024/06/Screenshot-2024-06-17-at-10.02.59%E2%80%AFPM.png?format=pjpg&auto=webp&quality=1&width=500)\nBased on these findings, we introduce the \u2018scaling factor\u2019 concept, a ratio to significantly boost query throughput by activating additional threads and vCPUs, thereby enhancing performance. Extending the results also for full-text search use cases, we can confirm that the theoretical scaling factor aligns closely with the empirical results.\n![](https://redis.io/wp-content/uploads/2024/06/Redis_Blog_ScalableSearchEngineering_InfographicIllustration_4.png?format=pjpg&auto=webp&quality=1&width=2043)\n### **We are just getting started.**\nWe continue to improve Redis Query Engine. Sometimes it\u2019s best not to wait for a background thread to pick up a task but to execute the query on a main thread straight away. For short and simple queries, this seems to be the case. Accessing the key space is still a bottleneck since it is required to lock the main thread, while the index has its own optimized data structures outside the keyspace. Sometimes, when a query explicitly requests loading data from the keyspace, projecting data (`RETURN` or `LOAD`), we need to conform to Redis and lock the keyspace (Redis lock mechanism). This also can be improved. \n## **We benchmarked using industry-standard methods.**\nThis section contains all the details about the setup, how we conducted the tests, and the reasons behind the dataset and tooling selection. Let\u2019s start with our benchmark principles.\n### **Principles for effective benchmarking**\nTo ensure that benchmarks are meaningful and actionable, we believe a good benchmark should adhere to the following principles:\n  * **Relevance:** Benchmarks should reflect real-world use cases and workloads. It should capture a wide range of metrics, including average, median, and percentile latencies and report on throughput rates under different conditions, such as varying recall levels and operation mixes among writes and queries.\n  * **Consistency:** Use standardized testing environments and configurations. Tests should be repeated to ensure the results are reliable and reproducible.\n  * **Transparency:** Document the benchmark setup, including hardware, software versions, and configurations. Provide access to benchmark scripts and methodologies.\n  * **Scalability** : Include tests that measure performance at various scales, from small datasets to large, complex workloads. Ensure that the system can maintain performance as data size and load increase.\n  * **Baseline comparisons:** Compare performance against baseline metrics or alternative solutions. Highlight improvements and regressions in subsequent benchmarks.\n\n\nBy adhering to these principles, we try to ensure that our benchmarks provide valuable insights when compared with other solutions, drive continuous improvement in our query engine, and uphold the Redis commitment to provide superior performance.\n### **We tested against the top vector database providers.**\nWe compared Redis against three groups: pure vector database providers, general-purpose databases with vector capabilities, and Redis imitators on cloud service providers.\nWe compared Redis with three segments of vector data providers: pure vector database providers, general-purpose databases with vector capabilities, and other Redis imitator cloud service providers. We found that the speed, extensibility, and enterprise features varied significantly across the three groups. The pure vector databases were better at performance, but worse at scaling. The general-purpose databases were much worse at performance, but better at other integrations. And Redis imitators were significantly slower without the latest improvements in speed. Many users assume that Redis imitators are equally fast, but we want to break that illusion for you, to save you time and money, by comparing them to Redis.Our main findings are summarized below:\nPure Play| General Purpose| Redis imitators providers  \n---|---|---  \nProviders| Qdrant, Weaviate, Milvus| Amazon Aurora ( PostgreSQL w/ pgvector), MongoDB, Amazon OpenSearch| Amazon MemoryDB, Google Cloud MemoryStore for Redis  \nMissing providers| Pinecone*| Couchbase*, ElasticSearch*  \nSummary of findings| Pure players claim they are built in purpose but often lack enterprise features and stability| Well established general-purpose databases do have these enterprise features but lack performance| If you understood that Redis is your solution, consider that the imitators aren\u2019t as performant as Redis  \nWe selected these providers based on inputs from our customers who wanted to see how we compete against them, and how popular they are on [DB Engine](https://db-engines.com/en/ranking_trend/vector+dbms).\n* We also wanted to analyze ElasticSearch, Pinecone, and Couchbase, however, those database providers incorporate a [DeWitt clause](https://en.wikipedia.org/wiki/David_DeWitt) in their service terms, which legally restricts any form of benchmarking. Specifically, this clause forbids not only the publication of benchmark results but also the actual conduct of any performance evaluations on their products. As a result, [ElasticSearch](https://www.elastic.co/de/legal/elastic-cloud-account-terms), [Pinecone](https://www.pinecone.io/user-agreement/), and [Couchbase](https://www.couchbase.com/capella-terms/) customers don\u2019t have any insight into their potential performance or effectiveness, which can be verified independently.\n### We tested each provider using identical hardware.\nWe tested competitors using identical hardware in the cloud services solutions for the client and server side. The configuration parameters were defined to equally allocate the same amount of vCPU and Memory. The table in the appendix summarizes the resource type for each solution tested.\nIt\u2019s important to emphasize that we took extra effort to ensure that all competitors had stable results that matched other sources of benchmark data. For example, Qdrant\u2019s cloud results from our benchmarking versus the one published by [Qdrant](https://qdrant.tech/benchmarks/#single-node-benchmarks) show that our testing is either equal to or sometimes even better than what they published. This gives us extra confidence in the data we\u2019re now sharing. The below image shows our testing for Qdrant versus their self-reported results.\n![](https://redis.io/wp-content/uploads/2024/06/Screenshot-2024-06-17-at-10.04.45%E2%80%AFPM.png?format=pjpg&auto=webp&quality=1&width=500)\nWe simplified the player\u2019s configuration setup to guarantee equal comparison. So we compared benchmarks on single-node and pure k-nearest neighbors (k-NN) queries. The benchmark evaluates the performance of different engines under the same resource limit and shows relative performance metrics. \nThe metrics include ingestion and indexing times, and query performance (throughput and latency) on a single node, with various datasets of different vector dimensionality and distance functions. Specifically for \u200cquery performance, we analyzed both:\n  * The per-operation average and P95 latencies (including RTT) by doing a single-client benchmark.\n  * The maximum achievable throughput per each of the solutions, by running 100 concurrent clients executing requests in parallel, and measuring how many requests it can handle per second.\n\n\n### **Our testing consisted of ingestion and search workloads.**\nOur testing delves into a variety of performance metrics, offering a holistic view of how Redis stands in the vector database landscape. These tests were conducted using two main operations:\n  * **Ingestion time** : On ingestion and indexing using the HNSW algorithm considering the following parameters (configuration details can be found [here](https://github.com/redis-performance/vector-db-benchmark/blob/master/experiments/configurations/redis-single-node.json)): \n    * EF_CONSTRUCTION \u2013 Number of maximum allowed potential outgoing edges candidates for each node in the graph, during the graph building.\n    * M \u2013 Number of maximum allowed outgoing edges for each node in the graph in each layer. On layer zero the maximal number of outgoing edges will be 2M. \n  * **Query throughput and latency** : On pure k-nearest neighbors (k-NN) we measure the query throughput in requests per second (RPS) and the average client-side latency including round trip time (RTT), based on precision, by varying parameters M and EF at index creation time as explained above, and also varying the EF at runtime: \n    * EF_RUNTIME \u2013 The number of maximum top candidates to hold during the KNN search. Higher values of EF_RUNTIME will lead to more accurate results at the expense of a longer runtime. \n\n\nFor all the benchmarked solutions, we\u2019ve varied M between 16, 32, and 64 outgoing edges, and EF between 64, 128, 256, and 512 both for the construction and runtime variations. To ensure reproducible results, each configuration was run 3 times, and the best results were chosen. [Read more](https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/vectors/) about the HNSW configuration parameters and query.\nPrecision measurement was done by comparing how accurate the returned results were when compared to the ground truth previously computed for each query, and available as part of each dataset.\nWe focused on measuring ingestion and query performance separately. This method allows for clear, precise insights and makes it easier to compare our results with other benchmarks from competitors, who typically also test insertion and query independently. We plan to address mixed workloads with updates, deletes, and regular searches in a follow-up article to provide a comprehensive view. \nGiven the varying levels of filtering support among the different benchmarked solutions, we are not delving into the complexities of filtered search benchmarks at this time. Filtering adds a significant layer of complexity and it\u2019s handled differently by each of the solutions. We will not dive into it here. \nFurthermore, we\u2019re actively extending [[see PRs here]](https://github.com/qdrant/vector-db-benchmark/pulls?q=is%3Apr+author%3Afilipecosta90+) the benchmark utility to cover memory usage breakdown per solution (already with a working example for any Redis-compatible solutions), as well as allowing for benchmarks of billions of vectors. \n### **We used datasets representing various use cases.**\nWe selected the following datasets to cover a wide range of dimensionalities and distance functions. This approach ensures that we can deliver valuable insights for various use cases, from simple image retrieval to sophisticated text embeddings in large-scale AI applications.\n**Datasets**| **Number of vectors**| **Vector dimensionality**| **Distance function**| **Description**  \n---|---|---|---|---  \n[gist-960-euclidean](http://corpus-texmex.irisa.fr/)| 1,000,000| 960| Euclidean| Image feature vectors are generated by using the [Texmex](http://corpus-texmex.irisa.fr/) dataset and the gastrointestinal stromal tumour (GIST) algorithm.  \n[glove-100-angular](https://nlp.stanford.edu/projects/glove/)| 1,183,514| 100| Cosine| Word vectors are generated by applying the GloVe algorithm to the text data from the Internet.  \n[deep-image-96-angular](http://sites.skoltech.ru/compvision/noimi/)| 9,990,000| 96| Cosine| Vectors that are extracted from the output layer of the GoogLeNet neural network with the ImageNet training dataset.  \n[dbpedia-openai-1M-angular](https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M)| 1,000,000| 1536| Cosine| OpenAI\u2019s text embeddings dataset of DBpedia entities, using the text-embedding-ada-002 model.  \n### Client version and infra setup were identical.\nOur benchmarks were done using the client on Amazon Web Services instances provisioned through our benchmark testing infrastructure. Each setup is composed of Vector DB Deployment and client VM. The benchmark client VM was a c6i.8xlarge instance, with 32 VCPUs and 10Gbps Network Bandwidth, to avoid any client-side bottleneck. The benchmarking client and database solutions were placed under optimal networking conditions (same region), achieving the low latency and stable network performance necessary for steady-state analysis.\nWe also ran baseline benchmarks on network, memory, CPU, and I/O to understand the underlying network and virtual machine characteristics, whenever applicable. During the entire set of benchmarks, the network performance was kept below the measured limits, both on bandwidth and PPS, to produce steady, stable ultra-low latency network transfers (p99 per packet < 100 microseconds).\n### **Industry-standard benchmarking tooling delivered our results.**\nFor the benchmark results showcased in this blog, we\u2019ve used [Qdrant\u2019s vector-db-benchmark](https://github.com/redis-performance/vector-db-benchmark) tool, mainly because it\u2019s well-maintained, and accepts contributions from multiple vendors (Redis, Qdrant, Weaviate, among others), nurturing replicability of results.\nFurthermore, it allows for multi-client benchmarks and can be easily extended to incorporate further vendors and datasets. \nDetailed configuration for each setup can be found in our [benchmark repo for evaluation](https://github.com/redis-performance/vector-db-benchmark/tree/update.redisearch/experiments/configurations). You can reproduce these benchmarks by following [these steps.](https://github.com/redis-performance/vector-db-benchmark?tab=readme-ov-file#run-the-client)\n### **Try the fastest vector database today.**\nYou can see for yourself the faster search and vector database with the new Redis Query Engine. It\u2019s generally available now in Redis Software and will be coming to Redis Cloud later this year. To get faster search speeds for your app today, [download Redis Software](https://redis.io/downloads/) and [contact your rep](https://redis.io/meeting/) for 16X more performance. \n## Appendix\n### Detailed Competitive player\u2019s setup\nSolution| Setup Type (Software or Cloud)| Version| Setup| Special Tuning  \n---|---|---|---|---  \nRedis| Redis Cloud| 7.4| Redis Cloud with underlying AWS m6i.2xlarge(8 Cores and 32GB memory)| Scaling factor set to 4 (6x vCPU)  \nQdrant| Qdrant Cloud| 1.7.4| Qdrant Cloud(8 Cores and 32GB memory)| Number of segments varying targeting higher qps or lower latency  \nWeaviate| Software| 1.25.1| ** operational issues on their cloud. Used Software:m6i.2xlarge(8 Cores and 32GB memory)  \nMilvus| Software| 2.4.1| ** operational issues on their cloud. Used Software:m6i.2xlarge(8 Cores and 32GB memory)  \nAmazon Aurora| AWS Cloud| PostgreSQL v16.1 and pgvector 0.5.1| db.r7g.2xlarge(8 Cores and 64GB memory)| We\u2019ve tested both the \u201cAurora Memory Optimized\u201d and \u201cAurora Read Optimized\u201d instances  \nMongoDB| MongoDB Atlas Search| MongoDB v7.0.8| M50 General Purpose deployment (8 Cores and 32GB memory)| We\u2019ve used a write concern of 1, and a read preference on the primary.  \nAmazon OpenSearch| AWS Cloud| OpenSearch 2.11 (lucene 9.7.0)| r6g.2xlarge.search(8 Cores and 64GB memory)| We\u2019ve tested both the OpenSearch default 5 primary shards setup and single shard setup.The 5 shard setup proved to be the best for OpenSearch results.  \nAmazon MemoryDB| AWS Cloud| 7.1| db.r7g.2xlarge(8 Cores and 53GB memory)  \nGCP MemoryStore for Redis| Google Cloud| 7.2.4| Standard Tier size 36 GB  \n### Findings among vector pure players\n#### Other providers had challenges with scaling on the cloud.\nWeaviate and Milvus showcased operational problems on the cloud setups that are supposed to be purpose-sized for the given datasets. Specifically:\n  * Milvus Cloud (Zilliz) presented 2 issues. One related to the availability of the service upon high ingestion load, discussed in [[here]](https://github.com/qdrant/vector-db-benchmark/issues/54), and another related to the precision of the search replies that never really surpassed 0.89/0.90. For that reason, we\u2019ve deployed Milvus on software and confirmed that the issues were not present. \n  * Weaviate cloud, on concurrent scenarios the Weaviate solution benchmark became unresponsive, once again leading to availability issues. For that reason, we\u2019ve deployed Weaviate on software and confirmed that the issues were not present.\n\n\nGiven the above issues, we\u2019ve decided to deploy software-based solutions of Milvus and Weaviate, giving the same amount of CPU and memory resources as the ones given to the other players, namely 8 VCPUs and 32GB of memory.\n#### Faster ingestion or faster queries? \nGiven Qdrant indexing results, we\u2019ve decided to understand the impact of multiple segments on the indexing and query performance and confirmed that it\u2019s a tradeoff between both. Reducing the segment count on Qdrant leads to higher search qps but lowers the precision and leads to higher indexing time. As mentioned above, it\u2019s a trade-off. We varied the segment count on Qdrant to the minimum ( 2 ) and noticed that indeed the recall lowered but the qps improvement on querying was not significant enough to reach Redis\u2013 keeping Redis with the large advantage on KNN search.\n### Findings among general-purpose DB players\n#### Aurora Memory Optimized vs Read Optimized:\nSpecifically for the benchmarks we\u2019ve run, the Read Optimized instance did not outperform the memory-optimized one. This is true because the entire dataset fits in memory in PostgresSQL, and it\u2019s backed by [Amazon\u2019s announcement](https://aws.amazon.com/blogs/database/accelerate-generative-ai-workloads-on-amazon-aurora-with-optimized-reads-and-pgvector/):\n_The local NVMe will only cache evicted pages that are unmodified, so if your vector data is updated frequently, you may not see the same speedup._**_Additionally, if your vector workload can fit entirely in memory, you may not need an Optimized Reads instance\u2014but running one will help your workload continue to scale on the same instance size._**\n#### MongoDB Atlas vector search limitations:\nCurrently, MongoDB Atlas Vector Search does not offer a way to configure EF_CONSTRUCT and M during index creation, and the only configurable option that could improve precisions is exposed during runtime via the numCandidates config (i.e. EF_RUNTIME). This design choice simplifies the user experience but limits customization and the quality of the replies for use cases that require higher precision. Even when following Mongo\u2019s official documentation:\n_We recommend that you specify a number higher than the number of documents to return (limit) to increase accuracy although this might impact latency_** _. For example, we recommend a ratio of ten to twenty nearest neighbors for a limit of only one document_** _._\nand using a **_numCandidates_** config that reaches 20x the expected reply limit (meaning our max EF_RUNTIME was 2000) precision was far from optimal for the MongoDB solution, not surpassing 0.82. \nNotes on vertical and horizontal scaling approach: This document showcases multiple use cases that benefit from vertical scaling. However, it\u2019s important to note that some use cases may not scale as effectively. To ensure a smoother scaling process, we will document the vertical scaling anti-patterns as part of the Redis docs, and Redis experts will help customers understand if vertical scaling will boost their use-case performance and guide them on whether to use vertical scaling and horizontal scaling\n![Close](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/close-icon.svg)\nIn compliance with our [Privacy Policy](https://redis.com/legal/privacy-policy/), Redis uses cookies to enhance your experience, provide personalized content, and analyze our traffic. By clicking \"Accept\", you consent to our use of cookies. To manage or opt-out, please select \"More Info\".\nAccept More Info\n[ ![redis logo](https://redis.io/wp-content/uploads/2024/04/footlogo.svg) ](https://redis.io/)\n[ ![facebook-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/facebook.svg) ](https://www.facebook.com/Redisinc) [ ![Vector-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/Vector.svg) ](https://www.youtube.com/c/redisinc) [ ![LinkedIn-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/LinkedIn.svg) ](https://www.linkedin.com/company/redisinc/) [ ![Instagram-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/Instagram.svg) ](https://www.instagram.com/redisinc/) [ ![X-icon](https://redis.io/wp-content/themes/redislabs-glide/assets/dist/images/X.svg) ](https://twitter.com/Redisinc)\n  * [Trust](https://trust.redis.io)\n  * [Privacy](https://redis.io/legal/privacy-policy/)\n  * [Terms of use](https://redis.io/legal/redis-website-terms-of-use/)\n  * [Legal notices](https://redis.io/legal/)\n\n\nSelect Language: Language English Espa\u00f1ol Fran\u00e7ais Deutsch \ud55c\uad6d\uc5b4 Portugu\u00eas\nUSE CASES\n  * [Vector database](https://redis.io/solutions/vector-database/)\n  * [Feature stores](https://redis.io/solutions/feature-stores/)\n  * [Semantic cache](https://redis.io/redis-for-ai/)\n  * [Caching](https://redis.io/solutions/caching/)\n  * [NoSQL database](https://redis.io/nosql/what-is-nosql/)\n  * [Leaderboards](https://redis.io/solutions/leaderboards/)\n  * [Data deduplication](https://redis.io/solutions/deduplication/)\n  * [Messaging](https://redis.io/solutions/messaging/)\n  * [Authentication token storage](https://redis.io/solutions/authentication-token-storage/)\n  * [Fast data ingest](https://redis.io/solutions/fast-data-ingest/)\n  * [Query caching](https://redis.io/solutions/query-caching-with-redis-enterprise/)\n  * [All solutions](https://redis.io/solutions/)\n\n\nINDUSTRIES\n  * [Financial Services](https://redis.io/industries/financial-services/)\n  * [Gaming](https://redis.io/industries/gaming/)\n  * [Healthcare](https://redis.io/industries/healthcare/)\n  * [Retail](https://redis.io/industries/retail/)\n  * [All industries](https://redis.io/industries/)\n\n\nCOMPARE\n  * [Redis vs. ElastiCache](https://redis.io/compare/elasticache/)\n  * [Redis vs. Memcached](https://redis.io/compare/memcached/)\n  * [Redis vs. Memorystore](https://redis.io/compare/memorystore/)\n  * [Redis vs. Source Available](https://redis.io/compare/community-edition/)\n\n\nCOMPANY\n  * [Mission & values](https://redis.io/company/)\n  * [Leadership](https://redis.io/company/team/)\n  * [Careers](https://redis.io/careers/)\n  * [News](https://redis.io/company/news/)\n\n\nCONNECT\n  * [Community](https://redis.io/community/)\n  * [Events & Webinars](https://redis.io/events/)\n\n\nPARTNERS\n  * [Amazon Web Services](https://redis.io/cloud-partners/aws/)\n  * [Google Cloud](https://redis.io/cloud-partners/google/)\n  * [Azure](https://redis.io/cloud-partners/azure/)\n  * [All partners](https://redis.io/partners/)\n\n\nSUPPORT\n  * [Professional Services](https://redis.io/services/professional-services/)\n  * [Support](https://redis.io/support/)\n\n\n  * [Trust](https://trust.redis.io)\n  * [Privacy](https://redis.io/legal/privacy-policy/)\n  * [Terms of use](https://redis.io/legal/redis-website-terms-of-use/)\n  * [Legal notices](https://redis.io/legal/)\n\n\nSelect Language: Language English Espa\u00f1ol Fran\u00e7ais Deutsch \ud55c\uad6d\uc5b4 Portugu\u00eas\n",
    "answer": "```markdown\n# TL;DR Summary\n\nRedis outperforms competitors in vector database benchmarks, achieving up to 3.4x higher queries per second and 4x lower latency compared to pure vector databases like Qdrant and Milvus. The new Redis Query Engine enhances performance by 16X through multi-threading and concurrent index access.\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\n[ Back to Articles](https://huggingface.co/blog)\n#  [ ](https://huggingface.co/blog/rlhf/#illustrating-reinforcement-learning-from-human-feedback-rlhf) Illustrating Reinforcement Learning from Human Feedback (RLHF) \nPublished December 9, 2022\n[Update on GitHub](https://github.com/huggingface/blog/blob/main/rlhf.md)\n[ Upvote 211 ](https://huggingface.co/login?next=%2Fblog%2Frlhf)\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1589104979708-5df82bcada6d0311fd3d5402.jpeg) ](https://huggingface.co/Chuanming \"Chuanming\")\n  * [![](https://huggingface.co/avatars/1a38d95b7d593535e6a5493fcb11332f.svg) ](https://huggingface.co/Ngit \"Ngit\")\n  * [![](https://huggingface.co/avatars/a6cf48befd61b49d24286350f3c894c4.svg) ](https://huggingface.co/kazuakey \"kazuakey\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/T762Ut0Y-w0sZB2ynvfbJ.jpeg) ](https://huggingface.co/ariG23498 \"ariG23498\")\n  * [![](https://huggingface.co/avatars/df430ada63760a97cf592b52ca80c843.svg) ](https://huggingface.co/Raman \"Raman\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/60f3289e5526f7cb110b33a9/TP3TzgNCm11Kr6rXTOHts.png) ](https://huggingface.co/rbgo \"rbgo\")\n  * +205\n\n\n[![Nathan Lambert's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/628e5f90a9a3c754c1f7c88f/iWqMY_l6dalrgRaJZWbK3.png)](https://huggingface.co/natolambert)\n[natolambert Nathan Lambert](https://huggingface.co/natolambert)\n[![Louis Castricato's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/1665155571839-60d35a0307da9c17c7270909.jpeg)](https://huggingface.co/LouisCastricato)\n[LouisCastricato Louis Castricato](https://huggingface.co/LouisCastricato)\nguest\n[![Leandro von Werra's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/5e48005437cb5b49818287a5/4uCXGGui-9QifAT4qelxU.png)](https://huggingface.co/lvwerra)\n[lvwerra Leandro von Werra](https://huggingface.co/lvwerra)\n[![Alex Havrilla's avatar](https://huggingface.co/avatars/eeb849eb29e249fd1b40e5c10d9bf9e5.svg)](https://huggingface.co/Dahoas)\n[Dahoas Alex Havrilla](https://huggingface.co/Dahoas)\nguest\n  * [RLHF: Let\u2019s take it step by step](https://huggingface.co/blog/rlhf/#rlhf-lets-take-it-step-by-step \"RLHF: Let\u2019s take it step by step\")\n  * [Open-source tools for RLHF](https://huggingface.co/blog/rlhf/#open-source-tools-for-rlhf \"Open-source tools for RLHF\")\n  * [What\u2019s next for RLHF?](https://huggingface.co/blog/rlhf/#whats-next-for-rlhf \"What\u2019s next for RLHF?\")\n\n\n_This article has been translated to Chinese[\u7b80\u4f53\u4e2d\u6587](https://huggingface.co/blog/zh/rlhf) and Vietnamese [\u0111\u1ecdc ti\u1ebfng vi\u1ec7t](https://trituenhantao.io/kien-thuc/minh-hoa-rlhf-vu-khi-dang-sau-gpt/)_. \nLanguage models have shown impressive capabilities in the past few years by generating diverse and compelling text from human input prompts. However, what makes a \"good\" text is inherently hard to define as it is subjective and context dependent. There are many applications such as writing stories where you want creativity, pieces of informative text which should be truthful, or code snippets that we want to be executable. \nWriting a loss function to capture these attributes seems intractable and most language models are still trained with a simple next token prediction loss (e.g. cross entropy). To compensate for the shortcomings of the loss itself people define metrics that are designed to better capture human preferences such as [BLEU](https://en.wikipedia.org/wiki/BLEU) or [ROUGE](https://en.wikipedia.org/wiki/ROUGE_\\(metric\\)). While being better suited than the loss function itself at measuring performance these metrics simply compare generated text to references with simple rules and are thus also limited. Wouldn't it be great if we use human feedback for generated text as a measure of performance or go even one step further and use that feedback as a loss to optimize the model? That's the idea of Reinforcement Learning from Human Feedback (RLHF); use methods from reinforcement learning to directly optimize a language model with human feedback. RLHF has enabled language models to begin to align a model trained on a general corpus of text data to that of complex human values.\nRLHF's most recent success was its use in [ChatGPT](https://openai.com/blog/chatgpt/). Given ChatGPT's impressive abilities, we asked it to explain RLHF for us:\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/chatgpt-explains.png)\nIt does surprisingly well, but doesn't quite cover everything. We'll fill in those gaps!\n##  [ ](https://huggingface.co/blog/rlhf/#rlhf-lets-take-it-step-by-step) RLHF: Let\u2019s take it step by step \nReinforcement learning from Human Feedback (also referenced as RL from human preferences) is a challenging concept because it involves a multiple-model training process and different stages of deployment. In this blog post, we\u2019ll break down the training process into three core steps:\n  1. Pretraining a language model (LM),\n  2. gathering data and training a reward model, and\n  3. fine-tuning the LM with reinforcement learning.\n\n\nTo start, we'll look at how language models are pretrained.\n####  [ ](https://huggingface.co/blog/rlhf/#pretraining-language-models) Pretraining language models \nAs a starting point RLHF use a language model that has already been pretrained with the classical pretraining objectives (see this [blog post](https://huggingface.co/blog/how-to-train) for more details). OpenAI used a smaller version of GPT-3 for its first popular RLHF model, [InstructGPT](https://openai.com/blog/instruction-following/). In their shared papers, Anthropic used transformer models from 10 million to 52 billion parameters trained for this task. DeepMind has documented using up to their 280 billion parameter model [Gopher](https://arxiv.org/abs/2112.11446). It is likely that all these companies use much larger models in their RLHF-powered products.\nThis initial model _can_ also be fine-tuned on additional text or conditions, but does not necessarily need to be. For example, OpenAI fine-tuned on human-generated text that was \u201cpreferable\u201d and Anthropic generated their initial LM for RLHF by distilling an original LM on context clues for their \u201chelpful, honest, and harmless\u201d criteria. These are both sources of what we refer to as expensive, _augmented_ data, but it is not a required technique to understand RLHF. Core to starting the RLHF process is having a _model that responds well to diverse instructions_.\nIn general, there is not a clear answer on \u201cwhich model\u201d is the best for the starting point of RLHF. This will be a common theme in this blog \u2013 the design space of options in RLHF training are not thoroughly explored.\nNext, with a language model, one needs to generate data to train a **reward model** , which is how human preferences are integrated into the system.\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/pretraining.png)\n####  [ ](https://huggingface.co/blog/rlhf/#reward-model-training) Reward model training \nGenerating a reward model (RM, also referred to as a preference model) calibrated with human preferences is where the relatively new research in RLHF begins. The underlying goal is to get a model or system that takes in a sequence of text, and returns a scalar reward which should numerically represent the human preference. The system can be an end-to-end LM, or a modular system outputting a reward (e.g. a model ranks outputs, and the ranking is converted to reward). The output being a **scalar** **reward** is crucial for existing RL algorithms being integrated seamlessly later in the RLHF process.\nThese LMs for reward modeling can be both another fine-tuned LM or a LM trained from scratch on the preference data. For example, Anthropic has used a specialized method of fine-tuning to initialize these models after pretraining (preference model pretraining, PMP) because they found it to be more sample efficient than fine-tuning, but no one base model is considered the clear best choice for reward models.\nThe training dataset of prompt-generation pairs for the RM is generated by sampling a set of prompts from a predefined dataset (Anthropic\u2019s data generated primarily with a chat tool on Amazon Mechanical Turk is [available](https://huggingface.co/datasets/Anthropic/hh-rlhf) on the Hub, and OpenAI used prompts submitted by users to the GPT API). The prompts are passed through the initial language model to generate new text.\nHuman annotators are used to rank the generated text outputs from the LM. One may initially think that humans should apply a scalar score directly to each piece of text in order to generate a reward model, but this is difficult to do in practice. The differing values of humans cause these scores to be uncalibrated and noisy. Instead, rankings are used to compare the outputs of multiple models and create a much better regularized dataset. \nThere are multiple methods for ranking the text. One method that has been successful is to have users compare generated text from two language models conditioned on the same prompt. By comparing model outputs in head-to-head matchups, an [Elo](https://en.wikipedia.org/wiki/Elo_rating_system) system can be used to generate a ranking of the models and outputs relative to each-other. These different methods of ranking are normalized into a scalar reward signal for training.\nAn interesting artifact of this process is that the successful RLHF systems to date have used reward language models with varying sizes relative to the text generation (e.g. OpenAI 175B LM, 6B reward model, Anthropic used LM and reward models from 10B to 52B, DeepMind uses 70B Chinchilla models for both LM and reward). An intuition would be that these preference models need to have similar capacity to understand the text given to them as a model would need in order to generate said text.\nAt this point in the RLHF system, we have an initial language model that can be used to generate text and a preference model that takes in any text and assigns it a score of how well humans perceive it. Next, we use **reinforcement learning (RL)** to optimize the original language model with respect to the reward model.\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/reward-model.png)\n####  [ ](https://huggingface.co/blog/rlhf/#fine-tuning-with-rl) Fine-tuning with RL \nTraining a language model with reinforcement learning was, for a long time, something that people would have thought as impossible both for engineering and algorithmic reasons. What multiple organizations seem to have gotten to work is fine-tuning some or all of the parameters of a **copy of the initial LM** with a policy-gradient RL algorithm, Proximal Policy Optimization (PPO). Some parameters of the LM are frozen because fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive (for more, see Low-Rank Adaptation ([LoRA](https://arxiv.org/abs/2106.09685)) for LMs or the [Sparrow](https://arxiv.org/abs/2209.14375) LM from DeepMind) -- depending on the scale of the model and infrastructure being used. The exact dynamics of how many parameters to freeze, or not, is considered an open research problem. PPO has been around for a relatively long time \u2013 there are [tons](https://spinningup.openai.com/en/latest/algorithms/ppo.html) of [guides](https://huggingface.co/blog/deep-rl-ppo) on how it works. The relative maturity of this method made it a favorable choice for scaling up to the new application of distributed training for RLHF. It turns out that many of the core RL advancements to do RLHF have been figuring out how to update such a large model with a familiar algorithm (more on that later).\nLet's first formulate this fine-tuning task as a RL problem. First, the **policy** is a language model that takes in a prompt and returns a sequence of text (or just probability distributions over text). The **action space** of this policy is all the tokens corresponding to the vocabulary of the language model (often on the order of 50k tokens) and the **observation space** is the distribution of possible input token sequences, which is also quite large given previous uses of RL (the dimension is approximately the size of vocabulary ^ length of the input token sequence). The **reward function** is a combination of the preference model and a constraint on policy shift.\nThe reward function is where the system combines all of the models we have discussed into one RLHF process. Given a prompt, _x_ , from the dataset, the text _y_ is generated by the current iteration of the fine-tuned policy. Concatenated with the original prompt, that text is passed to the preference model, which returns a scalar notion of \u201cpreferability\u201d, r\u03b8 r_\\theta r\u03b8\u200b. In addition, per-token probability distributions from the RL policy are compared to the ones from the initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback\u2013Leibler [(KL) divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between these sequences of distributions over tokens, rKL r_\\text{KL} rKL\u200b. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via sampling from both distributions (explained by John Schulman [here](http://joschu.net/blog/kl-approx.html)). The final reward sent to the RL update rule is r=r\u03b8\u2212\u03bbrKL r = r_\\theta - \\lambda r_\\text{KL} r=r\u03b8\u200b\u2212\u03bbrKL\u200b.\nSome RLHF systems have added additional terms to the reward function. For example, OpenAI experimented successfully on InstructGPT by mixing in additional pre-training gradients (from the human annotation set) into the update rule for PPO. It is likely as RLHF is further investigated, the formulation of this reward function will continue to evolve.\nFinally, the **update rule** is the parameter update from PPO that maximizes the reward metrics in the current batch of data (PPO is on-policy, which means the parameters are only updated with the current batch of prompt-generation pairs). PPO is a trust region optimization algorithm that uses constraints on the gradient to ensure the update step does not destabilize the learning process. DeepMind used a similar reward setup for Gopher but used [synchronous advantage actor-critic](http://proceedings.mlr.press/v48/mniha16.html?ref=https://githubhelp.com) (A2C) to optimize the gradients, which is notably different but has not been reproduced externally.\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/rlhf.png)\n_Technical detail note: The above diagram makes it look like both models generate different responses for the same prompt, but what really happens is that the RL policy generates text, and that text is fed into the initial model to produce its relative probabilities for the KL penalty. This initial model is untouched by gradient updates during training_.\nOptionally, RLHF can continue from this point by iteratively updating the reward model and the policy together. As the RL policy updates, users can continue ranking these outputs versus the model's earlier versions. Most papers have yet to discuss implementing this operation, as the deployment mode needed to collect this type of data only works for dialogue agents with access to an engaged user base. Anthropic discusses this option as _Iterated Online RLHF_ (see the original [paper](https://arxiv.org/abs/2204.05862)), where iterations of the policy are included in the ELO ranking system across models. This introduces complex dynamics of the policy and reward model evolving, which represents a complex and open research question.\n##  [ ](https://huggingface.co/blog/rlhf/#open-source-tools-for-rlhf) Open-source tools for RLHF \nThe first [code](https://github.com/openai/lm-human-preferences) released to perform RLHF on LMs was from OpenAI in TensorFlow in 2019.\nToday, there are already a few active repositories for RLHF in PyTorch that grew out of this. The primary repositories are Transformers Reinforcement Learning ([TRL](https://github.com/lvwerra/trl)), [TRLX](https://github.com/CarperAI/trlx) which originated as a fork of TRL, and Reinforcement Learning for Language models ([RL4LMs](https://github.com/allenai/RL4LMs)).\nTRL is designed to fine-tune pretrained LMs in the Hugging Face ecosystem with PPO. TRLX is an expanded fork of TRL built by [CarperAI](https://carper.ai/) to handle larger models for online and offline training. At the moment, TRLX has an API capable of production-ready RLHF with PPO and Implicit Language Q-Learning [ILQL](https://sea-snell.github.io/ILQL_site/) at the scales required for LLM deployment (e.g. 33 billion parameters). Future versions of TRLX will allow for language models up to 200B parameters. As such, interfacing with TRLX is optimized for machine learning engineers with experience at this scale.\n[RL4LMs](https://github.com/allenai/RL4LMs) offers building blocks for fine-tuning and evaluating LLMs with a wide variety of RL algorithms (PPO, NLPO, A2C and TRPO), reward functions and metrics. Moreover, the library is easily customizable, which allows training of any encoder-decoder or encoder transformer-based LM on any arbitrary user-specified reward function. Notably, it is well-tested and benchmarked on a broad range of tasks in [recent work](https://arxiv.org/abs/2210.01241) amounting up to 2000 experiments highlighting several practical insights on data budget comparison (expert demonstrations vs. reward modeling), handling reward hacking and training instabilities, etc. RL4LMs current plans include distributed training of larger models and new RL algorithms. \nBoth TRLX and RL4LMs are under heavy further development, so expect more features beyond these soon. \nThere is a large [dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf) created by Anthropic available on the Hub.\n##  [ ](https://huggingface.co/blog/rlhf/#whats-next-for-rlhf) What\u2019s next for RLHF? \nWhile these techniques are extremely promising and impactful and have caught the attention of the biggest research labs in AI, there are still clear limitations. The models, while better, can still output harmful or factually inaccurate text without any uncertainty. This imperfection represents a long-term challenge and motivation for RLHF \u2013 operating in an inherently human problem domain means there will never be a clear final line to cross for the model to be labeled as _complete_.\nWhen deploying a system using RLHF, gathering the human preference data is quite expensive due to the direct integration of other human workers outside the training loop. RLHF performance is only as good as the quality of its human annotations, which takes on two varieties: human-generated text, such as fine-tuning the initial LM in InstructGPT, and labels of human preferences between model outputs.\nGenerating well-written human text answering specific prompts is very costly, as it often requires hiring part-time staff (rather than being able to rely on product users or crowdsourcing). Thankfully, the scale of data used in training the reward model for most applications of RLHF (~50k labeled preference samples) is not as expensive. However, it is still a higher cost than academic labs would likely be able to afford. Currently, there only exists one large-scale dataset for RLHF on a general language model (from [Anthropic](https://huggingface.co/datasets/Anthropic/hh-rlhf)) and a couple of smaller-scale task-specific datasets (such as summarization data from [OpenAI](https://github.com/openai/summarize-from-feedback)). The second challenge of data for RLHF is that human annotators can often disagree, adding a substantial potential variance to the training data without ground truth.\nWith these limitations, huge swaths of unexplored design options could still enable RLHF to take substantial strides. Many of these fall within the domain of improving the RL optimizer. PPO is a relatively old algorithm, but there are no structural reasons that other algorithms could not offer benefits and permutations on the existing RLHF workflow. One large cost of the feedback portion of fine-tuning the LM policy is that every generated piece of text from the policy needs to be evaluated on the reward model (as it acts like part of the environment in the standard RL framework). To avoid these costly forward passes of a large model, offline RL could be used as a policy optimizer. Recently, new algorithms have emerged, such as [implicit language Q-learning](https://arxiv.org/abs/2206.11871) (ILQL) [[Talk](https://youtu.be/fGq4np3brbs) on ILQL at CarperAI], that fit particularly well with this type of optimization. Other core trade-offs in the RL process, like exploration-exploitation balance, have also not been documented. Exploring these directions would at least develop a substantial understanding of how RLHF functions and, if not, provide improved performance.\nWe hosted a lecture on Tuesday 13 December 2022 that expanded on this post; you can watch it [here](https://www.youtube.com/watch?v=2MBJOuVq380&feature=youtu.be)!\n####  [ ](https://huggingface.co/blog/rlhf/#further-reading) Further reading \nHere is a list of the most prevalent papers on RLHF to date. The field was recently popularized with the emergence of DeepRL (around 2017) and has grown into a broader study of the applications of LLMs from many large technology companies. Here are some papers on RLHF that pre-date the LM focus:\n  * [TAMER: Training an Agent Manually via Evaluative Reinforcement](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ICDL08-knox.pdf) (Knox and Stone 2008): Proposed a learned agent where humans provided scores on the actions taken iteratively to learn a reward model.\n  * [Interactive Learning from Policy-Dependent Human Feedback](http://proceedings.mlr.press/v70/macglashan17a/macglashan17a.pdf) (MacGlashan et al. 2017): Proposed an actor-critic algorithm, COACH, where human feedback (both positive and negative) is used to tune the advantage function.\n  * [Deep Reinforcement Learning from Human Preferences](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html) (Christiano et al. 2017): RLHF applied on preferences between Atari trajectories.\n  * [Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces](https://ojs.aaai.org/index.php/AAAI/article/view/11485) (Warnell et al. 2018): Extends the TAMER framework where a deep neural network is used to model the reward prediction.\n  * [A Survey of Preference-based Reinforcement Learning Methods](https://www.jmlr.org/papers/volume18/16-634/16-634.pdf) (Wirth et al. 2017): Summarizes efforts above with many, many more references.\n\n\nAnd here is a snapshot of the growing set of \"key\" papers that show RLHF's performance for LMs:\n  * [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593) (Zieglar et al. 2019): An early paper that studies the impact of reward learning on four specific tasks.\n  * [Learning to summarize with human feedback](https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html) (Stiennon et al., 2020): RLHF applied to the task of summarizing text. Also, [Recursively Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862) (OpenAI Alignment Team 2021), follow on work summarizing books.\n  * [WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332) (OpenAI, 2021): Using RLHF to train an agent to navigate the web.\n  * InstructGPT: [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (OpenAI Alignment Team 2022): RLHF applied to a general language model [[Blog post](https://openai.com/blog/instruction-following/) on InstructGPT].\n  * GopherCite: [Teaching language models to support answers with verified quotes](https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes) (Menick et al. 2022): Train a LM with RLHF to return answers with specific citations.\n  * Sparrow: [Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/abs/2209.14375) (Glaese et al. 2022): Fine-tuning a dialogue agent with RLHF\n  * [ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/) (OpenAI 2022): Training a LM with RLHF for suitable use as an all-purpose chat bot.\n  * [Scaling Laws for Reward Model Overoptimization](https://arxiv.org/abs/2210.10760) (Gao et al. 2022): studies the scaling properties of the learned preference model in RLHF.\n  * [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862) (Anthropic, 2022): A detailed documentation of training a LM assistant with RLHF.\n  * [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858) (Ganguli et al. 2022): A detailed documentation of efforts to \u201cdiscover, measure, and attempt to reduce [language models] potentially harmful outputs.\u201d\n  * [Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning](https://arxiv.org/abs/2208.02294) (Cohen at al. 2022): Using RL to enhance the conversational skill of an open-ended dialogue agent.\n  * [Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization](https://arxiv.org/abs/2210.01241) (Ramamurthy and Ammanabrolu et al. 2022): Discusses the design space of open-source tools in RLHF and proposes a new algorithm NLPO (Natural Language Policy Optimization) as an alternative to PPO.\n  * [Llama 2](https://arxiv.org/abs/2307.09288) (Touvron et al. 2023): Impactful open-access model with substantial RLHF details.\n\n\nThe field is the convergence of multiple fields, so you can also find resources in other areas:\n  * Continual learning of instructions ([Kojima et al. 2021](https://arxiv.org/abs/2108.04812), [Suhr and Artzi 2022](https://arxiv.org/abs/2212.09710)) or bandit learning from user feedback ([Sokolov et al. 2016](https://arxiv.org/abs/1601.04468), [Gao et al. 2022](https://arxiv.org/abs/2203.10079))\n  * Earlier history on using other RL algorithms for text generation (not all with human preferences), such as with recurrent neural networks ([Ranzato et al. 2015](https://arxiv.org/abs/1511.06732)), an actor-critic algorithm for text prediction ([Bahdanau et al. 2016](https://arxiv.org/abs/1607.07086)), or an early work adding human preferences to this framework ([Nguyen et al. 2017](https://arxiv.org/abs/1707.07402)).\n\n\n**Citation:** If you found this useful for your academic work, please consider citing our work, in text:\n```\nLambert, et al., \"Illustrating Reinforcement Learning from Human Feedback (RLHF)\", Hugging Face Blog, 2022.\n\n```\n\nBibTeX citation:\n```\n@article{lambert2022illustrating,\n author = {Lambert, Nathan and Castricato, Louis and von Werra, Leandro and Havrilla, Alex},\n title = {Illustrating Reinforcement Learning from Human Feedback (RLHF)},\n journal = {Hugging Face Blog},\n year = {2022},\n note = {https://huggingface.co/blog/rlhf},\n}\n\n```\n\n_Thanks to[Robert Kirk](https://robertkirk.github.io/) for fixing some factual errors regarding specific implementations of RLHF. Thanks to Stas Bekman for fixing some typos or confusing phrases Thanks to [Peter Stone](https://www.cs.utexas.edu/~pstone/), [Khanh X. Nguyen](https://machineslearner.com/) and [Yoav Artzi](https://yoavartzi.com/) for helping expand the related works further into history. Thanks to [Igor Kotenkov](https://www.linkedin.com/in/seeall/) for pointing out a technical error in the KL-penalty term of the RLHF procedure, its diagram, and textual description._\nMore Articles from our Blog\n[ ![](https://huggingface.co/blog/assets/166_trl_ddpo/thumbnail.png) Finetune Stable Diffusion Models with DDPO via TRL By  [metric-space](https://huggingface.co/metric-space) September 29, 2023 guest \u2022 13](https://huggingface.co/blog/trl-ddpo)\n[ ![](https://huggingface.co/blog/assets/putting_rl_back_in_rlhf_with_rloo/thumbnail.png) Putting RL back in RLHF By  [vwxyzjn](https://huggingface.co/vwxyzjn) June 12, 2024 \u2022 85](https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo)\n### Community\n![](https://huggingface.co/avatars/4555dc8337799a612b2a21bb0ffe122a.svg) [chironbang](https://huggingface.co/chironbang)\n[5 days ago](https://huggingface.co/blog/rlhf/#67e336cb2191d69019892ea7)\nIs the last figure caption correct? \"The above diagram makes it look like both models generate different responses for the same prompt, but what really happens is that the RL policy generates text, and that text is fed into the initial model to produce its relative probabilities for the KL penalty.\"\nTo me it makes more sense to feed the prompt to the reference and the policy models then to compare the reference model probability distribution with the policy's.Any insight would be appreciated.\nBest,Chiron\n+ \nReply\nEditPreview\nUpload images, audio, and videos by dragging in the text input, pasting, or clicking here. \nTap or paste here to upload images\nYour need to confirm your account before you can post a new comment. \nComment \n\u00b7 [Sign up](https://huggingface.co/join?next=%2Fblog%2Frlhf) or [log in](https://huggingface.co/login?next=%2Fblog%2Frlhf) to comment \n[ Upvote 211 ](https://huggingface.co/login?next=%2Fblog%2Frlhf)\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1589104979708-5df82bcada6d0311fd3d5402.jpeg) ](https://huggingface.co/Chuanming \"Chuanming\")\n  * [![](https://huggingface.co/avatars/1a38d95b7d593535e6a5493fcb11332f.svg) ](https://huggingface.co/Ngit \"Ngit\")\n  * [![](https://huggingface.co/avatars/a6cf48befd61b49d24286350f3c894c4.svg) ](https://huggingface.co/kazuakey \"kazuakey\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/T762Ut0Y-w0sZB2ynvfbJ.jpeg) ](https://huggingface.co/ariG23498 \"ariG23498\")\n  * [![](https://huggingface.co/avatars/df430ada63760a97cf592b52ca80c843.svg) ](https://huggingface.co/Raman \"Raman\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/60f3289e5526f7cb110b33a9/TP3TzgNCm11Kr6rXTOHts.png) ](https://huggingface.co/rbgo \"rbgo\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/60f69f2ba8e554af4289fdc8/vNj5GnvDmIwTHTyYdaS6q.jpeg) ](https://huggingface.co/bylin \"bylin\")\n  * [![](https://huggingface.co/avatars/83a387806142df3ee621cf14f250f0f4.svg) ](https://huggingface.co/swaption2009 \"swaption2009\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/613a226b0fd0cb3c9b9ede5a/Dhi8bmqnAiG6s1rqNxJjM.jpeg) ](https://huggingface.co/pulkitmehtawork \"pulkitmehtawork\")\n  * [![](https://huggingface.co/avatars/847abdb9facca33507d47268e99d9ac5.svg) ](https://huggingface.co/keyuchen2019 \"keyuchen2019\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1633286253722-noauth.jpeg) ](https://huggingface.co/Ajwad \"Ajwad\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/61656437d82f365f44993dce/YL7n7Alht967g2Wz6ycTz.jpeg) ](https://huggingface.co/varl42 \"varl42\")\n  * +199\n\n\nSystem theme \nCompany\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/) [](https://huggingface.co/)\nWebsite\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)\n",
    "answer": "```markdown\n# TL;DR: Illustrating Reinforcement Learning from Human Feedback (RLHF)\n\nThe article explains RLHF, a method to optimize language models using human feedback via reinforcement learning. It details three steps: pretraining, reward model training, and fine-tuning. Key insights include challenges in data quality, model alignment, and the need for improved RL algorithms. Open-source tools like TRL and RL4LMs are highlighted for practical implementation.\n```"
  },
  {
    "instruction": "[RunPod](https://www.runpod.io/)\n[Pricing](https://www.runpod.io/gpu-instance/pricing)[Serverless](https://www.runpod.io/serverless-gpu)[Blog](https://blog.runpod.io)[Docs](https://docs.runpod.io)\n[Contact sales](https://meetings.hubspot.com/runpodsales/runpod)[Sign Up](https://www.runpod.io/console/signup)[Login](https://www.runpod.io/console/login)\nAll in one cloud.\nTrain, fine-tune and deploy AImodels with RunPod.\n[Get started](https://www.runpod.io/console/signup)\nRunPod works with Startups, Academic Institutions, and Enterprises.\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2750%27%20height=%2760%27/%3e)![opencv logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2790%27%20height=%2753%27/%3e)![replika logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27152%27%20height=%2737%27/%3e)![datasciencedojo logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2735%27/%3e)![jina logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27130%27%20height=%2723%27/%3e)![defined ai logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27120%27%20height=%2744%27/%3e)![otovo logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2790%27%20height=%2735%27/%3e)![abzu logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27150%27%20height=%2727%27/%3e)![aftershoot logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27120%27%20height=%2733%27/%3e)![krnl logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2750%27%20height=%2760%27/%3e)![opencv logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2790%27%20height=%2753%27/%3e)![replika logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27152%27%20height=%2737%27/%3e)![datasciencedojo logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2780%27%20height=%2735%27/%3e)![jina logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27130%27%20height=%2723%27/%3e)![defined ai logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27120%27%20height=%2744%27/%3e)![otovo logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2790%27%20height=%2735%27/%3e)![abzu logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27150%27%20height=%2727%27/%3e)![aftershoot logo](https://www.runpod.io/)\n![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27120%27%20height=%2733%27/%3e)![krnl logo](https://www.runpod.io/)\n1\nDevelop\nGlobally distributed GPU cloud for your AI workloads\nDeploy any GPU workload seamlessly, so you can focus less on infrastructure and more on running ML models.\nPyTorch\nID: twnw98clgxxf2z\n$2.89/hour\n200 GB Disk: 200 GB Pod Volume\nVolume Path: /workspace\n1 x H100 PCIe\n9 vCPU 50 GB RAM\nCA\n8654 Mbps\n938 Mbps\n963 MBps\n0\n2025-03-30T17:42:45.806Z\ncreate pod network\n1\n2025-03-30T17:42:46.806Z\ncreate 20GB network volume\n2\n2025-03-30T17:42:47.806Z\ncreate container runpod/pytorch:3.10-2.0.0-117\n3\n2025-03-30T17:42:48.806Z\n3.10-2.0.0-117 Pulling from runpod/pytorch\n4\n2025-03-30T17:42:49.806Z\nDigest: sha256:2dbf81dd888d383620a486f83ad2ff47540c6cb5e02a61e74b8db03a715488d6\n5\n2025-03-30T17:42:50.806Z\nStatus: Image is up to date for runpod/pytorch:3.10-2.0.0-117\n6\n2025-03-30T17:42:51.806Z\nstart container\n## Spin up a GPU pod in seconds\nit's a pain to having to wait upwards of 10 minutes for your pods to spin up - we've cut the cold-boot time down to milliseconds, so you can start building within seconds of deploying your pods.\n[Spin up a pod](https://www.runpod.io/console/deploy)\n## Choose from 50+ templates ready out-of-the-box, or bring your own custom container.\nGet setup instantly with PyTorch, Tensorflow, or any other preconfigured environment you might need for your machine learning workflow. Along with managed and community templates, we also let you configure your own template to fit your deployment needs.\n[Browse templates](https://www.runpod.io/console/explore)\nPyTorch\n[Deploy](https://www.runpod.io/console/explore/runpod-torch-v220)\nTensorflow\n[Deploy](https://www.runpod.io/console/explore/runpod-tensorflow)\nDocker\n[Deploy](https://www.runpod.io/console/explore/runpod-kobold-united)\n![runpod logo](https://www.runpod.io/static/svg/runpod-template-logo.svg)\nRunpod\n[Deploy](https://www.runpod.io/console/explore/runpod-desktop)\n## Powerful & Cost-Effective GPUsfor Every Workload\n[See all GPUs](https://www.runpod.io/console/deploy)\nThousands of GPUs across 30+ Regions\nDeploy any container on Secure Cloud. Public and private image repos are supported. Configure your environment the way you want.\nZero fees for ingress/egress\nGlobal interoperability\n99.99% Uptime\n$0.05/GB/month Network Storage\n![amd](https://www.runpod.io/static/images/companies/amd.svg)\nStarting from $2.49/hr\nMI300X\n192GB VRAM\n283GB RAM\n24 vCPUs\n$2.49/hr\nSecure Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $1.99/hr\nH100 PCIe\n80GB VRAM\n188GB RAM\n16 vCPUs\n$2.39/hr\nSecure Cloud\n$1.99/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $1.19/hr\nA100 PCIe\n80GB VRAM\n117GB RAM\n8 vCPUs\n$1.64/hr\nSecure Cloud\n$1.19/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $1.89/hr\nA100 SXM\n80GB VRAM\n125GB RAM\n16 vCPUs\n$1.89/hr\nSecure Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.44/hr\nA40\n48GB VRAM\n50GB RAM\n9 vCPUs\n$0.44/hr\nSecure Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.99/hr\nL40\n48GB VRAM\n94GB RAM\n8 vCPUs\n$0.99/hr\nSecure Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.79/hr\nL40S\n48GB VRAM\n62GB RAM\n12 vCPUs\n$0.86/hr\nSecure Cloud\n$0.79/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.33/hr\nRTX A6000\n48GB VRAM\n50GB RAM\n8 vCPUs\n$0.59/hr\nSecure Cloud\n$0.33/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.16/hr\nRTX A5000\n24GB VRAM\n25GB RAM\n8 vCPUs\n$0.29/hr\nSecure Cloud\n$0.16/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.34/hr\nRTX 4090\n24GB VRAM\n29GB RAM\n6 vCPUs\n$0.69/hr\nSecure Cloud\n$0.34/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.22/hr\nRTX 3090\n24GB VRAM\n24GB RAM\n4 vCPUs\n$0.43/hr\nSecure Cloud\n$0.22/hr\nCommunity Cloud\n![nvidia](https://www.runpod.io/static/images/companies/nvidia.png)\nStarting from $0.20/hr\nRTX A4000 Ada\n20GB VRAM\n31GB RAM\n4 vCPUs\n$0.38/hr\nSecure Cloud\n$0.20/hr\nCommunity Cloud\n2\nScale\nScale ML inferencewith Serverless\nRun your AI models with autoscaling, job queueing and sub 250ms cold start time.\n[Deploy Now](https://www.runpod.io/console/serverless)\nAutoscale in seconds\nRespond to user demand in real time with GPU workers thatscale from 0 to 100s in seconds.\nFlex \nWorkers\nActive \nWorkers\n10 GPUs\n6:24AM\n100 GPUs\n11:34AM\n20 GPUs\n1:34PM\nUsage Analytics\nReal-time usage analytics for your endpoint with metrics on completed and failed requests. Useful for endpoints that have fluctuating usage profiles throughout the day.\n[See the console ](https://www.runpod.io/console/serverless)\nActive\nRequests\nCompleted:\n2,277\nRetried:\n21\nFailed:\n9\nExecution Time\nTotal:\n1,420s\nP70:\n8s\nP90:\n19s\nP98:\n22s\nExecution Time Analytics\nDebug your endpoints with detailed metrics on execution time. Useful for hosting models that have varying execution times, like large language models. You can also monitor delay time, cold start time, cold start count, GPU utilization, and more.\n[See the console ](https://www.runpod.io/console/serverless)\nReal-Time Logs\nGet descriptive, real-time logs to show you exactly what's happening across your active and flex GPU workers at all times.\n[See the console ](https://www.runpod.io/console/serverless)\nworker logs -- zsh\n2024-03-15T19:56:00.8264895Z INFO | Started job db7c792024-03-15T19:56:03.2667597Z0% | | 0/28 [00:00<?, ?it/s]12% |\u2588\u2588 | 4/28 [00:00<00:01, 12.06it/s]38% |\u2588\u2588\u2588\u2588 | 12/28 [00:00<00:01, 12.14it/s]77% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 22/28 [00:01<00:00, 12.14it/s]100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28/28 [00:02<00:00, 12.13it/s]2024-03-15T19:56:04.7438407Z INFO | Completed job db7c79 in 2.9s2024-03-15T19:57:00.8264895Z INFO | Started job ea1r142024-03-15T19:57:03.2667597Z0% | | 0/28 [00:00<?, ?it/s]15% |\u2588\u2588 | 4/28 [00:00<00:01, 12.06it/s]41% |\u2588\u2588\u2588\u2588 | 12/28 [00:00<00:01, 12.14it/s]80% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 22/28 [00:01<00:00, 12.14it/s]100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28/28 [00:02<00:00, 12.13it/s]2024-03-15T19:57:04.7438407Z INFO | Completed job ea1r14 in 2.9s2024-03-15T19:58:00.8264895Z INFO | Started job gn3a252024-03-15T19:58:03.2667597Z0% | | 0/28 [00:00<?, ?it/s]18% |\u2588\u2588 | 4/28 [00:00<00:01, 12.06it/s]44% |\u2588\u2588\u2588\u2588 | 12/28 [00:00<00:01, 12.14it/s]83% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 22/28 [00:01<00:00, 12.14it/s]100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28/28 [00:02<00:00, 12.13it/s]2024-03-15T19:58:04.7438407Z INFO | Completed job gn3a25 in 2.9s\n![cloud image everything header](https://www.runpod.io/static/images/everything-clouds.png)\nEverything your app needs. All in \none cloud.\n99.99%\nguaranteed uptime\n10PB+\nnetwork storage\n6,811,142,348\nrequests\nAI Inference\nWe handle millions of inference requests a day. Scale your machine learning inference while keeping costs low with RunPod serverless.\nAI Training\nRun machine learning training tasks that can take up to 7 days. Train on our available NVIDIA H100s and A100s or reserve AMD MI300Xs and AMD MI250s a year in advance.\nAutoscale\nServerless GPU workers scale from 0 to n with 8+ regions distributed globally. You only pay when your endpoint receives and processes a request.\nBring Your Own Container\nDeploy any container on our AI cloud. Public and private image repositories are supported. Configure your environment the way you want.\nZero Ops Overhead\nRunPod handles all the operational aspects of your infrastructure from deploying to scaling. You bring the models, let us handle the ML infra.\nNetwork Storage\nServerless workers can access network storage volume backed by NVMe SSD with up to 100Gbps network throughput. 100TB+ storage size is supported, contact us if you need 1PB+.\nEasy-to-use CLI\nUse our CLI tool to automatically hot reload local changes while developing, and deploy on Serverless when you\u2019re done tinkering.\nSecure & Compliant\nRunPod AI Cloud is built on enterprise-grade GPUs with world-class compliance and security to best serve your machine learning models.\nLightning Fast Cold-Start\nWith Flashboot, watch your cold-starts drop to sub 250 milliseconds. No more waiting for GPUs to warm up when usage is unpredictable.\nCertifications\nRunPod has obtained SOC2 Type 1 Certification as of February 2025. Our data center partners maintain leading compliance standards (including HIPAA, SOC2, and ISO 27001)\n![gpu background](https://www.runpod.io/static/images/soc2-light.webp)\nLaunch your AI application in minutes\nStart building with the most cost-effective platform for developing and scaling machine learning models.\n[Get started](https://www.runpod.io/console/signup)\nProducts\n[Secure Cloud](https://www.runpod.io/console/gpu-secure-cloud)[Community Cloud](https://www.runpod.io/console/gpu-cloud)[Serverless](https://www.runpod.io/console/serverless)\nResources\n[Docs](https://docs.runpod.io/overview)[FAQ](https://docs.runpod.io/faq)[Blog](https://blog.runpod.io)[Become a Host](https://www.runpod.io/console/host/docs/faq)[GPU Benchmarks](https://www.runpod.io/dir/gpu-benchmarks)[GPU Models](https://www.runpod.io/dir/gpu-models)[Articles](https://www.runpod.io/articles)[AI FAQ](https://www.runpod.io/ai-faq)[LLM Models](https://www.runpod.io/dir/llm-models)\nCompany\n[About](https://www.runpod.io/about)[Careers](https://job-boards.greenhouse.io/runpod)[Compliance](https://www.runpod.io/compliance)[Cookie Policy](https://www.runpod.io/legal/cookie-policy)[Disclaimer](https://www.runpod.io/legal/disclaimer)[Privacy Policy](https://www.runpod.io/legal/privacy-policy)[Terms of Service](https://www.runpod.io/legal/terms-of-service)\nContact\n[Contact Us](https://contact.runpod.io)[Discord](https://discord.gg/cUpRmau42V)help@runpod.ioreferrals@runpod.iopress@runpod.io\n[](https://github.com/runpod)[](https://discord.gg/cUpRmau42V)[](https://twitter.com/runpod_io)[](https://www.instagram.com/runpod.io)\n[RunPod](https://www.runpod.io/)\nCopyright \u00a9 2025. All rights reserved.\n",
    "answer": "```markdown\n## TL;DR Summary\n\nRunPod offers a cloud platform for training, fine-tuning, and deploying AI models with low latency and cost-effective GPU options. It supports various frameworks (PyTorch, TensorFlow) and provides serverless scaling, real-time analytics, and enterprise-grade security. Ideal for startups and enterprises.\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\nAmazon SageMaker documentation\nDeploy models to Amazon SageMaker\n# Amazon SageMaker\n\ud83c\udfe1 View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersDistilabelEvaluateGradioHubHub Python LibraryHugging Face Generative AI Services (HUGS)Huggingface.jsInference API (serverless)Inference Endpoints (dedicated)LeaderboardsLightevalOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jssmolagentstimm\nSearch documentation\n`\u2318K`\nmain EN [ 365](https://github.com/huggingface/hub-docs)\n[Hugging Face on Amazon SageMaker ](https://huggingface.co/docs/sagemaker/en/index)[Get started ](https://huggingface.co/docs/sagemaker/en/getting-started)[Run training on Amazon SageMaker ](https://huggingface.co/docs/sagemaker/en/train)[Deploy models to Amazon SageMaker ](https://huggingface.co/docs/sagemaker/en/inference)[Reference ](https://huggingface.co/docs/sagemaker/en/reference)\n![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\nJoin the Hugging Face community\nand get access to the augmented documentation experience \nCollaborate on models, datasets and Spaces \nFaster examples with accelerated inference \nSwitch between documentation themes \n[Sign Up](https://huggingface.co/join)\nto get started\n# [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-models-to-amazon-sagemaker) Deploy models to Amazon SageMaker\nDeploying a \ud83e\udd17 Transformers models in SageMaker for inference is as easy as:\nCopied\n```\nfrom sagemaker.huggingface import HuggingFaceModel\n# create Hugging Face Model Class and deploy it as SageMaker endpoint\nhuggingface_model = HuggingFaceModel(...).deploy()\n```\n\nThis guide will show you how to deploy models with zero-code using the [Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit). The Inference Toolkit builds on top of the [`pipeline` feature](https://huggingface.co/docs/transformers/main_classes/pipelines) from \ud83e\udd17 Transformers. Learn how to:\n  * [Install and setup the Inference Toolkit](https://huggingface.co/docs/sagemaker/en/inference/#installation-and-setup).\n  * [Deploy a \ud83e\udd17 Transformers model trained in SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-transformer-model-trained-in-sagemaker).\n  * [Deploy a \ud83e\udd17 Transformers model from the Hugging Face [model Hub](https://huggingface.co/models)](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-model-from-the-hub).\n  * [Run a Batch Transform Job using \ud83e\udd17 Transformers and Amazon SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#run-batch-transform-with-transformers-and-sagemaker).\n  * [Create a custom inference module](https://huggingface.co/docs/sagemaker/en/inference/#user-defined-code-and-modules).\n\n\n## [](https://huggingface.co/docs/sagemaker/en/inference/#installation-and-setup) Installation and setup\nBefore deploying a \ud83e\udd17 Transformers model to SageMaker, you need to sign up for an AWS account. If you don\u2019t have an AWS account yet, learn more [here](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html).\nOnce you have an AWS account, get started using one of the following:\n  * [SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html)\n  * [SageMaker notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)\n  * Local environment\n\n\nTo start training locally, you need to setup an appropriate [IAM role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\nUpgrade to the latest `sagemaker` version.\nCopied\n```\npip install sagemaker --upgrade\n```\n\n**SageMaker environment**\nSetup your SageMaker environment as shown below:\nCopied\n```\nimport sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n```\n\n_Note: The execution role is only available when running a notebook within SageMaker. If you run`get_execution_role` in a notebook not on SageMaker, expect a `region` error._\n**Local environment**\nSetup your local environment as shown below:\nCopied\n```\nimport sagemaker\nimport boto3\niam_client = boto3.client('iam')\nrole = iam_client.get_role(RoleName='role-name-of-your-iam-role-with-right-permissions')['Role']['Arn']\nsess = sagemaker.Session()\n```\n\n## [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a--transformers-model-trained-in-sagemaker) Deploy a \ud83e\udd17 Transformers model trained in SageMaker\nThere are two ways to deploy your Hugging Face model trained in SageMaker:\n  * Deploy it after your training has finished.\n  * Deploy your saved model at a later time from S3 with the `model_data`.\n\n\n\ud83d\udcd3 Open the [deploy_transformer_model_from_s3.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb) for an example of how to deploy a model from S3 to SageMaker for inference.\n### [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-after-training) Deploy after training\nTo deploy your model directly after training, ensure all required files are saved in your training script, including the tokenizer and the model.\nIf you use the Hugging Face `Trainer`, you can pass your tokenizer as an argument to the `Trainer`. It will be automatically saved when you call `trainer.save_model()`.\nCopied\n```\nfrom sagemaker.huggingface import HuggingFace\n############ pseudo code start ############\n# create Hugging Face Estimator for training\nhuggingface_estimator = HuggingFace(....)\n# start the train job with our uploaded datasets as input\nhuggingface_estimator.fit(...)\n############ pseudo code end ############\n# deploy model to SageMaker Inference\npredictor = hf_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")\n# example request: you always need to define \"inputs\"\ndata = {\n  \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n}\n# request\npredictor.predict(data)\n```\n\nAfter you run your request you can delete the endpoint as shown:\nCopied\n```\n# delete endpoint\npredictor.delete_endpoint()\n```\n\n### [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-with-modeldata) Deploy with model_data\nIf you\u2019ve already trained your model and want to deploy it at a later time, use the `model_data` argument to specify the location of your tokenizer and model weights.\nCopied\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n  model_data=\"s3://models/my-bert-model/model.tar.gz\", # path to your trained SageMaker model\n  role=role,                      # IAM role with permissions to create an endpoint\n  transformers_version=\"4.26\",              # Transformers version used\n  pytorch_version=\"1.13\",                # PyTorch version used\n  py_version='py39',                  # Python version used\n)\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n  initial_instance_count=1,\n  instance_type=\"ml.m5.xlarge\"\n)\n# example request: you always need to define \"inputs\"\ndata = {\n  \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n}\n# request\npredictor.predict(data)\n```\n\nAfter you run our request, you can delete the endpoint again with:\nCopied\n```\n# delete endpoint\npredictor.delete_endpoint()\n```\n\n### [](https://huggingface.co/docs/sagemaker/en/inference/#create-a-model-artifact-for-deployment) Create a model artifact for deployment\nFor later deployment, you can create a `model.tar.gz` file that contains all the required files, such as:\n  * `pytorch_model.bin`\n  * `tf_model.h5`\n  * `tokenizer.json`\n  * `tokenizer_config.json`\n\n\nFor example, your file should look like this:\nCopied\n```\nmodel.tar.gz/\n|- pytorch_model.bin\n|- vocab.txt\n|- tokenizer_config.json\n|- config.json\n|- special_tokens_map.json\n```\n\nCreate your own `model.tar.gz` from a model from the \ud83e\udd17 Hub:\n  1. Download a model:\n\n\nCopied\n```\ngit lfs install\ngit clone git@hf.co:{repository}\n```\n\n  1. Create a `tar` file:\n\n\nCopied\n```\ncd {repository}\ntar zcvf model.tar.gz *\n```\n\n  1. Upload `model.tar.gz` to S3:\n\n\nCopied\n```\naws s3 cp model.tar.gz <s3://{my-s3-path}>\n```\n\nNow you can provide the S3 URI to the `model_data` argument to deploy your model later.\n## [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-model-from-the--hub) Deploy a model from the \ud83e\udd17 Hub\nTo deploy a model directly from the \ud83e\udd17 Hub to SageMaker, define two environment variables when you create a `HuggingFaceModel`:\n  * `HF_MODEL_ID` defines the model ID which is automatically loaded from [huggingface.co/models](http://huggingface.co/models) when you create a SageMaker endpoint. Access 10,000+ models on he \ud83e\udd17 Hub through this environment variable.\n  * `HF_TASK` defines the task for the \ud83e\udd17 Transformers `pipeline`. A complete list of tasks can be found [here](https://huggingface.co/docs/transformers/main_classes/pipelines).\n\n\n> \u26a0\ufe0f ** Pipelines are not optimized for parallelism (multi-threading) and tend to consume a lot of RAM. For example, on a GPU-based instance, the pipeline operates on a single vCPU. When this vCPU becomes saturated with the inference requests preprocessing, it can create a bottleneck, preventing the GPU from being fully utilized for model inference. Learn more [here](https://huggingface.co/docs/transformers/en/pipeline_webserver#using-pipelines-for-a-webserver)\nCopied\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n# Hub model configuration <https://huggingface.co/models>\nhub = {\n 'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models\n 'HF_TASK':'question-answering'              # NLP task you want to use for predictions\n}\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n  env=hub,                        # configuration for loading model from Hub\n  role=role,                       # IAM role with permissions to create an endpoint\n  transformers_version=\"4.26\",               # Transformers version used\n  pytorch_version=\"1.13\",                 # PyTorch version used\n  py_version='py39',                   # Python version used\n)\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n  initial_instance_count=1,\n  instance_type=\"ml.m5.xlarge\"\n)\n# example request: you always need to define \"inputs\"\ndata = {\n\"inputs\": {\n\t\"question\": \"What is used for inference?\",\n\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n\t}\n}\n# request\npredictor.predict(data)\n```\n\nAfter you run our request, you can delete the endpoint again with:\nCopied\n```\n# delete endpoint\npredictor.delete_endpoint()\n```\n\n\ud83d\udcd3 Open the [deploy_transformer_model_from_hf_hub.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb) for an example of how to deploy a model from the \ud83e\udd17 Hub to SageMaker for inference.\n## [](https://huggingface.co/docs/sagemaker/en/inference/#run-batch-transform-with--transformers-and-sagemaker) Run batch transform with \ud83e\udd17 Transformers and SageMaker\nAfter training a model, you can use [SageMaker batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html) to perform inference with the model. Batch transform accepts your inference data as an S3 URI and then SageMaker will take care of downloading the data, running the prediction, and uploading the results to S3. For more details about batch transform, take a look [here](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html).\n\u26a0\ufe0f The Hugging Face Inference DLC currently only supports `.jsonl` for batch transform due to the complex structure of textual data.\n_Note: Make sure your`inputs` fit the `max_length` of the model during preprocessing._\nIf you trained a model using the Hugging Face Estimator, call the `transformer()` method to create a transform job for a model based on the training job (see [here](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform) for more details):\nCopied\n```\nbatch_job = huggingface_estimator.transformer(\n  instance_count=1,\n  instance_type='ml.p3.2xlarge',\n  strategy='SingleRecord')\n\nbatch_job.transform(\n  data='s3://s3-uri-to-batch-data',\n  content_type='application/json',  \n  split_type='Line')\n```\n\nIf you want to run your batch transform job later or with a model from the \ud83e\udd17 Hub, create a `HuggingFaceModel` instance and then call the `transformer()` method:\nCopied\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n# Hub model configuration <https://huggingface.co/models>\nhub = {\n\t'HF_MODEL_ID':'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n\t'HF_TASK':'text-classification'\n}\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n  env=hub,                        # configuration for loading model from Hub\n  role=role,                       # IAM role with permissions to create an endpoint\n  transformers_version=\"4.26\",               # Transformers version used\n  pytorch_version=\"1.13\",                 # PyTorch version used\n  py_version='py39',                   # Python version used\n)\n# create transformer to run a batch job\nbatch_job = huggingface_model.transformer(\n  instance_count=1,\n  instance_type='ml.p3.2xlarge',\n  strategy='SingleRecord'\n)\n# starts batch transform job and uses S3 data as input\nbatch_job.transform(\n  data='s3://sagemaker-s3-demo-test/samples/input.jsonl',\n  content_type='application/json',  \n  split_type='Line'\n)\n```\n\nThe `input.jsonl` looks like this:\nCopied\n```\n{\"inputs\":\"this movie is terrible\"}\n{\"inputs\":\"this movie is amazing\"}\n{\"inputs\":\"SageMaker is pretty cool\"}\n{\"inputs\":\"SageMaker is pretty cool\"}\n{\"inputs\":\"this movie is terrible\"}\n{\"inputs\":\"this movie is amazing\"}\n```\n\n\ud83d\udcd3 Open the [sagemaker-notebook.ipynb notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb) for an example of how to run a batch transform job for inference.\n## [](https://huggingface.co/docs/sagemaker/en/inference/#deploy-an-llm-to-sagemaker-using-tgi) Deploy an LLM to SageMaker using TGI\nIf you are interested in using a high-performance serving container for LLMs, you can use the Hugging Face TGI container. This utilizes the [Text Generation Inference](https://github.com/huggingface/text-generation-inference) library. A list of compatible models can be found [here](https://huggingface.co/docs/text-generation-inference/supported_models#supported-models).\nFirst, make sure that the latest version of SageMaker SDK is installed:\nCopied\n```\npip install sagemaker>=2.231.0\n```\n\nThen, we import the SageMaker Python SDK and instantiate a sagemaker_session to find the current region and execution role.\nCopied\n```\nimport sagemaker\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\nimport time\nsagemaker_session = sagemaker.Session()\nregion = sagemaker_session.boto_region_name\nrole = sagemaker.get_execution_role()\n```\n\nNext we retrieve the LLM image URI. We use the helper function get_huggingface_llm_image_uri() to generate the appropriate image URI for the Hugging Face Large Language Model (LLM) inference. The function takes a required parameter backend and several optional parameters. The backend specifies the type of backend to use for the model: \u201chuggingface\u201d refers to using Hugging Face TGI backend.\nCopied\n```\nimage_uri = get_huggingface_llm_image_uri(\n backend=\"huggingface\",\n region=region\n)\n```\n\nNow that we have the image uri, the next step is to configure the model object. We specify a unique name, the image_uri for the managed TGI container, and the execution role for the endpoint. Additionally, we specify a number of environment variables including the `HF_MODEL_ID` which corresponds to the model from the HuggingFace Hub that will be deployed, and the `HF_TASK` which configures the inference task to be performed by the model.\nYou should also define `SM_NUM_GPUS`, which specifies the tensor parallelism degree of the model. Tensor parallelism can be used to split the model across multiple GPUs, which is necessary when working with LLMs that are too big for a single GPU. To learn more about tensor parallelism with inference, see our previous blog post. Here, you should set `SM_NUM_GPUS` to the number of available GPUs on your selected instance type. For example, in this tutorial, we set `SM_NUM_GPUS` to 4 because our selected instance type ml.g4dn.12xlarge has 4 available GPUs.\nNote that you can optionally reduce the memory and computational footprint of the model by setting the `HF_MODEL_QUANTIZE` environment variable to `true`, but this lower weight precision could affect the quality of the output for some models.\nCopied\n```\nmodel_name = \"llama-3-1-8b-instruct\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\nhub = {\n  'HF_MODEL_ID':'meta-llama/Llama-3.1-8B-Instruct',\n  'SM_NUM_GPUS':'1',\n\t'HUGGING_FACE_HUB_TOKEN': '<REPLACE WITH YOUR TOKEN>',\n}\nassert hub['HUGGING_FACE_HUB_TOKEN'] != '<REPLACE WITH YOUR TOKEN>', \"You have to provide a token.\"\n\nmodel = HuggingFaceModel(\n  name=model_name,\n  env=hub,\n  role=role,\n  image_uri=image_uri\n)\n```\n\nNext, we invoke the deploy method to deploy the model.\nCopied\n```\npredictor = model.deploy(\n initial_instance_count=1,\n instance_type=\"ml.g5.2xlarge\",\n endpoint_name=model_name\n)\n```\n\nOnce the model is deployed, we can invoke it to generate text. We pass an input prompt and run the predict method to generate a text response from the LLM running in the TGI container.\nCopied\n```\ninput_data = {\n \"inputs\": \"The diamondback terrapin was the first reptile to\",\n \"parameters\": {\n  \"do_sample\": True,\n  \"max_new_tokens\": 100,\n  \"temperature\": 0.7,\n  \"watermark\": True\n }\n}\npredictor.predict(input_data)\n```\n\nWe receive the following auto-generated text response:\nCopied\n```\n[{'generated_text': 'The diamondback terrapin was the first reptile to make the list, followed by the American alligator, the American crocodile, and the American box turtle. The polecat, a ferret-like animal, and the skunk rounded out the list, both having gained their slots because they have proven to be particularly dangerous to humans.\\n\\nCalifornians also seemed to appreciate the new list, judging by the comments left after the election.\\n\\n\u201cThis is fantastic,\u201d one commenter declared.\\n\\n\u201cCalifornia is a very'}]\n```\n\nOnce we are done experimenting, we delete the endpoint and the model resources.\nCopied\n```\npredictor.delete_model()\npredictor.delete_endpoint()\n```\n\n## [](https://huggingface.co/docs/sagemaker/en/inference/#user-defined-code-and-modules) User defined code and modules\nThe Hugging Face Inference Toolkit allows the user to override the default methods of the `HuggingFaceHandlerService`. You will need to create a folder named `code/` with an `inference.py` file in it. See [here](https://huggingface.co/docs/sagemaker/en/inference/#create-a-model-artifact-for-deployment) for more details on how to archive your model artifacts. For example:\nCopied\n```\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n |- inference.py\n |- requirements.txt \n```\n\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:\n  * `model_fn(model_dir)` overrides the default method for loading a model. The return value `model` will be used in `predict` for predictions. `predict` receives argument the `model_dir`, the path to your unzipped `model.tar.gz`.\n  * `transform_fn(model, data, content_type, accept_type)` overrides the default transform function with your custom implementation. You will need to implement your own `preprocess`, `predict` and `postprocess` steps in the `transform_fn`. This method can\u2019t be combined with `input_fn`, `predict_fn` or `output_fn` mentioned below.\n  * `input_fn(input_data, content_type)` overrides the default method for preprocessing. The return value `data` will be used in `predict` for predictions. The inputs are:\n    * `input_data` is the raw body of your request.\n    * `content_type` is the content type from the request header.\n  * `predict_fn(processed_data, model)` overrides the default method for predictions. The return value `predictions` will be used in `postprocess`. The input is `processed_data`, the result from `preprocess`.\n  * `output_fn(prediction, accept)` overrides the default method for postprocessing. The return value `result` will be the response of your request (e.g.`JSON`). The inputs are:\n    * `predictions` is the result from `predict`.\n    * `accept` is the return accept type from the HTTP Request, e.g. `application/json`.\n\n\nHere is an example of a custom inference module with `model_fn`, `input_fn`, `predict_fn`, and `output_fn`:\nCopied\n```\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder\ndef model_fn(model_dir):\n  # implement custom code to load the model\n  loaded_model = ...\n  \n  return loaded_model \ndef input_fn(input_data, content_type):\n  # decode the input data (e.g. JSON string -> dict)\n  data = decoder_encoder.decode(input_data, content_type)\n  return data\ndef predict_fn(data, model):\n  # call your custom model with the data\n  outputs = model(data , ... )\n  return predictions\ndef output_fn(prediction, accept):\n  # convert the model output to the desired output format (e.g. dict -> JSON string)\n  response = decoder_encoder.encode(prediction, accept)\n  return response\n```\n\nCustomize your inference module with only `model_fn` and `transform_fn`:\nCopied\n```\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder\ndef model_fn(model_dir):\n  # implement custom code to load the model\n  loaded_model = ...\n  \n  return loaded_model \ndef transform_fn(model, input_data, content_type, accept):\n   # decode the input data (e.g. JSON string -> dict)\n  data = decoder_encoder.decode(input_data, content_type)\n  # call your custom model with the data\n  outputs = model(data , ... ) \n  # convert the model output to the desired output format (e.g. dict -> JSON string)\n  response = decoder_encoder.encode(output, accept)\n  return response\n```\n\n[< > Update on GitHub](https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md)\n[\u2190Run training on Amazon SageMaker](https://huggingface.co/docs/sagemaker/en/train) [Reference\u2192](https://huggingface.co/docs/sagemaker/en/reference)\n[Deploy models to Amazon SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#deploy-models-to-amazon-sagemaker) [Installation and setup](https://huggingface.co/docs/sagemaker/en/inference/#installation-and-setup) [Deploy a \ud83e\udd17 Transformers model trained in SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a--transformers-model-trained-in-sagemaker) [Deploy after training](https://huggingface.co/docs/sagemaker/en/inference/#deploy-after-training) [Deploy with model_data](https://huggingface.co/docs/sagemaker/en/inference/#deploy-with-modeldata) [Create a model artifact for deployment](https://huggingface.co/docs/sagemaker/en/inference/#create-a-model-artifact-for-deployment) [Deploy a model from the \ud83e\udd17 Hub](https://huggingface.co/docs/sagemaker/en/inference/#deploy-a-model-from-the--hub) [Run batch transform with \ud83e\udd17 Transformers and SageMaker](https://huggingface.co/docs/sagemaker/en/inference/#run-batch-transform-with--transformers-and-sagemaker) [Deploy an LLM to SageMaker using TGI](https://huggingface.co/docs/sagemaker/en/inference/#deploy-an-llm-to-sagemaker-using-tgi) [User defined code and modules](https://huggingface.co/docs/sagemaker/en/inference/#user-defined-code-and-modules)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis document provides a comprehensive guide on deploying Hugging Face models to Amazon SageMaker, covering installation, setup, and deployment methods. Key topics include zero-code deployment, batch transforms, and custom inference modules. It emphasizes the ease of using the Inference Toolkit and deploying models from the Hugging Face Hub.\n\n**Tags:** Generative AI, LLMs, Hugging Face, Amazon SageMaker\n```"
  },
  {
    "instruction": "## Select your cookie preferences\nWe use essential cookies and similar tools that are necessary to provide our site and services. We use performance cookies to collect anonymous statistics, so we can understand how customers use our site and make improvements. Essential cookies cannot be deactivated, but you can choose \u201cCustomize\u201d or \u201cDecline\u201d to decline performance cookies. If you agree, AWS and approved third parties will also use cookies to provide useful site features, remember your preferences, and display relevant content, including relevant advertising. To accept or decline all non-essential cookies, choose \u201cAccept\u201d or \u201cDecline.\u201d To make more detailed choices, choose \u201cCustomize.\u201d\nAcceptDeclineCustomize\n## Customize cookie preferences\nWe use cookies and similar tools (collectively, \"cookies\") for the following purposes.\n### Essential\nEssential cookies are necessary to provide our site and services and cannot be deactivated. They are usually set in response to your actions on the site, such as setting your privacy preferences, signing in, or filling in forms. \n### Performance\nPerformance cookies provide anonymous statistics about how customers navigate our site so we can improve site experience and performance. Approved third parties may perform analytics on our behalf, but they cannot use the data for their own purposes.\nAllow performance category\nAllowed\n### Functional\nFunctional cookies help us provide useful site features, remember your preferences, and display relevant content. Approved third parties may set these cookies to provide certain site features. If you do not allow these cookies, then some or all of these services may not function properly.\nAllow functional category\nAllowed\n### Advertising\nAdvertising cookies may be set through our site by us or our advertising partners and help us deliver relevant marketing content. If you do not allow these cookies, you will experience less relevant advertising.\nAllow advertising category\nAllowed\nBlocking some types of cookies may impact your experience of our sites. You may review and change your choices at any time by selecting Cookie preferences in the footer of this site. We and selected third-parties use cookies or similar technologies as specified in the [AWS Cookie Notice](https://aws.amazon.com/legal/cookies/).\nCancelSave preferences\n## Your privacy choices\nWe display ads relevant to your interests on AWS sites and on other properties, including cross-context behavioral advertising. Cross-context behavioral advertising uses data from one site or app to advertise to you on a different company\u2019s site or app.\nTo not allow AWS cross-context behavioral advertising based on cookies or similar technologies, select \u201cDon't allow\u201d and \u201cSave privacy choices\u201d below, or visit an AWS site with a legally-recognized decline signal enabled, such as the Global Privacy Control. If you delete your cookies or visit this site from a different browser or device, you will need to make your selection again. For more information about cookies and how we use them, please read our [AWS Cookie Notice](https://aws.amazon.com/legal/cookies/).\nCross-context behavioral ads\nAllowDon't allow\nTo not allow all other AWS cross-context behavioral advertising, [complete this form by email](https://pulse.aws/application/ZRPLWLL6?p=0).\nFor more information about how AWS handles your information, please read the [AWS Privacy Notice](https://aws.amazon.com/privacy/).\nCancelSave privacy choices\n## Unable to save cookie preferences\nWe will only store essential cookies at this time, because we were unable to save your cookie preferences.If you want to change your cookie preferences, try again later using the link in the AWS console footer, or contact support if the problem persists.\nDismiss\n[ Skip to main content](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/#aws-page-content-main)\n[Click here to return to Amazon Web Services homepage](https://aws.amazon.com/?nc2=h_lg)\n[About AWS](https://aws.amazon.com/about-aws/?nc2=h_header) [Contact Us](https://aws.amazon.com/contact-us/?nc2=h_header) [ Support ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/) [ English ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/) [ My Account ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\n[ Sign In](https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-signin)\n[ Create an AWS Account ](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html?nc2=h_ct&src=header_signup)\n[ ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\n[ ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\nClose\nProfile \nYour profile helps improve your interactions with select AWS experiences. \n[ Login](https://auth.aws.amazon.com/sign-in)\nClose\nProfile \nYour profile helps improve your interactions with select AWS experiences. \n[ View profile](https://aws.amazon.com/profile)\n[ Log out](https://auth.aws.amazon.com/sign-out)\n  * [Amazon Q](https://aws.amazon.com/q/?nc2=h_ql_prod_l1_q)\n  * [Products](https://aws.amazon.com/products/?nc2=h_ql_prod)\n  * [Solutions](https://aws.amazon.com/solutions/?nc2=h_ql_sol)\n  * [Pricing](https://aws.amazon.com/pricing/?nc2=h_ql_pr)\n  * [Documentation](https://aws.amazon.com/documentation-overview/?nc2=h_ql_doc_do)\n  * [Learn](https://aws.amazon.com/getting-started/?nc2=h_ql_le)\n  * [Partner Network](https://aws.amazon.com/partners/?nc2=h_ql_pn)\n  * [AWS Marketplace](https://aws.amazon.com/marketplace/?nc2=h_ql_mp)\n  * [Customer Enablement](https://aws.amazon.com/customer-enablement/?nc2=h_ql_ce)\n  * [Events](https://aws.amazon.com/events/?nc2=h_ql_ev)\n  * [Explore More ](https://aws.amazon.com/contact-us/?nc2=h_ql_exm)\n\n\nClose\n  * [\u0639\u0631\u0628\u064a](https://aws.amazon.com/ar/?nc1=h_ls)\n  * [Bahasa Indonesia](https://aws.amazon.com/id/?nc1=h_ls)\n  * [Deutsch](https://aws.amazon.com/de/?nc1=h_ls)\n  * [English](https://aws.amazon.com/?nc1=h_ls)\n  * [Espa\u00f1ol](https://aws.amazon.com/es/?nc1=h_ls)\n  * [Fran\u00e7ais](https://aws.amazon.com/fr/?nc1=h_ls)\n  * [Italiano](https://aws.amazon.com/it/?nc1=h_ls)\n  * [Portugu\u00eas](https://aws.amazon.com/pt/?nc1=h_ls)\n\n\n  * [Ti\u1ebfng Vi\u1ec7t](https://aws.amazon.com/vi/?nc1=f_ls)\n  * [T\u00fcrk\u00e7e](https://aws.amazon.com/tr/?nc1=h_ls)\n  * [\u03a1\u0443\u0441\u0441\u043a\u0438\u0439](https://aws.amazon.com/ru/?nc1=h_ls)\n  * [\u0e44\u0e17\u0e22](https://aws.amazon.com/th/?nc1=f_ls)\n  * [\u65e5\u672c\u8a9e](https://aws.amazon.com/jp/?nc1=h_ls)\n  * [\ud55c\uad6d\uc5b4](https://aws.amazon.com/ko/?nc1=h_ls)\n  * [\u4e2d\u6587 (\u7b80\u4f53)](https://aws.amazon.com/cn/?nc1=h_ls)\n  * [\u4e2d\u6587 (\u7e41\u9ad4)](https://aws.amazon.com/tw/?nc1=h_ls)\n\n\nClose\n  * [My Profile](https://aws.amazon.com/profile/?nc2=h_m_mc)\n  * [Sign out of AWS Builder ID](https://auth.aws.amazon.com/sign-out/?nc2=h_m_mc)\n  * [AWS Management Console](https://console.aws.amazon.com/?nc2=h_m_mc)\n  * [Account Settings](https://console.aws.amazon.com/billing/home#/account?nc2=h_m_ma)\n  * [Billing & Cost Management](https://console.aws.amazon.com/billing/home?nc2=h_m_bc)\n  * [Security Credentials](https://console.aws.amazon.com/iam/home?nc2=h_m_sc#security_credential)\n  * [AWS Personal Health Dashboard](https://phd.aws.amazon.com/?nc2=h_m_sc)\n\n\nClose\n  * [Support Center](https://console.aws.amazon.com/support/home/?nc2=h_ql_cu)\n  * [Expert Help](https://iq.aws.amazon.com/?utm=mkt.nav)\n  * [Knowledge Center](https://repost.aws/knowledge-center/?nc2=h_m_ma)\n  * [AWS Support Overview](https://aws.amazon.com/premiumsupport/?nc2=h_m_bc)\n  * [AWS re:Post](https://repost.aws/)\n\n\n[Click here to return to Amazon Web Services homepage](https://aws.amazon.com/?nc2=h_lg)\n[ ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\n[ ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\nClose\nProfile \nYour profile helps improve your interactions with select AWS experiences. \n[ Login](https://auth.aws.amazon.com/sign-in)\nClose\nProfile \nYour profile helps improve your interactions with select AWS experiences. \n[ View profile](https://aws.amazon.com/profile)\n[ Log out](https://auth.aws.amazon.com/sign-out)\nClose\nProfile \nYour profile helps improve your interactions with select AWS experiences. \n[ View profile](https://aws.amazon.com/profile)\n[ Log out](https://auth.aws.amazon.com/sign-out)\n[ Get Started for Free ](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html?nc2=h_mobile)\n[ Contact Us ](https://aws.amazon.com/contact-us/?nc2=h_mobile)\n  * [ Products ](https://aws.amazon.com/products/?nc2=h_mo)\n  * [ Solutions ](https://aws.amazon.com/solutions/?nc2=h_mo)\n  * [ Pricing ](https://aws.amazon.com/pricing/?nc2=h_mo)\n  * [ Introduction to AWS ](https://aws.amazon.com/what-is-aws/?nc2=h_mo)\n  * [ Getting Started ](https://aws.amazon.com/getting-started/?nc2=h_mo)\n  * [ Documentation ](https://aws.amazon.com/documentation-overview/?nc2=h_mo)\n  * [ Training and Certification ](https://aws.amazon.com/training/?nc2=h_mo)\n  * [ Developer Center ](https://aws.amazon.com/developer/?nc2=h_mo)\n  * [ Customer Success ](https://aws.amazon.com/solutions/case-studies/?nc2=h_mo)\n  * [ Partner Network ](https://aws.amazon.com/partners/?nc2=h_mo)\n  * [ AWS Marketplace ](https://aws.amazon.com/marketplace/?nc2=h_mo)\n  * [ Support ](https://console.aws.amazon.com/support/home?nc2=h_ql_cu)\n  * [ AWS re:Post ](https://repost.aws/)\n  * [ Log into Console ](https://console.aws.amazon.com/console/home)\n  * [ Download the Mobile App ](https://aws.amazon.com/console/mobile/)\n\n\n[ AWS Tutorials Directory](https://aws.amazon.com/tutorials/?ref=tutorials_navbar)\n[ Getting Started Resource Center](https://aws.amazon.com/getting-started/?ref=tutorials_navbar) [ Developer Center](https://aws.amazon.com/developer/?ref=tutorials_navbar) [ IT Pro Center](https://aws.amazon.com/it-pro/?ref=tutorials_navbar) [ Architecture Center](https://aws.amazon.com/architecture/?ref=tutorials_navbar) [ Tools & SDKs](https://aws.amazon.com/developer/tools/?ref=tutorials_navbar) [ More Resources ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\nClose [ Ask an Expert on AWS re:Post](https://repost.aws/?trk=859f3c3f-c0fa-4b16-9fd5-8f05cf9273de) [ Builders Library](https://aws.amazon.com/builders-library/?ref=tutorials_navbar) [ Documentation](https://docs.aws.amazon.com/?ref=tutorials_navbar)\n#  Build, train, deploy, and monitor a machine learning model\n##  with Amazon SageMaker Studio\n[Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html) is the first fully integrated development environment (IDE) for machine learning that provides a single, web-based visual interface to perform all the steps for ML development.\nIn this tutorial, you use Amazon SageMaker Studio to build, train, deploy, and monitor an [XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) model. You cover the entire machine learning (ML) workflow from feature engineering and model training to batch and live deployments for [ML models](https://aws.amazon.com/getting-started/hands-on/build-train-deploy-machine-learning-model-sagemaker/). \nIn this tutorial, you learn how to:\n  1. Set up the Amazon SageMaker Studio Control Panel\n  2. Download a public dataset using an Amazon SageMaker Studio Notebook and upload it to Amazon S3\n  3. Create an Amazon SageMaker Experiment to track and manage training and processing jobs\n  4. Run an Amazon SageMaker Processing job to generate features from raw data\n  5. Train a model using the built-in XGBoost algorithm\n  6. Test the model performance on the test dataset using Amazon SageMaker Batch Transform\n  7. Deploy the model as an endpoint, and set up a Monitoring job to monitor the model endpoint in production for data drift.\n  8. Visualize results and monitor the model using SageMaker Model Monitor to determine any differences between the training dataset and the deployed model.\n\n\nThe model will be trained on the [UCI Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients) dataset that contains information on customer demographics, payment history, and billing history.\n**About this Tutorial**  \n---  \n**Time** | 1 hour   \n**Cost** | Less than $10  \n**Use Case** | Machine Learning  \n**Products** | [Amazon SageMaker](https://aws.amazon.com/sagemaker/)  \n**Audience** | Developer, Data Scientist  \n**Level** | Intermediate  \n**Last Updated** | February 25, 2021  \n###  Step 1. Create an AWS Account\nThe resources created and used in this tutorial are [AWS Free Tier](https://aws.amazon.com/free/) eligible. The cost of this workshop is less than $10. \n[ Sign-up for AWS ](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html?client=lightsail&fid=3BE5EA8FA64943AD-0284EED1954F5F15)\nAlready have an account? [Sign-in](https://lightsail.aws.amazon.com/ls/)\n###  Step 2. Create your Amazon SageMaker Studio Control Panel\nComplete the following steps to onboard to Amazon SageMaker Studio and set up your Amazon SageMaker Studio Control Panel.\n**Note:** For more information, see [Get Started with Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio.html) in the Amazon SageMaker documentation. \na. Sign in to the [Amazon SageMaker console](https://console.aws.amazon.com/sagemaker/). \nNote: In the top right corner, make sure to select an AWS Region where SageMaker Studio is available. For a list of Regions, see [Onboard to Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html).\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/autopilot-region-selection.8d03ac86c93954a3f9feedd3316dd7c67261af18.png)\nb. In the **Amazon SageMaker** navigation pane, choose **Amazon SageMaker Studio**. \n**Note:** If you are using Amazon SageMaker Studio for the first time, you must complete the [Studio onboarding process](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html). When onboarding, you can choose to use either AWS Single Sign-On (AWS SSO) or AWS Identity and Access Management (IAM) for authentication methods. When you use IAM authentication, you can choose either the Quick start or the Standard setup procedure. If you are unsure of which option to choose, see [Onboard to Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html) and ask your IT administrator for assistance. For simplicity, this tutorial uses the **Quick start** procedure. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/autopilot-studio-selection.95603b17fb12513f282a9b2efd6bbeee8ec548bb.png)\nc. In the **Get started** box, choose **Quick start** and specify a user name. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/autopilot-quickstart.0d81f24ac6a2d84d274fa5859942edcaf5f06815.png)\nd. For **Execution role** , choose **Create an IAM role**. In the dialog box that appears, choose **Any S3 bucket** and choose **Create role**. \nAmazon SageMaker creates a role with the required permissions and assigns it to your instance. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/autopilot-create-iam-role.8ef3ad4ffeab2feb692e76dc838993e7dcfc01cd.png)\ne. Click **Submit**. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/autopilot-execution-role.4d28d46ee847169231967a7983fc02341e463394.png)\n###  Step 3. Download the dataset\nAmazon SageMaker Studio notebooks are one-click Jupyter notebooks that contain everything you need to build and test your training scripts. SageMaker Studio also includes experiment tracking and visualization so that it\u2019s easy to manage your entire machine learning workflow in one place.\nComplete the following steps to create a SageMaker Notebook, download the dataset, and then upload the dataset to Amazon S3.\n**Note:** For more information, see [Use Amazon SageMaker Studio Notebooks](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks.html) in the Amazon SageMaker documentation. \na. In the **Amazon SageMaker Studio Control Panel** , choose **Open Studio**. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/autopilot-open-studio.779522ed5a6e213544a7533b9d7642cf175df6e5.png)\nb. In **JupyterLab** , on the **File** menu, choose **New** , then **Notebook**. In the **Select Kernel** box, choose **Python 3 (Data Science)**. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/autopilot-select-kernel.b8683aa1b83df9d2bba63f794318bcb27599d2f3.png)\nc. First, verify your version of the [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/). Copy and paste the following code block into the code cell and select **Run**. \n**Note:** While the code runs, an * appears between the square brackets. After a few seconds, the code execution completes and the * is replaced with a number. \n```\nimport boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport sys\nimport IPython\nif int(sagemaker.__version__.split('.')[0]) == 2:\n  print(\"Installing previous SageMaker Version and restarting the kernel\")\n  !{sys.executable} -m pip install sagemaker==1.72.0\n  IPython.Application.instance().kernel.do_shutdown(True)\nelse:\n  print(\"Version is good\")\n\nrole = get_execution_role()\nsess = sagemaker.Session()\nregion = boto3.session.Session().region_name\nprint(\"Region = {}\".format(region))\nsm = boto3.Session().client('sagemaker')\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-import-libraries.446f3b0434e7a5b49edfc095d5b75053b6e1bd4c.png)\nNext, import libraries. Copy and paste the following code into the code cell and select **Run**. \n```\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nfrom time import sleep, gmtime, strftime\nimport json\nimport time\n\n```\n\nCode snippet copied\nCopy\nFinally, import the experiments. Copy and paste the following code into the code cell and select **Run**. \n```\n!pip install sagemaker-experiments \nfrom sagemaker.analytics import ExperimentAnalytics\nfrom smexperiments.experiment import Experiment\nfrom smexperiments.trial import Trial\nfrom smexperiments.trial_component import TrialComponent\nfrom smexperiments.tracker import Tracker\n\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-import-libraries-experiments.c2ac30bea7afced4a181768c00e685352a17dd50.png)\nd. Define the Amazon S3 buckets and folders for the project. Copy and paste the following code into the code cell and select **Run**. \n```\nrawbucket= sess.default_bucket() # Alternatively you can use our custom bucket here. \nprefix = 'sagemaker-modelmonitor' # use this prefix to store all files pertaining to this workshop.\ndataprefix = prefix + '/data'\ntraindataprefix = prefix + '/train_data'\ntestdataprefix = prefix + '/test_data'\ntestdatanolabelprefix = prefix + '/test_data_no_label'\ntrainheaderprefix = prefix + '/train_headers'\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-s3-bucket.5af937a58471f3dad23ac1aa696fae009ef71998.png)\ne. Download the dataset and import it using the pandas library. Copy and paste the following code into a new code cell and choose **Run**.\n```\n! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\ndata = pd.read_excel('default of credit card clients.xls', header=1)\ndata = data.drop(columns = ['ID'])\ndata.head()\n\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-dataset.84eb9011450307b0c1417122cbe53dcdeef67b1f.png)\nf. Rename the last column as **Label** and extract the label column separately. For the Amazon SageMaker built-in [XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html), the label column must be the first column in the dataframe. To make that change, copy and paste the following code into a new code cell and choose **Run**. \n```\ndata.rename(columns={\"default payment next month\": \"Label\"}, inplace=True)\nlbl = data.Label\ndata = pd.concat([lbl, data.drop(columns=['Label'])], axis = 1)\ndata.head()\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-dataset-columns.0d2617872331e776ce8de8887e489c331e1cb52e.png)\ng. Upload the CSV dataset into an [Amazon S3](https://aws.amazon.com/s3/) bucket. Copy and paste the following code into a new code cell and choose **Run**.\n```\nif not os.path.exists('rawdata/rawdata.csv'):\n  !mkdir rawdata\n  data.to_csv('rawdata/rawdata.csv', index=None)\nelse:\n  pass\n# Upload the raw dataset\nraw_data_location = sess.upload_data('rawdata', bucket=rawbucket, key_prefix=dataprefix)\nprint(raw_data_location)\n```\n\nCode snippet copied\nCopy\nYou're done! The code output displays the S3 bucket URI like the following example:\n```\ns3://sagemaker-us-east-2-ACCOUNT_NUMBER/sagemaker-modelmonitor/data\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-upload-data.5a4e18b23cf127a751f7b60667a485c4a22d390c.png)\n##  Step 4: Process the data using Amazon SageMaker Processing\nIn this step, you use Amazon SageMaker Processing to pre-process the dataset, including scaling the columns and splitting the dataset into train and test data. Amazon SageMaker Processing lets you run your preprocessing, postprocessing, and model evaluation workloads on fully managed infrastructure.\nComplete the following steps to processs the data and generate features using Amazon SageMaker Processing. \n**Note:** Amazon SageMaker Processing runs on separate compute instances from your notebook. This means you can continue to experiment and run code in your notebook while the processing job is under way. This will incur additional charges for the cost of the instance which is up and running for the duration of the processing job. The instances are automatically terminated by SageMaker once the processing job completes. For pricing details, see [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/).\n**Note:** For more information, see [Process Data and Evaluate Models](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) in the Amazon SageMaker documentation.\na. Import the [scikit-learn processing](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation) container. Copy and paste the following code into a new code cell and choose **Run**.\n**Note:** Amazon SageMaker provides a managed container for scikit-learn. For more information, see [Process Data and Evaluate Models with scikit-learn](https://docs.aws.amazon.com/sagemaker/latest/dg/use-scikit-learn-processing-container.html).\n```\nfrom sagemaker.sklearn.processing import SKLearnProcessor\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                   role=role,\n                   instance_type='ml.c4.xlarge',\n                   instance_count=1)\n```\n\nCode snippet copied\nCopy\n[ ![Import scikit-learn container](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-sklearn.ec336d7c7e2bf8bed959ebf9a7060241b2470758.png)](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\nb. Copy and paste the following pre-processing script into a new cell and choose **Run**. \n```\n%%writefile preprocessing.py\nimport argparse\nimport os\nimport warnings\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.exceptions import DataConversionWarning\nfrom sklearn.compose import make_column_transformer\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nif __name__=='__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n  parser.add_argument('--random-split', type=int, default=0)\n  args, _ = parser.parse_known_args()\n  \n  print('Received arguments {}'.format(args))\n  input_data_path = os.path.join('/opt/ml/processing/input', 'rawdata.csv')\n  \n  print('Reading input data from {}'.format(input_data_path))\n  df = pd.read_csv(input_data_path)\n  df.sample(frac=1)\n  \n  COLS = df.columns\n  newcolorder = ['PAY_AMT1','BILL_AMT1'] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n  \n  split_ratio = args.train_test_split_ratio\n  random_state=args.random_split\n  \n  X_train, X_test, y_train, y_test = train_test_split(df.drop('Label', axis=1), df['Label'], \n                            test_size=split_ratio, random_state=random_state)\n  \n  preprocess = make_column_transformer(\n    (['PAY_AMT1'], StandardScaler()),\n    (['BILL_AMT1'], MinMaxScaler()),\n  remainder='passthrough')\n  \n  print('Running preprocessing and feature engineering transformations')\n  train_features = pd.DataFrame(preprocess.fit_transform(X_train), columns = newcolorder)\n  test_features = pd.DataFrame(preprocess.transform(X_test), columns = newcolorder)\n  \n  # concat to ensure Label column is the first column in dataframe\n  train_full = pd.concat([pd.DataFrame(y_train.values, columns=['Label']), train_features], axis=1)\n  test_full = pd.concat([pd.DataFrame(y_test.values, columns=['Label']), test_features], axis=1)\n  \n  print('Train data shape after preprocessing: {}'.format(train_features.shape))\n  print('Test data shape after preprocessing: {}'.format(test_features.shape))\n  \n  train_features_headers_output_path = os.path.join('/opt/ml/processing/train_headers', 'train_data_with_headers.csv')\n  \n  train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_data.csv')\n  \n  test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_data.csv')\n  \n  print('Saving training features to {}'.format(train_features_output_path))\n  train_full.to_csv(train_features_output_path, header=False, index=False)\n  print(\"Complete\")\n  \n  print(\"Save training data with headers to {}\".format(train_features_headers_output_path))\n  train_full.to_csv(train_features_headers_output_path, index=False)\n         \n  print('Saving test features to {}'.format(test_features_output_path))\n  test_full.to_csv(test_features_output_path, header=False, index=False)\n  print(\"Complete\")\n```\n\nCode snippet copied\nCopy\nc. Copy the preprocessing code over to the Amazon S3 bucket using the following code, then choose **Run**.\n```\n# Copy the preprocessing code over to the s3 bucket\ncodeprefix = prefix + '/code'\ncodeupload = sess.upload_data('preprocessing.py', bucket=rawbucket, key_prefix=codeprefix)\nprint(codeupload)\n\n```\n\nCode snippet copied\nCopy\n[ ![Copy code and specify S3 bucket](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-parsed-dataset.b049c144237ed3eba2ee9f843d6487034f6e1cc6.png)](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\nd. Specify where you want to store your training and test data after the SageMaker Processing job completes. Amazon SageMaker Processing automatically stores the data in the specified location. \n```\ntrain_data_location = rawbucket + '/' + traindataprefix\ntest_data_location = rawbucket+'/'+testdataprefix\nprint(\"Training data location = {}\".format(train_data_location))\nprint(\"Test data location = {}\".format(test_data_location))\n\n```\n\nCode snippet copied\nCopy\n[ ![Copy code and specify S3 bucket](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-parsed-dataset.b049c144237ed3eba2ee9f843d6487034f6e1cc6.png)](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\ne. Copy and paste the following code to start the Processing job. This code starts the job by calling _sklearn_processor.run_ and extracts some optional metadata about the processing job, such as where the training and test outputs were stored. \n```\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\nsklearn_processor.run(code=codeupload,\n           inputs=[ProcessingInput(\n            source=raw_data_location,\n            destination='/opt/ml/processing/input')],\n           outputs=[ProcessingOutput(output_name='train_data',\n                        source='/opt/ml/processing/train',\n                destination='s3://' + train_data_location),\n                ProcessingOutput(output_name='test_data',\n                        source='/opt/ml/processing/test',\n                        destination=\"s3://\"+test_data_location),\n                ProcessingOutput(output_name='train_data_headers',\n                        source='/opt/ml/processing/train_headers',\n                        destination=\"s3://\" + rawbucket + '/' + prefix + '/train_headers')],\n           arguments=['--train-test-split-ratio', '0.2']\n           )\npreprocessing_job_description = sklearn_processor.jobs[-1].describe()\noutput_config = preprocessing_job_description['ProcessingOutputConfig']\nfor output in output_config['Outputs']:\n  if output['OutputName'] == 'train_data':\n    preprocessed_training_data = output['S3Output']['S3Uri']\n  if output['OutputName'] == 'test_data':\n    preprocessed_test_data = output['S3Output']['S3Uri']\n\n\n```\n\nCode snippet copied\nCopy\nNote the locations of the code, train and test data in the outputs provided to the processor. Also, note the arguments provided to the processing scripts. \n[ ![Start processing job](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-processor.cd7cba4b42a5bccb909283f6f0d18753b4828cf7.png)](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\n##  Step 5: Create an Amazon SageMaker Experiment\nNow that you have downloaded and staged your dataset in Amazon S3, you can create an Amazon SageMaker Experiment. An experiment is a collection of processing and training jobs related to the same machine learning project. Amazon SageMaker Experiments automatically manages and tracks your training runs for you. \nComplete the following steps to create a new experiment.\n**Note:** For more information, see [Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) in the Amazon SageMaker documentation. \na. Copy and paste the following code to create an experiment named _Build-train-deploy-_. \n```\n# Create a SageMaker Experiment\ncc_experiment = Experiment.create(\n  experiment_name=f\"Build-train-deploy-{int(time.time())}\", \n  description=\"Predict credit card default from payments data\", \n  sagemaker_boto_client=sm)\nprint(cc_experiment)\n\n```\n\nCode snippet copied\nCopy\nEvery training job is logged as a _trial_. Each trial is an iteration of your end-to-end training job. In addition to the training job, it can also track pre-processing and post-processing jobs as well as datasets and other metadata. A single experiment can include multiple trials which makes it easy for you to track multiple iterations over time within the Amazon SageMaker Studio Experiments pane.\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-create-experiment.5dc4402f2042c47cbc266aeb544bc9e3b9bd9a1c.png)\nb. Copy and paste the following code to track your pre-processing job under Experiments as well as a step in the training pipeline.\n```\n# Start Tracking parameters used in the Pre-processing pipeline.\nwith Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n  tracker.log_parameters({\n    \"train_test_split_ratio\": 0.2,\n    \"random_state\":0\n  })\n  # we can log the s3 uri to the dataset we just uploaded\n  tracker.log_input(name=\"ccdefault-raw-dataset\", media_type=\"s3/uri\", value=raw_data_location)\n  tracker.log_input(name=\"ccdefault-train-dataset\", media_type=\"s3/uri\", value=train_data_location)\n  tracker.log_input(name=\"ccdefault-test-dataset\", media_type=\"s3/uri\", value=test_data_location)\n\n```\n\nCode snippet copied\nCopy\nc. View the details of the experiment: In the **Experiments** pane, right-click the experiment named **Build-train-deploy-** and choose **Open in trial components list**.\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-trial-components.a29bc386558de92b10e5681c9f17ab6f58578495.png)\nd. Copy and paste the following code and choose **Run**. Then, take a closer look at the code:\nTo train an XGBoost classifier, you first import the [XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) container maintained by Amazon SageMaker. Then, you log the training run under a **Trial** so SageMaker Experiments can track it under a **Trial** name. The pre-processing job is included under the same trial name since it is part of the pipeline. Next, create a SageMaker Estimator object, which automatically provisions the underlying instance type of your choosing, copies over the training data from the specified output location from the processing job, trains the model, and outputs the model artifacts.\n```\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost', '1.0-1')\ns3_input_train = sagemaker.s3_input(s3_data='s3://' + train_data_location, content_type='csv')\npreprocessing_trial_component = tracker.trial_component\ntrial_name = f\"cc-default-training-job-{int(time.time())}\"\ncc_trial = Trial.create(\n    trial_name=trial_name, \n      experiment_name=cc_experiment.experiment_name,\n    sagemaker_boto_client=sm\n  )\ncc_trial.add_trial_component(preprocessing_trial_component)\ncc_training_job_name = \"cc-training-job-{}\".format(int(time.time()))\nxgb = sagemaker.estimator.Estimator(container,\n                  role, \n                  train_instance_count=1, \n                  train_instance_type='ml.m4.xlarge',\n                  train_max_run=86400,\n                  output_path='s3://{}/{}/models'.format(rawbucket, prefix),\n                  sagemaker_session=sess) # set to true for distributed training\nxgb.set_hyperparameters(max_depth=5,\n            eta=0.2,\n            gamma=4,\n            min_child_weight=6,\n            subsample=0.8,\n            verbosity=0,\n            objective='binary:logistic',\n            num_round=100)\nxgb.fit(inputs = {'train':s3_input_train},\n    job_name=cc_training_job_name,\n    experiment_config={\n      \"TrialName\": cc_trial.trial_name, #log training job in Trials for lineage\n      \"TrialComponentDisplayName\": \"Training\",\n    },\n    wait=True,\n  )\ntime.sleep(2)\n\n```\n\nCode snippet copied\nCopy\nThe training job will take about 70 seconds to complete. You should see the following output.\n```\nCompleted - Training job completed\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-get-image.20072e3101167eb9b0dc9b9186788b40cc24e5fd.png)\ne. In the left toolbar, choose **Experiment**. Right-click the **Build-train-deploy-** experiment and choose **Open in trial components list**. Amazon SageMaker Experiments captures all the runs including any failed training runs. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-job-list.f741f928cbf3c41cfb829696e624a30cbad11d9f.png)\nf. Right-click one of the completed **Training jobs** and choose **Open in Trial Details** to explore the associated metadata with the training job. \n**Note:** You may need to refresh the page to see the latest results. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-training-metadata.57bcdc852f9f9f450dbb3a1d2a675ee9a27dae90.png)\n##  Step 6: Deploy the model for offline inference\nIn your preprocessing step, you generated some test data. In this step, you generate offline or batch inference from the trained model to evaluate the model performance on unseen test data. \nComplete the following steps to deploy the model for offline inference.\n**Note:** For more information, see [Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) in the Amazon SageMaker documentation. \na. Copy and paste the following code and choose **Run**.\nThis step copies the test dataset over from the Amazon S3 location into your local folder. \n```\ntest_data_path = 's3://' + test_data_location + '/test_data.csv'\n! aws s3 cp $test_data_path .\n\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-copy-dataset.cba162066d4319117deb98a5d9358eda159ca83b.png)\nb. Copy and paste the following code and choose **Run**.\n```\ntest_full = pd.read_csv('test_data.csv', names = [str(x) for x in range(len(data.columns))])\ntest_full.head()\n\n```\n\nCode snippet copied\nCopy\nc. Copy and paste the following code and choose **Run**. This step extracts the label column. \n```\nlabel = test_full['0'] \n```\n\nCode snippet copied\nCopy\nd. Copy and paste the following code and choose **Run** to create the Batch Transform job. Then, take a closer look at the code:\nLike the training job, SageMaker provisions all the underlying resources, copies over the trained model artifacts, sets up a Batch endpoint locally, copies over the data, and runs inferences on the data and pushes the outputs to Amazon S3. Note that by setting the **input_filter** , you are letting Batch Transform know to neglect the first column in the test data which is the label column. \n```\n%%time\nsm_transformer = xgb.transformer(1, 'ml.m5.xlarge', accept = 'text/csv')\n# start a transform job\nsm_transformer.transform(test_data_path, split_type='Line', input_filter='$[1:]', content_type='text/csv')\nsm_transformer.wait()\n\n```\n\nCode snippet copied\nCopy\nThe Batch Transform job will take about 4 minutes to complete after which you can evaluate the model results. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-input-filter.36e2fc6554ce24555355d3c3e0d6bfa429010e92.png)\ne. Copy and run the following code to evaluate the model metrics. Then, take a closer look at the code:\nFirst, you define a function that pulls the output of the Batch Transform job, which is contained in a file with a **.out** extension from the Amazon S3 bucket. Then, you extract the predicted labels into a dataframe and append the true labels to this dataframe. \n```\nimport json\nimport io\nfrom urllib.parse import urlparse\ndef get_csv_output_from_s3(s3uri, file_name):\n  parsed_url = urlparse(s3uri)\n  bucket_name = parsed_url.netloc\n  prefix = parsed_url.path[1:]\n  s3 = boto3.resource('s3')\n  obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_name))\n  return obj.get()[\"Body\"].read().decode('utf-8')\noutput = get_csv_output_from_s3(sm_transformer.output_path, 'test_data.csv.out')\noutput_df = pd.read_csv(io.StringIO(output), sep=\",\", header=None)\noutput_df.head(8)\noutput_df['Predicted']=np.round(output_df.values)\noutput_df['Label'] = label\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nconfusion_matrix = pd.crosstab(output_df['Predicted'], output_df['Label'], rownames=['Actual'], colnames=['Predicted'], margins = True)\nconfusion_matrix\n\n```\n\nCode snippet copied\nCopy\nYou should see an output similar to the image, which shows the total number of **Predicted** True and False values compared to the **Actual** values. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-output-predicted-actual.07c3626191859ac332d35a22fbfe0ce6fdd0dadd.png)\nf. Use the following code to extract both the baseline model accuracy and the model accuracy. \n**Note:** A helpful model for the baseline accuracy can be the fraction of non-default cases. A model that always predicts that a user will not default has that accuracy. \n```\nprint(\"Baseline Accuracy = {}\".format(1- np.unique(data['Label'], return_counts=True)[1][1]/(len(data['Label']))))\nprint(\"Accuracy Score = {}\".format(accuracy_score(label, output_df['Predicted'])))\n\n```\n\nCode snippet copied\nCopy\nThe results show that a simple model can already beat the baseline accuracy. In order to improve the results, you can tune the hyperparameters. You can use hyperparameter optimization (HPO) on SageMaker for automatic model tuning. To learn more, see [How Hyperparameter Tuning Works](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html). \n**Note:** Although it is not included in this tutorial, you also have the option of including Batch Transform as part of your trial. When you call the .transform function, simply pass in the experiment_config as you did for the Training job. Amazon SageMaker automatically associates the Batch Transform as a trial component.\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-accuracy.06e79010a31766ea2511406de7618f2f1f3c1960.png)\n##  Step 7: Deploy the model as an endpoint and set up data capture\nIn this step, you deploy the model as a RESTful HTTPS endpoint to serve live inferences. Amazon SageMaker automatically handles the model hosting and creation of the endpoint for you. \nComplete the following steps to deploy the model as an endpoint and set up data capture.\n**Note:** For more information, see [Deploy Models for Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html) in the Amazon SageMaker documentation. \na. Copy and paste the following code and choose **Run**.\n```\nfrom sagemaker.model_monitor import DataCaptureConfig\nfrom sagemaker import RealTimePredictor\nfrom sagemaker.predictor import csv_serializer\nsm_client = boto3.client('sagemaker')\nlatest_training_job = sm_client.list_training_jobs(MaxResults=1,\n                        SortBy='CreationTime',\n                        SortOrder='Descending')\ntraining_job_name=TrainingJobName=latest_training_job['TrainingJobSummaries'][0]['TrainingJobName']\ntraining_job_description = sm_client.describe_training_job(TrainingJobName=training_job_name)\nmodel_data = training_job_description['ModelArtifacts']['S3ModelArtifacts']\ncontainer_uri = training_job_description['AlgorithmSpecification']['TrainingImage']\n# create a model.\ndef create_model(role, model_name, container_uri, model_data):\n  return sm_client.create_model(\n    ModelName=model_name,\n    PrimaryContainer={\n    'Image': container_uri,\n    'ModelDataUrl': model_data,\n    },\n    ExecutionRoleArn=role)\n  \ntry:\n  model = create_model(role, training_job_name, container_uri, model_data)\nexcept Exception as e:\n    sm_client.delete_model(ModelName=training_job_name)\n    model = create_model(role, training_job_name, container_uri, model_data)\n    \nprint('Model created: '+model['ModelArn'])\n\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-create-model-created.bbeba188f4609843e78c7430f1f499eddcc1b30d.png)\nb. To specify the data configuration settings, copy and paste the following code and choose **Run**.\nThis code tells SageMaker to capture 100% of the inference payloads received by the endpoint, capture both inputs and outputs, and also note the input content type as csv.\n```\ns3_capture_upload_path = 's3://{}/{}/monitoring/datacapture'.format(rawbucket, prefix)\ndata_capture_configuration = {\n  \"EnableCapture\": True,\n  \"InitialSamplingPercentage\": 100,\n  \"DestinationS3Uri\": s3_capture_upload_path,\n  \"CaptureOptions\": [\n    { \"CaptureMode\": \"Output\" },\n    { \"CaptureMode\": \"Input\" }\n  ],\n  \"CaptureContentTypeHeader\": {\n    \"CsvContentTypes\": [\"text/csv\"],\n    \"JsonContentTypes\": [\"application/json\"]}}\n\n```\n\nCode snippet copied\nCopy\nc. Copy and paste the following code and choose **Run**. This step creates an endpoint configuration and deploys the endpoint. In the code, you can specify instance type and whether you want to send all the traffic to this endpoint, etc. \n```\ndef create_endpoint_config(model_config, data_capture_config): \n  return sm_client.create_endpoint_config(\n                        EndpointConfigName=model_config,\n                        ProductionVariants=[\n                            {\n                              'VariantName': 'AllTraffic',\n                              'ModelName': model_config,\n                              'InitialInstanceCount': 1,\n                              'InstanceType': 'ml.m4.xlarge',\n                              'InitialVariantWeight': 1.0,\n                        },\n                          \n                          ],\n                        DataCaptureConfig=data_capture_config\n                        )\n\n\ntry:\n  endpoint_config = create_endpoint_config(training_job_name, data_capture_configuration)\nexcept Exception as e:\n  sm_client.delete_endpoint_config(EndpointConfigName=endpoint)\n  endpoint_config = create_endpoint_config(training_job_name, data_capture_configuration)\nprint('Endpoint configuration created: '+ endpoint_config['EndpointConfigArn'])\n\n```\n\nCode snippet copied\nCopy\nd. Copy and paste the following code and choose **Run** to create the endpoint. \n```\n# Enable data capture, sampling 100% of the data for now. Next we deploy the endpoint in the correct VPC.\nendpoint_name = training_job_name\ndef create_endpoint(endpoint_name, config_name):\n  return sm_client.create_endpoint(\n                  EndpointName=endpoint_name,\n                  EndpointConfigName=training_job_name\n                )\n\ntry:\n  endpoint = create_endpoint(endpoint_name, endpoint_config)\nexcept Exception as e:\n  sm_client.delete_endpoint(EndpointName=endpoint_name)\n  endpoint = create_endpoint(endpoint_name, endpoint_config)\nprint('Endpoint created: '+ endpoint['EndpointArn'])\n\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-endpoint-created.462d228b19d0aa86c03bb1a72d1af56a71bb34f5.png)\ne. In the left toolbar, choose **Endpoints**. The **Endpoints** list displays all of the endpoints in service.\nNotice the _build-train-deploy_ endpoint shows a status of _Creating_. To deploy the model, Amazon SageMaker must first copy your model artifacts and inference image onto the instance and set up a HTTPS endpoint to inferface with client applications or RESTful APIs. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-endpoint-status.52db936293c7da09a14e87bb0323ff66cd07c721.png)\nOnce the endpoint is created, the status changes to _InService_. (Note that creating an endpoint may take about 5-10 minutes.) \n**Note:** You may need to click Refresh to get the updated status.\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-endpoints-inservice.e905d3fa36dd3a238515588c6fd57cfe1928e6d3.png)\nf. In the **JupyterLab Notebook** , copy and run the following code to take a sample of the test dataset. This code takes the first 10 rows. \n```\n!head -10 test_data.csv > test_sample.csv\n```\n\nCode snippet copied\nCopy\ng. Run the following code to send some inference requests to this endpoint.\n**Note:** If you specified a different endpoint name, you will need to replace _endpoint_ below with your endpoint name. \n```\nfrom sagemaker import RealTimePredictor\nfrom sagemaker.predictor import csv_serializer\npredictor = RealTimePredictor(endpoint=endpoint_name, content_type = 'text/csv')\nwith open('test_sample.csv', 'r') as f:\n  for row in f:\n    payload = row.rstrip('\\n')\n    response = predictor.predict(data=payload[2:])\n    sleep(0.5)\nprint('done!')\n\n```\n\nCode snippet copied\nCopy\nh. Run the following code to verify that Model Monitor is correctly capturing the incoming data.\nIn the code, the _current_endpoint_capture_prefix_ captures the directory path where your ModelMonitor outputs are stored. Navigate to your Amazon S3 bucket, to see if the prediction requests are being captured. Note that this location should match the _s3_capture_upload_path_ in the code above.\n```\n# Extract the captured json files.\ndata_capture_prefix = '{}/monitoring'.format(prefix)\ns3_client = boto3.Session().client('s3')\ncurrent_endpoint_capture_prefix = '{}/datacapture/{}/AllTraffic'.format(data_capture_prefix, endpoint_name)\nprint(current_endpoint_capture_prefix)\nresult = s3_client.list_objects(Bucket=rawbucket, Prefix=current_endpoint_capture_prefix)\ncapture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\nprint(\"Found Capture Files:\")\nprint(\"\\n \".join(capture_files))\n\ncapture_files[0]\n\n```\n\nCode snippet copied\nCopy\nThe captured output indicates that data capture is configured and saving the incoming requests. \n**Note:** If you initially see a Null response, the data may not have been synchronously loaded onto the Amazon S3 path when you first initialized the data capture. Wait about a minute and try again.\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-extract-json.b8d96a4ad7f35a372350477cd0d44732d817dd35.png)\ni. Run the following code to extract the content of one of the json files and view the captured outputs. \n```\n# View contents of the captured file.\ndef get_obj_body(bucket, obj_key):\n  return s3_client.get_object(Bucket=rawbucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\ncapture_file = get_obj_body(rawbucket, capture_files[0])\nprint(json.dumps(json.loads(capture_file.split('\\n')[5]), indent = 2, sort_keys =True))\n\n```\n\nCode snippet copied\nCopy\nThe output indicates that data capture is capturing both the input payload and the output of the model. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-data-capture.ce3385bec24cc4418bbd5619bfacbda0b9172362.png)\n##  Step 8: Monitor the endpoint with SageMaker Model Monitor\nIn this step, you enable SageMaker Model Monitor to monitor the deployed endpoint for data drift. To do so, you compare the payload and outputs sent to the model against a baseline and determine whether there is any drift in the input data, or the label. \nComplete the following steps to enable model monitoring.\n**Note:** For more information, see [Amazon SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html) in the Amazon SageMaker documentation. \na. Run the following code to create a folder in your Amazon S3 bucket to store the outputs of the Model Monitor. \nThis code creates two folders: one folder stores the baseline data which you used for training your model; the second folder stores any violations from that baseline.\n```\nmodel_prefix = prefix + \"/\" + endpoint_name\nbaseline_prefix = model_prefix + '/baselining'\nbaseline_data_prefix = baseline_prefix + '/data'\nbaseline_results_prefix = baseline_prefix + '/results'\nbaseline_data_uri = 's3://{}/{}'.format(rawbucket,baseline_data_prefix)\nbaseline_results_uri = 's3://{}/{}'.format(rawbucket, baseline_results_prefix)\ntrain_data_header_location = \"s3://\" + rawbucket + '/' + prefix + '/train_headers'\nprint('Baseline data uri: {}'.format(baseline_data_uri))\nprint('Baseline results uri: {}'.format(baseline_results_uri))\nprint(train_data_header_location)\n\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-monitor-baseline.abcd340eba0425223dc460d618c529d5afc42ef7.png)\nb. Run the following code to set up a baseline job for Model Monitor to capture the statistics of the training data. To do this, Model Monitor uses the [deequ](https://github.com/awslabs/deequ) library built on top of Apache Spark for conducting unit tests on data. \n```\nfrom sagemaker.model_monitor import DefaultModelMonitor\nfrom sagemaker.model_monitor.dataset_format import DatasetFormat\nmy_default_monitor = DefaultModelMonitor(\n  role=role,\n  instance_count=1,\n  instance_type='ml.m5.xlarge',\n  volume_size_in_gb=20,\n  max_runtime_in_seconds=3600)\nmy_default_monitor.suggest_baseline(\n  baseline_dataset=os.path.join(train_data_header_location, 'train_data_with_headers.csv'),\n  dataset_format=DatasetFormat.csv(header=True),\n  output_s3_uri=baseline_results_uri,\n  wait=True\n)\n\n```\n\nCode snippet copied\nCopy\nModel Monitor sets up a separate instance, copies over the training data, and generates some statistics. The service generates a lot of Apache Spark logs, which you can ignore. Once the job is completed, you will see a _Spark job completed_ output. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-baseline-suggestion-job.94077d3556b0cec3439b0fae3cdd90e02ba6308d.png)\nc. Run the following code to look at the outputs generated by the baseline job. \n```\ns3_client = boto3.Session().client('s3')\nresult = s3_client.list_objects(Bucket=rawbucket, Prefix=baseline_results_prefix)\nreport_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\nprint(\"Found Files:\")\nprint(\"\\n \".join(report_files))\nbaseline_job = my_default_monitor.latest_baselining_job\nschema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\nschema_df\n\n```\n\nCode snippet copied\nCopy\nYou will see two files: **constraints.json** and **statistics.json**. Next, dive deeper into their contents.\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-found-files.140817729d769be046cdcce8e9d077fd90cbb094.png)\nThe code above converts the json output in /**statistics.json** into a pandas dataframe. Note how the deequ library infers the data type of the column, the presence or absence of Null or missing values, and statistical parameters such as the mean, min, max, sum, standard deviation, and sketch parameters for an input data stream.\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-pandas-dataframe.05626b92265719c2560ca0fea5cd8b764e82c033.png)\nLikewise, the **constraints.json** file consists of a number of constraints the training dataset obeys such as non-negativity of values, and the data type of the feature field.\n```\nconstraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\nconstraints_df\n\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-constraints.f6ff33dcef9c1683e56876735748e2239936e443.png)\nd. Run the following code to set up the frequency for endpoint monitoring.\nYou can specify daily or hourly. This code specifies an hourly frequency, but you may want to change this for production applications as hourly frequency will generate a lot of data. Model Monitor will produce a report consisting of all the violations it finds. \n```\nreports_prefix = '{}/reports'.format(prefix)\ns3_report_path = 's3://{}/{}'.format(rawbucket,reports_prefix)\nprint(s3_report_path)\nfrom sagemaker.model_monitor import CronExpressionGenerator\nfrom time import gmtime, strftime\nmon_schedule_name = 'Built-train-deploy-model-monitor-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nmy_default_monitor.create_monitoring_schedule(\n  monitor_schedule_name=mon_schedule_name,\n  endpoint_input=predictor.endpoint,\n  output_s3_uri=s3_report_path,\n  statistics=my_default_monitor.baseline_statistics(),\n  constraints=my_default_monitor.suggested_constraints(),\n  schedule_cron_expression=CronExpressionGenerator.hourly(),\n  enable_cloudwatch_metrics=True,\n)\n\n```\n\nCode snippet copied\nCopy\nNote that this code enables Amazon CloudWatch Metrics, which instructs Model Monitor to send outputs to CloudWatch. You can use this approach to trigger alarms using CloudWatch Alarms to let engineers or admins know when data drift has been detected. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-monitoring-schedule.18c63636ce89f1afef2d023f7661170c17de0694.png)\n##  Step 9: Test SageMaker Model Monitor performance\nIn this step, you evaluate Model Monitor against some sample data. Instead of sending the test payload as is, you modify the distribution of several features in the test payload to test that Model Monitor can detect the change. \nComplete the following steps to test the Model Monitor performance.\n**Note:** For more information, see [Amazon SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html) in the Amazon SageMaker documentation. \na. Run the following code to import the test data and generate some modified sample data.\n```\nCOLS = data.columns\ntest_full = pd.read_csv('test_data.csv', names = ['Label'] +['PAY_AMT1','BILL_AMT1'] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n)\ntest_full.head()\n\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-test-model-monitor.6e99408ff7c5f40815eae5f7da1a76fc1e876085.png)\nb. Run the following code to change a few columns. Note the differences marked in red in the image here from the previous step. Drop the label column and save the modified sample test data. \n```\nfaketestdata = test_full\nfaketestdata['EDUCATION'] = -faketestdata['EDUCATION'].astype(float)\nfaketestdata['BILL_AMT2']= (faketestdata['BILL_AMT2']//10).astype(float)\nfaketestdata['AGE']= (faketestdata['AGE']-10).astype(float)\nfaketestdata.head()\nfaketestdata.drop(columns=['Label']).to_csv('test-data-input-cols.csv', index = None, header=None)\n\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-changed-data.9f7dd607cda6a7ab60a657e1b7aec16370ace339.png)\nc. Run the following code to repeatedly invoke the endpoint with this modified dataset.\n```\nfrom threading import Thread\nruntime_client = boto3.client('runtime.sagemaker')\n# (just repeating code from above for convenience/ able to run this section independently)\ndef invoke_endpoint(ep_name, file_name, runtime_client):\n  with open(file_name, 'r') as f:\n    for row in f:\n      payload = row.rstrip('\\n')\n      response = runtime_client.invoke_endpoint(EndpointName=ep_name,\n                     ContentType='text/csv', \n                     Body=payload)\n      time.sleep(1)\n      \ndef invoke_endpoint_forever():\n  while True:\n    invoke_endpoint(endpoint, 'test-data-input-cols.csv', runtime_client)\n    \nthread = Thread(target = invoke_endpoint_forever)\nthread.start()\n# Note that you need to stop the kernel to stop the invocations\n\n```\n\nCode snippet copied\nCopy\nd. Run the following code to check the status of the Model Monitor job.\n```\ndesc_schedule_result = my_default_monitor.describe_schedule()\nprint('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))\n\n```\n\nCode snippet copied\nCopy\nYou should see an output of _Schedule status: Scheduled_\ne. Run the following code to check every 10 minutes if any monitoring outputs have been generated. Note that the first job may run with a buffer of about 20 minutes. \n```\nmon_executions = my_default_monitor.list_executions()\nprint(\"We created ahourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\\nWe will have to wait till we hit the hour...\")\nwhile len(mon_executions) == 0:\n  print(\"Waiting for the 1st execution to happen...\")\n  time.sleep(600)\n  mon_executions = my_default_monitor.list_executions()\n\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-check-outputs.ff1fd4f19f425b4c550173751822b43d1d88c224.png)\nf. In the left toolbar of **Amazon SageMaker Studio** , choose **Endpoints**. Right-click the **build-train-deploy** endpoint and choose **Describe Endpoint**.\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-describe-endpoint.9fce4a1fc1edf1d313f047fcf3d7a08c3436342a.png)\ng. Choose **Monitoring job history**. Notice that the **Monitoring status** shows **In progress**.\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-job-in-progress.71f53f2b4495a140a46185926cec91d755081e84.png)\nOnce the job is complete, the **Monitoring status** displays **Issue found** (for any issues found). \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-job-complete.b737bd9c81ebba5e7965cfdcb503383e1f39eff2.png)\nh. Double-click the issue to view details. You can see that Model Monitor detected large baseline drifts in the **EDUCATION** and **BILL_AMT2** fields that you previously modified.\nModel Monitor also detected some differences in data types in two other fields. The training data consists of integer labels, but the XGBoost model predicts a probability score. Therefore, Model Monitor reported a mismatch. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-monitoring-details.b7ec04489c35ab8e250881100cb8783d084fe1b4.png)\ni. In your **JupyterLab Notebook** , run the following cells to see the output from Model Monitor.\n```\nlatest_execution = mon_executions[-1] # latest execution's index is -1, second to last is -2 and so on..\ntime.sleep(60)\nlatest_execution.wait(logs=False)\nprint(\"Latest execution status: {}\".format(latest_execution.describe()['ProcessingJobStatus']))\nprint(\"Latest execution result: {}\".format(latest_execution.describe()['ExitMessage']))\nlatest_job = latest_execution.describe()\nif (latest_job['ProcessingJobStatus'] != 'Completed'):\n    print(\"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\")\n\n```\n\nCode snippet copied\nCopy\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-job-complete-in-notebook.02e6e572fc344c036e6eb17bf5f89544474d04e6.png)\nj. Run the following code to view the reports generated by Model Monitor. \n```\nreport_uri=latest_execution.output.destination\nprint('Report Uri: {}'.format(report_uri))\nfrom urllib.parse import urlparse\ns3uri = urlparse(report_uri)\nreport_bucket = s3uri.netloc\nreport_key = s3uri.path.lstrip('/')\nprint('Report bucket: {}'.format(report_bucket))\nprint('Report key: {}'.format(report_key))\ns3_client = boto3.Session().client('s3')\nresult = s3_client.list_objects(Bucket=rawbucket, Prefix=report_key)\nreport_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\nprint(\"Found Report Files:\")\nprint(\"\\n \".join(report_files))\n\n```\n\nCode snippet copied\nCopy\nYou can see that in addition to **statistics.json** and **constraints.json** , there is a new file generated named **constraint_violations.json**. The contents of this file were displayed above in Amazon SageMaker Studio (Step g).\n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-constraints-violation.bb9c8bfcfc663443b50f70ba6445c25bdd728c59.png)\n**Note:** Once you set up data capture, Amazon SageMaker Studio automatically creates a notebook for you that contains the code above to run monitoring jobs. To access the notebook, right-click the endpoint and choose **Describe Endpoint**. On the **Monitoring results** tab, choose **Enable Monitoring**. This step automatically opens a Jupyter notebook containing the code you authored above. \n![](https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-model-monitor.3d2d7aa2dbe60180cffe0fb4ba2dedb6a79c5273.png)\n###  Step 10. Clean up\nIn this step, you terminate the resources you used in this lab. \n**Important:** Terminating resources that are not actively being used reduces costs and is a best practice. Not terminating your resources will result in charges to your account.\na. **Delete monitoring schedules:** In your Jupyter notebook, copy and paste the following code and choose **Run**.\n**Note:** You cannot delete the Model Monitor endpoint until all of the monitoring jobs associated with the endpoint are deleted.\n```\nmy_default_monitor.delete_monitoring_schedule()\ntime.sleep(10) # actually wait for the deletion\n\n```\n\nCode snippet copied\nCopy\nb. **Delete your endpoint:** In your Jupyter notebook, copy and paste the following code and choose **Run**.\n**Note:** Make sure you have first deleted all monitoring jobs associated with the endpoint.\n```\nsm.delete_endpoint(EndpointName = endpoint_name)\n```\n\nCode snippet copied\nCopy\nIf you want to clean up all training artifacts (models, preprocessed data sets, etc.), copy and paste the following code into your code cell and choose **Run**.\n**Note:** Make sure to replace ACCOUNT_NUMBER with your account number. \n```\n%%sh\naws s3 rm --recursive s3://sagemaker-us-east-2-ACCOUNT_NUMBER/sagemaker-modelmonitor/data\n\n```\n\nCode snippet copied\nCopy\n##  Conclusion\nCongratulations! You created, trained, deployed, and monitored a machine learning model with Amazon SageMaker Studio. \nYou can continue your machine learning journey with SageMaker by following the next steps section below. \n###  Was this page helpful?\n[ ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\n[ ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\n[ Feedback ](https://docs-feedback.aws.amazon.com/feedback.jsp?feedback_destination_id=29e3b6b0-4a70-4d70-a6ec-6459755d2383&topic_url=https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio)\n[ ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\n[ ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\n[ ](https://docs-feedback.aws.amazon.com/feedback.jsp?feedback_destination_id=29e3b6b0-4a70-4d70-a6ec-6459755d2383&topic_url=https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio \"Feedback\")\nClose\nThank you for your feedback \nFeedback helps us improve our experience. If you would like to share more details on the feedback, please click the feedback button below. \n[ Feedback ](https://docs-feedback.aws.amazon.com/feedback.jsp?feedback_destination_id=29e3b6b0-4a70-4d70-a6ec-6459755d2383&topic_url=https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio)\n[ Explore example ML notebooks  Explore example notebooks that show how to apply machine learning, deep learning and reinforcement learning in Amazon SageMaker.  ](https://github.com/awslabs/amazon-sagemaker-examples)\n[ **Take a tour of Amazon SageMaker Studio** See the Amazon SageMaker UI overview.  ](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-ui.html)\n[ Learn how to build, train, and deploy ML models automatically with Amazon SageMaker Autopilot  Complete the 10-minute tutorial to create a machine learning model automatically with Amazon SageMaker Autopilot.  ](https://aws.amazon.com/getting-started/hands-on/create-machine-learning-model-automatically-sagemaker-autopilot/)\n[ Sign In to the Console ](https://console.aws.amazon.com/console/home?nc1=f_ct&src=footer-signin-mobile)\n###  Learn About AWS\n  * [What Is AWS?](https://aws.amazon.com/what-is-aws/?nc1=f_cc)\n  * [What Is Cloud Computing?](https://aws.amazon.com/what-is-cloud-computing/?nc1=f_cc)\n  * [AWS Accessibility](https://aws.amazon.com/accessibility/?nc1=f_cc)\n  * [What Is DevOps?](https://aws.amazon.com/devops/what-is-devops/?nc1=f_cc)\n  * [What Is a Container?](https://aws.amazon.com/containers/?nc1=f_cc)\n  * [What Is a Data Lake?](https://aws.amazon.com/what-is/data-lake/?nc1=f_cc)\n  * [What is Artificial Intelligence (AI)?](https://aws.amazon.com/what-is/artificial-intelligence/?nc1=f_cc)\n  * [What is Generative AI?](https://aws.amazon.com/what-is/generative-ai/?nc1=f_cc)\n  * [What is Machine Learning (ML)?](https://aws.amazon.com/what-is/machine-learning/?nc1=f_cc)\n  * [AWS Cloud Security](https://aws.amazon.com/security/?nc1=f_cc)\n  * [What's New](https://aws.amazon.com/new/?nc1=f_cc)\n  * [Blogs](https://aws.amazon.com/blogs/?nc1=f_cc)\n  * [Press Releases](https://press.aboutamazon.com/press-releases/aws \"Press Releases\")\n\n\n###  Resources for AWS\n  * [Getting Started](https://aws.amazon.com/getting-started/?nc1=f_cc)\n  * [Training and Certification](https://aws.amazon.com/training/?nc1=f_cc)\n  * [AWS Trust Center](https://aws.amazon.com/trust-center/?nc1=f_cc)\n  * [AWS Solutions Library](https://aws.amazon.com/solutions/?nc1=f_cc)\n  * [Architecture Center](https://aws.amazon.com/architecture/?nc1=f_cc)\n  * [Product and Technical FAQs](https://aws.amazon.com/faqs/?nc1=f_dr)\n  * [Analyst Reports](https://aws.amazon.com/resources/analyst-reports/?nc1=f_cc)\n  * [AWS Partners](https://aws.amazon.com/partners/work-with-partners/?nc1=f_dr)\n\n\n###  Developers on AWS\n  * [Developer Center](https://aws.amazon.com/developer/?nc1=f_dr)\n  * [SDKs & Tools](https://aws.amazon.com/developer/tools/?nc1=f_dr)\n  * [.NET on AWS](https://aws.amazon.com/developer/language/net/?nc1=f_dr)\n  * [Python on AWS](https://aws.amazon.com/developer/language/python/?nc1=f_dr)\n  * [Java on AWS](https://aws.amazon.com/developer/language/java/?nc1=f_dr)\n  * [PHP on AWS](https://aws.amazon.com/developer/language/php/?nc1=f_cc)\n  * [JavaScript on AWS](https://aws.amazon.com/developer/language/javascript/?nc1=f_dr)\n\n\n###  Help\n  * [Contact Us](https://aws.amazon.com/contact-us/?nc1=f_m)\n  * [Get Expert Help](https://iq.aws.amazon.com/?utm=mkt.foot/?nc1=f_m)\n  * [File a Support Ticket](https://console.aws.amazon.com/support/home/?nc1=f_dr)\n  * [AWS re:Post](https://repost.aws/?nc1=f_dr)\n  * [Knowledge Center](https://repost.aws/knowledge-center/?nc1=f_dr)\n  * [AWS Support Overview](https://aws.amazon.com/premiumsupport/?nc1=f_dr)\n  * [Legal](https://aws.amazon.com/legal/?nc1=f_cc)\n  * [AWS Careers](https://aws.amazon.com/careers/)\n\n\n[ Create an AWS Account ](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html?nc1=f_ct&src=default)\n[ ](https://twitter.com/awscloud \"Twitter\")\n[ ](https://www.facebook.com/amazonwebservices \"Facebook\")\n[ ](https://www.linkedin.com/company/amazon-web-services/ \"Linkedin\")\n[ ](https://www.instagram.com/amazonwebservices/ \"Instagram\")\n[ ](https://www.twitch.tv/aws \"Twitch\")\n[ ](https://www.youtube.com/user/AmazonWebServices/Cloud/ \"YouTube\")\n[ ](https://aws.amazon.com/podcasts/ \"Podcast\")\n[ ](https://pages.awscloud.com/communication-preferences?trk=homepage \"Email\")\nAmazon is an Equal Opportunity Employer: _Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age._\n  * Language\n  * [\u0639\u0631\u0628\u064a](https://aws.amazon.com/ar/?nc1=h_ls)\n  * [Bahasa Indonesia](https://aws.amazon.com/id/?nc1=h_ls)\n  * [Deutsch](https://aws.amazon.com/de/?nc1=h_ls)\n  * [English](https://aws.amazon.com/?nc1=h_ls)\n  * [Espa\u00f1ol](https://aws.amazon.com/es/?nc1=h_ls)\n  * [Fran\u00e7ais](https://aws.amazon.com/fr/?nc1=h_ls)\n  * [Italiano](https://aws.amazon.com/it/?nc1=h_ls)\n  * [Portugu\u00eas](https://aws.amazon.com/pt/?nc1=h_ls)\n  * [Ti\u1ebfng Vi\u1ec7t](https://aws.amazon.com/vi/?nc1=f_ls)\n  * [T\u00fcrk\u00e7e](https://aws.amazon.com/tr/?nc1=h_ls)\n  * [\u03a1\u0443\u0441\u0441\u043a\u0438\u0439](https://aws.amazon.com/ru/?nc1=h_ls)\n  * [\u0e44\u0e17\u0e22](https://aws.amazon.com/th/?nc1=f_ls)\n  * [\u65e5\u672c\u8a9e](https://aws.amazon.com/jp/?nc1=h_ls)\n  * [\ud55c\uad6d\uc5b4](https://aws.amazon.com/ko/?nc1=h_ls)\n  * [\u4e2d\u6587 (\u7b80\u4f53)](https://aws.amazon.com/cn/?nc1=h_ls)\n  * [\u4e2d\u6587 (\u7e41\u9ad4)](https://aws.amazon.com/tw/?nc1=h_ls)\n\n\n  * [Privacy](https://aws.amazon.com/privacy/?nc1=f_pr)\n  * |\n  * [Accessibility](https://aws.amazon.com/accessibility/?nc1=f_acc)\n  * |\n  * [Site Terms](https://aws.amazon.com/terms/?nc1=f_pr)\n  * |\n  * [ Cookie Preferences ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/)\n  * |\n  * \u00a9 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved.\n\n\n####  Ending Support for Internet Explorer\n[ Got it ](https://aws.amazon.com/tutorials/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?trk=ha_a134p000004f0SDAAY~ha_awssm-5821_all-users/ \"Close\")\nAWS support for Internet Explorer ends on 07/31/2022. Supported browsers are Chrome, Firefox, Edge, and Safari. [Learn more \u00bb](https://aws.amazon.com/blogs/aws/heads-up-aws-support-for-internet-explorer-11-is-ending/)\nGot it\n",
    "answer": "```markdown\n## TL;DR Summary\n\nThis tutorial guides users through building, training, deploying, and monitoring a machine learning model using Amazon SageMaker Studio. It covers dataset preparation, model training with XGBoost, batch inference, and setting up monitoring for data drift. Key insights include leveraging SageMaker's integrated tools for efficient ML workflows and the importance of monitoring model performance post-deployment.\n```"
  },
  {
    "instruction": "[Skip to main content](https://www.databricks.com/product/unity-catalog/#main)\n[![](https://www.databricks.com/product/unity-catalog/)](https://www.databricks.com/)\n[Login](https://login.databricks.com/?dbx_source=www&itm=main-cta-login&l=en-EN)\n[![](https://www.databricks.com/product/unity-catalog/)](https://www.databricks.com/)\n  * Why Databricks \n    *       * Discover\n        * [For Executives](https://www.databricks.com/why-databricks/executives)\n        * [For Startups ](https://www.databricks.com/product/startups)\n        * [Lakehouse Architecture ](https://www.databricks.com/product/data-lakehouse)\n        * [Mosaic Research](https://www.databricks.com/research/mosaic)\n      * Customers\n        * [Featured Stories](https://www.databricks.com/customers)\n        * [See All Customers](https://www.databricks.com/customers/all)\n      * Partners\n        * [Cloud ProvidersDatabricks on AWS, Azure, and GCP](https://www.databricks.com/company/partners/cloud-partners)\n        * [Consulting & System IntegratorsExperts to build, deploy and migrate to Databricks](https://www.databricks.com/company/partners/consulting-and-si)\n        * [Technology PartnersConnect your existing tools to your Lakehouse](https://www.databricks.com/company/partners/technology-partner-program)\n        * [C&SI Partner ProgramBuild, deploy or migrate to the Lakehouse](https://www.databricks.com/company/partners/consulting-and-si/candsi-partner-program)\n        * [Data PartnersAccess the ecosystem of data consumers](https://www.databricks.com/company/partners/data-partner-program)\n        * [Partner SolutionsFind custom industry and migration solutions](https://www.databricks.com/company/partners/consulting-and-si/partner-solutions)\n        * [Built on DatabricksBuild, market and grow your business](https://www.databricks.com/company/partners/built-on-partner-program)\n  * Product \n    *       * Databricks Platform\n        * [Platform OverviewA unified platform for data, analytics and AI](https://www.databricks.com/product/data-intelligence-platform)\n        * [Data ManagementData reliability, security and performance](https://www.databricks.com/product/delta-lake-on-databricks)\n        * [SharingAn open, secure, zero-copy sharing for all data](https://www.databricks.com/product/delta-sharing)\n        * [Data WarehousingServerless data warehouse for SQL analytics](https://www.databricks.com/product/databricks-sql)\n        * [GovernanceUnified governance for all data, analytics and AI assets](https://www.databricks.com/product/unity-catalog)\n        * [Real-Time AnalyticsReal-time analytics, AI and applications made simple](https://www.databricks.com/product/data-streaming)\n        * [Artificial IntelligenceBuild and deploy ML and GenAI applications](https://www.databricks.com/product/artificial-intelligence)\n        * [Data EngineeringETL and orchestration for batch and streaming data](https://www.databricks.com/solutions/data-engineering)\n        * [Business IntelligenceIntelligent analytics for real-world data](https://www.databricks.com/product/business-intelligence)\n        * [Data ScienceCollaborative data science at scale](https://www.databricks.com/product/data-science)\n      * Integrations and Data\n        * [MarketplaceOpen marketplace for data, analytics and AI](https://www.databricks.com/product/marketplace)\n        * [IDE IntegrationsBuild on the Lakehouse in your favorite IDE](https://www.databricks.com/product/data-science/ide-integrations)\n        * [Partner ConnectDiscover and integrate with the Databricks ecosystem](https://www.databricks.com/partnerconnect)\n      * Pricing\n        * [Databricks PricingExplore product pricing, DBUs and more](https://www.databricks.com/product/pricing)\n        * [Cost CalculatorEstimate your compute costs on any cloud](https://www.databricks.com/product/pricing/product-pricing/instance-types)\n      * Open Source\n        * [Open Source TechnologiesLearn more about the innovations behind the platform](https://www.databricks.com/product/open-source)\n  * Solutions \n    *       * Databricks for Industries\n        * [Communications](https://www.databricks.com/solutions/industries/communications)\n        * [Media and Entertainment](https://www.databricks.com/solutions/industries/media-and-entertainment)\n        * [Financial Services](https://www.databricks.com/solutions/industries/financial-services)\n        * [Public Sector](https://www.databricks.com/solutions/industries/public-sector)\n        * [Healthcare & Life Sciences](https://www.databricks.com/solutions/industries/healthcare-and-life-sciences)\n        * [Retail](https://www.databricks.com/solutions/industries/retail-industry-solutions)\n        * [Manufacturing](https://www.databricks.com/solutions/industries/manufacturing-industry-solutions)\n        * [See All Industries](https://www.databricks.com/solutions)\n      * Cross Industry Solutions\n        * [Customer Data Platform](https://www.databricks.com/solutions/industries/customer-experience)\n        * [Cyber Security](https://www.databricks.com/solutions/industries/cybersecurity)\n      * Migration & Deployment\n        * [Data Migration](https://www.databricks.com/solutions/migration)\n        * [Professional Services](https://www.databricks.com/professional-services)\n      * Solution Accelerators\n        * [Explore AcceleratorsMove faster toward outcomes that matter](https://www.databricks.com/solutions/accelerators)\n  * Resources \n    *       * Training and Certification\n        * [Learning OverviewHub for training, certification, events and more](https://www.databricks.com/learn)\n        * [Training OverviewDiscover curriculum tailored to your needs](https://www.databricks.com/learn/training/home)\n        * [Databricks AcademySign in to the Databricks learning platform](https://www.databricks.com/learn/training/login)\n        * [CertificationGain recognition and differentiation](https://www.databricks.com/learn/training/certification)\n        * [University AllianceWant to teach Databricks? See how.](https://www.databricks.com/university)\n      * Events\n        * [Data + AI Summit](https://www.databricks.com/dataaisummit)\n        * [Data + AI World Tour](https://www.databricks.com/dataaisummit/worldtour)\n        * [Data Intelligence Days](https://www.databricks.com/lp/data-intelligence-days)\n        * [Event Calendar](https://www.databricks.com/events)\n      * Blog and Podcasts\n        * [Databricks BlogExplore news, product announcements, and more](https://www.databricks.com/blog)\n        * [Databricks Mosaic Research BlogDiscover the latest in our Gen AI research](https://www.databricks.com/blog/category/generative-ai/mosaic-research)\n        * [Data Brew PodcastLet\u2019s talk data!](https://www.databricks.com/discover/data-brew)\n        * [Champions of Data + AI PodcastInsights from data leaders powering innovation](https://www.databricks.com/discover/champions-of-data-and-ai)\n      * Get Help\n        * [Customer Support](https://www.databricks.com/support)\n        * [Documentation](https://docs.databricks.com/en/index.html)\n        * [Community](https://community.databricks.com/s/)\n      * Dive Deep\n        * [Resource Center](https://www.databricks.com/resources)\n        * [Demo Center](https://www.databricks.com/resources/demos)\n  * About \n    *       * Company\n        * [Who We Are](https://www.databricks.com/company/about-us)\n        * [Our Team](https://www.databricks.com/company/leadership-team)\n        * [Databricks Ventures](https://www.databricks.com/databricks-ventures)\n        * [Contact Us](https://www.databricks.com/company/contact)\n      * Careers\n        * [Working at Databricks](https://www.databricks.com/company/careers)\n        * [Open Jobs](https://www.databricks.com/company/careers/open-positions)\n      * Press\n        * [Awards and Recognition](https://www.databricks.com/company/awards-and-recognition)\n        * [Newsroom](https://www.databricks.com/company/newsroom)\n      * Security and Trust\n        * [Security and Trust](https://www.databricks.com/trust)\n\n\n  * Ready to get started?\n  * [Get a Demo](https://www.databricks.com/resources/demos)\n\n\n  * [Login](https://login.databricks.com/?dbx_source=www&itm=main-cta-login&l=en-EN)\n  * [Contact Us](https://www.databricks.com/company/contact)\n  * [Try Databricks](https://login.databricks.com/?dbx_source=www&itm_data=databricks-web-nav&intent=SIGN_UP&l=en-EN)\n\n\n# \nUnity Catalog\n### Unified and open governance for data and AI\n[Get started](https://login.databricks.com/?dbx_source=www&intent=SIGN_UP \"Get started\")[Watch demo](https://www.databricks.com/resources/demos/videos/data-governance/unity-catalog-overview \"Watch demo\")\n[![](https://www.databricks.com/sites/default/files/2025-02/2023-10-EB-Data-and-AI-Governance-PC-107x107-2x.png?v=1740752502) A Comprehensive Guide to Data and AI Governance Build a unified governance strategy for your data and AI estate. Get the eBook](https://www.databricks.com/resources/ebook/data-analytics-and-ai-governance?itm_data=product-unity-catalog-pc-feb25)\nDatabricks Unity Catalog is the industry\u2019s only unified and open governance solution for data and AI, built into the Databricks Data Intelligence Platform. With Unity Catalog, organizations can seamlessly govern both structured and unstructured data in any format, as well as machine learning models, notebooks, dashboards and files across any cloud or platform. Data scientists, analysts and engineers can securely discover, access and collaborate on trusted data and AI assets across platforms, leveraging AI to boost productivity and unlock the full potential of the lakehouse environment. This unified and open approach to governance promotes interoperability and accelerates data and AI initiatives while simplifying regulatory compliance.\n### \u201cDatabricks Unity Catalog is now an integral part of the PepsiCo Data Foundation, our centralized global system that consolidates over 6 petabytes of data worldwide. It streamlines the onboarding process for more than 1,500 active users and enables unified data discovery for our 30+ digital product teams across the globe, supporting both business intelligence and artificial intelligence applications.\u201d\n\u2014 Bhaskar Palit, Senior Director, Data and Analytics\n[Learn more](https://www.databricks.com/blog/how-pepsico-established-enterprise-grade-data-intelligence-platform-powered-databricks-unity)\n![pepsico](https://www.databricks.com/sites/default/files/2024-06/pepsico-customer-image.png?v=1717761445)\n### \u201cUnity Catalog distills permissioning into its important pieces by removing the complexity introduced by IAM policies and other data control platforms, allowing us to focus on what is important to our business use case.\u201d\n\u2014 Joseph Kesting, Software Engineer\n[Learn more](https://www.databricks.com/customers/block/unity-catalog)\n![Block](https://www.databricks.com/sites/default/files/2023-06/Block.png?v=1687639080)\n### \u201cUnity Catalog has enabled us to centralize our data governance, providing robust security features and streamlined compliance processes. This centralization reduced the complexities associated with managing permissions across disparate systems and helped ensure that only authorized personnel had access to sensitive data, which is crucial for adhering to stringent data protection laws, including GDPR.\u201d\n\u2014 Michael Ewins, Director of Engineering\n[Learn more](https://www.databricks.com/blog/data-destinations-how-skyscanner-optimizes-traveler-experiences-databricks-unity-catalog)\n![Skyscanner](https://www.databricks.com/sites/default/files/2024-06/skyscanner-customer-image.png?v=1717760942)\n### \u201cWith Unity Catalog, we are centralizing data catalog and access management across various teams and workspaces, which has simplified governance.\u201d\n\u2014 Venkatesh Sivasubramanian, Senior Director, Big Data\n[Learn more](https://www.databricks.com/customers/rivian)\n![Rivian](https://www.databricks.com/sites/default/files/2023-06/Rivian.png?v=1687639181)\n### \u201cDistributing data governance through Databricks Unity Catalog transformed Akamai\u2019s approach to managing and governing data. With Unity Catalog, we are now managing and governing over 6 petabytes of data with fine-grained access controls on rows and columns.\u201d\n\u2014 Gilad Asulin, Big Data Team Leader\n[Learn more](https://medium.com/@agilad_3118/how-akamai-leverages-databricks-unity-catalog-for-distributed-data-governance-d1eda1c5851)\n![Akamai](https://www.databricks.com/sites/default/files/2023-06/Akamai.png?v=1687639211)\n![PepsiCo logo](https://www.databricks.com/sites/default/files/2024-06/logo-color-pepsico.svg?v=1717720292)\n![block](https://www.databricks.com/sites/default/files/2023-09/logo-color-block-image.png?v=1724434337)\n![Skyscanner](https://www.databricks.com/sites/default/files/2024-06/logo-color-skyscanner.svg?v=1724667400)\n![Rivian logo](https://www.databricks.com/sites/default/files/2023-03/logo-color-rivian.svg?v=1678822598)\n![Akamai](https://www.databricks.com/sites/default/files/2023-03/akamai-logo.svg?v=1724432005)\n![UC](https://www.databricks.com/sites/default/files/2023-06/unity-catalog1.png?v=1687797543)\n### Unified visibility into data and AI\nEasily discover and classify both structured and unstructured data in any format, including machine learning models, notebooks, dashboards and files across all cloud platforms. Seamlessly manage, govern and query data from external databases and data warehouses like MySQL, PostgreSQL, Amazon Redshift, Snowflake, Azure SQL, Azure Synapse, Google BigQuery and catalogs such as HMS and AWS Glue in one place. Accelerate your data and AI initiatives with a single point of access for data exploration. Improve productivity with intelligent search, discovery and automatically generated data insights and documentation.\n![Single permission model for data and AI](https://www.databricks.com/sites/default/files/2024-06/uc-image-02.png?v=1717757695)\n### Single permission model for data and AI\nSimplify access management with a unified interface to define access policies on data and AI assets and consistently apply and audit these policies on any cloud or data platform. Securely access data from other computing platforms using open interfaces, with consistent permissions managed in one place. Enhance security with fine-grained control on rows and columns, while efficiently managing access through low-code attribute-based access policies that scale seamlessly.\n![A dashboard displaying data and graphs.](https://www.databricks.com/sites/default/files/2023-06/unity-catalog-3.png?v=1703004975)\n### AI-powered monitoring and observability\nHarness the power of AI to automate monitoring, diagnose errors and uphold data and ML model quality. Benefit from proactive alerts that automatically detect personally identifiable information (PII) data, track model drift, and effectively resolve issues within your data and AI pipelines to maintain accuracy and integrity. Streamline debugging, root cause analysis, and impact assessment with automated column-level data lineage. Gain comprehensive observability into your data and AI with operational intelligence utilizing built-in system tables for billing, auditing, lineage and more.\n![Open accessibility](https://www.databricks.com/sites/default/files/2024-06/uc-image-04.png?v=1717759484)\n### Open accessibility\nSecurely access your data and AI assets from any compute engine using open APIs and standard interfaces. Share data and AI assets across clouds, regions and platforms with open source Delta Sharing. Securely collaborate with anyone, anywhere to unlock new revenue streams and drive business value, without relying on proprietary formats, complex ETL processes or costly data replication.\n## Here's more to explore\n[![](https://www.databricks.com/sites/default/files/2024-03/2023-05-wb-introducing-generative-ai-ty-tn-362x190-2x.png?v=1711095832)TrainingGenerative AI FundamentalsStart now](https://www.databricks.com/resources/learn/training/generative-ai-fundamentals?itm_data=cro-fpd-target-360)[![](https://www.databricks.com/sites/default/files/2023-11/2023-04-lakehouse-fundamentals-build-your-skill-ty-tn-362x190-2x.png?v=1700688856)TrainingLakehouse FundamentalsStart now](https://www.databricks.com/resources/learn/training/lakehouse-fundamentals?itm_data=cro-fpd-target-360)[![](https://www.databricks.com/sites/default/files/2024-10/2022-09-Databricks-on-Azure-3-part-training-series-TY-TN-360x188-2x.jpg?v=1729099703)TrainingFree Azure Databricks TrainingWatch now](https://www.databricks.com/resources/webinar/azure-databricks-free-training-series?itm_data=cro-fpd-target-360)\n## Here's more to explore\n[![](https://www.databricks.com/sites/default/files/2024-03/2023-05-wb-introducing-generative-ai-ty-tn-362x190-2x.png?v=1711095832)TrainingGenerative AI FundamentalsStart now](https://www.databricks.com/resources/learn/training/generative-ai-fundamentals?itm_data=cro-fpd-target-360)[![](https://www.databricks.com/sites/default/files/2023-11/2023-04-lakehouse-fundamentals-build-your-skill-ty-tn-362x190-2x.png?v=1700688856)TrainingLakehouse FundamentalsStart now](https://www.databricks.com/resources/learn/training/lakehouse-fundamentals?itm_data=cro-fpd-target-360)[![](https://www.databricks.com/sites/default/files/2024-07/2024-07-eb-big-book-of-data-engineering-3rd-ed-ty-tn-360x188-2x.png?v=1721029189)eBookBig Book of Data EngineeringRead now](https://www.databricks.com/resources/ebook/big-book-of-data-engineering?itm_data=cro-fpd-target-360)\n## Resources\n[![](https://www.databricks.com/sites/default/files/2023-10/2023-10-eb-data-and-ai-governance-ad-1200x628.png?v=1702986485)eBookA Comprehensive Guide to Data and AI GovernanceRead now](https://www.databricks.com/resources/ebook/data-analytics-and-ai-governance)[![](https://www.databricks.com/sites/default/files/2023-06/2022-06-EB-New-Approach-to-Data-Sharing-Ad-1200x628.png?v=1687654866)eBookA New Approach to Data SharingRead now](https://www.databricks.com/resources/ebook/a-new-approach-to-data-sharing)[![](https://www.databricks.com/sites/default/files/2024-04/2024-02-wp-databricks-ai-security-framework-cover-ty-tn-360x188-2x.png?v=1712042992)WhitepaperDatabricks AI Security FrameworkRead now](https://www.databricks.com/resources/whitepaper/databricks-ai-security-framework-dasf)\n## Integrations\nUnity Catalog works with your existing data catalogs, data storage systems and governance solutions so you can leverage your existing investments and build a future-proof governance model without expensive migration costs.\n[![](https://www.databricks.com/sites/default/files/2025-02/LP-hero-image-2023-09-EB-A-New-Approach-to-Data-Sharing-460x460-2x.jpg?v=1738573853) Share data and AI securely Learn the easiest way to share across platforms. Download now](https://www.databricks.com/resources/ebook/a-new-approach-to-data-sharing?itm_data=product-unity-catalog-pc2-feb5)\n## Resources\n[![ebook: A Comprehensive Guide to Data and AI Governance](https://www.databricks.com/sites/default/files/2023-10/2023-10-eb-data-and-ai-governance-ad-1200x628.png)EbookA Comprehensive Guide to Data and AI Governance Read now](https://www.databricks.com/resources/ebook/data-analytics-and-ai-governance)\n[![eBook: A New Approach to Data Sharing](https://www.databricks.com/sites/default/files/2023-06/2022-06-EB-New-Approach-to-Data-Sharing-Ad-1200x628.png)eBookA New Approach to Data Sharing Read now](https://www.databricks.com/resources/ebook/a-new-approach-to-data-sharing)\n[![Databricks AI Security Framework](https://www.databricks.com/sites/default/files/2024-04/2024-02-wp-databricks-ai-security-framework-cover-ty-tn-360x188-2x.png)WhitepaperDatabricks AI Security Framework Read now](https://www.databricks.com/resources/whitepaper/databricks-ai-security-framework-dasf)\n### Featured Talks\n[[Keynote] Evolving Data Governance With Unity Catalog Presented by Matei Zaharia at Data + AI Summit 2024](https://www.youtube.com/watch?v=lZbmBAxa3O4&list=PLTPXxbhUt-YXh57uL3QUQiKLwTf39IoT1&index=17)\n[What\u2019s New in Unity Catalog - With Live Demos](https://youtu.be/vnznr18oGfI)\n[Technical Deep Dive for Practitioners: Databricks Unity Catalog from A-Z](https://www.youtube.com/watch?v=iMGG1Ng7sy4)\n[Introducing Unity Catalog Open Source: Unified Governance and Interoperability for Data + AI ](https://youtu.be/COnxxM0gJmw)\n[Attribute-Based Access Controls in Unity Catalog - Building a Scalable Access Management Framework](https://www.youtube.com/watch?v=KufwTp7MQD4)\n[Lakehouse Federation: Discover, Query and Govern Any Data with Unity Catalog](https://www.youtube.com/watch?v=xGImEGpw0uE)\n[Lakehouse Monitoring GA: Profiling, Diagnosing, and Enforcing Data Quality with Intelligence](https://www.youtube.com/watch?v=aDBPoKyA0DQ)\n[Upgrading to Unity Catalog With Ease, Using UCX](https://www.youtube.com/watch?v=pmW9jOFE0qI)\n### Blogs\n[What\u2019s New with Databricks Unity Catalog at Data + AI Summit 2024](https://www.databricks.com/blog/whats-new-databricks-unity-catalog-data-ai-summit-2024)\n[Open Sourcing Unity Catalog](https://www.databricks.com/blog/open-sourcing-unity-catalog)\n[What\u2019s New with Data Sharing and Collaboration](https://www.databricks.com/blog/whats-new-data-sharing-and-collaboration)\n[Unity Catalog Lakeguard: Industry-first and only data governance for multi-user Apache Spark\u2122 clusters](https://www.databricks.com/blog/unity-catalog-lakeguard-industry-first-and-only-data-governance-multi-user-apachetm-spark)\n[Introducing Lakehouse Federation Capabilities in Unity Catalog](https://www.databricks.com/blog/introducing-lakehouse-federation-capabilities-unity-catalog)\n[Built-In Governance for Your Databricks Workspace](https://www.databricks.com/blog/built-governance-your-databricks-workspace)\n### Getting Started\n[AWS Documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n[Azure Documentation](https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/)\n[GCP Documentation](https://docs.gcp.databricks.com/data-governance/unity-catalog/index.html)\n## Ready to get started?\n[Try Databricks for free](https://login.databricks.com/?dbx_source=www&intent=SIGN_UP)\n[![databricks logo](https://www.databricks.com/sites/default/files/2023-08/databricks-default.png?v=1712162038)](https://www.databricks.com/)\nWhy Databricks\nDiscover\n  * [For Executives](https://www.databricks.com/why-databricks/executives)\n  * [For Startups](https://www.databricks.com/product/startups)\n  * [Lakehouse Architecture](https://www.databricks.com/product/data-lakehouse)\n  * [Mosaic Research](https://www.databricks.com/research/mosaic)\n\n\nCustomers\n  * [Featured](https://www.databricks.com/customers)\n  * [See All](https://www.databricks.com/customers/all)\n\n\nPartners\n  * [Cloud Providers](https://www.databricks.com/company/partners/cloud-partners)\n  * [Technology Partners](https://www.databricks.com/company/partners/technology-partner-program)\n  * [Data Partners](https://www.databricks.com/company/partners/data-partner-program)\n  * [Built on Databricks](https://www.databricks.com/company/partners/built-on-partner-program)\n  * [Consulting & System Integrators](https://www.databricks.com/company/partners/consulting-and-si)\n  * [C&SI Partner Program](https://www.databricks.com/company/partners/consulting-and-si/candsi-partner-program)\n  * [Partner Solutions](https://www.databricks.com/company/partners/consulting-and-si/partner-solutions)\n\n\nWhy Databricks\nDiscover\n  * [For Executives](https://www.databricks.com/why-databricks/executives)\n  * [For Startups](https://www.databricks.com/product/startups)\n  * [Lakehouse Architecture](https://www.databricks.com/product/data-lakehouse)\n  * [Mosaic Research](https://www.databricks.com/research/mosaic)\n\n\nCustomers\n  * [Featured](https://www.databricks.com/customers)\n  * [See All](https://www.databricks.com/customers/all)\n\n\nPartners\n  * [Cloud Providers](https://www.databricks.com/company/partners/cloud-partners)\n  * [Technology Partners](https://www.databricks.com/company/partners/technology-partner-program)\n  * [Data Partners](https://www.databricks.com/company/partners/data-partner-program)\n  * [Built on Databricks](https://www.databricks.com/company/partners/built-on-partner-program)\n  * [Consulting & System Integrators](https://www.databricks.com/company/partners/consulting-and-si)\n  * [C&SI Partner Program](https://www.databricks.com/company/partners/consulting-and-si/candsi-partner-program)\n  * [Partner Solutions](https://www.databricks.com/company/partners/consulting-and-si/partner-solutions)\n\n\nProduct\nDatabricks Platform\n  * [Platform Overview](https://www.databricks.com/product/data-intelligence-platform)\n  * [Sharing](https://www.databricks.com/product/delta-sharing)\n  * [Governance](https://www.databricks.com/product/unity-catalog)\n  * [Artificial Intelligence](https://www.databricks.com/product/artificial-intelligence)\n  * [Business Intelligence](https://www.databricks.com/product/business-intelligence)\n  * [Data Management](https://www.databricks.com/product/delta-lake-on-databricks)\n  * [Data Warehousing](https://www.databricks.com/product/databricks-sql)\n  * [Real-Time Analytics](https://www.databricks.com/product/data-streaming)\n  * [Data Engineering](https://www.databricks.com/solutions/data-engineering)\n  * [Data Science](https://www.databricks.com/product/data-science)\n\n\nPricing\n  * [Pricing Overview](https://www.databricks.com/product/pricing)\n  * [Pricing Calculator](https://www.databricks.com/product/pricing/product-pricing/instance-types)\n\n\n[Open Source](https://www.databricks.com/product/open-source)\nIntegrations and Data\n  * [Marketplace](https://www.databricks.com/product/marketplace)\n  * [IDE Integrations](https://www.databricks.com/product/data-science/ide-integrations)\n  * [Partner Connect](https://www.databricks.com/partnerconnect)\n\n\nProduct\nDatabricks Platform\n  * [Platform Overview](https://www.databricks.com/product/data-intelligence-platform)\n  * [Sharing](https://www.databricks.com/product/delta-sharing)\n  * [Governance](https://www.databricks.com/product/unity-catalog)\n  * [Artificial Intelligence](https://www.databricks.com/product/artificial-intelligence)\n  * [Business Intelligence](https://www.databricks.com/product/business-intelligence)\n  * [Data Management](https://www.databricks.com/product/delta-lake-on-databricks)\n  * [Data Warehousing](https://www.databricks.com/product/databricks-sql)\n  * [Real-Time Analytics](https://www.databricks.com/product/data-streaming)\n  * [Data Engineering](https://www.databricks.com/solutions/data-engineering)\n  * [Data Science](https://www.databricks.com/product/data-science)\n\n\nPricing\n  * [Pricing Overview](https://www.databricks.com/product/pricing)\n  * [Pricing Calculator](https://www.databricks.com/product/pricing/product-pricing/instance-types)\n\n\nOpen Source\nIntegrations and Data\n  * [Marketplace](https://www.databricks.com/product/marketplace)\n  * [IDE Integrations](https://www.databricks.com/product/data-science/ide-integrations)\n  * [Partner Connect](https://www.databricks.com/partnerconnect)\n\n\nSolutions\nDatabricks For Industries\n  * [Communications](https://www.databricks.com/solutions/industries/communications)\n  * [Financial Services](https://www.databricks.com/solutions/industries/financial-services)\n  * [Healthcare and Life Sciences](https://www.databricks.com/solutions/industries/healthcare-and-life-sciences)\n  * [Manufacturing](https://www.databricks.com/solutions/industries/manufacturing-industry-solutions)\n  * [Media and Entertainment](https://www.databricks.com/solutions/industries/media-and-entertainment)\n  * [Public Sector](https://www.databricks.com/solutions/industries/federal-government)\n  * [Retail](https://www.databricks.com/solutions/industries/retail-industry-solutions)\n  * [View All](https://www.databricks.com/solutions)\n\n\nCross Industry Solutions\n  * [Customer Data Platform](https://www.databricks.com/solutions/industries/customer-experience)\n  * [Cyber Security](https://www.databricks.com/solutions/industries/cybersecurity)\n\n\n[Data Migration](https://www.databricks.com/solutions/migration)\n[Professional Services](https://www.databricks.com/professional-services)\n[Solution Accelerators](https://www.databricks.com/solutions/accelerators)\nSolutions\nDatabricks For Industries\n  * [Communications](https://www.databricks.com/solutions/industries/communications)\n  * [Financial Services](https://www.databricks.com/solutions/industries/financial-services)\n  * [Healthcare and Life Sciences](https://www.databricks.com/solutions/industries/healthcare-and-life-sciences)\n  * [Manufacturing](https://www.databricks.com/solutions/industries/manufacturing-industry-solutions)\n  * [Media and Entertainment](https://www.databricks.com/solutions/industries/media-and-entertainment)\n  * [Public Sector](https://www.databricks.com/solutions/industries/federal-government)\n  * [Retail](https://www.databricks.com/solutions/industries/retail-industry-solutions)\n  * [View All](https://www.databricks.com/solutions)\n\n\nCross Industry Solutions\n  * [Customer Data Platform](https://www.databricks.com/solutions/industries/customer-experience)\n  * [Cyber Security](https://www.databricks.com/solutions/industries/cybersecurity)\n\n\nData Migration\nProfessional Services\nSolution Accelerators\nResources\n[Documentation](https://docs.databricks.com/en/index.html)\n[Customer Support](https://www.databricks.com/support)\n[Community](https://community.databricks.com/)\nTraining and Certification\n  * [Learning Overview](https://www.databricks.com/learn)\n  * [Training Overview](https://www.databricks.com/learn/training/home)\n  * [Certification](https://www.databricks.com/learn/training/certification)\n  * [University Alliance](https://www.databricks.com/university)\n  * [Databricks Academy Login](https://www.databricks.com/learn/training/login)\n\n\nEvents\n  * [Data + AI Summit](https://www.databricks.com/dataaisummit)\n  * [Data + AI World Tour](https://www.databricks.com/dataaisummit/worldtour)\n  * [Data Intelligence Days](https://www.databricks.com/lp/data-intelligence-days)\n  * [Full Calendar](https://www.databricks.com/events)\n\n\nBlog and Podcasts\n  * [Databricks Blog](https://www.databricks.com/blog)\n  * [Databricks Mosaic Research Blog](https://www.databricks.com/blog/category/generative-ai/mosaic-research)\n  * [Data Brew Podcast](https://www.databricks.com/discover/data-brew)\n  * [Champions of Data & AI Podcast](https://www.databricks.com/discover/champions-of-data-and-ai)\n\n\nResources\nDocumentation\nCustomer Support\nCommunity\nTraining and Certification\n  * [Learning Overview](https://www.databricks.com/learn)\n  * [Training Overview](https://www.databricks.com/learn/training/home)\n  * [Certification](https://www.databricks.com/learn/training/certification)\n  * [University Alliance](https://www.databricks.com/university)\n  * [Databricks Academy Login](https://www.databricks.com/learn/training/login)\n\n\nEvents\n  * [Data + AI Summit](https://www.databricks.com/dataaisummit)\n  * [Data + AI World Tour](https://www.databricks.com/dataaisummit/worldtour)\n  * [Data Intelligence Days](https://www.databricks.com/lp/data-intelligence-days)\n  * [Full Calendar](https://www.databricks.com/events)\n\n\nBlog and Podcasts\n  * [Databricks Blog](https://www.databricks.com/blog)\n  * [Databricks Mosaic Research Blog](https://www.databricks.com/blog/category/generative-ai/mosaic-research)\n  * [Data Brew Podcast](https://www.databricks.com/discover/data-brew)\n  * [Champions of Data & AI Podcast](https://www.databricks.com/discover/champions-of-data-and-ai)\n\n\nAbout\nCompany\n  * [Who We Are](https://www.databricks.com/company/about-us)\n  * [Our Team](https://www.databricks.com/company/leadership-team)\n  * [Databricks Ventures](https://www.databricks.com/databricks-ventures)\n  * [Contact Us](https://www.databricks.com/company/contact)\n\n\nCareers\n  * [Open Jobs](https://www.databricks.com/company/careers/open-positions)\n  * [Working at Databricks](https://www.databricks.com/company/careers)\n\n\nPress\n  * [Awards and Recognition](https://www.databricks.com/company/awards-and-recognition)\n  * [Newsroom](https://www.databricks.com/company/newsroom)\n\n\n[Security and Trust](https://www.databricks.com/trust)\nAbout\nCompany\n  * [Who We Are](https://www.databricks.com/company/about-us)\n  * [Our Team](https://www.databricks.com/company/leadership-team)\n  * [Databricks Ventures](https://www.databricks.com/databricks-ventures)\n  * [Contact Us](https://www.databricks.com/company/contact)\n\n\nCareers\n  * [Open Jobs](https://www.databricks.com/company/careers/open-positions)\n  * [Working at Databricks](https://www.databricks.com/company/careers)\n\n\nPress\n  * [Awards and Recognition](https://www.databricks.com/company/awards-and-recognition)\n  * [Newsroom](https://www.databricks.com/company/newsroom)\n\n\nSecurity and Trust\n[![databricks logo](https://www.databricks.com/sites/default/files/2023-08/databricks-default.png?v=1712162038)](https://www.databricks.com/)\nDatabricks Inc. 160 Spear Street, 15th Floor San Francisco, CA 94105 1-866-330-0121\n  * [](https://www.linkedin.com/company/databricks)\n  * [](https://www.facebook.com/pages/Databricks/560203607379694)\n  * [](https://twitter.com/databricks)\n  * [](https://www.databricks.com/feed)\n  * [](https://www.glassdoor.com/Overview/Working-at-Databricks-EI_IE954734.11,21.htm)\n  * [](https://www.youtube.com/@Databricks)\n\n\n![](https://www.databricks.com/sites/default/files/2021/02/telco-icon-2.png?v=1715274112)\n[See Careers at Databricks](https://www.databricks.com/company/careers)\n  * [](https://www.linkedin.com/company/databricks)\n  * [](https://www.facebook.com/pages/Databricks/560203607379694)\n  * [](https://twitter.com/databricks)\n  * [](https://www.databricks.com/feed)\n  * [](https://www.glassdoor.com/Overview/Working-at-Databricks-EI_IE954734.11,21.htm)\n  * [](https://www.youtube.com/@Databricks)\n\n\n\u00a9 Databricks 2025. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the [Apache Software Foundation.](https://www.apache.org/)\n  * [Privacy Notice](https://www.databricks.com/legal/privacynotice)\n  * |[Terms of Use](https://www.databricks.com/terms-of-use)\n  * |[Modern Slavery Statement](https://www.databricks.com/legal/modern-slavery-policy-statement)\n  * |[California Privacy](https://www.databricks.com/legal/supplemental-privacy-notice-california-residents)\n  * |[Your Privacy Choices](https://www.databricks.com/product/unity-catalog/#yourprivacychoices)\n  * ![](https://www.databricks.com/sites/default/files/2022-12/gpcicon_small.png)\n\n\n",
    "answer": "```markdown\n# TL;DR Summary\n\n**Unity Catalog** by Databricks offers unified governance for data and AI, enabling seamless management of structured and unstructured data across platforms. It simplifies access management, enhances security, and automates monitoring, promoting productivity and compliance for organizations like PepsiCo and Akamai.\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\n[ Back to Articles](https://huggingface.co/blog)\n#  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#making-llms-even-more-accessible-with-bitsandbytes-4-bit-quantization-and-qlora) Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA \nPublished May 24, 2023\n[Update on GitHub](https://github.com/huggingface/blog/blob/main/4bit-transformers-bitsandbytes.md)\n[ Upvote 133 ](https://huggingface.co/login?next=%2Fblog%2F4bit-transformers-bitsandbytes)\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/5f0988ad19cb630495b8147a/I5T-tPlEXzsXR7iI8d4u1.png) ](https://huggingface.co/ucalyptus \"ucalyptus\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/600988773adca878cda1d294/-fm8VEoWgqFvwfYbpRhEF.jpeg) ](https://huggingface.co/nezubn \"nezubn\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/6032802e1f993496bc14d9e3/w6hr-DEQot4VVkoyRIBiy.png) ](https://huggingface.co/osanseviero \"osanseviero\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/6065a9cbe43e52694178ed78/TwnrOfTacrwtLnBC8cS8e.jpeg) ](https://huggingface.co/emanuelevivoli \"emanuelevivoli\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/T762Ut0Y-w0sZB2ynvfbJ.jpeg) ](https://huggingface.co/ariG23498 \"ariG23498\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1664223180691-60c9687eed9842b2fc0f5357.png) ](https://huggingface.co/tushifire \"tushifire\")\n  * +127\n\n\n[![Younes Belkada's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png)](https://huggingface.co/ybelkada)\n[ybelkada Younes Belkada](https://huggingface.co/ybelkada)\n[![Tim Dettmers's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/1638462736111-noauth.png)](https://huggingface.co/timdettmers)\n[timdettmers Tim Dettmers](https://huggingface.co/timdettmers)\nguest\n[![Artidoro Pagnoni's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/6279d059812ee439d9c8057c/2E3-rFQganxuz3jDL-Esc.jpeg)](https://huggingface.co/artidoro)\n[artidoro Artidoro Pagnoni](https://huggingface.co/artidoro)\nguest\n[![Sylvain Gugger's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg)](https://huggingface.co/sgugger)\n[sgugger Sylvain Gugger](https://huggingface.co/sgugger)\n[![Sourab Mangrulkar's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/1638132956881-5fca176d1d7a08cb34d79d5d.jpeg)](https://huggingface.co/smangrul)\n[smangrul Sourab Mangrulkar](https://huggingface.co/smangrul)\n  * [Resources](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#resources \"Resources\")\n  * [Introduction](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#introduction \"Introduction\")\n    * [FP8 format](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#fp8-format \"FP8 format\")\n    * [FP4 precision in a few words](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#fp4-precision-in-a-few-words \"FP4 precision in a few words\")\n  * [QLoRA paper, a new way of democratizing quantized large transformer models](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#qlora-paper-a-new-way-of-democratizing-quantized-large-transformer-models \"QLoRA paper, a new way of democratizing quantized large transformer models\")\n  * [How to use it in transformers?](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#how-to-use-it-in-transformers \"How to use it in transformers?\")\n    * [Getting started](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#getting-started \"Getting started\")\n    * [Quickstart](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#quickstart \"Quickstart\")\n    * [Advanced usage](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#advanced-usage \"Advanced usage\")\n    * [Common questions](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#common-questions \"Common questions\")\n  * [Playground](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#playground \"Playground\")\n  * [Acknowledgements](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#acknowledgements \"Acknowledgements\")\n\n\nLLMs are known to be large, and running or training them in consumer hardware is a huge challenge for users and accessibility. Our [LLM.int8 blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) showed how the techniques in the [LLM.int8 paper](https://arxiv.org/abs/2208.07339) were integrated in transformers using the `bitsandbytes` library. As we strive to make models even more accessible to anyone, we decided to collaborate with bitsandbytes again to allow users to run models in 4-bit precision. This includes a large majority of HF models, in any modality (text, vision, multi-modal, etc.). Users can also train adapters on top of 4bit models leveraging tools from the Hugging Face ecosystem. This is a new method introduced today in the QLoRA paper by Dettmers et al. The abstract of the paper is as follows: \n> We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimizers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\n##  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#resources) Resources \nThis blogpost and release come with several resources to get started with 4bit models and QLoRA:\n  * [Original paper](https://arxiv.org/abs/2305.14314)\n  * [Basic usage Google Colab notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing) - This notebook shows how to use 4bit models in inference with all their variants, and how to run GPT-neo-X (a 20B parameter model) on a free Google Colab instance \ud83e\udd2f \n  * [Fine tuning Google Colab notebook](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing) - This notebook shows how to fine-tune a 4bit model on a downstream task using the Hugging Face ecosystem. We show that it is possible to fine tune GPT-neo-X 20B on a Google Colab instance!\n  * [Original repository for replicating the paper's results](https://github.com/artidoro/qlora)\n  * [Guanaco 33b playground](https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi) - or check the playground section below\n\n\n##  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#introduction) Introduction \nIf you are not familiar with model precisions and the most common data types (float16, float32, bfloat16, int8), we advise you to carefully read the introduction in [our first blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) that goes over the details of these concepts in simple terms with visualizations. \nFor more information we recommend reading the fundamentals of floating point representation through [this wikibook document](https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers#:~:text=In%20decimal%2C%20very%20large%20numbers,be%20used%20for%20binary%20numbers.). \nThe recent QLoRA paper explores different data types, 4-bit Float and 4-bit NormalFloat. We will discuss here the 4-bit Float data type since it is easier to understand. \nFP8 and FP4 stand for Floating Point 8-bit and 4-bit precision, respectively. They are part of the minifloats family of floating point values (among other precisions, the minifloats family also includes bfloat16 and float16).\nLet\u2019s first have a look at how to represent floating point values in FP8 format, then understand how the FP4 format looks like.\n###  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#fp8-format) FP8 format \nAs discussed in our previous blogpost, a floating point contains n-bits, with each bit falling into a specific category that is responsible for representing a component of the number (sign, mantissa and exponent). These represent the following.\nThe FP8 (floating point 8) format has been first introduced in the paper [\u201cFP8 for Deep Learning\u201d](https://arxiv.org/pdf/2209.05433.pdf) with two different FP8 encodings: E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa).\n[![fp8_scheme](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/bitsandbytes/FP8-scheme.png)](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/bitsandbytes/FP8-scheme.png)  \n---  \n**Overview of Floating Point 8 (FP8) format. Source: Original content from[`sgugger`](https://huggingface.co/sgugger) **  \nAlthough the precision is substantially reduced by reducing the number of bits from 32 to 8, both versions can be used in a variety of situations. Currently one could use [Transformer Engine library](https://github.com/NVIDIA/TransformerEngine) that is also integrated with HF ecosystem through accelerate.\nThe potential floating points that can be represented in the E4M3 format are in the range -448 to 448, whereas in the E5M2 format, as the number of bits of the exponent increases, the range increases to -57344 to 57344 - but with a loss of precision because the number of possible representations remains constant. It has been empirically proven that the E4M3 is best suited for the forward pass, and the second version is best suited for the backward computation\n###  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#fp4-precision-in-a-few-words) FP4 precision in a few words \nThe sign bit represents the sign (+/-), the exponent bits a base two to the power of the integer represented by the bits (e.g. `2^{010} = 2^{2} = 4`), and the fraction or mantissa is the sum of powers of negative two which are \u201cactive\u201d for each bit that is \u201c1\u201d. If a bit is \u201c0\u201d the fraction remains unchanged for that power of `2^-i` where i is the position of the bit in the bit-sequence. For example, for mantissa bits 1010 we have `(0 + 2^-1 + 0 + 2^-3) = (0.5 + 0.125) = 0.625`. To get a value, we add _1_ to the fraction and multiply all results together, for example, with 2 exponent bits and one mantissa bit the representations 1101 would be:\n`-1 * 2^(2) * (1 + 2^-1) = -1 * 4 * 1.5 = -6`\nFor FP4 there is no fixed format and as such one can try combinations of different mantissa/exponent combinations. In general, 3 exponent bits do a bit better in most cases. But sometimes 2 exponent bits and a mantissa bit yield better performance.\n##  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#qlora-paper-a-new-way-of-democratizing-quantized-large-transformer-models) QLoRA paper, a new way of democratizing quantized large transformer models \nIn few words, QLoRA reduces the memory usage of LLM finetuning without performance tradeoffs compared to standard 16-bit model finetuning. This method enables 33B model finetuning on a single 24GB GPU and 65B model finetuning on a single 46GB GPU.\nMore specifically, QLoRA uses 4-bit quantization to compress a pretrained language model. The LM parameters are then frozen and a relatively small number of trainable parameters are added to the model in the form of Low-Rank Adapters. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training. Read more about LoRA in the [original LoRA paper](https://arxiv.org/abs/2106.09685).\nQLoRA has one storage data type (usually 4-bit NormalFloat) for the base model weights and a computation data type (16-bit BrainFloat) used to perform computations. QLoRA dequantizes weights from the storage data type to the computation data type to perform the forward and backward passes, but only computes weight gradients for the LoRA parameters which use 16-bit bfloat. The weights are decompressed only when they are needed, therefore the memory usage stays low during training and inference.\nQLoRA tuning is shown to match 16-bit finetuning methods in a wide range of experiments. In addition, the Guanaco models, which use QLoRA finetuning for LLaMA models on the [OpenAssistant dataset (OASST1)](https://huggingface.co/datasets/OpenAssistant/oasst1), are state-of-the-art chatbot systems and are close to ChatGPT on the Vicuna benchmark. This is an additional demonstration of the power of QLoRA tuning.\nFor a more detailed reading, we recommend you read the [QLoRA paper](https://arxiv.org/abs/2305.14314).\n##  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#how-to-use-it-in-transformers) How to use it in transformers? \nIn this section let us introduce the transformers integration of this method, how to use it and which models can be effectively quantized. \n###  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#getting-started) Getting started \nAs a quickstart, load a model in 4bit by (at the time of this writing) installing accelerate and transformers from source, and make sure you have installed the latest version of bitsandbytes library (0.39.0).\n```\npip install -q -U bitsandbytes\npip install -q -U git+https://github.com/huggingface/transformers.git\npip install -q -U git+https://github.com/huggingface/peft.git\npip install -q -U git+https://github.com/huggingface/accelerate.git\n\n```\n\n###  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#quickstart) Quickstart \nThe basic way to load a model in 4bit is to pass the argument `load_in_4bit=True` when calling the `from_pretrained` method by providing a device map (pass `\"auto\"` to get a device map that will be automatically inferred).\n```\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_4bit=True, device_map=\"auto\")\n...\n\n```\n\nThat's all you need!\nAs a general rule, we recommend users to not manually set a device once the model has been loaded with `device_map`. So any device assignment call to the model, or to any model\u2019s submodules should be avoided after that line - unless you know what you are doing.\nKeep in mind that loading a quantized model will automatically cast other model's submodules into `float16` dtype. You can change this behavior, (if for example you want to have the layer norms in `float32`), by passing `torch_dtype=dtype` to the `from_pretrained` method.\n###  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#advanced-usage) Advanced usage \nYou can play with different variants of 4bit quantization such as NF4 (normalized float 4 (default)) or pure FP4 quantization. Based on theoretical considerations and empirical results from the paper, we recommend using NF4 quantization for better performance.\nOther options include `bnb_4bit_use_double_quant` which uses a second quantization after the first one to save an additional 0.4 bits per parameter. And finally, the compute type. While 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit and here any combination can be chosen (float16, bfloat16, float32 etc). \nThe matrix multiplication and training will be faster if one uses a 16-bit compute dtype (default torch.float32). One should leverage the recent `BitsAndBytesConfig` from transformers to change these parameters. An example to load a model in 4bit using NF4 quantization below with double quantization with the compute dtype bfloat16 for faster training:\n```\nfrom transformers import BitsAndBytesConfig\n\nnf4_config = BitsAndBytesConfig(\n  load_in_4bit=True,\n  bnb_4bit_quant_type=\"nf4\",\n  bnb_4bit_use_double_quant=True,\n  bnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\n\n```\n\n####  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#changing-the-compute-dtype) Changing the compute dtype \nAs mentioned above, you can also change the compute dtype of the quantized model by just changing the `bnb_4bit_compute_dtype` argument in `BitsAndBytesConfig`.\n```\nimport torch\nfrom transformers import BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(\n  load_in_4bit=True,\n  bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n```\n\n####  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#nested-quantization) Nested quantization \nFor enabling nested quantization, you can use the `bnb_4bit_use_double_quant` argument in `BitsAndBytesConfig`. This will enable a second quantization after the first one to save an additional 0.4 bits per parameter. We also use this feature in the training Google colab notebook.\n```\nfrom transformers import BitsAndBytesConfig\ndouble_quant_config = BitsAndBytesConfig(\n  load_in_4bit=True,\n  bnb_4bit_use_double_quant=True,\n)\nmodel_double_quant = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=double_quant_config)\n\n```\n\nAnd of course, as mentioned in the beginning of the section, all of these components are composable. You can combine all these parameters together to find the optimial use case for you. A rule of thumb is: use double quant if you have problems with memory, use NF4 for higher precision, and use a 16-bit dtype for faster finetuning. For instance in the [inference demo](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing), we use nested quantization, bfloat16 compute dtype and NF4 quantization to fit gpt-neo-x-20b (40GB) entirely in 4bit in a single 16GB GPU.\n###  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#common-questions) Common questions \nIn this section, we will also address some common questions anyone could have regarding this integration.\n####  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#does-fp4-quantization-have-any-hardware-requirements) Does FP4 quantization have any hardware requirements? \nNote that this method is only compatible with GPUs, hence it is not possible to quantize models in 4bit on a CPU. Among GPUs, there should not be any hardware requirement about this method, therefore any GPU could be used to run the 4bit quantization as long as you have CUDA>=11.2 installed. Keep also in mind that the computation is not done in 4bit, the weights and activations are compressed to that format and the computation is still kept in the desired or native dtype.\n####  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#what-are-the-supported-models) What are the supported models? \nSimilarly as the integration of LLM.int8 presented in [this blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) the integration heavily relies on the `accelerate` library. Therefore, any model that supports accelerate loading (i.e. the `device_map` argument when calling `from_pretrained`) should be quantizable in 4bit. Note also that this is totally agnostic to modalities, as long as the models can be loaded with the `device_map` argument, it is possible to quantize them. \nFor text models, at this time of writing, this would include most used architectures such as Llama, OPT, GPT-Neo, GPT-NeoX for text models, Blip2 for multimodal models, and so on. \nAt this time of writing, the models that support accelerate are: \n```\n[\n  'bigbird_pegasus', 'blip_2', 'bloom', 'bridgetower', 'codegen', 'deit', 'esm', \n  'gpt2', 'gpt_bigcode', 'gpt_neo', 'gpt_neox', 'gpt_neox_japanese', 'gptj', 'gptsan_japanese', \n  'lilt', 'llama', 'longformer', 'longt5', 'luke', 'm2m_100', 'mbart', 'mega', 'mt5', 'nllb_moe', \n  'open_llama', 'opt', 'owlvit', 'plbart', 'roberta', 'roberta_prelayernorm', 'rwkv', 'switch_transformers', \n  't5', 'vilt', 'vit', 'vit_hybrid', 'whisper', 'xglm', 'xlm_roberta'\n] \n\n```\n\nNote that if your favorite model is not there, you can open a Pull Request or raise an issue in transformers to add the support of accelerate loading for that architecture.\n####  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#can-we-train-4bit8bit-models) Can we train 4bit/8bit models? \nIt is not possible to perform pure 4bit training on these models. However, you can train these models by leveraging parameter efficient fine tuning methods (PEFT) and train for example adapters on top of them. That is what is done in the paper and is officially supported by the PEFT library from Hugging Face. We also provide a [training notebook](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing) and recommend users to check the [QLoRA repository](https://github.com/artidoro/qlora) if they are interested in replicating the results from the paper.\n[![lora-gif](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/133_trl_peft/lora-animated.gif)](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/133_trl_peft/lora-animated.gif)  \n---  \n**The output activations original (frozen) pretrained weights (left) are augmented by a low rank adapter comprised of weight matrics A and B (right).**  \n####  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#what-other-consequences-are-there) What other consequences are there? \nThis integration can open up several positive consequences to the community and AI research as it can affect multiple use cases and possible applications. In RLHF (Reinforcement Learning with Human Feedback) it is possible to load a single base model, in 4bit and train multiple adapters on top of it, one for the reward modeling, and another for the value policy training. A more detailed blogpost and announcement will be made soon about this use case.\nWe have also made some benchmarks on the impact of this quantization method on training large models on consumer hardware. We have run several experiments on finetuning 2 different architectures, Llama 7B (15GB in fp16) and Llama 13B (27GB in fp16) on an NVIDIA T4 (16GB) and here are the results\nModel name | Half precision model size (in GB) | Hardware type / total VRAM | quantization method (CD=compute dtype / GC=gradient checkpointing / NQ=nested quantization) | batch_size | gradient accumulation steps | optimizer | seq_len | Result  \n---|---|---|---|---|---|---|---|---  \n<10B scale models  \ndecapoda-research/llama-7b-hf | 14GB | 1xNVIDIA-T4 / 16GB | LLM.int8 (8-bit) + GC | 1 | 4 | AdamW | 512 | **No OOM**  \ndecapoda-research/llama-7b-hf | 14GB | 1xNVIDIA-T4 / 16GB | LLM.int8 (8-bit) + GC | 1 | 4 | AdamW | 1024 | OOM  \ndecapoda-research/llama-7b-hf | 14GB | 1xNVIDIA-T4 / 16GB | 4bit + NF4 + bf16 CD + no GC | 1 | 4 | AdamW | 512 | **No OOM**  \ndecapoda-research/llama-7b-hf | 14GB | 1xNVIDIA-T4 / 16GB | 4bit + FP4 + bf16 CD + no GC | 1 | 4 | AdamW | 512 | **No OOM**  \ndecapoda-research/llama-7b-hf | 14GB | 1xNVIDIA-T4 / 16GB | 4bit + NF4 + bf16 CD + no GC | 1 | 4 | AdamW | 1024 | OOM  \ndecapoda-research/llama-7b-hf | 14GB | 1xNVIDIA-T4 / 16GB | 4bit + FP4 + bf16 CD + no GC | 1 | 4 | AdamW | 1024 | OOM  \ndecapoda-research/llama-7b-hf | 14GB | 1xNVIDIA-T4 / 16GB | 4bit + NF4 + bf16 CD + GC | 1 | 4 | AdamW | 1024 | **No OOM**  \n10B+ scale models  \ndecapoda-research/llama-13b-hf | 27GB | 2xNVIDIA-T4 / 32GB | LLM.int8 (8-bit) + GC | 1 | 4 | AdamW | 512 | **No OOM**  \ndecapoda-research/llama-13b-hf | 27GB | 1xNVIDIA-T4 / 16GB | LLM.int8 (8-bit) + GC | 1 | 4 | AdamW | 512 | OOM  \ndecapoda-research/llama-13b-hf | 27GB | 1xNVIDIA-T4 / 16GB | 4bit + FP4 + bf16 CD + no GC | 1 | 4 | AdamW | 512 | OOM  \ndecapoda-research/llama-13b-hf | 27GB | 1xNVIDIA-T4 / 16GB | 4bit + FP4 + fp16 CD + no GC | 1 | 4 | AdamW | 512 | OOM  \ndecapoda-research/llama-13b-hf | 27GB | 1xNVIDIA-T4 / 16GB | 4bit + NF4 + fp16 CD + GC | 1 | 4 | AdamW | 512 | **No OOM**  \ndecapoda-research/llama-13b-hf | 27GB | 1xNVIDIA-T4 / 16GB | 4bit + NF4 + fp16 CD + GC | 1 | 4 | AdamW | 1024 | OOM  \ndecapoda-research/llama-13b-hf | 27GB | 1xNVIDIA-T4 / 16GB | 4bit + NF4 + fp16 CD + GC + NQ | 1 | 4 | AdamW | 1024 | **No OOM**  \nWe have used the recent `SFTTrainer` from TRL library, and the benchmarking script can be found [here](https://gist.github.com/younesbelkada/f48af54c74ba6a39a7ae4fd777e72fe8)\n##  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#playground) Playground \nTry out the Guananco model cited on the paper on [the playground](https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi) or directly below\n##  [ ](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#acknowledgements) Acknowledgements \nThe HF team would like to acknowledge all the people involved in this project from University of Washington, and for making this available to the community. \nThe authors would also like to thank [Pedro Cuenca](https://huggingface.co/pcuenq) for kindly reviewing the blogpost, [Olivier Dehaene](https://huggingface.co/olivierdehaene) and [Omar Sanseviero](https://huggingface.co/osanseviero) for their quick and strong support for the integration of the paper's artifacts on the HF Hub.\nMore Articles from our Blog\n[ ![](https://huggingface.co/blog/assets/169_quanto_intro/thumbnail.png) quanto: a pytorch quantization toolkit By  [dacorvo](https://huggingface.co/dacorvo) March 18, 2024 \u2022 35](https://huggingface.co/blog/quanto-introduction)\n[ ![](https://huggingface.co/blog/assets/163_overview_quantization_transformers/thumbnail.jpg) Overview of natively supported quantization schemes in \ud83e\udd17 Transformers By  [ybelkada](https://huggingface.co/ybelkada) September 12, 2023 \u2022 12](https://huggingface.co/blog/overview-quantization-transformers)\n### Community\n![](https://huggingface.co/avatars/9849aa31fa73a2d59dde2d2a6b9af4b7.svg) [asd1asd1](https://huggingface.co/asd1asd1)\n[24 days ago](https://huggingface.co/blog/4bit-transformers-bitsandbytes/#67ca86ac8e32b0571e17cacd)\nhola\n+ \nReply\nEditPreview\nUpload images, audio, and videos by dragging in the text input, pasting, or clicking here. \nTap or paste here to upload images\nYour need to confirm your account before you can post a new comment. \nComment \n\u00b7 [Sign up](https://huggingface.co/join?next=%2Fblog%2F4bit-transformers-bitsandbytes) or [log in](https://huggingface.co/login?next=%2Fblog%2F4bit-transformers-bitsandbytes) to comment \n[ Upvote 133 ](https://huggingface.co/login?next=%2Fblog%2F4bit-transformers-bitsandbytes)\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/5f0988ad19cb630495b8147a/I5T-tPlEXzsXR7iI8d4u1.png) ](https://huggingface.co/ucalyptus \"ucalyptus\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/600988773adca878cda1d294/-fm8VEoWgqFvwfYbpRhEF.jpeg) ](https://huggingface.co/nezubn \"nezubn\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/6032802e1f993496bc14d9e3/w6hr-DEQot4VVkoyRIBiy.png) ](https://huggingface.co/osanseviero \"osanseviero\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/6065a9cbe43e52694178ed78/TwnrOfTacrwtLnBC8cS8e.jpeg) ](https://huggingface.co/emanuelevivoli \"emanuelevivoli\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/T762Ut0Y-w0sZB2ynvfbJ.jpeg) ](https://huggingface.co/ariG23498 \"ariG23498\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1664223180691-60c9687eed9842b2fc0f5357.png) ](https://huggingface.co/tushifire \"tushifire\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1642355231679-60d4f9efc742185d32980048.jpeg) ](https://huggingface.co/vangenugtenr \"vangenugtenr\")\n  * [![](https://huggingface.co/avatars/3372193b29c48ddaa67b34680d492643.svg) ](https://huggingface.co/Naruke \"Naruke\")\n  * [![](https://huggingface.co/avatars/0e9c6507b999f7eedb86463482b967a7.svg) ](https://huggingface.co/moeinshariatnia \"moeinshariatnia\")\n  * [![](https://huggingface.co/avatars/7aff6bf2845176720adfe3d051b76560.svg) ](https://huggingface.co/cs-fxr \"cs-fxr\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1631978236667-612b80d69459fc9129a984ba.jpeg) ](https://huggingface.co/duongna \"duongna\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png) ](https://huggingface.co/ybelkada \"ybelkada\")\n  * +121\n\n\nSystem theme \nCompany\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/) [](https://huggingface.co/)\nWebsite\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nHugging Face introduces QLoRA, enabling 4-bit quantization for LLMs, allowing efficient finetuning of large models (up to 65B parameters) on consumer GPUs. This method reduces memory usage without sacrificing performance, achieving state-of-the-art results in chatbot benchmarks.\n```"
  },
  {
    "instruction": "[ Tell 120+K peers about your AI research \u2192 Learn more \ud83d\udca1 ![](https://neptune.ai/wp-content/themes/neptune/img/icon-cancel.svg) ](https://neptune.ai/neurips-2024)\n[ ![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg) ](https://neptune.ai \"neptune.ai\")\n  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [Walkthrough [2 min]](https://neptune.ai/walkthrough)\n      * [Play with public sandbox](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n      * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [Neptune vs WandB](https://neptune.ai/vs/wandb)\n      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n      * [Other comparisons](https://neptune.ai/vs)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune Scale projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [AI Researcher](https://neptune.ai/product/ai-researcher)\n      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n      * [Academia & Kagglers](https://neptune.ai/research)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [Monitor training](https://neptune.ai/product/monitor-training)\n      * [Compare experiments](https://neptune.ai/product/compare-experiments)\n      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)\n      * [Reports](https://neptune.ai/product/reports)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/deepsense-ai)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2025/01/Artera-color-signet.png?fit=129%2C97&ssl=1)Case studyHow Neptune Helps Artera Bring AI Solutions to Market Faster ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/artera)\n[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](https://neptune.ai/customers)\n  * [Documentation](https://docs.neptune.ai/)\n  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [Menu Item](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [Blog](https://neptune.ai/blog)\n      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Grici\u016bnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)\n  * [Pricing](https://neptune.ai/pricing)\n  * [Enterprise](https://neptune.ai/product/enterprise)\n  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [Menu Item](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [About us](https://neptune.ai/about-us)\n      * [Customers](https://neptune.ai/customers)\n      * [Careers](https://neptune.ai/jobs)\n      * [Security](https://security.neptune.ai/)\n      * [Contact us](https://neptune.ai/contact-us)\n\n\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find? \nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/) > [Blog](https://neptune.ai/blog) > [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [Categories](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n      * [MLOps](https://neptune.ai/blog/category/mlops)\n      * [LLMOps](https://neptune.ai/blog/category/llmops)\n      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)\n    * [Categories](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)\n      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)\n      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)\n\n\n[ MLOps Learning Hub ![chevron](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://neptune.ai/mlops-learn-hub)\nSearch in Blog... \n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg) ![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog... \n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [Categories](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n      * [MLOps](https://neptune.ai/blog/category/mlops)\n      * [LLMOps](https://neptune.ai/blog/category/llmops)\n      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)\n    * [Categories](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)\n      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)\n      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)\n\n\n[ MLOps Learning Hub ![chevron](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://neptune.ai/mlops-learn-hub)\n  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [Walkthrough [2 min]](https://neptune.ai/walkthrough)\n      * [Play with public sandbox](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n      * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [Neptune vs WandB](https://neptune.ai/vs/wandb)\n      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n      * [Other comparisons](https://neptune.ai/vs)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune Scale projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [AI Researcher](https://neptune.ai/product/ai-researcher)\n      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n      * [Academia & Kagglers](https://neptune.ai/research)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [Monitor training](https://neptune.ai/product/monitor-training)\n      * [Compare experiments](https://neptune.ai/product/compare-experiments)\n      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)\n      * [Reports](https://neptune.ai/product/reports)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/deepsense-ai)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2025/01/Artera-color-signet.png?fit=129%2C97&ssl=1)Case studyHow Neptune Helps Artera Bring AI Solutions to Market Faster ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/artera)\n[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](https://neptune.ai/customers)\n  * [Documentation](https://docs.neptune.ai/)\n  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [Menu Item](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [Blog](https://neptune.ai/blog)\n      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Grici\u016bnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)\n  * [Pricing](https://neptune.ai/pricing)\n  * [Enterprise](https://neptune.ai/product/enterprise)\n  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [Menu Item](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n      * [About us](https://neptune.ai/about-us)\n      * [Customers](https://neptune.ai/customers)\n      * [Careers](https://neptune.ai/jobs)\n      * [Security](https://security.neptune.ai/)\n      * [Contact us](https://neptune.ai/contact-us)\n\n\n[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)\n[Neptune Blog](https://neptune.ai/blog)\n#  MLOps Landscape in 2024: Top Tools and Platforms \n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Stephen-Oladele.png?fit=560%2C560&ssl=1)\n[ Stephen Oladele  ](https://neptune.ai/blog/author/stephen-oladele)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg) 26 min \n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg) 14th March, 2025 \n[ML Tools](https://neptune.ai/blog/category/machine-learning-tools)[MLOps](https://neptune.ai/blog/category/mlops)\n![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 3 2'%3E%3C/svg%3E)\nAs you delve into the landscape of [MLOps](https://neptune.ai/blog/mlops) in 2024, you will find a plethora of tools and platforms that have gained traction and are shaping the way models are developed, deployed, and monitored. To provide you with a comprehensive overview, this article explores the key players in the MLOps and FMOps (or LLMOps) ecosystems, encompassing both open-source and closed-source tools, with a focus on highlighting their key features and contributions.\n## MLOps landscape\nOne of the defining characteristics of the MLOps landscape in 2024 is the coexistence of both open-source and closed-source solutions. [Open-source MLOps tools](https://neptune.ai/blog/best-open-source-mlops-tools) have gained significant traction due to their flexibility, community support, and adaptability to various workflows. On the other hand, closed-source platforms often provide enterprise-grade features, enhanced security, and dedicated user support.\nHere\u2019s an overview diagram of what the landscape looks like in 2024:\n[![MLOps and LLMOps landscape in 2024](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/09/MLOps-Landscape.png?resize=1920%2C926&ssl=1)](https://neptune.ai/mlops-landscape-3)_MLOps and LLMOps landscape in 202_ 4\nThe rest of this article will focus on highlighting over 90 MLOps tools and platforms on the market in 2024 in the following categories:\n  * End-to-end Machine Learning Operations (MLOps) platforms\n  * Experiment tracking, model metadata storage and management\n  * Dataset labeling and annotation\n  * Data storage and versioning\n  * Data quality monitoring and management\n  * Feature stores\n  * Model hubs\n  * Model quality testing\n  * Workflow orchestration and pipelining tools\n  * Model deployment and serving\n  * Model observability\n  * Responsible AI\n  * Compute and infrastructure\n  * GPU Cloud Servers\n  * [NEW] Serverless GPUs\n  * [NEW] Vector databases and data retrieval\n  * [NEW] Foundation model training frameworks\n\n\nBy providing an inclusive overview of the LLMOps and MLOps tools and MLOps platforms that emerged in 2024, this article will equip you with a better understanding of the diverse tooling landscape, enabling you to make informed decisions in your MLOps journey.\n## How to evaluate MLOps tools and platforms\nLike every software solution, evaluating [MLOps (Machine Learning Operations)](https://neptune.ai/blog/mlops) tools and platforms can be a complex task as it requires consideration of varying factors. Below, you will find some key factors to consider when assessing MLOps tools and platforms, depending on your needs and preferences.\n  * 1 Cloud and technology strategy \n  * 2 Alignment to other tools in the organization\u2019s tech stack \n  * 3 Commercial details \n  * 4 Knowledge and skills in the organization \n  * 5 Key use cases and/or user journeys \n  * 6 User support arrangements \n  * 7 Active user community and future roadmap \n\n\n### Cloud and technology strategy\nChoose an MLOps tool that aligns with your cloud provider or technology stack and supports the frameworks and languages you use for ML development. For example, if you use AWS, you may prefer Amazon SageMaker as an MLOps platform that integrates with other AWS services.\n### Alignment to other tools in the organization\u2019s tech stack\nConsider how well the MLOps tool integrates with your existing tools and workflows, such as data sources, data engineering platforms, code repositories, CI/CD pipelines, monitoring systems, etc. For example, [neptune.ai as an experiment tracker](https://neptune.ai/product/experiment-tracking), integrates with over 30 MLOps tools and platforms.\n### Commercial details\nConsider the commercial details when evaluating MLOps tools and platforms. Assess the pricing models, including any hidden costs, and ensure they fit your budget and scaling requirements. Review vendor support and maintenance terms (SLAs and SLOs), contractual agreements, and negotiation flexibility to align with your organization\u2019s needs. Free trials or proof of concepts (PoCs) can help you evaluate the tool\u2019s value before committing to a commercial agreement.\n### Knowledge and skills in the organization\nEvaluate the level of expertise and experience of your ML team and choose a tool that matches their skill set and learning curve. For example, if your team is proficient in Python and R, you may want an MLOps tool that supports open data formats like Parquet, JSON, CSV, etc., and Pandas or Apache Spark DataFrames.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  The Best Tools, Libraries, Frameworks and Methodologies that ML Teams Actually Use [Things We Learned from 41 ML Startups]  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/tools-libraries-frameworks-methodologies-ml-startups-roundup)\n### Key use cases and/or user journeys\nIdentify the main business problems and the data scientist\u2019s needs that you want to solve with ML, and choose a tool that can handle them effectively. For example, if your team works on recommender systems or [natural language processing](https://neptune.ai/blog/category/natural-language-processing) applications, you may want an MLOps tool that has built-in algorithms or templates for these use cases.\n### User support arrangements\nConsider the availability and quality of support from the provider or vendor, including documentation, tutorials, forums, customer service, etc. Also, check the frequency and stability of updates and improvements to the tool.\n### Active user community and future roadmap\nConsider a tool that has a strong and active community of users and developers who can provide feedback, insights, and best practices. In addition to considering the vendor\u2019s reputation, ensure you can be positioned to receive updates, see the roadmap of the tool, and see how they align with your goal.\n## End-to-end MLOps platforms\nEnd-to-end MLOps platforms provide a unified ecosystem that streamlines the entire ML workflow, from data preparation and model development to deployment and monitoring. \n### Core features of end-to-end MLOps platforms\nEnd-to-end MLOps platforms combine a wide range of essential capabilities and tools, which should include:\n  * **Data management and preprocessing** : Provide capabilities for data ingestion, storage, and preprocessing, allowing you to efficiently manage and prepare data for training and evaluation. This includes features for data labeling, data versioning, data augmentation, and integration with popular data storage systems.\n  * **Experimentation and model development:** Platforms should offer features for you to design and run experiments, explore different algorithms and architectures, and optimize model performance. This includes features for hyperparameter tuning, automated model selection, and visualization of model metrics.\n  * **Model deployment and serving** : Enable seamless model deployment and serving by providing features for containerization, API management, and scalable serving infrastructure.\n  * **Model monitoring and performance tracking** : Platforms should include capabilities to monitor and track the performance of deployed ML models in real-time. This includes features for logging, monitoring model metrics, detecting anomalies, and alerting, allowing you to ensure the reliability, stability, and optimal performance of your models.\n  * **Collaboration and version control** : Support collaboration among data and ML teams, allowing them to share code, models, and experiments. They should also offer version control capabilities to manage the changes and revisions of ML artifacts, ensuring reproducibility and facilitating effective teamwork.\n  * **Automated pipelining and workflow orchestration:** Platforms should provide tools for automated pipelining and workflow orchestration, enabling you to define and manage complex ML pipelines. This includes features for dependency management, task scheduling, and error handling, simplifying the management and execution of ML workflows.\n  * **Model governance and compliance** : They should address model governance and compliance requirements, so you can implement ethical considerations, privacy safeguards, and regulatory compliance into your ML solutions. This includes features for model explainability, fairness assessment, privacy preservation, and compliance tracking.\n  * **Integration with ML tools and libraries:** Provide you with flexibility and extensibility. This allows you to leverage your preferred ML tools and access a wide range of resources, enhancing productivity and enabling the use of cutting-edge techniques.\n\n\n![](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/03/MLOps-Landscape-in-2023-Top-Tools-and-Platforms.png?resize=1200%2C628&ssl=1)_Some popular end-to-end MLOps platforms in 202_ 4\n### Amazon SageMaker\n[Amazon SageMaker](https://aws.amazon.com/sagemaker/) provides a unified interface for data preprocessing, model training, and experimentation, allowing data scientists to collaborate and share code easily. SageMaker Studio offers built-in algorithms, automated model tuning, and seamless integration with AWS services, making it a powerful platform for developing and deploying machine learning solutions at scale.\n### Microsoft Azure ML Platform\nThe [Azure Machine Learning](https://azure.microsoft.com/en-au/products/machine-learning) platform provides a collaborative workspace that supports various programming languages and frameworks. With Azure Machine Learning, data scientists can leverage pre-built models, automate machine learning tasks, and seamlessly integrate with other Azure services, making it an efficient and scalable solution for machine learning projects in the cloud.\n### Google Cloud Vertex AI\n[Google Cloud Vertex AI](https://cloud.google.com/vertex-ai) provides a unified environment for both automated model development with AutoML and custom model training using popular frameworks. With built-in components and integration with Google Cloud services, Vertex AI simplifies the end-to-end machine learning process, making it easier for data science teams to build and deploy models at scale.\n### Modelbit\n[Modelbit](https://www.modelbit.com/) is an MLOps platform that covers model training, deployment, and lifecycle management. It provides auto-scaling infrastructure, allowing users to access CPU and GPU resources as needed while abstracting the complex GPU/CUDA setup.\nModelbit\u2019s endpoints [support request aliasing, splitting, and mirroring](https://doc.modelbit.com/endpoints/), allowing for complex rollout and testing patterns. The platform comes with built-in dataset management and feature store capabilities and integrates with [dbt](https://doc.modelbit.com/deployments/calling-from-dbt/), [Snowflake](https://doc.modelbit.com/warehouses/snowflake/), [Amazon Redshift](https://doc.modelbit.com/warehouses/redshift/), and [Amazon Athena](https://doc.modelbit.com/warehouses/athena/).\n### Domino Enterprise MLOps Platform\nThe [Domino Enterprise MLOps Platform](https://www.dominodatalab.com/product/domino-enterprise-mlops-platform) provides:\n  * A **system of record** for reproducible and reusable workflows.\n  * An **integrated model factory** to develop, deploy, and monitor models in one place using your preferred tools and languages.\n  * A **self-service infrastructure portal** for infrastructure and governance.\n\n\n### Databricks\n[Databricks](https://www.databricks.com/) is a cloud-native platform for big data processing, machine learning, and analytics built using the Data Lakehouse architecture. The platform gives you a unified set of tools for enterprise\u2011grade solutions for everything you need to do with data, including building, deploying, sharing, and maintaining solutions that have to do with data.\n### DataRobot\n[DataRobot MLOps](https://www.datarobot.com/platform/mlops/) offers features such as automated model deployment, monitoring, and governance. DataRobot MLOps facilitates collaboration between data scientists, data engineers, and IT operations, ensuring smooth integration of models into the production environment.\n### W&B (Weights & Biases)\n[W&B](https://wandb.ai/site) is a machine learning platform for your data science teams to track experiments, version and iterate on datasets, evaluate model performance, reproduce models, visualize results, spot regressions, and share findings with colleagues. The platform also offers features for hyperparameter optimization, automating model training workflows, model management, prompt engineering, and no-code ML app development.\n### Valohai\n[Valohai](https://valohai.com/product/) provides a collaborative environment for managing and automating machine learning projects. With Valohai, you can define pipelines, track changes, and run experiments on cloud resources or your own infrastructure. It simplifies the machine learning workflow and offers features for version control, data management, and scalability.\n### TrueFoundry\n[TrueFoundry](http://www.truefoundry.com/) is a cloud-native ML training and deployment PaaS on top of Kubernetes that enables ML teams to train and Deploy models at speed of Big Tech with 100% reliability and scalability \u2013 allowing them to save cost and release Models to production faster.\nIt abstracts Kubernetes from Data Scientists and enable them to work with infrastructure comfortably. It also allows teams to deploy and fine-tune large language models seamlessly with full security and cost optimization.\nTrueFoundry is open-ended, API-driven and integrates with the internal systems. You can also deploy it on a company\u2019s internal infrastructure and ensures complete Data Privacy and DevSecOps practices. Take a look at [this introductory article](https://blog.truefoundry.com/is-truefoundry-ml-platform-right-for-you/) to learn more about TrueFoundry.\n### Kubeflow\n[Kubeflow](https://www.kubeflow.org/) is an open-source machine learning platform built for running scalable and portable ML workloads on Kubernetes. It provides tools and components to facilitate end-to-end ML workflows, including data preprocessing, training, serving, and monitoring. \nKubeflow integrates with popular ML frameworks, supports versioning and collaboration, and simplifies the deployment and management of ML pipelines on Kubernetes clusters. Check out the [Kubeflow documentation](https://www.kubeflow.org/docs/).\n### Metaflow\n[Metaflow](https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9#:~:text=Over%20the%20past%20two%20years%2C%20Metaflow%20has%20been%20used%20internally%20at%20Netflix%20to%20build%20and%20manage%20hundreds%20of%20data%2Dscience%20projects%20from%20natural%20language%20processing%20to%20operations%20research.) helps data scientists and machine learning engineers build, manage, and deploy data science projects. It provides a high-level API that makes it easy to define and execute data science workflows. It also provides a number of features that help improve the reproducibility and reliability of data science projects. Netflix runs hundreds to thousands of ML projects on Metaflow\u2014that\u2019s how scalable it is.\nYou can use Metaflow for research, development, and production and integrate it with a variety of other tools and services. Check out the [Metaflow Docs](https://docs.metaflow.org/introduction/what-is-metaflow).\n[ ![](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/05/blog_feature_image_051678_7_0_9_0.jpg?fit=200%2C105&ssl=1) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) May interest you  Building a Machine Learning Platform [Definitive Guide]  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/ml-platform-guide)\n## Experiment tracking, model metadata storage, and management\n[Experiment tracking](https://neptune.ai/blog/ml-experiment-tracking) and model metadata management tools provide you with the ability to track experiment parameters, metrics, and visualizations, ensuring reproducibility and facilitating collaboration. \nWhen thinking about a tool for metadata storage and management, you should consider:\n  * **General business-related items** : Pricing model, security, and support.\n  * **Setup** : How much infrastructure is needed, and how easy is it to plug into your workflow?\n  * **Flexibility, speed, and accessibility** : can you customize the metadata structure? Is it accessible from your language/framework/infrastructure, framework, or infrastructure? Is it fast and reliable enough for your workflow?\n  * **Model versioning, lineage, and packaging** : Can you version and reproduce models and experiments? Can you see the complete model lineage with data/models/experiments used downstream?\n  * **Log and display of metadata** : what metadata types are supported in the API and UI? Can you render audio/video? What do you get out of the box for your frameworks?\n  * **Comparing and visualizing experiments and models** : what visualizations are supported, and does it have parallel coordinate plots? Can you compare images? Can you debug system information?\n  * **Organizing and searching experiments, models, and related metadata** : can you manage your workflow in a clean way in the tool? Can you customize the UI to your needs? Can you find experiments and models easily?\n  * **Model review, collaboration, and sharing** : can you approve models automatically and manually before moving to production? Can you comment and discuss experiments with your team?\n  * **CI/CD/CT compatibility** : how well does it work with CI/CD tools? Does it support continuous training/testing (CT)?\n  * **Integrations and support** : does it integrate with your model training frameworks? Can you use it inside orchestration and pipeline tools?\n\n\nDepending on whether your model metadata problems are on the side of research or productization, you may want to compare and choose a more specific solution: \n  * [Experiment tracking tools](https://neptune.ai/blog/best-ml-experiment-tracking-tools)\n  * [Model registry](https://neptune.ai/blog/ml-model-registry-best-tools)\n  * [ML metadata store](https://neptune.ai/blog/best-metadata-store-solutions)\n\n\n![Some popular experiment tracking, model metadata storage, and management tools in the 2023 MLOps landscape](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/8.png?resize=1200%2C628&ssl=1)_Some popular experiment tracking, model metadata storage, and management tools in the 2024 MLOps landscape_\n### MLflow\n[MLflow](https://github.com/mlflow/mlflow) is an open-source platform for managing the end-to-end machine learning lifecycle. It provides experiment tracking, versioning, and deployment capabilities. With MLflow, data science teams can easily log and compare experiments, track metrics, and organize their models and artifacts.\n### neptune.ai\n[neptune.ai](https://neptune.ai/) is the experiment tracker designed with a [strong focus on collaboration](https://neptune.ai/product/team-collaboration) and scalability. The tool is known for its user-friendly interface and [seamlessly integrates with popular ML frameworks](https://docs.neptune.ai/integrations/), enabling teams to adopt it into their existing workflows with minimal disruption. Neptune gives users a lot of flexibility when defining data structures and tracking metadata. \n![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n##  **Might be useful**\nUnlike manual, homegrown, or open-source solutions, neptune.ai is a scalable full-fledged component with user access management, developer-friendly UX, and advanced collaboration features. \nThat\u2019s especially valuable for ML/AI teams. Here\u2019s an example of how Neptune helped Waabi optimize their experiment tracking workflow. \n> ![](https://neptune.ai/wp-content/themes/neptune/img/icon-quote-small.svg)\n> The product has been very helpful for our experimentation workflows. Almost all the projects in our company are now using Neptune for experiment tracking, and it seems to satisfy all our current needs. It\u2019s also great that all these experiments are available to view for everyone in the organization, making it very easy to reference experimental runs and share results.  James Tu, Research Scientist at Waabi \n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/app-screenshot/bar-buttons.svg)\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button--test-tube.svg) See in app  ](https://demo.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9c57c497-1131-4644-827f-0fcff4f28ad2&dash=charts&compare=IwGl7SOrYhmOSYwCwCYSIJyQKy4zyHKmiaipnUqx54gVhV5VXlO1ewDsNpTTA2Co%2B3WpiaJQ0gY2ToS4-stUAGYSq2qk2bIjwR0DKgDZE6ABxhrEXGLCZ7YeKbOVLQA) ![zoom](https://neptune.ai/wp-content/themes/neptune/img/icon-zoom.svg) Full screen preview \n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nFull [case study with Waabi](https://neptune.ai/customers/waabi)\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nDive into[ documentation](https://docs.neptune.ai/)\n  * ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Get in touch](https://neptune.ai/contact-us) if you\u2019d like to go through a custom demo with your team\n\n\n### Comet ML\n[Comet ML](https://www.comet.com/site/) is a cloud-based experiment tracking and optimization platform. It enables data scientists to log, compare, and visualize experiments, track code, hyperparameters, metrics, and outputs. Comet offers interactive visualizations, collaboration features, and integration with popular ML libraries, making it a comprehensive solution for experiment tracking.\n### AimStack\n[AimStack](https://aimstack.io/) is an open-source AI metadata tracking tool designed to handle thousands of tracked metadata sequences. It provides a performant and intuitive UI for exploring and comparing training runs, prompt sessions, and more. It can help you track the progress of your experiments, compare different approaches, and identify areas for improvement.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Recommended  From Research to Production: Building The Most Scalable Experiment Tracker For Foundation Models  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)\n## Dataset labeling and annotation\nDataset labeling and annotation tools form a critical component of machine learning (ML) systems, enabling you to prepare high-quality training data for their models. These tools provide a streamlined workflow for annotating data, ensuring accurate and consistent labeling that fuels model training and evaluation. \n### Core features of dataset labeling and annotation tools\nDataset labeling and annotation tools should include: \n  * **Support for your data modalities** : Support for multiple data types, including audio, parquet, video, text data, and special dataset types like sensor readings and 3D magnetic resonance imaging (MRI) medical datasets.\n  * **Efficient collaboration** : They must facilitate seamless collaboration among annotators, enabling multiple users to work simultaneously, track progress, assign tasks, and communicate effectively, ensuring efficient annotation workflows.\n  * **Robust and customizable annotation interfaces** : User-friendly and customizable annotation interfaces empower annotators to easily label and annotate data, offering features like bounding boxes, polygons, keypoints, and text labels, enhancing the accuracy and consistency of annotations.\n  * **Integration with ML frameworks:** Seamless integration with popular ML frameworks allows annotated datasets to be directly used for model training and evaluation, eliminating data transformation complexities and enhancing the ML development workflow.\n  * **Versioning and auditing** : Provide features to track and manage different versions of annotations, along with comprehensive auditing capabilities, ensuring transparency, reproducibility, and accountability throughout the annotation process.\n  * **Data quality control:** Robust dataset labeling and annotation tools incorporate quality control mechanisms such as inter-annotator agreement analysis, review workflows, and data validation checks to ensure the accuracy and reliability of annotations.\n  * **Seamless data export:** Dataset labeling and annotation tools should support the seamless export of annotated data in various formats (e.g., JSON, CSV, TFRecord) compatible with downstream ML pipelines, facilitating the integration of annotated datasets into ML workflows.\n\n\nThe [alternatives for data labeling](https://neptune.ai/blog/data-labeling-software) in 2024 range from tools and services that support expert labelers to crowdsourcing services, third-party annotators, and programmatic labeling. \n![Some of the most popular data labeling and annotation MLOps tools in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/07/MLOps-Landscape-in-2023-Top-Tools-and-Platforms-3.png?resize=1200%2C628&ssl=1)_Some of the most popular data labeling and annotation MLOps tools in 202_ 4\n### Labelbox\n[Labelbox](https://labelbox.com/) is a data labeling platform that provides a range of features and capabilities to streamline the data labeling process and ensure high-quality annotations, such as collaborative annotation, quality control, and automation capabilities.\n### Amazon SageMaker Ground Truth\n[SageMaker Ground Truth](https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html) is a fully managed data labeling service designed to help you efficiently label and annotate your training data with high-quality annotations. Some of its features include a data labeling workforce, annotation workflows, active learning and auto-labeling, scalability and infrastructure, and so on.\n### Scale AI\n[Scale AI](https://scale.com/) is a data annotation platform that provides various annotation tools for image, video, and text data, including object detection, semantic segmentation, and natural language processing. Scale AI combines human annotators and machine learning algorithms to deliver efficient and reliable annotations for your team.\n### SuperAnnotate\n[SuperAnnotate](https://www.superannotate.com/) specializes in image and video annotation tasks. The platform provides a comprehensive set of annotation tools, including object detection, segmentation, and classification. \nWith features like collaborative annotation, quality control, and customizable workflows, SuperAnnotate empowers data science and machine learning teams to efficiently annotate their training data with high accuracy and precision. \n### Snorkel Flow\n[Snorkel Flow](https://snorkel.ai/snorkel-flow/) is a data-centric AI platform for automated data labeling, integrated model training and analysis, and enhanced domain expert collaboration. The platform\u2019s labeling capabilities include flexible label function creation, auto-labeling, active learning, and so on.\n### Kili\n[Kili](https://kili-technology.com/) is a cloud-based platform that can be accessed from anywhere for data scientists, machine learning engineers, and business users to label data more efficiently and effectively. It provides a variety of features that can help improve the quality and accuracy of labeled data, including:\n  * Labeling tools.\n  * Quality control.\n  * Collaboration.\n  * Reporting.\n\n\n### Encord Annotate\n[Encord Annotate](https://encord.com/automate/) is an automated annotation platform that performs AI-assisted image annotation, video annotation, and dataset management. It is part of the Encord suite of products alongside Encord Active. The key features of Encord Annotate include:\n  * Support for all annotation types.\n  * Auto-annotation tools such as Meta\u2019s [Segment Anything Model](https://encord.com/blog/segment-anything-model-explained/) and other [AI-assisted labeling techniques](https://encord.com/glossary/ai-assisted-labeling-definition/).\n  * MLOps workflows for [computer vision](https://neptune.ai/blog/category/computer-vision) and ML teams\n  * Use-case-centric annotations.\n  * Easy collaboration, annotator management, and QA workflows.\n  * Robust security functionality.\n\n\n### Superb AI\n[Superb AI](https://superb-ai.com/) provides a suite of tools to help data practitioners and ML Engineers effortlessly label, manage, and curate training datasets for computer vision. The labelling component supports and extensive range of annotation types and collaborative features for all stakeholders involved in the process (SMEs, data practitioners, engineers, ec cetera). You can also automate the process using a no-code UI and limited ground truth data with the help of models fine-tuned to your use case.\nOther features let yoy manage your team and datasets with quality control tools, project analytics, and user reports to ensure accurate and consistent annotations. [Here](https://docs.superb-ai.com/docs#label) you can learn more about Superb AI.\n## Data storage and versioning \nYou need data storage and versioning tools to maintain data integrity, enable collaboration, facilitate the reproducibility of experiments and analyses, and ensure accurate [ML model development](https://neptune.ai/blog/category/machine-learning-model-development) and deployment. Versioning allows you to trace and compare different iterations of datasets.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  Managing Dataset Versions in Long-Term ML Projects  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/managing-dataset-versions-in-long-term-ml-projects)\n### Core features of dataset storage and versioning tools\nRobust dataset storage and versioning tools should provide: \n  * **Secure and scalable storage** : Dataset storage and versioning tools should provide a secure and scalable infrastructure to store large volumes of data, ensuring data privacy and availability for you to access and manage datasets.\n  * **Dataset version control:** The ability to track, manage, and version datasets is crucial for reproducibility and experimentation. Tools should allow you to easily create, update, compare, and revert dataset versions, enabling efficient management of dataset changes throughout the ML development process.\n  * **Metadata management** : Robust metadata management capabilities enable you to associate relevant information, such as dataset descriptions, annotations, preprocessing steps, and licensing details, with the datasets, facilitating better organization and understanding of the data.\n  * **Collaborative workflows** : Dataset storage and versioning tools should support collaborative workflows, allowing multiple users to access and contribute to datasets simultaneously, ensuring efficient collaboration among ML engineers, data scientists, and other stakeholders.\n  * **Data Integrity and consistency** : These tools should ensure data integrity by implementing checksums or hash functions to detect and prevent data corruption, maintaining the consistency and reliability of the datasets over time.\n  * **Integration with ML frameworks** : Seamless integration with popular ML frameworks allows you to directly access and utilize the stored datasets within your ML pipelines, simplifying data loading, preprocessing, and model training processes.\n\n\n![Some popular data storage and versioning MLOps tools available for data teams in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/2.png?resize=1200%2C628&ssl=1)_Some popular data storage and versioning MLOps tools available for data teams in 202_ 4\n### DVC\n[DVC](https://dvc.org/) is an open-source tool for versioning datasets and models. It integrates with [Git](https://git-scm.com/) and provides a Git-like interface for data versioning, allowing you to track changes, manage branches, and collaborate with data teams effectively.\n### Dolt\n[Dolt](https://www.dolthub.com/) is an open-source relational database system built on Git. It combines the capabilities of a traditional database with the versioning and collaboration features of Git. Dolt allows you to [version (integration with DVC)](https://www.dolthub.com/blog/2021-04-16-dolt-dvc/) and manage structured data, making tracking changes, collaborating, and maintaining data integrity easier.\n### LakeFS\n[LakeFS](https://github.com/treeverse/lakeFS) is an open-source platform that provides [data lake](https://en.wikipedia.org/wiki/Data_lake) versioning and management capabilities. It sits between the data lake and cloud object storage, allowing you to version and control changes to data lakes at scale. LakeFS facilitates data reproducibility, collaboration, and data governance within the data lake environment.\n### Pachyderm\n[Pachyderm](https://github.com/pachyderm/pachyderm) is an open-source data versioning and lineage tool focusing on large-scale data processing and versioning. It provides data lineage tracking, versioning, and reproducibility features, making it suitable for managing complex data science workflows.\n### Delta Lake\n[Delta Lake](https://delta.io/) is an open-source storage layer that provides reliability, ACID transactions, and data versioning for big data processing frameworks such as [Apache Spark](https://spark.apache.org/). Your data team can manage large-scale, structured, and unstructured data with high performance and durability. Delta Lake helps ensure data consistency and enables efficient versioning and management within big data workflows.\n## Data quality monitoring and management\nYou may want to continuously observe data quality, consistency, and distribution to identify anomalies or shifts that may impact model performance. [Data monitoring tools](https://whylabs.ai/blog/posts/choosing-the-right-data-quality-monitoring-solution) help monitor the quality of the data. Data management encompasses organizing, storing, and governing data assets effectively, ensuring accessibility, security, and compliance. \nThese practices are vital for maintaining data integrity, enabling collaboration, facilitating reproducibility, and supporting reliable and accurate machine learning model development and deployment.\n### Core features of data quality monitoring and management tools\nData quality monitoring and management offer capabilities such as:\n  * **Data profiling** : Tools should provide comprehensive data profiling capabilities, allowing you to analyze and understand the characteristics, statistics, and distributions of your datasets, enabling better insights into data quality issues.\n  * **Anomaly detection** : Effective anomaly detection mechanisms can enable you to identify and flag outliers, missing values, and other data anomalies that could impact the accuracy and performance of ML models.\n  * **Data validation** : Tools should facilitate data validation by allowing you to define validation rules and perform checks to ensure that the dataset adheres to predefined criteria and standards.\n  * **Data cleansing** : The ability to detect and correct data errors, inconsistencies, and outliers is crucial for maintaining high-quality datasets. Tools should offer features for data cleansing, including data imputation, outlier removal, and noise reduction techniques.\n  * **Integration with ML workflows** : Integration with ML workflows and pipelines can enable you to incorporate data quality monitoring and management processes into your overall ML development workflow, ensuring ongoing monitoring and improvement of data quality.\n  * **Automation and alerting** : Tools should provide automation capabilities to streamline data quality monitoring tasks, along with alerting mechanisms to notify you of potential data quality issues, facilitating timely remediation.\n  * **Documentation and auditing** : The availability of documentation and auditing features allows ML engineers to track data quality changes over time, ensuring transparency, reproducibility, and compliance with data governance policies.\n\n\n![Some popular data quality monitoring and management MLOps tools available for data science and ML teams in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/4.png?resize=1200%2C628&ssl=1)_Some popular data quality monitoring and management MLOps tools available for data science and ML teams in 202_ 4\n### Great Expectations\n[Great Expectations](https://greatexpectations.io/) is an open-source library for data quality validation and monitoring. You can define expectations about data quality, track data drift, and monitor changes in data distributions over time. Great Expectations provides data profiling, anomaly detection, and validation features, ensuring high-quality data for machine learning workflows.\n### Talend Data Quality\n[Talend Data Quality](https://www.talend.com/products/data-quality/) is a comprehensive data quality management tool with data profiling, cleansing, and monitoring features. With Talend, you can assess data quality, identify anomalies, and implement data cleansing processes.\n### Monte Carlo\n[Monte Carlo](https://www.montecarlodata.com/) is a popular data observability platform that provides real-time monitoring and alerting for data quality issues. It could help you detect and prevent data pipeline failures, data drift, and anomalies. Montecarlo offers data quality checks, profiling, and monitoring capabilities to ensure high-quality and reliable data for machine learning and analytics.\n### Soda Core\n[Soda Core](https://github.com/sodadata/soda-core) is an open-source data quality management framework for SQL, Spark, and Pandas-accessible data. You can define and validate data quality checks, monitor data pipelines, and identify anomalies in real-time.\n### Metaplane\n[Metaplane](https://www.metaplane.dev/) is a data quality monitoring and management platform offering features for data profiling, quality checks, and lineage. It provides visibility into data pipelines, monitors data quality in real-time, and can help you identify and address data issues. Metaplane supports collaboration, anomaly detection, and data quality rule management.\n### Databand\n[Databand](https://databand.ai/) is a data pipeline observability platform that monitors and manages data workflows. It offers features for data lineage, data quality monitoring, and data pipeline orchestration. You can track data quality, identify performance bottlenecks, and improve the reliability of their data pipelines.\n## Feature stores\n[Feature stores](https://neptune.ai/blog/feature-stores-components-of-a-data-science-factory-guide) provide a centralized repository for storing, managing, and serving ML features, enabling you to find and share feature values for both model training and serving.\n### Core features of feature stores\nRobust feature store tools should offer capabilities such as:\n  * **Feature engineering pipelines** : Effective feature store tools allow you to define and manage feature engineering pipelines that include data transformation and feature extraction steps to generate high-quality ML features.\n  * **Feature serving** : Feature store tools should offer efficient serving capabilities, so you can retrieve and serve ML features for model training, inference, and real-time predictions.\n  * **Scalability and performance** : Feature store tools should provide scalability and performance optimizations to handle large volumes of data and support real-time feature retrieval, ensuring efficient and responsive ML workflows.\n  * **Feature versioning** : Tools should support versioning of ML features, allowing you to track changes, compare different versions, and ensure features processing techniques are consistent for training and serving ML models.\n  * **Feature validation** : Tools should provide mechanisms for validating the quality and integrity of ML features, enabling you to detect data inconsistencies, missing values, and outliers that may impact the accuracy and performance of ML models.\n  * **Feature metadata management** : Tools should support managing metadata associated with ML features, including descriptions, data sources, transformation logic, and statistical properties, to enhance transparency and documentation.\n  * **Integration with ML workflows** : Integration with ML workflows and pipelines facilitate the integration of feature engineering and feature serving processes into the overall ML development lifecycle. This can help you make model development workflows reproducible.\n\n\nMore companies started building feature stores and self-serve[ feature platforms](https://huyenchip.com/2023/01/08/self-serve-feature-platforms.html#feature-platform-comparison) to allow sharing and discovery of features across teams and projects.\n![Some popular feature stores available for data science and machine learning teams in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/Feature-stores.png?resize=1200%2C628&ssl=1)_Some popular feature stores available for data science and machine learning teams in 202_ 4\n### Feast\n[Feast](https://github.com/feast-dev/feast) is an open-source feature store with a centralized and scalable platform for managing, serving, and discovering features in MLOps workflows. You can define, store, and serve features for training and inference in machine learning models. Feast supports batch and real-time feature serving, enabling teams to efficiently access and reuse features across different stages of the ML lifecycle.\n### Tecton\n[Tecton](https://www.tecton.ai/feature-store/) is a feature platform designed to manage the end-to-end lifecycle of features. It integrates with existing data stores and provides components for feature engineering, feature storage, serving, and monitoring, helping your team improve productivity and operationalize their ML pipelines.\n### Hopsworks Feature Store\n[Hopsworks Feature Store](https://www.hopsworks.ai/the-python-centric-feature-store) is an open-source feature platform for data-intensive ML workloads. You can use Hopsworks Feature Store to build, manage, and serve features for machine learning models while ensuring data lineage, governance, and collaboration. This provides end-to-end support for data engineering and MLOps workflows.\n### Featureform\n[Featureform](https://github.com/featureform/featureform) is an open-source virtual feature store that can be used with any data infrastructure. It can help data science teams:\n  * Break feature engineering silos,\n  * Manage features over time through versioning.\n  * Share features across the organization.\n  * Provide tools for managing feature quality, including data profiling, feature drift detection, and feature impact analysis.\n\n\n### Databricks Feature Stores\n[Databricks Feature Store](https://docs.databricks.com/machine-learning/feature-store/index.html) is a centralized and scalable solution for managing features in machine learning workflows. You can leverage its unified repository to store, discover, and serve features, eliminating duplication and promoting code reusability. Integration with Apache Spark and Delta Lake enables efficient data processing and ensures data integrity and versioning. It offers [offline](https://docs.databricks.com/machine-learning/feature-store/concepts.html#offline-store) (primarily for batch inference) and [online](https://docs.databricks.com/machine-learning/feature-store/concepts.html#offline-store) stores (low-latency DB for real-time scoring).\nWith features like versioning, metadata management, point-in-time lookups, and data lineage, Databricks Feature Store enhances collaboration, improves productivity, and allows your data scientists to focus on model development rather than repetitive feature engineering tasks.\n### Google Cloud Vertex AI Feature Store\n[Vertex AI Feature Store](https://cloud.google.com/vertex-ai/docs/featurestore) is a feature management service that can provide your team with the capabilities for storing, discovering, and serving features for machine learning workloads.\nWith the Vertex AI Feature Store, your data scientists can access and reuse features across projects, leverage versioning and metadata management capabilities, and integrate seamlessly with other Google Cloud services to streamline their MLOps pipelines.\n## Model hubs\nModel hubs provide a centralized platform for managing, sharing, and deploying ML models. They empower you to streamline model management, foster collaboration, and accelerate the deployment of ML models.\n### Core features of model hubs\nModel hubs should offer features such as:\n  * **Model discovery** : Model hub tools offer search and discovery functionalities to explore and find relevant models based on criteria such as performance metrics, domain, architecture, or specific requirements.\n  * **Model sharing** : Tools should provide mechanisms for sharing ML models with other team members or across the organization, fostering collaboration, knowledge sharing, and reuse of pre-trained models.\n  * **Model metadata management** : Tools should support the management of metadata associated with ML models, including descriptions, the kinds of tasks they solve, performance metrics, training configurations, and version history, facilitating model documentation and reproducibility.\n  * **Integration with ML workflows** : Integration with ML workflows and pipelines allows you to incorporate model hub functionalities into your ML development lifecycle, simplifying model training, evaluation, and deployment processes.\n  * **Model governance and access control:** Model hub tools should provide governance features to set access controls, usage licenses, permissions, and sharing policies to ensure data privacy, security, and compliance with regulatory requirements. A good implementation of this can be the inclusion of [model cards](https://www.nocode.ai/ai-model-cards-101-an-introduction-to-the-key-concepts-and-terminology/).\n  * **Model deploymen** t: Model hub tools should provide inference APIs to test the model\u2019s capabilities and enable seamless deployment of ML models to various environments, including cloud platforms, edge devices, or on-premises infrastructure.\n  * **Model versioning** : Tools should support versioning of ML models within the model hub to track changes, compare different versions, and ensure reproducibility when training and deploying ML models.\n\n\n![Popular model hubs and repositories for pre-trained models in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/7.png?resize=1200%2C628&ssl=1)_Popular model hubs and repositories for pre-trained models in 202_ 4\n### Hugging Face Models Hubs\nThe [Hugging Face Model Hub](https://huggingface.co/models) is a popular platform and ecosystem for sharing, discovering, and utilizing pre-trained models for different ML tasks. Members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing. It offers a vast collection of models, including cutting-edge architectures like transformers, for tasks such as text classification, sentiment analysis, and question-answering. \nWith extensive language support and integration with major deep learning frameworks, the Model Hub simplifies the integration of [pre-trained models](https://huggingface.co/models) and [libraries](https://huggingface.co/docs/hub/models-libraries) into existing workflows, making it a valuable resource for researchers, developers, and data scientists.\n### Kaggle Models\n[Kaggle Models](https://www.kaggle.com/models) enable your data scientists to search and discover hundreds of trained, ready-to-deploy machine learning models in Kaggle and share pre-trained models from competitions. They can use pre-trained models to quickly and easily build machine learning models.\n### Tensorflow Hub\n[TensorFlow Hub](https://www.tensorflow.org/hub) is a repository of machine learning models that have been trained on specific datasets, or you can even contribute models that have been created for your use case. It enables transfer learning by making various ML models freely available as libraries or web API calls. The entire model can be downloaded to your source code\u2019s runtime with a single line of code.\nThe problem domains are broken down into: \n  * **Text** : language modelings, texts retrieval, question answering, text generation, and summarization.\n  * **Images** : classification, object detection, and style transfer, among several others,\n  * **Video** : video classification, generation, audio, and text, \n  * **Audio** : speech-to-text embeddings and speech synthesis, amongst others.\n\n\n## Hyperparameter optimization\nThe [hyperparameter optimization tooling landscape](https://neptune.ai/blog/best-tools-for-model-tuning-and-hyperparameter-optimization) so far hasn\u2019t changed much. The usual suspects are still the top tools in the industry. \n![Some popular hyperparameter optimization MLOps tools in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/Hyperparameter-optimization.png?resize=1200%2C628&ssl=1)_Some popular hyperparameter optimization MLOps tools in 202_ 4\n### Optuna\n[Optuna](https://optuna.org/) is an open-source hyperparameter optimization framework in Python. It offers a flexible and scalable solution for automating the search for optimal hyperparameter configurations. Optuna supports various optimization algorithms, including tree-structured Parzen estimators (TPE) and grid search, and provides a user-friendly interface for defining search spaces and objective functions.\n### Hyperopt\n[Hyperopt](http://hyperopt.github.io/hyperopt/) is another open-source library for hyperparameter optimization. It employs a combination of random search, tree of Parzen estimators (TPE), and other optimization algorithms. Hyperopt provides a simple interface for defining search spaces and objective functions and is particularly suitable for optimizing complex hyperparameter configurations. \n### SigOpt\n[SigOpt](https://sigopt.com/) is a commercial hyperparameter optimization platform designed to help data science and machine learning teams optimize their models. It offers a range of optimization algorithms, including Bayesian optimization, to efficiently explore the hyperparameter space. \nThe platform integrates well with popular machine learning libraries and frameworks, enabling easy incorporation into existing workflows. One notable feature of SigOpt is its ability to handle \u201cblack box\u201d optimization, making it suitable for optimizing models with proprietary or sensitive architectures.\n## Model quality testing\n[Model quality testing tools](https://neptune.ai/blog/tools-ml-model-testing) provide features to ensure the reliability, robustness, and accuracy of ML models.\n### Core features of model quality testing tools\nModel quality testing tools should offer capabilities such as:\n  * **Model evaluation techniques** : Evaluation methodologies to assess the performance of ML models, including metrics such as accuracy, precision, recall, F1-score, and area under the curve (AUC) to objectively assess model effectiveness.\n  * **Performance metrics:** Tools should offer a range of performance metrics to evaluate model quality across different domains and tasks and measure model performance specific to their use cases. Metrics such as AUC, F1-scores for classification problems, mean average precision (mAP) for object detection, and perplexity for language models.\n  * **Error analysis** : Model quality testing tools should facilitate error analysis to understand and identify the types of errors made by ML models, helping you gain insights into model weaknesses and prioritize areas for improvement.\n  * **Model versioning and comparison** : Model quality testing tools should support model versioning and comparison to compare the performance of different model versions and track the impact of changes on model quality over time.\n  * **Documentation and reporting:** The tools should provide features for documenting model quality testing processes, capturing experimental configurations, and generating reports, facilitating transparency, reproducibility, and collaboration.\n  * **Integration with ML workflows** : Integration with ML workflows and pipelines to incorporate model quality testing processes into your overall ML development lifecycle, ensuring continuous testing and improvement of model quality.\n  * **Fairness testing:** In the context of ethical AI, tools should provide capabilities for fairness testing to evaluate and mitigate biases and disparities in model predictions across different demographic groups or sensitive attributes.\n\n\n![Some popular MLOps tools to setup production ML model quality testing in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/9.png?resize=1200%2C628&ssl=1)_Some popular MLOps tools to setup production ML model quality testing in 202_ 4\n### Deepchecks\n[Deepchecks](https://deepchecks.com/) is a Python package for comprehensively validating your machine learning models and data with minimal effort. This includes checks related to various issues, such as model performance, data integrity, distribution mismatches, and more. \n### Truera\n[Truera](https://truera.com/) is a model intelligence platform designed to enable the trust and transparency of machine learning models. It focuses on model quality assurance and helps data science teams identify and mitigate model risks. Truera offers capabilities such as model debugging, explainability, and fairness assessment to gain insights into model behavior and identify potential issues or biases. Learn more from the documentation.\n### Kolena\n[Kolena](https://docs.kolena.io/) is a platform for rigorous testing and debugging to build team alignment and trust. It also includes an online platform to log the results and insights. Kolena focuses mostly on the ML unit testing and validation process at scale. It provides:\n  * **Data Studio** to search for testing scenarios in your project and identify edge cases\n  * **Test Case Manager** to manage and control test suites and cases and provide visibility into test coverage.\n  * **Debugger** to analyze model errors and identify new testing scenarios.\n\n\nYou interface with it through the web at app.kolena.io and programmatically via the [Kolena Python client](https://docs.kolena.io/testing-with-kolena/using-kolena-client).\n## Workflow orchestration and pipelining tools\nWorkflow orchestration and pipelining tools are essential components for streamlining and automating complex ML workflows. \n### Core features of workflow orchestration and pipelining tools\nWorkflow orchestration and pipelining tools should provide:\n  * **Task scheduling and dependency management** : Workflow orchestration and pipelining tools should provide robust scheduling capabilities to define dependencies between tasks and automatically execute them in the correct order, ensuring smooth workflow execution.\n  * **Workflow monitoring and visualization** : Workflow orchestration and pipelining tools should offer monitoring and visualization features to track the progress of workflows, monitor resource usage, and visualize workflow dependencies for better insights and troubleshooting.\n  * **Reproducibility and versioning** : Workflow orchestration and pipelining tools should support reproducibility by capturing the entire workflow configuration, including code versions, datasets, and dependencies. This will help you track past executions for reproducibility and debugging purposes.\n  * **Integration with ML frameworks** : Integration with popular ML frameworks so you can leverage your preferred ML libraries and tools within the workflow orchestration and pipelining system, ensuring compatibility and flexibility in model development.\n  * **Error handling and retry mechanisms** : The tools should provide robust error handling and retry mechanisms to handle failures, retry failed tasks, and exceptional cases gracefully, ensuring the reliability and resilience of ML workflows.\n  * **Distributed computing and scalability** : Have distributed computing capabilities to handle large-scale ML workflows, so you can leverage distributed computing frameworks or cloud infrastructure to scale your workflows and process massive amounts of data.\n\n\n![Some popular workflow orchestration and pipelining MLOps tools in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/10.png?resize=1200%2C628&ssl=1)_Some popular workflow orchestration and pipelining MLOps tools in 202_ 4\n### ZenML\n[ZenML](https://zenml.io/) is an extensible, open-source MLOps framework for building portable, production-ready MLOps pipelines. It\u2019s built for data scientists and MLOps engineers to collaborate as they develop for production. Learn more about the core concepts of ZenML in their [documentation](https://docs.zenml.io/getting-started/core-concepts).\n### Kedro Pipelines\n[Kedro](https://kedro.org/) is a Python library for building modular data science pipelines. Kedro assists you in creating data science workflows composed of reusable components, each with a \u201csingle responsibility,\u201d to speed up data pipelining, improve data science prototyping, and promote pipeline reproducibility. Check out the [Kedro\u2019s Docs.](https://docs.kedro.org/en/0.17.1/03_tutorial/06_visualise_pipeline.html)\n### Flyte\n[Flyte](https://flyte.org/) is a platform for orchestrating ML pipelines at scale. You can use Flyte for deployment, maintenance, lifecycle management, version control, and training. You can integrate it with platforms like [Feast](https://feast.dev/) and packages like [PyTorch](https://github.com/pytorch/pytorch), [TensorFlow](https://www.tensorflow.org/), and [Whylogs](https://github.com/whylabs/whylogs) to do tasks for the whole model lifecycle.\n[This article](https://mlops.community/flyte-mlops-simplified/) by Samhita Alla, a software engineer and tech evangelist at Union.ai, provides a simplified walkthrough of the applications of Flyte in MLOps. Check out [the documentation](https://docs.flyte.org/projects/cookbook/en/latest/index.html) to get started.\n### Prefect\n[Prefect](https://www.prefect.io/) is an open-source workflow management system that simplifies the orchestration of data pipelines and complex workflows. It offers features like task scheduling, dependency management, and error handling, ensuring efficient and reliable execution of data workflows. \nWith its Python-based infrastructure and user-friendly dashboard compared to [Airflow](https://airflow.apache.org/), Prefect enhances productivity and reproducibility for data engineering and data science teams.\n### Mage AI\n[Mage](https://www.mage.ai/) is an open-source tool to build, run, and manage data pipelines for transforming and integrating data. The features include:\n  * **Orchestration** to schedule and manage data pipelines with observability.\n  * **Notebook** for interactive Python, SQL, and R editors for coding data pipelines.\n  * **Data integrations** allow you to sync data from third-party sources to your internal destinations.\n  * **Streaming pipelines** to ingest and transform real-time data.\n  * **Integration** with [dbt](https://www.getdbt.com/) to build, run, and manage DBT models.\n\n\n## Model deployment and serving\nModel deployment and [model serving tools ](https://neptune.ai/blog/ml-model-serving-best-tools)enable you to deploy trained models into production environments and serve predictions to end-users or downstream systems.\n### Core features of model deployment and serving tools\nModel deployment and serving tools should offer capabilities such as:\n  * **Integration with deployment platforms** : Compatibility and integration with deployment platforms, such as cloud services or container orchestration frameworks, allow you to deploy and manage ML models on your preferred infrastructure.\n  * **Model versioning and management:** Have robust versioning and management capabilities to deploy and serve different versions of ML models, track model performance, and roll back to previous versions if needed.\n  * **API and endpoint management:** Include API and endpoint management features to define and manage endpoints, handle authentication and authorization, and provide a convenient interface for accessing the deployed ML models.\n  * **Automated scaling and load balancing:** Provide automated scaling and load balancing capabilities to handle varying workloads and distribute incoming requests efficiently across multiple instances of deployed models.\n  * **Model configuration and runtime flexibility** : Include flexibility in model configuration and runtime environments, so you can customize model settings, adjust resource allocation, and choose the runtime environment that best suits their deployment requirements.\n  * **Support different deployment patterns:** The tool should support batch processing, real-time (streaming) inference, and inference processors (in the form of REST APIs or function calls).\n\n\n![Some of the top MLOps tools for model serving and inference in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/MLOps-Landscape-in-2023-Top-Tools-and-Platforms-1.png?resize=1200%2C628&ssl=1)_Some of the top MLOps tools for model serving and inference in 202_ 4\n### BentoML\n[BentoML](https://github.com/bentoml/BentoML) is an open platform for machine learning in production. It simplifies model packaging and model management, optimizes model serving workloads to run at production scale, and accelerates the creation, deployment, and monitoring of prediction services.\n### Seldon Core\n[Seldon Core](https://www.seldon.io/solutions/open-source-projects/core) is an open-source platform with a framework that makes deploying your machine learning models and experiments at scale on Kubernetes easier and faster.\nIt\u2019s a cloud-agnostic, secure, reliable, and robust system maintained through a consistent security and update policy.\nSeldon Core summary:\n  * Easy way to containerize ML models using our pre-packaged inference servers, custom servers, or language wrappers.\n  * Powerful and rich inference graphs of predictors, transformers, routers, combiners, and more.\n  * Metadata provenance to ensure each model can be traced back to its respective training system, data, and metrics.\n  * Advanced and customizable metrics with integration to [Prometheus](https://prometheus.io/) and [Grafana](https://grafana.com/).\n  * Full auditing through model input-output request (logging integration with [Elasticsearch](https://www.elastic.co/)).\n\n\n### NVIDIA Triton Inference Server\n[NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) is open-source software that provides a unified management and serving interface for deep learning models. You can deploy and scale machine learning models in production, and it supports a wide variety of deep learning frameworks, including TensorFlow, PyTorch, and ONNX.\nTriton Inference Server is a valuable tool for data scientists and machine learning engineers because it can help them:\n  * Deploy machine learning models in production quickly and easily.\n  * Scale machine learning models to meet demand.\n  * Manage multiple machine learning models from a single interface.\n  * Monitor the performance of machine learning models.\n\n\n### NVIDIA TensorRT\n[NVIDIA TensorRT](https://developer.nvidia.com/tensorrt) is a high-performance deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications. You can use it to speed up the inference of deep learning models on NVIDIA GPUs.\nTensorRT is relevant to data scientists and machine learning engineers because it can help them to:\n  * **Improve the inference performance of their models**. TensorRT can optimize deep learning models for inference on NVIDIA GPUs, which can lead to significant performance improvements.\n  * **Reduce the size of their models**. TensorRT can also reduce the size of deep learning models, which can make them easier to deploy and use.\n  * **Make their models more efficient**. TensorRT can make deep learning models more efficient by optimizing them for specific hardware platforms.\n\n\n### OctoML\n[OctoML](https://octoml.ai/) is a machine learning acceleration platform that helps engineers quickly deploy machine learning models on any hardware, cloud provider, or edge device. It is built on top of the open-source Apache TVM compiler framework project.\nOctoML provides several features that make it a good choice for engineers who want to deploy machine learning models. These features include:\n  * A **unified model format** that makes it easy to deploy models on different hardware and cloud providers.\n  * A **pre-trained model repository** so you can find and deploy pre-trained models.\n  * A **model deployment pipeline** to ease deploying models to production.\n  * A **model monitoring dashboard** to monitor the performance of deployed models.\n\n\n## Model observability\nModel observability tools can allow you to gain insights into the behavior, performance, and health of your deployed ML models. \n### Core features of model observability tools\nModel observability tools should offer capabilities such as:\n  * **Logging and monitoring** : Enable logging and monitoring of key metrics, events, and system behavior related to the deployed ML models, facilitating real-time visibility into model performance, resource usage, and predictions.\n  * **Model performance tracking** : Track and analyze model performance over time, including metrics such as accuracy, precision, recall, or custom-defined metrics, providing a comprehensive view of model effectiveness.\n  * **Data drift and concept drift detection** : Include capabilities to detect and monitor data drift (changes in the input data distribution) and concept drift (changes in the relationship between inputs and outputs), so you can identify and address issues related to changing data patterns.\n  * **Alerting and anomaly detection** : Tools should provide alerting mechanisms to notify ML engineers of critical events, performance deviations, or anomalies in model behavior, enabling timely response and troubleshooting.\n  * **Visualization and dashboards:** Offer visualization capabilities and customizable dashboards to create informative and interactive visual representations of model behavior, performance trends, or feature importance.\n  * **Model debugging and root cause analysis:** Facilitate model debugging and root cause analysis by providing tools to investigate and diagnose issues related to model performance, predictions, or input data.\n  * **Compliance and regulatory requirements:** Provide features to address compliance and regulatory requirements, such as data privacy, explainability, or fairness, to ensure that deployed models adhere to ethical and legal standards.\n  * **Integration with ML workflow and deployment pipeline:** This enables you to incorporate model observability processes into the development lifecycle, ensuring continuous monitoring and improvement of deployed ML models.\n\n\n![Some model observability tools in the MLOps landscape in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/12.png?resize=1200%2C628&ssl=1)_Some model observability tools in the MLOps landscape in 202_ 4\n### WhyLabs\n[WhyLabs](https://whylabs.ai/) is an AI observability platform that helps data scientists and machine learning engineers monitor the health of their AI models and the data pipelines that fuel them. It provides various tools for monitoring model performance, detecting drift, and identifying issues with data quality.\nWhyLabs is relevant to data scientists and machine learning engineers because it can help them:\n  * Ensure the quality and accuracy of their models.\n  * Detect data drift.\n  * Identify issues with data quality.\n\n\n### Arize AI\n[Arize AI](https://arize.com/) is a machine learning observability platform that helps data scientists and machine learning engineers monitor and troubleshoot their models in production. It provides various tools for monitoring model performance, detecting drift, and identifying issues with data quality.\n### Mona\n[Mona](https://www.monalabs.io/platform) provides data scientists and machine learning engineers with an end-to-end monitoring solution that boosts visibility in their AI systems. It starts with ensuring a single source of information for the systems\u2019 behavior over time. It continues with ongoing tracking of key performance indicators and proactive insights about pockets of misbehavior \u2013 enabling teams to take preemptive, efficient corrective measures. \nBy providing real-time insights, Mona enables teams to detect issues weeks or months before they come to the surface, allowing them to troubleshoot and resolve the anomalies quickly.\n### Superwise\n[Superwise](https://superwise.ai/) is a model observability platform that helps data scientists and machine learning engineers monitor and troubleshoot their models in production. It provides various tools for monitoring model performance, detecting drift, and identifying issues with data quality.\nSuperwise is a powerful tool that can help your data scientists and machine learning engineers ensure the quality and accuracy of their AI models.\n### Evidently AI\n[Evidently AI](https://github.com/evidentlyai/evidently) is an open-source ML model monitoring system. It helps analyze machine learning models during development, validation, or production monitoring. The tool generates interactive reports from Pandas DataFrame. \n### Aporia\n[Aporia](https://www.aporia.com/product/ml-monitoring/) is a platform for machine learning observability. Data science and machine learning teams from various industries use Aporia to monitor model behavior, guarantee peak model performance, and easily scale production ML. It supports all machine learning use cases and model types by allowing you to completely customize your ML observability experience.\n[ ![](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/07/blog_feature_image_103615_4_1_5_8.jpg?fit=200%2C105&ssl=1) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Related post  LLM Observability: Fundamentals, Practices, and Tools  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/llm-observability)\n## Responsible AI\nYou can use responsible AI tools to deploy ML models through ethical, fair, and accountable techniques.\n### Core features of responsible AI tools\nResponsible AI tools should provide capabilities such as:\n  * **Fairness assessment:** Capabilities to assess and measure the fairness of ML models, identifying potential biases and discriminatory behavior across different demographic groups or sensitive attributes.\n  * **Explainability and interpretability:** Features that enable you to explain and interpret the decisions made by ML models.\n  * **Transparency and auditing:** Facilitate transparency and auditing of ML models, enabling you to track and document the entire model development and deployment process.\n  * **Robustness and security:** Address the robustness and security of ML models, including techniques to defend against adversarial attacks or model tampering, safeguarding ML systems from malicious exploitation or unintended vulnerabilities.\n  * **Regulatory compliance:** Help you adhere to regulatory requirements and industry standards, such as data protection regulations (e.g., GDPR), industry-specific guidelines, or fairness regulations.\n  * **Ethics and governance:** Provide guidelines and frameworks for you to incorporate ethical considerations and governance practices into your ML systems. \n  * **Bias mitigation** : Include techniques and algorithms to mitigate biases in ML models so you can address and reduce unwanted biases that may be present in your training data or model predictions.\n\n\n![Some of the responsible AI MLOps tools and platforms in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/13.png?resize=1200%2C628&ssl=1)_Some of the responsible AI MLOps tools and platforms in 202_ 4\n### Arthur AI\n[Arthur AI](https://www.arthur.ai/explainability) is a machine learning explainability platform that helps data scientists and machine learning engineers understand how their models work. It provides a variety of tools for explaining model predictions, including:\n  * **Feature importance** to show how important each feature is in a model\u2019s prediction.\n  * **Sensitivity analysis** to show how a model\u2019s prediction changes when a single feature is changed.\n  * **Counterfactual explanations** to show what changes would need to be made to an input in order to change a model\u2019s prediction.\n\n\n### Fiddler AI\n[Fiddler AI](https://www.fiddler.ai/ml-model-monitoring) is a model monitoring and explainable AI platform that helps data scientists and machine learning engineers understand how their models work. It provides a variety of tools for explaining model predictions, including:\n  * **Feature importance** to show how important each feature is in a model\u2019s prediction.\n  * **Sensitivity analysis** to show how a model\u2019s prediction changes when a single feature is changed.\n  * **Counterfactual explanation** to show what changes would need to be made to input in order to change a model\u2019s prediction.\n\n\n## Infrastructure: compute, tools, and technologies\nThe compute and infrastructure component is a vital aspect of machine learning (ML) systems, providing the necessary resources and environment to train, deploy, and run ML models at scale. \n### Core features of compute and infrastructure tools\nInfrastructure tools should provide capabilities such as:\n  * **Resource management:** Offer capabilities for efficient resource management, allowing you to allocate and provision computing resources such as CPUs, GPUs, or TPUs based on the requirements of their ML workloads. This ensures optimal resource utilization and cost efficiency.\n  * **Distributed computing:** Support distributed computing frameworks and technologies to leverage parallel processing, distributed training, or data partitioning for model training and inference.\n  * **Monitoring and performance optimization:** Provide monitoring and performance optimization features to track the performance of ML workloads, monitor resource usage, detect compute bottlenecks, and optimize the overall performance of ML systems.\n  * **High availability and fault tolerance** : Ensure high availability and fault tolerance by providing mechanisms to handle hardware failures, network disruptions, or system crashes. This helps maintain the reliability and uninterrupted operation of ML systems.\n  * **Integration with cloud and on-premises infrastructure:** Integrate with cloud platforms, on-premises infrastructure, or hybrid environments to leverage the advantages of different deployment models and infrastructure options based on their specific needs and preferences.\n  * **Security and data privacy:** Incorporate security measures and data privacy safeguards, including encryption, access controls, and compliance with data protection regulations. This ensures the confidentiality and integrity of data during ML operations.\n  * **Containerization and virtualization:** Facilitate containerization and virtualization technologies, enabling you to package your ML models, dependencies, and runtime environments into portable containers.\n  * **Scalability and elasticity** : Provide scalability and elasticity features, enabling you to easily scale up or down your computing resources based on the demand of your ML workloads. \n\n\n![Some popular MLOps tools for compute and infrastructure in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/14.png?resize=1200%2C628&ssl=1)_Some popular MLOps tools for compute and infrastructure in 202_ 4\n### Ray Open Source\n[Anyscale](https://www.anyscale.com/) is the developer of [Ray](https://github.com/ray-project/ray), a unified compute framework for scalable computing. Ray Open Source is an open-source, unified, and distributed framework for scaling AI and Python applications. You can effortlessly scale any workload or application from a laptop to the cloud without the cost or expertise required to build complex infrastructure.\n### Nuclio\n[Nuclio](https://github.com/nuclio/nuclio) is a high-performance \u201cserverless\u201d framework focused on data, I/O, and compute intensive workloads. It is well integrated with popular data science tools, such as [Jupyter](https://neptune.ai/blog/should-you-use-jupyter-notebooks-in-production) and [Kubeflow](https://neptune.ai/blog/kubeflow-pipelines-experiment-tracking); supports a variety of data and streaming sources, and supports execution over CPUs and GPUs.\n### Run:ai\n[Run.ai](https://www.run.ai/) optimizes and orchestrates GPU compute resources for AI and deep learning workloads. It builds a virtualization layer for AI workloads by abstracting workloads from the underlying infrastructure, creating a shared pool of resources that can be provisioned on the fly, enabling full utilization of expensive GPUs to compute. \nYou retain control and gain real-time visibility\u2014including seeing and provisioning run-time, queuing, and GPU utilization\u2014from a single web-based UI. \n### MosaicML Platform\nThe [MosaicML platform](https://www.mosaicml.com/training) provides you with the following key benefits when you want to fine-tune LLMs:\n  * **Multiple cloud providers** to leverage GPUs from different cloud providers without the overhead of setting up an account and all of the required integrations.\n  * **LLM training configurations**. The [composer library](https://www.mosaicml.com/composer) has a number of well-tuned configurations for training a variety of models and for different types of training objectives.\n  * **Managed infrastructure** for orchestration, efficiency optimizations, and fault tolerance (i.e., recovery from node failures).\n\n\n## GPU cloud servers\n[GPU Cloud vendors](https://fullstackdeeplearning.com/cloud-gpus/) have also exploded in popularity. The vendor offerings are divided into two classes:\n  * **GPU Cloud Servers** are long-running (but possibly pre-emptible) machines.\n  * **Severless GPUs** are machines that scale-to-zero in the absence of traffic.\n\n\n![Some GPU cloud platforms and offerings in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/15.png?resize=1200%2C628&ssl=1)_Some GPU cloud platforms and offerings in 202_ 4\n### Paperspace\n[Paperspace](https://www.paperspace.com/) is a high-performance cloud computing platform that provides GPU-accelerated virtual machines for building, training, and deploying models. It offers pre-configured instances with popular frameworks and tools, simplifying the setup process for data scientists. \nWith its user-friendly interface and flexible pricing options, Paperspace enables easy access to powerful GPU resources, facilitating faster training and inference of machine learning models in the cloud.\n### Lambda\n[Lambda GPU Cloud](https://lambdalabs.com/service/gpu-cloud) is a cloud-based platform from Lambda Labs that offers GPU-accelerated virtual machines for machine learning and deep learning tasks. It provides pre-installed frameworks, a user-friendly interface, and flexible pricing options. With Lambda GPU Cloud, you can easily access powerful GPU resources in the cloud, simplifying the development and deployment of machine learning models.\n## Serverless GPUs\n### Modal\n[Modal](https://modal.com/) is a platform that provides a solution for cloud-based encryption. You can write and run code in the cloud and launch custom containers. You can either [define a container environment](https://modal.com/docs/guide/custom-container) in their code or leverage the pre-built backend.\n### Baseten\n[Baseten](https://docs.baseten.co/) is a serverless backend for building ML-powered applications with auto-scaling, GPU access, CRON jobs, and serverless functions. It is agnostic to model training workflows and will work with any model trained using any framework.\n[ ![](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/08/Podcast-logo-1-e1691414642676.png?fit=200%2C119&ssl=1) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--resource.svg) Learn more  Deploying ML Models on GPU With Kyle Morris  Read more  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/deploying-ml-models-on-gpu-with-kyle-morris)\n## Vector databases and data retrieval\n[Vector databases](https://zilliz.com/what-is-milvus) are a new category of a database management system designed to search across images, video, text, audio, and other forms of unstructured data via their content rather than human-generated labels or tags. There are a few open-source and paid solutions that have exploded in usage by data and software teams over the past few years.\n![Popular vector databases and data retrieval tools in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/3.png?resize=1200%2C628&ssl=1)_Popular vector databases and data retrieval tools in 202_ 4\n### Pinecone\n[Pinecone](https://www.pinecone.io/) is a vector database built on top of the open-source [Lucene library](https://lucene.apache.org/core/) that makes it easy to build high-performance vector search applications. It provides a simple API that makes it easy to index and search vectors, and it also supports a variety of advanced features, such as fuzzy search and autocomplete.\n### Qdrant\n[Qdrant](https://github.com/qdrant/qdrant) is a vector similarity search engine and vector database written in [Rust](https://www.rust-lang.org/). It provides a production-ready service with a convenient API to store, search, and manage embeddings. It makes it useful for all sorts of neural-network or semantic-based matching, faceted search, and other applications.\n### Weviate\n[Weaviate](https://weaviate.io/) is an open-source vector database that stores both objects and vectors. It enables you to combine vector search with structured filtering while leveraging the fault-tolerance and scalability of a cloud-native database, all of which are accessible via GraphQL, REST, and various language clients.\n### Chroma\n[Chroma](https://github.com/chroma-core/chroma) is an open source vector store and embeddings database designed to make it easy to build AI applications with embeddings. It is fully-typed, integrates with programming frameworks like [LangChain](https://langchain.com/) and [LlamaIndex](https://github.com/jerryjliu/llama_index), and provides a single API to develop, test, and run your production AI applications.\n### Activeloop\nActiveloop\u2019s [Deep Lake](https://github.com/activeloopai/deeplake) is a vector database that powers foundational model training and integrates with popular tools such as [LangChain](https://langchain.com/), [LlamaIndex](https://github.com/jerryjliu/llama_index), [Weights & Biases](https://wandb.ai/site), and many more. It can: \n  * Use multi-modal datasets to fine-tune your LLMs,\n  * Store both the embeddings and the original data with automatic version control, so no embedding re-computation is needed.\n\n\n### Milvus\n[Milvus](https://github.com/milvus-io/milvus) is an open-source vector database built to power embedding similarity search and AI applications. Milvus makes unstructured data search more accessible and provides a consistent user experience regardless of the deployment environment.\n## LLMOps and foundation model training frameworks \nApart from the \u201ctraditional\u201d [model training frameworks](https://neptune.ai/blog/top-10-best-machine-learning-tools-for-model-training) like [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/), [TensorFlow 2](https://www.tensorflow.org/versions), and other model training tools that have remained consistent in the landscape over the past decade, some new tools have emerged in 2024 for training and fine-tuning foundation models.\n![Some LLMOps and foundation model training frameworks in 2023](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/06/5.png?resize=1200%2C628&ssl=1)Some LLMOps and foundation model training frameworks in 2024\n### Guardrails\n[Guardrails](https://shreyar.github.io/guardrails/#rail-spec) is an open-source Python package that lets your data scientist add structure, type, and quality guarantees to the outputs of large language models (LLMs). Guardrails:\n\u2013 Does pydantic-style validation of LLM outputs. This includes semantic validation such as checking for bias in generated text, checking for bugs in generated code, etc.\n\u2013 Takes corrective actions (e.g. asking LLM again) when validation fails,\n\u2013 Enforces structure and type guarantees (e.g., JSON).\n### LangChain\n[LangChain](https://github.com/hwchase17/langchain) is an open-source framework for building applications that use large language models (LLMs). It provides a number of features that make it easy to use LLMs, including:\n  * An API for interacting with LLMs.\n  * Out-of-the-box pre-trained LLMs.\n  * Tools for fine-tuning LLMs for specific tasks.\n  * Example applications that use LLMs.\n\n\n### LLamaIndex\n[LlamaIndex](https://github.com/jerryjliu/llama_index) is a simple, flexible interface between your external data and LLMs. It provides the following tools in an easy-to-use fashion:\n  * Data connectors to your existing data sources and data formats (API\u2019s, PDF\u2019s, docs, SQL, etc.)\n  * Indices over your unstructured and structured data for use with LLM\u2019s. These indices help to abstract away common boilerplate and pain points for in-context learning: \n    * Storing context in an easy-to-access format for prompt insertion.\n    * Dealing with prompt limitations (e.g. 4096 tokens for Davinci) when context is too big.\n    * Dealing with text splitting.\n  * An interface for users to query the index (feed in an input prompt) and obtain a knowledge-augmented output.\n  * A comprehensive toolset, trading off cost and performance.\n\n\n### DUST\n[Dust](https://github.com/dust-tt/dust) is designed to provide a flexible framework to define and deploy large language model apps without having to write any execution code. It is specifically intended to ease:\n  * Working on multiple examples simultaneously while designing a large language model app.\n  * Introspecting model outputs produced by intermediary steps of large language model apps.\n  * Iterating on the design of large language model apps by providing a granular and automated versioning system.\n\n\n## Conclusion\nThe MLOps and LLMOps landscape featured a diverse array of tools and platforms aimed at enabling organizations and individuals to effectively manage either part or the full end-to-end machine learning lifecycle. The dynamic ecosystem encompassed both open-source and commercial offerings, addressing various stages of the ML workflow. The field was rapidly evolving, giving practitioners plenty of choices to operationalize machine learning effectively.\n## MLOps tools and platforms FAQ\n### What devops tools are used in machine learning in 2024?\nSome of the popular DevOps tools in the machine learning space include:\n  * C[ontinuous integration and deployment (CI/CD) tools](https://neptune.ai/blog/continuous-integration-continuous-deployment-tools-for-machine-learning) like [Jenkins](https://www.jenkins.io/), [GitLab CI/CD](https://docs.gitlab.com/ee/ci/), and [CircleCI](https://circleci.com/) are gaining more adoption to enable automated testing, integration, and deployment of machine learning models.\n  * [Containerization tools ](https://neptune.ai/blog/kubernetes-vs-docker-for-machine-learning-engineer)such as Docker and Kubernetes used to package machine learning models, dependencies, and infrastructure configurations are still dominating.\n  * Configuration management tools like [Ansible](https://github.com/ansible/ansible), [Puppet](https://www.puppet.com/), and [Chef](https://www.chef.io/) used to automate the configuration and provisioning of infrastructure, are seeing lesser uptake as more operable and maintainable MLOps platforms emerge. \n\n\n### What MLOps frameworks work with sensitive data?\nThere are several MLOps frameworks that prioritize data privacy and can be used with sensitive data. Some of these frameworks include:\n[TensorFlow Privacy](https://github.com/tensorflow/privacy) provides tools and techniques for training models on sensitive data in TensorFlow while incorporating privacy safeguards like differential privacy and federated learning.\n[PySyft](https://github.com/OpenMined/PySyft) enables secure and private machine learning by implementing techniques such as federated learning, homomorphic encryption, and secure multi-party computation (MPC).[Intel OpenVINO](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html) (Open Visual Inference and Neural Network Optimization) toolkit provides optimizations for running machine learning models on Intel hardware. It includes features for enhancing privacy and security, such as model encryption, tamper-resistant model execution, and secure inference.\n##  Was the article useful? \n![yes](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--yes.svg) Yes  ![no](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--no.svg) No \n![](https://neptune.ai/wp-content/themes/neptune/img/icon-bulb.svg) Suggest changes \nYour email Your message (optional)\nThis site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply. \nI am familiar with the [Privacy Policy](https://neptune.staginglab.eu/privacy-policy)*\nSubmit\n\u0394\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/i-box/header-icon.svg)\n### **More about** MLOps Landscape in 2024: Top Tools and Platforms \n####  Check out our  **product resources** and  **related articles** below: \n[ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  Learnings From Teams Training Large-Scale Models: Challenges and Solutions For Monitoring at Hyperscale  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/learnings-from-teams-training-large-scale-models) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  From Research to Production: Building The Most Scalable Experiment Tracker For Foundation Models  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  LLMOps: What It Is, Why It Matters, and How to Implement It  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/llmops) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  Observability in LLMOps: Different Levels of Scale  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/observability-in-llmops)\n###  Explore more content topics: \n[ Computer Vision ](https://neptune.ai/blog/category/computer-vision) [ General ](https://neptune.ai/blog/category/general) [ LLMOps ](https://neptune.ai/blog/category/llmops) [ ML Model Development ](https://neptune.ai/blog/category/machine-learning-model-development) [ ML Tools ](https://neptune.ai/blog/category/machine-learning-tools) [ MLOps ](https://neptune.ai/blog/category/mlops) [ Natural Language Processing ](https://neptune.ai/blog/category/natural-language-processing) [ Paper Reflections ](https://neptune.ai/blog/category/paper-reflections) [ Product Updates ](https://neptune.ai/blog/category/product-updates) [ Reinforcement Learning ](https://neptune.ai/blog/category/reinforcement-learning) [ Tabular Data ](https://neptune.ai/blog/category/tabular-data) [ Time Series ](https://neptune.ai/blog/category/time-series-forecasting)\nAbout neptune.ai ![chevron](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1920 1080'%3E%3C/svg%3E) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-play.svg)\nNeptune is the experiment tracker purpose-built for foundation model training. \nIt lets you monitor and visualize thousands of per-layer metrics\u2014losses, gradients, and activations\u2014at any scale. Drill down into logs and debug training issues fast. Keep your model training stable while reducing wasted GPU cycles. \n[ Play with a live project  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://demo.neptune.ai/) [ See Docs  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://docs.neptune.ai/)\nTable of contents\n![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-small.svg)\n  1. MLOps landscape\n  2. How to evaluate MLOps tools and platforms\n  3. End-to-end MLOps platforms\n  4. Experiment tracking, model metadata storage, and management\n  5. Dataset labeling and annotation\n  6. Data storage and versioning \n  7. Data quality monitoring and management\n  8. Feature stores\n  9. Model hubs\n  10. Hyperparameter optimization\n  11. Model quality testing\n  12. Workflow orchestration and pipelining tools\n  13. Model deployment and serving\n  14. Model observability\n  15. Responsible AI\n  16. Infrastructure: compute, tools, and technologies\n  17. GPU cloud servers\n  18. Serverless GPUs\n  19. Vector databases and data retrieval\n  20. LLMOps and foundation model training frameworks \n  21. Conclusion\n  22. MLOps tools and platforms FAQ\n\n\n![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-cancel.svg) ![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-more.svg)\nCheck also: [Building a Machine Learning Platform [Definitive Guide]](https://neptune.ai/blog/ml-platform-guide)\n##  **Monitor your model training at scale**\nJoin 60,000+ researchers and practitioners who use Neptune to debug training failures, spot anomalies, and compare experiments.\n[ Try for free  ](https://app.neptune.ai/register) [ Get early access to Neptune Scale  ](https://neptune.ai/early-access)\n[ Take an interactive product tour  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)\n![](https://neptune.ai/wp-content/themes/neptune/img/footer-post-illustration.svg)\n#### Newsletter\nTop articles, case studies, events (and more) in your inbox every month.\nGet Newsletter\n  * [Product](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [Walkthrough](https://neptune.ai/walkthrough)\n    * [Documentation](https://docs.neptune.ai/)\n    * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [Pricing](https://neptune.ai/pricing)\n    * [Service status](https://status.neptune.ai/)\n\n\n  * [Solutions](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [AI Researcher](https://neptune.ai/product/ai-researcher)\n    * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n    * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n    * [Enterprise](https://neptune.ai/product/enterprise)\n    * [Academic Research](https://neptune.ai/research)\n\n\n  * [Compare](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [Neptune vs Weights & Biases](https://neptune.ai/vs/wandb)\n    * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n    * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n    * [Other comparisons](https://neptune.ai/vs)\n    * [ML experiment tracking tools](https://neptune.ai/blog/best-ml-experiment-tracking-tools)\n\n\n  * [Community](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [Blog](https://neptune.ai/blog)\n    * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n    * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n    * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n    * [How to Build an Experiment Tracker](https://neptune.ai/blog/build-experiment-tracking-tool)\n\n\n  * [Company](https://neptune.ai/blog/mlops-tools-platforms-landscape/)\n    * [Customers](https://neptune.ai/customers)\n    * [About us](https://neptune.ai/about-us)\n    * [Careers](https://neptune.ai/jobs)\n    * [Security portal and SOC 2](https://security.neptune.ai/)\n    * [Contact us](https://neptune.ai/contact-us)\n\n\n[ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-linked-in.svg) ](https://www.linkedin.com/company/neptuneai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-twitter.svg) ](https://twitter.com/neptune_ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-github.svg) ](https://github.com/neptune-ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-facebook.svg) ](https://www.facebook.com/neptuneAI) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-youtube.svg) ](https://www.youtube.com/channel/UCvOJU-ubyUqxGSDRN7xK4Ng) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-spotify.svg) ](https://open.spotify.com/show/4kGi82i4wTYgHbWmVMri5x) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-apple-podcast.svg) ](https://podcasts.apple.com/us/podcast/mlops-live/id1634179447)\n[ Terms of Service ](https://neptune.ai/terms-of-service) [ Privacy Policy ](https://neptune.ai/privacy-policy) [ SLA ](https://neptune.ai/service-level-agreement)\nCopyright \u00a9 2025 Neptune Labs. All rights reserved.\n![](https://neptune.ai/wp-content/themes/neptune/img/footer-stars--414w.png) ![](https://pixel.wp.com/g.gif?v=ext&blog=211928962&post=24506&tz=0&srv=neptune.ai&hp=atomic&ac=2&amp=0&j=1%3A14.5-a.9&host=neptune.ai&ref=&fcp=1628&rand=0.11827935663881428) ![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=fdeb5362-d7be-491c-ac2c-ef5ee9101643&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=d59ac064-ac2b-455d-b93b-0441245e7b3e&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=fdeb5362-d7be-491c-ac2c-ef5ee9101643&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=d59ac064-ac2b-455d-b93b-0441245e7b3e&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=2abff38c-cd53-46ef-a2a9-f3352eac1626&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=d59ac064-ac2b-455d-b93b-0441245e7b3e&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=2abff38c-cd53-46ef-a2a9-f3352eac1626&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=d59ac064-ac2b-455d-b93b-0441245e7b3e&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=a3f77a45-12f8-4238-862e-e5c11c41cfe9&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=d59ac064-ac2b-455d-b93b-0441245e7b3e&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=a3f77a45-12f8-4238-862e-e5c11c41cfe9&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=d59ac064-ac2b-455d-b93b-0441245e7b3e&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=40574ade-916d-4923-99d0-3803af5e697e&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=d59ac064-ac2b-455d-b93b-0441245e7b3e&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=40574ade-916d-4923-99d0-3803af5e697e&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=d59ac064-ac2b-455d-b93b-0441245e7b3e&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=48bcb9d9-fe79-4546-b22a-04137fb4f689&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=d59ac064-ac2b-455d-b93b-0441245e7b3e&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=48bcb9d9-fe79-4546-b22a-04137fb4f689&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=d59ac064-ac2b-455d-b93b-0441245e7b3e&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=7568fd91-7c28-4e5c-ac51-cc1aba2bddc0&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=d59ac064-ac2b-455d-b93b-0441245e7b3e&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=7568fd91-7c28-4e5c-ac51-cc1aba2bddc0&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=d59ac064-ac2b-455d-b93b-0441245e7b3e&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)\n![](https://bat.bing.com/action/0?ti=97011748&tm=gtm002&Ver=2&mid=21d59ac0-27ba-4c01-9189-bb1adefe56c0&bo=1&sid=37ef08200d8e11f0a36ed785bff3ae58&vid=37ef08800d8e11f089240dd13df19027&vids=0&msclkid=N&pi=918639831&lg=en-US&sw=1080&sh=600&sc=30&tl=MLOps%20Landscape%20in%202024%3A%20Top%20Tools%20and%20Platforms&p=https%3A%2F%2Fneptune.ai%2Fblog%2Fmlops-tools-platforms-landscape&r=&lt=3598&evt=pageLoad&sv=1&cdb=AQkR&rn=491797)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe 2024 MLOps landscape features a diverse array of tools for managing the ML lifecycle, including both open-source and commercial platforms. Key areas include experiment tracking, model deployment, data management, and responsible AI practices. Evaluating tools involves assessing integration, support, and alignment with organizational needs.\n```"
  },
  {
    "instruction": "This website stores cookies on your computer. These cookies are used to collect information about how you interact with our website and allow us to remember you. We use this information in order to improve and customize your browsing experience and for analytics and metrics about our visitors both on this website and other media. To find out more about the cookies we use, see our Privacy Policy\nIf you decline, your information won\u2019t be tracked when you visit this website. A single cookie will be used in your browser to remember your preference not to be tracked.\nCookies settings\nAcceptDecline\n[ ![Arize AI Logo](https://arize.com/wp-content/themes/arize-2022/images/logo2.svg) ](https://arize.com)\n  * [Products](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [![](https://arize.com/wp-content/uploads/2024/07/Group-11691.svg)PRODUCT](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Arize AX - Generative](https://arize.com/generative-ai/)\n    * [Arize AX - ML & CV](https://arize.com/ml-cv-observability)\n    * [Arize PX - Phoenix OSS](https://arize.com/phoenix-oss/)\n[](https://arize.com/resource/arize-platform-demo-2/)\n[![](https://arize.com/wp-content/uploads/2024/07/Platform-demo-thumbnail-2024.jpg)](https://arize.com/resource/arize-platform-demo-2/)\n#### Arize demo\nSee how it works\n  * [Pricing](https://arize.com/pricing)\n  * [Docs](https://docs.arize.com/arize)\n  * [Learn](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [![](https://arize.com/wp-content/uploads/2024/07/Layer_1-1.svg)LEARN](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Blog](https://arize.com/blog/)\n    * [Agents Hub](https://arize.com/ai-agents/)\n    * [LLM Evals Hub](https://arize.com/llm-evaluation)\n    * [AI Fundamentals](https://arize.com/blog-course/introduction-fundamentals/)\n    * [Paper Readings](https://arize.com/ai-research-papers/)\n    * [Video Tutorials](https://www.youtube.com/@arizeai/featured)\n    * [Courses](https://courses.arize.com/courses/)\n    * [Community & Events](https://arize.com/community/)\n[](https://arize.com/observe-2025/)\n[![](https://arize.com/wp-content/uploads/2025/02/Observe-Reg-open-nav-promo-2.jpg)](https://arize.com/observe-2025/)\n#### Arize:Observe is Back!\nGet Special Pricing Until April 30\n  * [Company](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [![](https://arize.com/wp-content/uploads/2024/07/Layer_1-2.svg)COMPANY](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [About](https://arize.com/about-us)\n    * [Careers](https://arize.com/careers/)\n    * [Partners](https://arize.com/partners/)\n    * [Press](https://arize.com/press/)\n    * [Security](https://arize.com/trust-center/)\n[](https://arize.com/customers/)\n[![](https://arize.com/wp-content/uploads/2024/07/Group-11696.png)](https://arize.com/customers/)\n#### Customers\nSee who is achieving better outcomes in production.\n  * [Star](https://github.com/Arize-ai/phoenix)\n  * [Get a Demo](https://arize.com/request-a-demo/)\n  * [Sign in](https://app.arize.com/auth/login?redirect_uri=/)\n  * [Sign Up](https://app.arize.com/auth/join)\n\n\n![](https://arize.com/wp-content/themes/arize-2022/images/icon-menu.svg)\n\u00d7\n###### ARIZE MACHINE LEARNING COURSE\n####  ![](https://arize.com/wp-content/themes/arize-2022/images/icon-courses.svg) The Fundamentals \n  * [ML Observability Course Overview](https://arize.com/blog-course/)\n  * [ML Observability Basics](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [What Is Observability?](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [ML Observability: The Essentials](https://arize.com/blog-course/machine-learning-observability-essentials/)\n    * [Tracing In Machine Learning](https://arize.com/blog-course/performance-tracing-machine-learning/)\n  * [Model Evaluation Metrics](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Top Model Evaluation Metrics](https://arize.com/blog-course/model-evaluation-metrics/)\n    * [Binary Cross Entropy](https://arize.com/blog-course/binary-cross-entropy-log-loss/)\n    * [Precision](https://arize.com/blog-course/precision-ml/)\n    * [Recall](https://arize.com/blog-course/precision-vs-recall/)\n    * [F1 Score](https://arize.com/blog-course/f1-score/)\n    * [Calibration Curve](https://arize.com/blog-course/what-is-calibration-reliability-curve/)\n    * [PR AUC](https://arize.com/blog/what-is-pr-auc/)\n    * [AUC ROC](https://arize.com/blog/what-is-auc/)\n    * [Mean Absolute Error](https://arize.com/blog-course/mean-absolute-error-in-machine-learning-what-you-need-to-know/)\n    * [Mean Absolute Percentage Error (MAPE)](https://arize.com/blog-course/mean-absolute-percentage-error-mape-what-you-need-to-know/)\n    * [R Squared](https://arize.com/blog-course/r-squared-understanding-the-coefficient-of-determination/)\n    * [RMSE](https://arize.com/blog-course/root-mean-square-error-rmse-what-you-need-to-know/)\n    * [Normalized Discounted Cumulative Gain (NDCG)](https://arize.com/blog-course/ndcg/)\n    * [Other Rank Aware Evaluation Metrics](https://arize.com/blog-course/monitoring-collaborative-filtering-recsys/)\n    * [Bleu Score and LLM Metrics](https://arize.com/blog-course/generative-ai-metrics-bleu-score/)\n  * [Drift Metrics](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Drift Metrics: a Quickstart Guide](https://arize.com/blog-course/drift/)\n    * [Data Binning](https://arize.com/blog-course/data-binning-production/)\n    * [Population Stability Index (PSI)](https://arize.com/blog-course/population-stability-index-psi/)\n    * [KL Divergence](https://arize.com/blog-course/kl-divergence/)\n    * [Jensen Shannon Divergence](https://arize.com/blog-course/jensen-shannon-divergence/)\n    * [Kolmogorov Smirnov Test](https://arize.com/blog-course/kolmogorov-smirnov-test/)\n  * [Model Fairness](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Algorithmic Bias](https://arize.com/blog-course/algorithmic-bias-examples-tools/)\n    * [Building Transparent AI Software Systems](https://arize.com/blog-course/transparent-ethical-ai-software-systems/)\n  * [Data Quality](https://arize.com/blog-course/data-quality-management-for-mlops/)\n  * [Service-Level Monitoring](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Service Monitoring Metrics](https://arize.com/blog-course/service-monitoring-metrics/)\n    * [ML Service-Level Performance Monitoring Essentials](https://arize.com/blog-course/ml-service-level-performance-monitoring/)\n  * [Explainability](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Explainability](https://arize.com/blog-course/explainability-xai-primer/)\n    * [Explainability Techniques](https://arize.com/blog-course/explainability-techniques-shap/)\n  * [Monitoring Image and Language Models and Embeddings](https://arize.com/blog-course/embeddings-meaning-examples-and-how-to-compute/)\n    * [KNN Algorithm](https://arize.com/blog-course/knn-algorithm-k-nearest-neighbor/)\n    * [Tokenization](https://arize.com/blog-course/tokenization/)\n    * [Embedding Versioning](https://arize.com/blog-course/embedding-versioning/)\n    * [Dimensionality Reduction](https://arize.com/blog-course/reduction-of-dimensionality-top-techniques/)\n    * [What Is LLMOps?](https://arize.com/blog-course/llmops-operationalizing-llms-at-scale/)\n    * [Monitoring Embedding/Vector Drift](https://arize.com/blog-course/embedding-drift-euclidean-distance/)\n    * [BERT](https://arize.com/blog-course/unleashing-bert-transformer-model-nlp/)\n    * [Fine-Tuning LLM Applications with LangChain](https://arize.com/blog-course/langchain-llm-agent-monitoring/)\n    * [Evals from OpenAI](https://arize.com/blog-course/evals-openai-simplifying-llm-evaluation/)\n    * [Guide To LLMs Evaluating LLMs](https://arize.com/blog-course/assessing-large-language-models/)\n\n\n  * [Certifications](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [LLM Certification](https://courses.arize.com/courses/)\n    * [Agents, Tools, and Chains](https://courses.arize.com/p/agents-tools-and-chains)\n    * [LLM Evaluations](https://courses.arize.com/p/llm-evaluations)\n    * [Traces and Spans](https://courses.arize.com/p/llm-observability-traces-spans)\n    * [ML Observability Fundamentals](https://arize.com/ml-observability-fundamentals/)\n    * [Advanced Model Performance Metrics](https://courses.arize.com/p/ml-observability-advanced-metrics-course)\n  * [New to Arize?](https://arize.com/blog-course/what-is-observability-data-machine-learning/)\n    * [Arize onboarding](https://docs.arize.com/arize/quickstart)\n\n\n#  What Is Observability? \n![](https://arize.com/wp-content/uploads/2020/12/bg-speaker-98x98.png)\n[ Aparna Dhinakaran, ](https://arize.com/author/aparna-dhinakaran/) Co-founder & Chief Product Officer  | Published February 02, 2023\nIn 1969, humans first stepped on the moon thanks to a lot of clever engineering and 150,000 lines of code. Among other things, this code enabled engineers at mission control to have the full view of the mission and make near-real-time decisions. The amount of code was so small that engineers were able to thoroughly debug the software, and its performance was nearly flawless. Today\u2019s search engines, on the other hand, execute billions of lines of code, making it much more likely that engineers will make mistakes or extraordinary circumstances will crop up somewhere in the system. On top of that, both data and machine learning models (code that computers essentially write for themselves) come with their own challenges. In short, it is nearly impossible to guarantee that a modern software system will always perform as you expect. That\u2019s why observability is crucial on all production systems.\nLuckily, we can automate observability of complex software systems so you can have timely and actionable information about your software, just like NASA engineers had about the Apollo missions.\n## What Is Observability?\nObservability is the practice of quickly finding out what the problem is and getting to the root cause. Most modern software systems include infrastructure, data, and machine learning models. All three need observability, but each has different requirements, workflows, and personas. Let\u2019s take a closer look!\n![](https://arize.com/wp-content/uploads/2021/05/observability-iceberg-2.png)\n## What are the Types of Observability?\nObservability is of course not unique to technical personas like DevOps, data engineers, and MLOps engineers. It is the same discipline used in tracking business metrics and industry-specific requirements, but in this post we will focus on what observability means to people in these three more technical roles.\nThe three types of engineers use observability to solve different problems with distinct workflows. At the highest level, infrastructure observability is concerned with software problems that often revolve around latency, failures, and resource utilization. Data observability, on the other hand, cares more about changes in data that your application is using, either at rest in the database or in any step that feeds into your data storage system. Machine learning observability is a newer discipline; it focuses on the performance of your models, measuring how well they predict values either on aggregate in certain slices of the data or on the level of individual predictions with techniques like SHAP analysis.\n![](https://arize.com/wp-content/uploads/2022/06/observability-personas-examples.jpg)\n## Infrastructure Observability\n### Persona\nSoftware/DevOps engineers are responsible for development and operations of software infrastructure. Their code is the mortar that holds all the bricks of modern software systems together.\n### Problem\nMany modern applications rely on several systems coming together, and a small problem in one system can show up as a significant slowdown in the overall application.\nA new version of the code can have unexpected bugs. Configuration changes, passwords, and rare occurrences like leap seconds can all cause troubles at this layer. Additionally, DevOps engineers are often responsible for software systems they did not design. Virtual machines or Kubernetes clusters running their code can fail.\n### Workflow\nSeveral workflows can be employed at this layer, depending on where the problem manifests. Let\u2019s look at one of them.\nSay you are a DevOps engineer for a large ecommerce site\u2014let\u2019s call it Nile. Suddenly, pages start taking just a little bit longer to load. It is well established that a 1% slowdown on such an application can cause a 15% drop in customer satisfaction, which translates to huge loss of revenue.\nNo sweat\u2014you have integrated your application with a market-leading observability solution. You get notified about the problem right away and can start troubleshooting.\nFirst, since you logged your page load times as well as individual query times for each query on the page, you can identify which pages load slower than others. Then you can look at individual query times and sort them by which are taking the longest.\nDeeper down in your system, you can identify sub-queries that are slowing you down. By analyzing the timing around these sub-traces, DevOps engineers like you are easily able to see patterns that point them to the root cause of the problem. In this case, a recent code change introduced more latency in history lookup for customers with more than 300 purchases in the last year, and thanks to tail latency amplification, all your customers paid the price. You back out the change until your team can figure out how to deploy it without incurring such costs.\nProblem solved.\n![](https://arize.com/wp-content/uploads/2022/06/system-infra-observability-4.jpg)\n## Data Observability\n### Persona\nData Engineers and Analytics Engineers are responsible for building and operating data pipelines. They ensure the timeliness, completeness, and other quality characteristics of the data that your application receives.\n### Problem\nWhen the original source of the data makes a change, it may alter the schema, contents, or arrival times of the data into the data lake or warehouse. For example, say your data vendor switched from US zip codes (number) to global postal codes (strings). Suddenly a step in your pipeline malfunctions because it relies on a zip code being a number or being exactly 5 digits long, and you get partial or null postal code for all your Canadian customers.\nOther problems can include stale data (from a pipeline not running on time), volume (accidentally dropped or duplicated records), and distribution (a negative value for a person\u2019s age or a new categorical value). These can be caused by changes in the data\u2019s original source, issues with the data infrastructure, or changes to the code in various transformation steps the data goes through as it moves down the pipeline.\nIdentifying these issues across all data pipelines, 24/7 is crucial to preventing data outages from reaching the data used by models for training and serving, and quickly scales beyond manual approaches like spot checks and even semi-automated approaches like data pipeline tests.\n### Workflow\nNow let\u2019s say you are a data engineer for a fast-growing analytics company. You got your first huge customer, and they have just launched a crucial marketing campaign that depends on your company\u2019s customer segmentation model. The recommendations your model serves over the next 24 hours will make a huge difference to their business and yours. And that model depends on fresh audience data to serve up.\nYou get an alert from your data observability tool: a table that is supposed to update every 60 minutes has not been updated in 75. Clicking the link in the alert takes you to the SLA for one of your third party data providers. You see that the data is stale, and that a similar issue happened a few months ago. Reading the incident notes your teammate left when they fixed this last time, you see which provider this data comes from, and their phone number. You pick up the phone and give them a call. Guess what? They didn\u2019t even know they had an issue until you called. Their SSL certificate expired and the cron job they are using to renew it got stuck. They rerun the script manually, and your data observability tool confirms that everything is back to normal. Time to detect: 15 minutes. Time to fix: 10 minutes. Customer impact: negligible. No one even knows you just saved your company\u2019s biggest deal to date.\n![](https://arize.com/wp-content/uploads/2022/06/data-observability-5.png)\n## Machine Learning Observability\n### Persona\nMachine learning engineers are responsible for developing, productionizing, and operating machine learning models.\n### Problem\nMore and more software systems rely on machine learning models. More and more economic value relies on machine learning models. A poorly performing model can lead to poor decisions or worse, loss of reputation and revenue for many customers.\nUnlike traditional software, however, machine learning models are not static. Performance usually degrades over time. In fact, [one survey](https://arize.com/resource/survey-machine-learning-observability-results/) has shown that on average models degrade within 10 days of being in production. It is vital to have the tools to detect problems, establish root cause, and improve models as quickly as possible.\n### Workflow\nFor this example, let\u2019s say you are a machine learning engineer maintaining a [fraud model](https://arize.com/blog/best-practices-in-ml-observability-for-monitoring-mitigating-and-preventing-fraud/) for a major bank. Your model labels transactions as fraudulent (or not). One day your [ML observability](https://arize.com/ml-observability/) system sends you an alert. You log in and can quickly arrive at the very slice of inputs that seem to generate unexpected output distribution. Among the inputs, you can identify a new scammer. Perhaps this new scammer exhibits behaviors the model has not seen before, but you can retrain it with this new data included and catch them and anyone else who tries this scheme in the future.\nCollecting useful information is once again key. In this case, you are looking for model inputs and outputs over time. Once you have the data you need, having some idea of baseline performance is very useful.\nWhen the model performance goes outside the baseline bounds, you can zoom in on the timeframe where the issues are present. Next you would sort your data by slices and evaluate the problematic ones.\n![](https://arize.com/wp-content/uploads/2022/06/ml-observe-example-workflow.jpg)\n## Conclusion\nAs software becomes more integral in the world at large and in your company in particular, the need for observability grows. The level of observability required for a complex software system today necessitates automated tools. Which tools you need depends on what problems you are solving. Typically software and DevOps engineers will mainly rely on system and infrastructure observability tools. Data engineers have specific challenges that require solutions tailored for data observability. Machine learning engineers require a whole different set of tools to troubleshoot degrading model performance. And as your team and company scale, you will probably find yourself needing all three.\n**_This post is written in partnership with Kyle Kirwan, Co-founder and CEO at data observability platform_[ _Bigeye_](https://www.bigeye.com/)**\n##### On this page\n  * [ What Is Observability? ](https://arize.com/blog-course/what-is-observability-data-machine-learning#what-is-observability)\n  * [ Types of Observability ](https://arize.com/blog-course/what-is-observability-data-machine-learning#types-of-observability)\n  * [ Infrastructure Observability ](https://arize.com/blog-course/what-is-observability-data-machine-learning#infrastructure-observability)\n  * [ Data Observability ](https://arize.com/blog-course/what-is-observability-data-machine-learning#data-observability)\n  * [ Machine Learning Observability ](https://arize.com/blog-course/what-is-observability-data-machine-learning#machine-learning-observability)\n\n\n[ ![Arize icon](https://arize.com/wp-content/uploads/2023/02/Arize-icon@2x.png) Try Arize  ](https://app.arize.com/auth/join)\n[ ![Slack icon](https://arize.com/wp-content/uploads/2023/02/Slack-icon@2x.png) Ask any AI question  ](https://arize-ai.slack.com/)\n## Sign up for our monthly newsletter, The Evaluator.\n[Subscribe](https://arize.com/blog-course/what-is-observability-data-machine-learning/#blog-subscribe-modal)\n[ ![arize logo](https://arize.com/wp-content/themes/arize-2022/images/Arize-logo.svg) ](https://arize.com)\n  * [Platform](https://arize.com)\n  * [Docs](https://docs.arize.com/arize)\n  * [Pricing](https://arize.com/pricing)\n  * [Phoenix](https://phoenix.arize.com/)\n\n\nLearn\n  * [Course](https://arize.com/elearning/)\n  * [Community & Events](https://arize.com/community/)\n  * [Blog](https://arize.com/blog/)\n\n\nTopics\n  * [LLM Evaluation](https://arize.com/llm-evaluation)\n  * [AI Agents](https://arize.com/ai-agents/)\n  * [LLM Tracing](https://docs.arize.com/phoenix/tracing/llm-traces)\n  * [RAG Evaluation](https://arize.com/blog-course/rag-evaluation/)\n\n\nCompany\n  * [About](https://arize.com/about-company/)\n  * [Customers](https://arize.com/customers/)\n  * [Careers](https://arize.com/careers/)\n  * [Press](https://arize.com/press/)\n  * [Security](https://arize.com/trust-center/)\n\n\n  * [Try now](https://app.arize.com/auth/join)\n  * [Book a Demo](https://arize.com/request-a-demo/)\n  * [Sign In](https://app.arize.com/auth/login)\n  * [Contact](https://arize.com/contact/)\n  * [Privacy Policy](https://arize.com/privacy-policy/)\n  * [Twitter](https://twitter.com/arizeai?lang=en)\n  * [Linkedin](https://www.linkedin.com/company/arizeai/)\n\n\nCopyright \u00a9 2025 Arize AI, Inc\nJoin the Arize ML Observability Community \nSubscribe to the Arize blog \nSubscribe to get the latest news, expertise, and product updates from Arize. Your inbox is sacred, so we\u2019ll only curate and send the best stuff. \nUTM Campaign\nUTM Content\nUTM Medium\nUTM Source\nUTM Term\n*We\u2019re committed to your privacy. Arize uses the information you provide to contact you about relevant content, products, and services. You may unsubscribe from these communications at any time. For more information, check out our [privacy policy](https://arize.com/privacy-policy/).\nContact us \nLike what you see? Let\u2019s chat. Fill out this form and we will be in contact with you soon! \n",
    "answer": "```markdown\n# TL;DR Summary\n\nObservability is essential for modern software systems, focusing on infrastructure, data, and machine learning. Each type has unique challenges and workflows. Automated tools are crucial for timely problem detection and resolution, ensuring optimal performance and reliability.\n\n**Tags:** #GenerativeAI #LLMs #MachineLearning #Observability\n```"
  },
  {
    "instruction": "[ Tell 120+K peers about your AI research \u2192 Learn more \ud83d\udca1 ![](https://neptune.ai/wp-content/themes/neptune/img/icon-cancel.svg) ](https://neptune.ai/neurips-2024)\n[ ![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg) ](https://neptune.ai \"neptune.ai\")\n  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [Walkthrough [2 min]](https://neptune.ai/walkthrough)\n      * [Play with public sandbox](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n      * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [Neptune vs WandB](https://neptune.ai/vs/wandb)\n      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n      * [Other comparisons](https://neptune.ai/vs)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune Scale projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [AI Researcher](https://neptune.ai/product/ai-researcher)\n      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n      * [Academia & Kagglers](https://neptune.ai/research)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [Monitor training](https://neptune.ai/product/monitor-training)\n      * [Compare experiments](https://neptune.ai/product/compare-experiments)\n      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)\n      * [Reports](https://neptune.ai/product/reports)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/deepsense-ai)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2025/01/Artera-color-signet.png?fit=129%2C97&ssl=1)Case studyHow Neptune Helps Artera Bring AI Solutions to Market Faster ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/artera)\n[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](https://neptune.ai/customers)\n  * [Documentation](https://docs.neptune.ai/)\n  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [Menu Item](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [Blog](https://neptune.ai/blog)\n      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Grici\u016bnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)\n  * [Pricing](https://neptune.ai/pricing)\n  * [Enterprise](https://neptune.ai/product/enterprise)\n  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [Menu Item](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [About us](https://neptune.ai/about-us)\n      * [Customers](https://neptune.ai/customers)\n      * [Careers](https://neptune.ai/jobs)\n      * [Security](https://security.neptune.ai/)\n      * [Contact us](https://neptune.ai/contact-us)\n\n\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find? \nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/) > [Blog](https://neptune.ai/blog) > [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [Categories](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n      * [MLOps](https://neptune.ai/blog/category/mlops)\n      * [LLMOps](https://neptune.ai/blog/category/llmops)\n      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)\n    * [Categories](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)\n      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)\n      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)\n\n\nSearch in Blog... \n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg) ![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog... \n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n  * [Topics![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [Categories](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n      * [MLOps](https://neptune.ai/blog/category/mlops)\n      * [LLMOps](https://neptune.ai/blog/category/llmops)\n      * [ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n      * [Computer Vision](https://neptune.ai/blog/category/computer-vision)\n    * [Categories](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\n      * [Reinforcement Learning](https://neptune.ai/blog/category/reinforcement-learning)\n      * [Tabular Data](https://neptune.ai/blog/category/tabular-data)\n      * [Time Series](https://neptune.ai/blog/category/time-series-forecasting)\n\n\n  * [Product![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/09/show.svg)Overview](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [Walkthrough [2 min]](https://neptune.ai/walkthrough)\n      * [Play with public sandbox](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n      * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/compare-1.svg)Compare](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [Neptune vs WandB](https://neptune.ai/vs/wandb)\n      * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n      * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n      * [Other comparisons](https://neptune.ai/vs)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/07/bg-5.jpg?fit=768%2C432&ssl=1)Live Neptune Scale projectPlay with a public example project that showcases Neptune's upcoming product release. It offers enhanced scalability and exciting new features. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908)\n  * [Solutions![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/bulb.svg)By role](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [AI Researcher](https://neptune.ai/product/ai-researcher)\n      * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n      * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n      * [Academia & Kagglers](https://neptune.ai/research)\n    * [![Menu icon](https://neptune.ai/wp-content/uploads/2023/08/star.svg)By use case](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [Monitor training](https://neptune.ai/product/monitor-training)\n      * [Compare experiments](https://neptune.ai/product/compare-experiments)\n      * [Collaborate with a team](https://neptune.ai/product/team-collaboration)\n      * [Reports](https://neptune.ai/product/reports)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/deepsense.ai-logo-e1667753808279.png?fit=75%2C75&ssl=1)Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/deepsense-ai)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2025/01/Artera-color-signet.png?fit=129%2C97&ssl=1)Case studyHow Neptune Helps Artera Bring AI Solutions to Market Faster ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/customers/artera)\n[See all case studies ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg)](https://neptune.ai/customers)\n  * [Documentation](https://docs.neptune.ai/)\n  * [Resources![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [Menu Item](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [Blog](https://neptune.ai/blog)\n      * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n      * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n      * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n      * [100 Second Research Playlist](https://www.youtube.com/watch?v=_sKZsx7Iprg&list=PLKePQLVx9tOcAGAKvmRuQ5Y1_k6wqG0cX&pp=iAQB)\n[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/12/Building-The-Most-Scalable-Experiment-Tracker-For-Foundation-Models.png?fit=768%2C403&ssl=1)ArticleFrom Research to Production: Building The Most Scalable Experiment Tracker For Foundation ModelsAurimas Grici\u016bnas discusses the journey and challenges behind building the most scalable experiment tracker for foundation model training. ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://neptune.ai/blog/building-the-most-scalable-experiment-tracker-for-foundation-models)[ ![Menu thumbnail](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/11/reasercher-3-levels-challenge.webp?fit=768%2C432&ssl=1)VideoBreaking Down AI Research Across 3 Levels of DifficultyWe challenged AI/ML researchers to explain their work across 3 levels of difficulty: for a young learner, university student, and a fellow researcher.  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/nav-article-arrow-right.svg) ](https://www.youtube.com/watch?v=zSEv3KBGlJQ&list=PLKePQLVx9tOfqC8ho2g_tQVxRga-XbIWa&index=3&t=9s)\n  * [Pricing](https://neptune.ai/pricing)\n  * [Enterprise](https://neptune.ai/product/enterprise)\n  * [Company![](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-with-margin.svg)](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [Menu Item](https://neptune.ai/blog/ml-model-serving-best-tools/)\n      * [About us](https://neptune.ai/about-us)\n      * [Customers](https://neptune.ai/customers)\n      * [Careers](https://neptune.ai/jobs)\n      * [Security](https://security.neptune.ai/)\n      * [Contact us](https://neptune.ai/contact-us)\n\n\n[ Log in  ](https://app.neptune.ai/login) [ Sign up  ](https://app.neptune.ai/register) [ Contact us  ](https://neptune.ai/contact-us)\n[Neptune Blog](https://neptune.ai/blog)\n#  Best Tools For ML Model Serving \n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/05/axel-profile-picture.png?fit=800%2C800&ssl=1)\n[ Axel Mendoza  ](https://neptune.ai/blog/author/axel-mendoza)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg) 15 min \n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg) 21st August, 2024 \n[ML Tools](https://neptune.ai/blog/category/machine-learning-tools)\n![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 3 2'%3E%3C/svg%3E)\n###  TL;DR \n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/note/list-arrow.svg)\nModel serving is an essential step in building machine-learning products. It comprises packaging models, building APIs, monitoring performance, and scaling to adjust to incoming requests.\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/note/list-arrow.svg)\nThe choice of a model-serving tool depends on specific project and team needs, such as frameworks and infrastructure compatibility, ease of use, inference optimization features, monitoring capabilities, and required deployment strategies.\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/note/list-arrow.svg)\nBroadly speaking, model-serving tools can be divided into two categories: model-serving runtimes that package ML models into inference-optimized containers and model-serving platforms that focus on deploying and scaling these models.\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/note/list-arrow.svg)\nVarious tools exist on the market today, each with specific strengths and weaknesses. BentoML, TensorFlow Serving, TorchServe, Nvidia Triton, and Titan Takeoff are leaders in the model-serving runtime category. When it comes to model-serving platforms, KServe, Seldon Core, Bento Cloud, and cloud providers\u2019 integrated solutions are the top contenders.\nChoosing the right model-serving tool is crucial for the success of any machine learning project. Deploying a model to production is a critical step in the machine-learning lifecycle. After all, we train models to solve problems, and only a deployed model can provide predictions to downstream consumers.\nAt its core, model serving involves making a trained machine-learning model available to receive inputs and serve the resulting predictions. The challenge in serving a machine learning model lies in packaging it, exposing it through an inference API, and maintaining its performance. Each project will have unique demands regarding latency, throughput, and scalability, which adds to the complexity.\nPlenty of frameworks and platforms have been developed. It is difficult to get an overview, understand the differences, and pick the right solution. But don\u2019t worry! After reading this article, you will \u2026\n  * \u2026 know the most important criteria for choosing the right tool for your team and project.\n  * \u2026 have a deep understanding of the model serving landscape.\n  * \u2026 understand the pros and cons of the leading model serving tools on the market.\n\n\n## Understanding model serving\n![Overview of the canonical model serving architecture and components](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/Model-Serving-Runtime-Serving-Platform.jpg?resize=1200%2C1200&ssl=1)Overview of the canonical model serving architecture and components. Given the model code and artifacts, we create a Docker image based on a model serving runtime. This Docker image, which contains the model server, is deployed to a model-serving platform that provides scalability and exposes the model to downstream users. | Source: [Author](https://www.axelmendoza.com/)\nIn the [MLOps](https://neptune.ai/blog/mlops) community, there\u2019s often confusion about terms related to model serving. Professionals frequently use serving and deployment interchangeably, which can lead to misunderstandings.\nHere\u2019s our attempt to define and distinguish the different components and their roles. (But remember, these definitions are based on our perspective and are by no means absolute truths.)\n  * **Model Serving Runtime:** Packaging a trained machine learning model into a container and setting up APIs so it can handle incoming requests. This allows the model to be used in a production environment, responding to data inputs with predictions (inference).\n  * **Model Serving Platform:** An environment designed to dynamically scale the number of model containers in response to incoming traffic. Tools like KServe are examples of serving platforms. They manage the infrastructure needed to deploy and scale models efficiently, responding to varying traffic without manual intervention.\n  * **Model Deployment:** The process of integrating a packaged model into a serving platform and connecting it to the broader infrastructure, such as databases and downstream services. This ensures the model can access necessary data, perform its intended functions, and deliver inference results to consumers.\n\n\nTo help you understand the roles and relationships better, let\u2019s consider this typical ML model lifecycle:\n  1. **Train:** Suppose we trained an LLM on articles on Neptune\u2019s blog to assist MLOps engineers in their decision-making.\n  2. **Package:** We use a model-serving runtime like BentoML to package our LLM into a Docker image, wrapped with a standardized, functional API.\n  3. **Deploy:** We deploy our model packaged with BentoML to the KServe model-serving platform. This Kubernetes-based platform auto-scales the model containers according to incoming requests.\n  4. **Integrate:** We connect the model with a chat widget in the sidebar of Neptune\u2019s blog so that users can type questions and receive the answers generated by our LLM. This requires us to integrate the necessary API requests into the website frontend code and ensure that our model\u2019s API can be reached publicly via the internet.\n  5. **Bring value:** Finally,the model is ready to assist many neptune.ai blog readers simultaneously.\n\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Recommended  How to Solve the Model Serving Component of the MLOps Stack  Read also  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/model-serving-component-mlops-stack)\n## Do you need a model-serving runtime?\nWhy is a serving runtime necessary when you could take a Docker base image to package your model together with a simple API that you\u2019ve quickly coded up using [Flask](https://flask.palletsprojects.com/en/3.0.x/) or [FastAPI](https://fastapi.tiangolo.com/)?\n### Three reasons why you need a model-serving runtime\n  1. **Optimized base images** : Model serving runtimes provide optimized Docker images tailored for inference. The images support intelligent optimization techniques for the hardware and ML framework you are using, ensuring that your model runs as efficiently as possible. The years of wisdom and optimization that went into the ML-optimized Docker base images are hard to replicate by yourself.\n  2. **Time-saving utilities** : Model-serving runtimes simplify the task of packaging your model into optimized Docker images. They often include utilities that help convert your model into a format more suitable for fast, efficient inference. This makes the deployment process smoother than if you had to do all that manually.\n  3. **Well-designed, clearly defined APIs:** These frameworks ease the process of integrating models by providing unified, well-designed APIs tailored for ML model inference. A model-serving runtime typically covers a wide range of machine-learning use cases, including support for data frames, images, and JSON payloads.\n\n\nHowever, there are also scenarios where you\u2019d be better off using a custom solution or looking for a fully managed offering.\n### Three reasons to avoid using a model-serving runtime\n  1. **Skill gap** : Some model-serving runtimes require significant software engineering skills on your team\u2019s part. If your team does not bring sufficient experience to the table, this can lead to challenges in setup, ongoing maintenance, and integration.\n  2. **Batch processing** : When you don\u2019t need real-time inference, but all computations can be batch-processed, simpler solutions may be more straightforward and cost-effective than implementing a solution with a full-fledged serving runtime.\n  3. **No scaling needs:** If your model does not need to be scaled because of low inference time or request volume, the benefits of using an ML-optimized container might not outweigh its engineering costs.\n\n\n## Criteria for selecting model-serving tools\n![Overview of the key criteria for selecting a model serving tool](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/Model-Serving-Mind-Map.jpg?resize=1200%2C628&ssl=1)Overview of the key criteria for selecting a model serving tool. Key considerations include framework compatibility, integrations, implementation complexity, performance, monitoring capabilities, cost, and licensing. | Source: Author\nFinding a model serving tool that meets your team\u2019s and project\u2019s specific needs can be challenging. This section will guide you through various criteria to consider when surveying the market and making a decision.\n### Framework compatibility\nWhen choosing a model-serving tool, it\u2019s crucial to consider the range of machine-learning frameworks it supports, such as [scikit-learn](https://scikit-learn.org/), [TensorFlow](https://tensorflow.org/), or [PyTorch](https://pytorch.org/). It would be unfortunate to select and begin setting up TorchServe only to discover later that it does not support the Keras model your colleague has trained.Additionally, it\u2019s essential to consider whether the tool provides GPU support and works with the CUDA version you\u2019re on. This is particularly important if you work with large deep-learning models.\nSupport for distributed processing is crucial if you plan to scale your models across multiple machines to handle larger workloads.\n### Integration\nAssessing how a model-serving tool aligns with your current MLOps stack and compute infrastructure or cloud environment is paramount. Suppose you already have a Kubernetes cluster running. That would be a strong argument to use a Kubernetes-native solution like KServe instead of a fully managed solution like Google\u2019s Vertex AI.\nThis applies not only to your infrastructure but also at the framework level. For example, if you plan to use [ArizeAI](https://arize.com/) for model observability, it would be better to use BentoML, which has out-of-the-box integration, instead of Tensorflow Serving, which does not.\n### Implementation complexity\nWhen evaluating model serving tools, it\u2019s crucial to recognize that not every framework is suitable for every team, as the complexity of implementation and required background knowledge can vary significantly. \nBefore deciding on a serving tool, consider the learning curve involved and your team\u2019s technical skills. A tool that is difficult to use can slow down progress, especially if you are not familiar with the required technologies.\nBroadly speaking, tools that provide high flexibility tend to be more complex and have a steeper learning curve. This complexity arises because these tools offer more options and control to the user. While this allows for better adaptation to specific needs, it also requires a deeper understanding.\nIdeally, you should choose the simplest tool that meets your team\u2019s and project\u2019s needs. This approach ensures you don\u2019t overcomplicate your setup with unnecessary features or struggle with a tool that\u2019s too limited for your requirements.\n### Performance\nModel-serving tools are designed to optimize inference performance. However, the extent of this optimization varies across frameworks. Determining a framework\u2019s efficiency before implementation is challenging, as efficiency depends on many factors, including the specific use case, model, and hardware.\nHowever, it is possible to obtain a first estimate of a tool\u2019s performance by examining its documentation. Sections that discuss the tool\u2019s architecture, key concepts, or specific inference features can provide insights into the expected performance.\nWhen it comes to model-serving runtimes, here are the main features to look at:\n  * **Concurrent model execution** : Spawns multiple instances of the same model to run simultaneously on a single hardware processor (GPU or CPU) and load balances the incoming requests across the instances. This way, multiple smaller models can share one processor, saving costs.\n  * **Inference parallelization** : Distributes inference tasks across multiple hardware processors (GPU or CPU) to speed up processing.\n  * **Adaptive batching** : Allows the server to combine multiple inference requests into a single batch dynamically, optimizing throughput and latency.\n  * **High-performance runtime support** : Compute-intensive models benefit from conversion to a more efficient runtime such as [TensorRT](https://developer.nvidia.com/tensorrt).\n  * **Asynchronous APIs** : Enable non-blocking requests, allowing the system to handle multiple requests at the same time. This improves responsiveness as the system does not process the requests sequentially.\n  * **gRPC inference protocol** : Offers a more efficient alternative to traditional HTTP/REST for communication between services. In fact, the [gRPC protocol has shown to be superior than REST](https://www.imaginarycloud.com/blog/grpc-vs-rest/#:~:text=Both%20gRPC%20and%20REST%20API,interacting%20with%20multiple%20programming%20languages.) in terms of response time.\n\n\n### Monitoring\nEvaluating a model-serving tool\u2019s built-in monitoring and logging features is essential. These features allow you to ensure the health and performance of your model containers, help diagnose issues, and optimize resource usage effectively. When analyzing your monitoring requirements, think about the level of detail you need and how easy it is to access the monitoring data.\nThe model serving runtimes discussed in this article all produce Prometheus metrics. To monitor your model performance in production, you need a Prometheus server that can consume the logs. You have two main options for this: [deploy a Prometheus server](https://prometheus.io/docs/prometheus/latest/installation/) or use a fully managed option.\nAnother aspect to investigate is the integration with external monitoring systems and observability platforms. Using fully managed monitoring tools such as [Arize AI](https://arize.com/), [Fiddler AI](https://www.fiddler.ai/), or [Evidently](https://www.evidentlyai.com/) can significantly improve your ability to manage your model\u2019s performance in production without having to support a complex infrastructure.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Recommended  Best Tools to Do ML Model Monitoring  Read also  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/ml-model-monitoring-best-tools)\n### Cost and licensing\nThe next criterion on the list is to anticipate the costs related to a model-serving tool:\n  * **Pricing structure:** Some model-serving tools are subscription-based, some require a one-time payment, some charge based on resource utilization, and others are open-source.\n  * **Licensing:** Some model-serving tools impose limitations on the deployment or distribution of your model containers, particularly in commercial settings. For example, [in early 2024, Seldon Core changed its license to Business Source License v1.1](https://www.seldon.io/strengthening-our-commitment-to-open-core) (BSL), rendering it free for non-production use but requiring a yearly subscription for production deployments.\n  * **Total cost** : Evaluating the total cost associated with a model-serving tool involves looking beyond the price tag. This is easily forgotten, in particular when settling for an open-source tool that\u2019s free to download and run. You have to consider costs for ongoing activities like support, updates, and infrastructure requirements. For example, KServe is open-source and thus free to use, but it requires deploying and managing a Kubernetes cluster to operate.\n\n\n### Support and documentation\nThe final criteria on our list revolve around support and documentation:\n  * **Support** : Choosing a tool with an active community or provider support is beneficial as it\u2019s invaluable to get suggestions or bug fixes from experts during implementation. For open-source tools, you can assess the quality of support by investigating the interactions on Slack or the developers\u2019 responsiveness to issues on their GitHub repository.\n  * **Documentation** : Before setting up a tool, it doesn\u2019t hurt to check the clarity and readability of the documentation. It\u2019s not to be underestimated, as the documentation will be your main companion for a while.\n  * **Learning resources** : The presence of extensive learning materials, such as tutorials, FAQs, and code examples, is essential. These resources can significantly ease your team\u2019s learning process and enhance the overall user experience with the tool.\n\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Recommended  Model Deployment Challenges: 6 Lessons From 6 ML Engineers  Read also  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/model-deployment-challenges-lessons-from-ml-engineers)\n## The top model-serving tools in 2024 \n![Overview of the model-serving runtimes and model-serving platforms included in our review ](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/Overview-of-the-model-serving-runtimes-and-model-serving-platforms-included-in-our-review.png?resize=1200%2C628&ssl=1)Overview of the model-serving runtimes and model-serving platforms included in our review | Source: Author\nLet\u2019s review a selection of model-serving tools that stand out for their capabilities and widespread use. We separated the comparison into two categories: serving runtimes and serving platforms.\n## Model-serving runtimes\nThe role of a serving runtimeis to package the model code and artifacts into a container and to build APIs optimized for model inference.\nEvery tool discussed in this category supports the following:\n  * **Parallel processing** : Supports parallel processing to handle multiple tasks simultaneously.\n  * **Asynchronous APIs** : Allows for non-blocking requests, enabling simultaneous request handling for faster response than sequential processing.\n  * **Adaptive batching** : Enables the server to merge incoming inference requests into batches for better throughput and reduced latency.\n  * **REST APIs** : Handles client-server communication using HTTP verbs such as POST, GET, PUT, and DELETE.\n  * **gRPC:** A high-performance, low-latency remote-procedure-call framework for service communication based on HTTP/2.\n  * **Monitoring logs** : Every model-serving runtime we review produces [Prometheus](https://prometheus.io/) logs that can be ingested to analyze hardware and model performance metrics.\n\n\n### BentoML\n![Chart showing BentoML components](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/best-ml-model-serving-tools-2.png?resize=1400%2C913&ssl=1)A \u201cBento\u201d is an archive containing all the necessary components to build a Docker image of your model: A requirements file that defines the dependencies, the source code for loading and running the model, the inference API, the model artifact(s), and the ML model definition. | [Source](https://towardsdatascience.com/10-ways-bentoml-can-help-you-serve-and-scale-machine-learning-models-4060f1e59d0d)\n[BentoML](https://github.com/bentoml/BentoML) is an open-source framework that simplifies the process of packaging models into ML-optimized Docker images.\nFirst released in 2019, BentoML introduced the concept of \u201cBentos\u201d: an archive containing all the necessary components to package a model, such as source code, model architecture, and configurations.\nThe tool provides a Python SDK with utilities to build Bentos. Users develop Python classes that inherit from BentoML interfaces to generate API servers. This is very handy as it allows you to test and debug those classes prior to creating a containerized version of your model.\n#### Reasons for choosing BentoML\n  * **Ease of use:** BentoML is one of the most straightforward frameworks to use. Since the release of 1.2, it has become possible to build a Bento with a few lines of code.\n  * **ML Framework support** : BentoML supports all the leading machine learning frameworks, such as PyTorch, Keras, TensorFlow, and scikit-learn.\n  * **Concurrent model execution:**[BentoML supports fractional GPU allocation](https://github.com/bentoml/BentoML/blob/main/src/_bentoml_impl/server/allocator.py). In other words, you can spawn multiple instances of a model on a single GPU to distribute the processing.\n  * **Integration:** BentoML comes with integrations for ZenML, Spark, MLflow, fast.ai, Triton Inference Server, and more.\n  * **Flexibility:** BentoML is \u201cPythonic\u201d and allows you to package any pre-trained model that you can import with Python, such as Large Language Models (LLMs), Stable Diffusion, or CLIP.\n  * **Clear documentation:** The documentation is easy to read, well-structured, and contains plenty of helpful examples.\n  * **Monitoring:** BentoML integrates withArizeAI and Prometheus metrics.\n\n\n#### Key limitations and drawbacks of BentoML\n  * **Requires extra implementation:** As BentoML is \u201cPythonic,\u201d you are required to implement model loading and inference methods on your own.\n  * **Native support for high-performance runtime:** BentoML runs on Python. Therefore, it is not as optimal as Tensorflow Serving or TorchServe, both of which run on backends written in C++ that are compiled to machine code. However, it is possible to use the [ONNX Python API](https://docs.bentoml.com/en/latest/reference/frameworks/onnx.html) to speed up the inference time.\n\n\n#### Summary\nOverall, BentoML is a great tool that will fit most use cases and teams. The main drawbacks are the need to re-implement a Python service for every model and the potential complexity of integrating a model from a high-performance runtime.\nTo learn more about this framework, read my [in-depth review of BentoML](https://www.axelmendoza.com/posts/bentoml-core-concepts-and-features/). You\u2019ll also want to check out BentoCloud, a fully managed model-serving platform specifically designed to scale BentoML containers.\n### TensorFlow Serving (TFX)\n![Graph showing th elifecycle of TenorFlow Serving models](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/best-ml-model-serving-tools-3.png?resize=1571%2C816&ssl=1)The lifecycle of TensorFlow Serving models. A \u201cSource\u201d detects new model weights. It creates a \u201cLoader\u201d that contains a pointer to the model on disk. The \u201cSource\u201d notifies the \u201cDynamicManager,\u201d which tells the \u201cLoader\u201d to instantiate the TensorFlow graph with the new weights. |[ Source](https://www.tensorflow.org/tfx/serving/architecture)\n[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) (TFX) is an open-source, high-performance serving runtime specifically designed to package TensorFlow and Keras models. It provides an optimized Docker image that connects your TensorFlow exported models to REST APIs.\n#### Reasons for choosing TensorFlow serving\n  * **Ease of use:** For TensorFlow models, the packaging process is as simple as using [one CLI command with Docker](https://www.tensorflow.org/tfx/serving/serving_basic#load_exported_model_with_standard_tensorflow_modelserver) and a [few lines of Python](https://www.tensorflow.org/tfx/serving/serving_basic#train_and_export_tensorflow_model). However, if you want to include custom pre-processing or post-processing into the servable, you will need to build a [custom signature](https://keras.io/examples/keras_recipes/tf_serving/#custom-signature).\n  * **High-performance runtime:** Once the model is exported from Python, we can package it with Docker. The TensorFlow Serving containers use a C++ runtime under the hood, making TensorFlow Serving one of the best-performing model-serving runtimes.\n  * **Customization:** This framework provides a clear abstraction for customizing the serving modules. However, to support models with custom operations, serve specific data associated with your model, or implement custom feature transformation logic, you need some knowledge of C++.\n\n\n#### Key limitations and drawbacks of TensorFlow Serving\n  * **ML framework support:** This tool only supports TensorFlow and Keras models.\n  * **Documentation:** We found the documentation somewhat simplistic and not very intuitive. It does not walk you through the concepts in order, and it feels like you are left exploring on your own.\n  * **No concurrent model execution:** TensorFlow Serving does not support intelligent load balancing on multiple models per device.\n\n\n#### Summary\nTensorFlow Serving (TFX) is your go-to framework if you use TensorFlow or Keras for model training. The tool provides a simple way to convert your model to a TensorFlow-specific high-performance runtime.\nHowever, if TensorFlow or Keras are not your framework of choice, TensorFlow Serving is not an option. While extending it to [support other ML frameworks](https://www.tensorflow.org/tfx/guide/non_tf) is possible, this approach lacks clear advantages as it will require additional implementation, while alternative model serving runtimes offer native support out of the box.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Recommended  How to Serve Machine Learning Models With TensorFlow Serving and Docker  Read also  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/how-to-serve-machine-learning-models-with-tensorflow-serving-and-docker)\n### TorchServe\n![Graph showing TorchServe architecture ](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/best-ml-model-serving-tools-4.png?resize=1920%2C1082&ssl=1)The TorchServe architecture for optimized model inference | [Source](https://pytorch.org/serve/)\n[TorchServe](https://pytorch.org/serve/) is a model-serving runtime designed to serve PyTorch models in production environments. It aims to provide utilities to ease the process of building Docker images for your models, equipped with APIs and designed for optimal model inference.\nThe steps to serve a model in PyTorch are the following:\n  1. **Export** : From the PyTorch definition of a model, we need to use [TorchScript](https://pytorch.org/docs/stable/jit.html) to export the model into a format that TorchServe can handle.\n  2. **Package:** Next, we use the `torch-model-archiver` utility to archive the model.\n  3. **Build the container:** Finally, we create a Docker image from the archive using the Docker CLI.\n\n\n#### Reasons for choosing TorchServe\n  * **Ease of use** : For simple use cases, serving a model with TorchServe is just a few CLI commands away. However, if your use case is not supported by the default handlers, you will need to develop your own handler in Python.\n  * **High-performance runtime:** TorchServe is among the top performers when it comes to model inference. The containers run models on a native runtime implemented in C++, resulting in amazing performance.\n  * **Customization:** The [TorchServe custom service guide](https://pytorch.org/serve/custom_service.html) is well thought out and provides many examples of how to extend its abstractions.\n\n\n#### Key limitations and drawbacks of TorchServe\n  * **ML Framework support:** This tool only supports PyTorch models.\n\n\n  * **No concurrent model execution:** TorchServe doesn\u2019t support serving multiple instances of the same model on a single GPU or CPU.\n  * **Documentation:** The documentation for TorchServe, which is part of the broader PyTorch documentation, is difficult to navigate.\n\n\n#### Summary\nTorchServe is a mature and robust tool for teams training their model with PyTorch. Similar to TensorFlow Serving, being able to convert your model to a C++ runtime easily is a huge plus.\n### Triton Inference Server\n![The Triton Inference Server\u2019s architecture](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/best-ml-model-serving-tools-5.png?resize=691%2C874&ssl=1)The Triton Inference Server\u2019s architecture. It comprises multiple scheduling and batching algorithms that can be configured on a model-by-model basis. | [Source](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html)\n[Triton Inference Server](https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/) is an open-source serving runtime developed by Nvidia. It is the most performant framework because it fully exploits the underlying hardware.\nThe [Triton architecture](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html) is undeniably the most sophisticated one among serving runtimes. After all, who is better to trust for optimization than Nvidia, the leading GPU manufacturer?\n#### Reasons for choosing Triton Inference Server\n  * **Concurrent model execution:**[Triton\u2019s instance group](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#instance-groups) feature allows multiple instances of a model to be loaded onto a single GPU. This enables an increase in performance proportional to the number of replicas on the same hardware. However, it\u2019s important to remember that this method doesn\u2019t increase the GPU\u2019s vRAM. In other words, your GPU must have sufficient memory to handle at least two replicas of the model to achieve a performance gain this way.\n  * **ML framework support** : Triton offers the most extensive ML framework support among the tools on our list. Read more about the [deep learning frameworks it supports](https://github.com/triton-inference-server/backend#where-can-i-find-all-the-backends-that-are-available-for-triton) and its [machine learning framework integrations](https://github.com/triton-inference-server/fil_backend).\n  * **Advanced optimization:** Triton has many advanced features, such as [sequence batching](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#sequence-batcher) for stateful models or an [ensemble scheduler](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#ensemble-scheduler) to pass tensors between models.\n  * **In-depth monitoring** : Triton produces Prometheus [advanced monitoring metrics](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/metrics.html#metrics).\n\n\n  * **Advanced utilities** : Triton has been designed with performance in mind. It provides multiple utilities to reduce the latency and increase the throughput of your models:\n\n\n  * [Model Analyzer](https://github.com/triton-inference-server/model_analyzer) helps you optimize your model performance by finding the optimal configuration for your hardware, such as max batch size, dynamic batching, and [instance group parameters](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#multiple-model-instances).\n  * Performance Analyzer enables debugging performance issues.\n  * Model Warmup can reduce the loading time of your models.\n\n\n  * **Documentation:** This framework has in-depth and comprehensive documentation.\n\n\n#### Key limitations and drawbacks of Triton Inference Server\n  * **Complexity:** Setting up and configuring Triton can be challenging. Within the model-serving domain, this framework has the most demanding learning curve, as users must become familiar with multiple concepts and abstractions.\n  * **Hardware dependency:** This tool is mainly designed for high-end Nvidia GPUs. Running on AMD is not supported, and running on a CPU is not worth the effort.\n\n\n#### Conclusion\nTriton is the prime choice for teams with robust software skills needing the best performance on Nvidia GPUs. It has no contender for large-scale scenarios demanding high throughput and low latency, as it is the only tool offering concurrent model execution on inference-optimized runtimes. However, the development costs associated with Triton and maintenance costs are not to be underestimated.\nSeveral model-serving tools provide integrations with Triton. Integrating [BentoML with Triton](https://docs.bentoml.com/en/v1.1.11/integrations/triton.html#why-integrating-bentoml-with-triton-inference-server) standardizes the model packaging process and versioning while increasing inference speed compared to standard BentoML. On the other hand, [Triton on Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/using-nvidia-triton) does not reduce the development and maintenance overhead of Triton but scales Triton instances for even better performance.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Recommended  Optimizing GPU usage during model training with neptune.ai  Read also  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/optimizing-gpu-usage-during-model-training-with-neptune)\n### Titan Takeoff Inference Server\n![The Titan Takeoff Playground where the user can prompt the LLMs](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/best-ml-model-serving-tools-6.png?resize=1109%2C792&ssl=1)The Titan Takeoff Playground where the user can prompt the LLMs | [Source](https://docs.titanml.co/docs/Docs/quickstart)\n[Titan Takekoff](https://docs.titanml.co/docs/intro) is a closed-source serving runtime tailored for the deployment and self-hosting of Large Language Models (LLMs). It has been designed for teams with [data privacy concerns](https://docs.titanml.co/blog/what-can-i-build-with-titan-takeoff-server#control-over-data-andmodels), it supports both cloud and on-premise deployment.\nThis tool provides proprietary Docker images specialized for LLM inference. It supports [most text generation and embedding models](https://docs.titanml.co/docs/Docs/launching/supported_models) from HuggingFace. \nTitan Takeoff provides a [model memory calculator](https://docs.titanml.co/docs/0.10.x/Docs/interfacing/gui/#model-memory-calculator) to assist in choosing your hardware.. Moreover, it uses quantization techniques to compress your LLMs to support [larger models on your existing hardware](https://docs.titanml.co/blog/what-can-i-build-with-titan-takeoff-server#inference-optimisation).\n#### Reasons for choosing Titan Takeoff Inference Server\n  * **Inference for LLMs:** Features a proprietary inference engine for top-tier inference speed and throughput optimized for LLMs. However, TitanML does not share any details concerning their engine.\n  * **Simplified deployment** : This tool provides ready-made Docker images for easy self-hosting. For [supported models](https://docs.titanml.co/docs/Docs/launching/supported_models/), the container comes with the model already packaged inside of it. For custom models, there is [documentation to import your models](https://docs.titanml.co/docs/0.10.x/Docs/launching/supported_models#how-can-i-use-a-model-i-have-saved-locally) into a TitanML container.\n  * **Inference optimization:** Titan Takeoff tool offers multi-GPU support and quantization, with additional utilities to optimize model performance for specific hardware.\n  * **User-friendly interface:** Titan Takeoff Includes a GUI for model testing and management.\n  * **No cloud-provider lock-in:** This framework enables you to deploy your models on Amazon SageMaker, Vertex AI, EC2, CloudRun, LangChain API, or a Kubernetes cluster.\n\n\n#### Key limitations and drawbacks of Titan Takeoff Inference Server\n  * **Pricing:** The [pricing section on the TitanML website](https://www.titanml.co/pricing) does not provide any tangible information on the pricing structure or ranges.\n  * **Specialized focus:** Titan Takeoff is primarily designed for LLMs.\n  * **New product and company:** Titan Takeoff is relatively new, and the company behind it is still a small startup. It is yet to be seen if the product and its developers establish themselves as a serious contender.\n\n\n#### Summary\nTitan Takeoff Inference Server can be a reliable option for teams prioritizing data privacy for custom LLM deployments. In any case, this platform is worth watching for everyone interested in serving LLMs, given its early stage and growth potential.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Recommended  Deep Learning Model Optimization Methods  Read also  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/deep-learning-model-optimization-methods)\n### Comparison of model-serving runtimes\nTo help you navigate the model-serving runtimes we\u2019ve reviewed, here\u2019s an overview of their core features at a glance:\nServing Runtimes  |  Multi-framework support  |  Complexity  |  Native high-performance runtime support(1)  |  Concurrent model execution  |  Model Versioning  |  LLM support  |  Pricing   \n---|---|---|---|---|---|---|---  \n**BentoML** |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Low |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Free +Paid for fully-managed  \n**TensorFlow Serving** |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Medium |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Free  \n**TorchServe** |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Medium |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Free  \n**Nvidia Triton** |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  High |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Free  \n**TitanML** |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Low |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Paid  \nAs all the serving runtimes we\u2019ve considered support REST APIs, gRPC, adaptive batching, asynchronous APIs, and produce Prometheus logs. We\u2019ve decided not to add columns for these features in our comparison table to keep it concise and informative.\n## Model-serving platforms\nThe role of a model-serving platform is to manage the infrastructure for deploying and scaling machine-learning models.\nIt\u2019s essential to understand that the decision isn\u2019t between choosing a serving platform or a serving runtime. In most scenarios, you\u2019ll need both. In fact, a model packaged with a serving runtime can then be deployed on a serving platform for scaling and monitoring.\nIt\u2019s also worth mentioning that most serving platforms have their own native serving runtimes that you can choose to use or substitute for an external one.\nLet\u2019s take the example of the Vertex AI serving platform:\n  * **Native serving runtime:** You can deploy your model using a [pre-built model container](https://cloud.google.com/vertex-ai/docs/model-registry/import-model#container-type) provided by Google.\n  * **External serving runtime:** Another option is to use a [custom container](https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container) that you created with a model serving runtime such as BentoML.\n\n\n### Cloud-provider platforms (Amazon SageMaker, Vertex AI, Azure Machine Learning)\n![Comparison of model-serving components on AWS, Azure, and GCP](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/best-ml-model-serving-tools-7.png?resize=1400%2C634&ssl=1)Comparison of model-serving components on AWS, Azure, and GCP | [Source](https://medium.com/@vineetjaiswal/introduction-comparison-of-mlops-platforms-aws-sagemaker-azure-machine-learning-gcp-vertex-ai-9c1153399c8e)\nThe model-serving platforms by the three big cloud providers \u2013 Amazon SageMaker, Vertex AI, and Azure Machine Learning \u2013 are very similar. They are part of end-to-end machine-learning platforms that manage the entire lifecycle from data preparation over experimentation and training to deployment.\n#### Reasons for choosing a cloud-provider platform\n  * **Ease of use:** The simplicity of these platforms enables even small and relatively inexperienced teams to deploy, monitor, and scale ML models at a fast pace.\n  * **Tight integration:** Simplifies the integration of machine learning models with the services and tools of the respective cloud provider. For example, Vertex AI has full integration with the Google Cloud Platform, while SageMaker works seamlessly with many other AWS services.\n  * **Managed Infrastructure** : Requires little setup and maintenance for scaling ML models. The platform will commission and manage the necessary compute resources on your behalf.\n  * **Auto-scaling endpoints:** Model endpoints are automatically adjusted according to the incoming traffic with minimal effort. (Among the three we discuss here, Amazon SageMaker is the only solution that enables its machine-learning inference endpoints to scale to zero.)\n  * **Support** : It is possible to receive extensive support from the respective provider with an additional subscription.\n  * **Built-in monitoring:** These platforms do not need additional infrastructure to monitor the model containers but come with integrated model metrics that are sufficient in many scenarios.\n  * **Documentation** : Provides comprehensive and regularly updated documentation. However, the often vast documentation is notoriously cumbersome to navigate.\n\n\n#### Key limitations and drawbacks of cloud-provider platforms\n  * **Vendor lock-in** : The tight integration creates a strong dependence on the respective cloud provider. Migrating to other platforms is often equivalent to re-engineering large parts of the setup.\n  * **High cost:** These platforms are more expensive than self-managed infrastructure, particularly if your application needs high-end GPUs. Cloud providers charge a significant premium compared to their regular infrastructure prices.\n  * **Complex pricing:** It is usually difficult to evaluate costs fully due to the multiple factors that contribute to the overall expense, such as compute resources, storage needs, and network bandwidth, in addition to the premium charged for using the fully managed solution.\n  * **Operational constraints:** The cloud-provider platforms enforce vendor-specific formats and procedures. This limits flexibility and customizability as you are obligated to follow the constraints of the cloud providers.\n\n\n#### Summary\nCloud-provider platforms are ideal for small to medium teams with limited MLOps expertise. They\u2019re also a good fit for companies that are committed to a cloud platform for the long term and prefer to delegate most of the maintenance of their environment. However, they must be prepared to pay the high costs associated with them.\n[ ![](https://neptune.ai/wp-content/themes/neptune/img/icon-related--article.svg) Recommended  The Best Amazon SageMaker Alternatives [for Experiment Tracking and Model Management]  Read also  ![](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/amazon-sagemaker-alternatives)\n### KServe\n![Overview of KServe\u2019s ModelMesh architecture for high-scale serving.](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/best-ml-model-serving-tools-8.png?resize=1134%2C704&ssl=1)Overview of KServe\u2019s ModelMesh architecture for high-scale serving. A controller Pod orchestrates multiple Kubernetes Deployments, which load and serve multiple models. A routing layer spawns the runtime Pods and distributes incoming requests. | [Source](https://kserve.github.io/website/0.11/modelserving/mms/modelmesh/overview/)\n[KServe](https://kserve.github.io/website) is an open-source tool that focuses on serving and scaling machine-learning models on [Kubernetes](https://kubernetes.io/).\nPreviously called KFServing, this tool originated from the open-source [Kubeflow](https://www.kubeflow.org/) project. It has been renamed KServe and now operates as a standalone server.\n#### Reasons for choosing KServe\n  * **Auto-scaling:** This platform offers auto-scaling capabilities out-of-the-box. Additionally, it supports scaling to zero to optimize resource costs.\n  * **Online prediction:** The [KServe architecture](https://kserve.github.io/website/0.11/modelserving/control_plane/) enables efficient real-time inference.\n  * **Batch prediction:** KServe implements a sophisticated [inference batcher](https://kserve.github.io/website/master/modelserving/batcher/batcher/) providing high-performance batch predictions.\n  * **Complex inference graphs** : KServe provides an elegant design to handle complex [inference graphs](https://kserve.github.io/website/0.9/modelserving/inference_graph/#concepts) efficiently.\n  * **ML framework support** : Supports a variety of ML frameworks, such as TensorFlow and PyTorch.\n  * **Integration** : Integrates well with a broad array of tools such as ZenML, Kafka, Nvidia Triton, Grafana, and more.\n  * **Deployment strategies:** KServe offers advanced deployment strategies like Multi-Armed Bandits, A/B testing, and Canary deployments.\n  * **Community support** : This platform benefits from an active and supportive community.\n\n\n#### Key limitations and drawbacks of KServe\n  * **Complexity of Kubernetes:** KServe requires you to deploy and maintain your own Kubernetes cluster, which can be challenging without a dedicated DevOps team.\n  * **Lack of built-in monitoring** : KServe does not include a built-in solution for model monitoring. KServe containers produce Prometheus logs, but the user is left to install and maintain a Prometheus server. However, as KServe is already running on K8s, adding an extra component shouldn\u2019t be a problem.\n\n\n#### Summary\nThis platform is ideal for teams with solid Kubernetes knowledge that prioritize advanced deployment features and customization to tailor their MLOps infrastructure to their application.\n### Seldon Core\n![Schematic depiction of a simple and a complex inference graph with Seldon Core](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/best-ml-model-serving-tools-9.png?resize=1920%2C988&ssl=1)Schematic depiction of a simple and a complex inference graph with Seldon Core | [Source](https://github.com/SeldonIO/seldon-core)\n[Seldon Core](https://www.seldon.io/solutions/seldon-core) is a model-serving platform to deploy and scale machine-learning models on [Kubernetes](https://kubernetes.io). This platform is well-known for its advanced deployment features.\nUntil January 22, 2024, Seldon Core was available as a free open-source tool. However, it transitioned to a Business Source License (BSL) 1.1. Companies now require a yearly subscription fee of $18,000 for commercializing products designed with Seldon Core versions released after the 22 of January 2024.\n#### Reasons for choosing Seldon Core\n  * **Online prediction:** Offers a robust online prediction solution with [native Kafka integration.](https://docs.seldon.io/projects/seldon-core/en/latest/streaming/kafka.html)\n  * **Batch prediction:** This tool provides a well-structured approach for [batch prediction](https://docs.seldon.io/projects/seldon-core/en/latest/servers/batch.html#servers-batch--page-root).\n  * **Advanced deployment:** Supports Multi-Armed Bandit, Canary deployments, and A/B testing.\n\n\n#### Key limitations and drawbacks of Seldon Core\n  * **Expensive subscription:**[Seldon\u2019s pricing](https://www.seldon.io/pricing) starts at $18,000 a year without provider support.\n  * **Complexity of Kubernetes:** Seldon Core requires you to deploy and maintain your own Kubernetes cluster, which can be challenging without a dedicated DevOps team.\n  * **Auto-scaling:** Auto-scaling requires extra setup through [KEDA](https://keda.sh/) and does not support scale-to-zero.\n\n\n#### Summary\nSeldon Core is a viable and reliable alternative for teams looking to scale machine learning models on Kubernetes and use advanced deployment features.\n![The BentoCloud user interface ](https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/best-ml-model-serving-tools-10.png?resize=1920%2C1729&ssl=1)The BentoCloud user interface displaying supported pre-packaged ML models | Source: Screenshot Author\n### BentoCloud\nThe BentoCloud user interface displaying supported pre-packaged ML models | Source: Screenshot Author\n[BentoCloud](https://www.bentoml.com/cloud) is a proprietary model-serving platform for scaling BentoML containers, designed and operated by the same company that built BentoML. Its goal is performance and cost-efficiency.\nBentoCloud leverages the BentoML serving runtime to provide pre-built model containers and high-level APIs to scale machine-learning models with a few lines of code.\n#### Reasons for choosing BentoCloud\n  * **Ease of use:** BentoCloud provides a simple yet effective CLI experience for developers to deploy BentoML containers [on various cloud providers](https://docs.bentoml.org/en/latest/bentocloud/how-tos/byoc.html).\n  * **Complex inference graphs:** BentoCloud allows the building of distributed inference graphs with multiple models.\n  * **Auto-scaling:** The BentoCloud platform supports auto-scaling out of the box and can scale to zero.\n  * **Advanced deployment strategies:** BentoCloud supports canary deployments and A/B testing.\n  * **ML framework support:** As this platform scales and manages BentoML containers, it inherits its broad machine-learning framework support.\n  * **No vendor lock-in:** Enterprise customers can[ deploy BentoCloud to their cloud provider of choice](https://docs.bentoml.org/en/latest/bentocloud/how-tos/byoc.html). Further, teams can always deploy their BentoML Docker images outside of BentoCloud.\n  * **Built-in monitoring:** Model metrics are accessible from the BentoCloud UI without requiring additional setup.\n\n\n#### Key limitations and drawbacks of BentoCloud\n  * **Cost:** BentoCloud platform is not open-source. The fully managed variant operates on a pay-as-you-go pricing model. An enterprise subscription (pricing not publicly available) allows deploying BentoCloud on your own cloud infrastructure.\n  * **Requires BentoML:** BentoCloud only supports models packaged with the BentoML model-serving runtime\n  * **No multi-armed bandits:** BentoCloud does not support the multi-armed bandit deployment strategy\n\n\n#### Summary\nBentoCloud is a strong choice for teams willing to use BentoML as their serving runtime and are looking for a fully managed platform that is easy to use.\n### Comparison of model-serving platforms\nTo help you navigate the model-serving platforms we\u2019ve discussed, here\u2019s an overview of their core features at a glance:\nServing Platforms  |  Multi-framework support  |  Complexity  |  Auto Scaling  |  Needs K8s  |  Scales to zero  |  Vendor Lock-in  |  Multi Armed Bandits  |  A/B Testing  |  Canary  |  Built-in Monitoring  |  Pricing   \n---|---|---|---|---|---|---|---|---|---|---|---  \n**Amazon SageMaker** |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Medium |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Paid  \n**Vertex AI** |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Medium |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Paid  \n**Azure Machine Learning** |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Medium |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Paid  \n**KServe** |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  High |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Free  \n**Seldon Core** |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  High |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Paid  \n**BentoCloud** |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Low |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) |  Paid  \n## Conclusion\nChoosing the right model-serving tool is essential for turning machine learning models into applications that bring value.\nIn this article, we provided an overview of serving runtimes and platforms, highlighting their features, benefits, and limitations. If you made it this far, you likely understood the difficulty of finding your company\u2019s optimal model-serving stack.\nRemember that your choice should be based on your project\u2019s needs, your team\u2019s skills, and how much control you need over deployment and scaling. Factors like framework compatibility, integration capabilities, and the trade-off between complexity and functionality are key to this decision as well.\nHere\u2019s our last bit of advice: start by narrowing down which tools can match your specific use case. Then, take some time to build a proof of concept (PoC) for each potential option. There is no better way to get the feel of a framework than to start a quick implementation.\n##  Was the article useful? \n![yes](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--yes.svg) Yes  ![no](https://neptune.ai/wp-content/themes/neptune/img/icon-article-rating--no.svg) No \n![](https://neptune.ai/wp-content/themes/neptune/img/icon-bulb.svg) Suggest changes \nYour email Your message (optional)\nThis site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply. \nI am familiar with the [Privacy Policy](https://neptune.staginglab.eu/privacy-policy)*\nSubmit\n\u0394\n![](https://neptune.ai/wp-content/themes/neptune/img/blocks/i-box/header-icon.svg)\n### **More about** Best Tools For ML Model Serving \n####  Check out our  **product resources** and  **related articles** below: \n[ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  MLOps Landscape in 2024: Top Tools and Platforms  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/mlops-tools-platforms-landscape) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  13 Best Tools for ML Experiment Tracking and Management in 2024  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/best-ml-experiment-tracking-tools) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  How to Optimize GPU Usage During Model Training With neptune.ai  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/optimizing-gpu-usage-during-model-training-with-neptune) [ ![](https://neptune.ai/wp-content/themes/neptune/img/ibox-related.svg) Related article  Building a Machine Learning Platform [Definitive Guide]  Read more  ![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-button-arrow-right.svg) ](https://neptune.ai/blog/ml-platform-guide)\n###  Explore more content topics: \n[ Computer Vision ](https://neptune.ai/blog/category/computer-vision) [ General ](https://neptune.ai/blog/category/general) [ LLMOps ](https://neptune.ai/blog/category/llmops) [ ML Model Development ](https://neptune.ai/blog/category/machine-learning-model-development) [ ML Tools ](https://neptune.ai/blog/category/machine-learning-tools) [ MLOps ](https://neptune.ai/blog/category/mlops) [ Natural Language Processing ](https://neptune.ai/blog/category/natural-language-processing) [ Paper Reflections ](https://neptune.ai/blog/category/paper-reflections) [ Product Updates ](https://neptune.ai/blog/category/product-updates) [ Reinforcement Learning ](https://neptune.ai/blog/category/reinforcement-learning) [ Tabular Data ](https://neptune.ai/blog/category/tabular-data) [ Time Series ](https://neptune.ai/blog/category/time-series-forecasting)\nAbout neptune.ai ![chevron](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![](data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1920 1080'%3E%3C/svg%3E) ![](https://neptune.ai/wp-content/themes/neptune/img/icon-play.svg)\nNeptune is the experiment tracker purpose-built for foundation model training. \nIt lets you monitor and visualize thousands of per-layer metrics\u2014losses, gradients, and activations\u2014at any scale. Drill down into logs and debug training issues fast. Keep your model training stable while reducing wasted GPU cycles. \n[ Play with a live project  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://demo.neptune.ai/) [ See Docs  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://docs.neptune.ai/)\nTable of contents\n![chevron](https://neptune.ai/wp-content/themes/neptune/img/icon-chevron-down-small.svg)\n  1. Understanding model serving\n  2. Do you need a model-serving runtime?\n  3. Criteria for selecting model-serving tools\n  4. The top model-serving tools in 2024 \n  5. Model-serving runtimes\n     * BentoML\n     * TensorFlow Serving (TFX)\n     * TorchServe\n     * Triton Inference Server\n     * Titan Takeoff Inference Server\n     * Comparison of model-serving runtimes\n  6. Model-serving platforms\n     * Cloud-provider platforms (Amazon SageMaker, Vertex AI, Azure Machine Learning)\n     * KServe\n     * Seldon Core\n     * BentoCloud\n     * Comparison of model-serving platforms\n  7. Conclusion\n\n\n![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-cancel.svg) ![chevron](https://neptune.ai/wp-content/themes/neptune/img/popup-more.svg)\nCheck also: [A Quickstart Guide to Auto-Sklearn (AutoML) For Machine Learning Practitioners](https://neptune.ai/blog/a-quickstart-guide-to-auto-sklearn-automl-for-machine-learning-practitioners)\n##  **Monitor your model training at scale**\nJoin 60,000+ researchers and practitioners who use Neptune to debug training failures, spot anomalies, and compare experiments.\n[ Try for free  ](https://app.neptune.ai/register) [ Get early access to Neptune Scale  ](https://neptune.ai/early-access)\n[ Take an interactive product tour  ![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg) ](https://scale.neptune.ai/o/neptune/org/LLM-training-example/runs/compare?viewId=9d0e032a-5a78-4a0e-81d1-98e0a7c81a8f&detailsTab=metadata&dash=charts&type=run&experimentOnly=true&compare=u0MsW4a1PJIUJ75nglpjHa9XUKFfAmcBRbLhNatCHX20)\n![](https://neptune.ai/wp-content/themes/neptune/img/footer-post-illustration.svg)\n#### Newsletter\nTop articles, case studies, events (and more) in your inbox every month.\nGet Newsletter\n  * [Product](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [Walkthrough](https://neptune.ai/walkthrough)\n    * [Documentation](https://docs.neptune.ai/)\n    * [Deployment options](https://neptune.ai/product/deployment-options)\n    * [Pricing](https://neptune.ai/pricing)\n    * [Service status](https://status.neptune.ai/)\n\n\n  * [Solutions](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [AI Researcher](https://neptune.ai/product/ai-researcher)\n    * [ML Team Lead](https://neptune.ai/product/ml-team-lead)\n    * [ML Platform Engineer](https://neptune.ai/product/ml-platform)\n    * [Enterprise](https://neptune.ai/product/enterprise)\n    * [Academic Research](https://neptune.ai/research)\n\n\n  * [Compare](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [Neptune vs Weights & Biases](https://neptune.ai/vs/wandb)\n    * [Neptune vs MLflow](https://neptune.ai/vs/mlflow)\n    * [Neptune vs TensorBoard](https://neptune.ai/vs/tensorboard)\n    * [Other comparisons](https://neptune.ai/vs)\n    * [ML experiment tracking tools](https://neptune.ai/blog/best-ml-experiment-tracking-tools)\n\n\n  * [Community](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [Blog](https://neptune.ai/blog)\n    * [Experiment Tracking Learning Hub](https://neptune.ai/experiment-tracking-learn-hub)\n    * [LLMOps Learning Hub](https://neptune.ai/llmops-learning-hub)\n    * [MLOps Learning Hub](https://neptune.ai/mlops-learn-hub)\n    * [How to Build an Experiment Tracker](https://neptune.ai/blog/build-experiment-tracking-tool)\n\n\n  * [Company](https://neptune.ai/blog/ml-model-serving-best-tools/)\n    * [Customers](https://neptune.ai/customers)\n    * [About us](https://neptune.ai/about-us)\n    * [Careers](https://neptune.ai/jobs)\n    * [Security portal and SOC 2](https://security.neptune.ai/)\n    * [Contact us](https://neptune.ai/contact-us)\n\n\n[ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-linked-in.svg) ](https://www.linkedin.com/company/neptuneai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-twitter.svg) ](https://twitter.com/neptune_ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-github.svg) ](https://github.com/neptune-ai) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-facebook.svg) ](https://www.facebook.com/neptuneAI) [ ![social icon](https://neptune.ai/wp-content/uploads/2022/08/icon-youtube.svg) ](https://www.youtube.com/channel/UCvOJU-ubyUqxGSDRN7xK4Ng) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-spotify.svg) ](https://open.spotify.com/show/4kGi82i4wTYgHbWmVMri5x) [ ![social icon](https://neptune.ai/wp-content/uploads/2023/01/icon-apple-podcast.svg) ](https://podcasts.apple.com/us/podcast/mlops-live/id1634179447)\n[ Terms of Service ](https://neptune.ai/terms-of-service) [ Privacy Policy ](https://neptune.ai/privacy-policy) [ SLA ](https://neptune.ai/service-level-agreement)\nCopyright \u00a9 2025 Neptune Labs. All rights reserved.\n![](https://neptune.ai/wp-content/themes/neptune/img/footer-stars--414w.png) ![](https://pixel.wp.com/g.gif?v=ext&blog=211928962&post=3737&tz=0&srv=neptune.ai&hp=atomic&ac=2&amp=0&j=1%3A14.5-a.9&host=neptune.ai&ref=&fcp=572&rand=0.835273875602804)![](https://t.co/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=0e667256-d081-4e19-bed0-f7b214a5dbe5&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=ddb7f659-6d78-4028-9b85-90be07bccc62&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-serving-best-tools&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=America%2FBoise%26en-US%26Google%20Inc.%26MacIntel%26127%261080%26600%2610%2630%261080%26600%260%26na&eci=3&event=%7B%7D&event_id=0e667256-d081-4e19-bed0-f7b214a5dbe5&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=ddb7f659-6d78-4028-9b85-90be07bccc62&tw_document_href=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-serving-best-tools&tw_iframe_status=0&txn_id=o35a7&type=javascript&version=2.3.31)\n![](https://bat.bing.com/action/0?ti=97011748&tm=gtm002&Ver=2&mid=a1cd6a70-59c6-4907-a3c9-3c8a778747dc&bo=1&sid=37ef08200d8e11f0a36ed785bff3ae58&vid=37ef08800d8e11f089240dd13df19027&vids=0&msclkid=N&pi=918639831&lg=en-US&sw=1080&sh=600&sc=30&tl=Best%20Tools%20For%20ML%20Model%20Serving&p=https%3A%2F%2Fneptune.ai%2Fblog%2Fml-model-serving-best-tools&r=&lt=822&evt=pageLoad&sv=1&cdb=AQkR&rn=804419)\n",
    "answer": "```markdown\n# TL;DR\n\nModel serving is crucial for deploying ML models, involving packaging, API creation, and performance monitoring. Key tools include BentoML, TensorFlow Serving, and Triton. Choose based on project needs, framework compatibility, and team skills for optimal results.\n```"
  },
  {
    "instruction": "# Resources [Community]\n\n\t<child_page>\n\t# Number of samples for fine-tuning based on general, domain, task specific\n\t\n\t\ud83d\udd17\u00a0[Number of samples for fine-tuning based on general, domain, task specific](https://www.linkedin.com/posts/maxime-labonne_the-term-fine-tuning-includes-completely-activity-7267499732669796352-AwSE?utm_source=share&utm_medium=member_desktop)\n\t---\n\t\n\t\ud83d\udd27 The term \"fine-tuning\" includes completely different categories, with different objectives.\n\t1\ufe0f\u20e3 General-purpose fine-tuning corresponds to the instruct models produced by LLM providers. This approach is used both at\n\t[Liquid AI](https://www.linkedin.com/company/liquid-ai-inc/)\n\tand by organizations like Meta that release open-weight models.\n\tHere, the objective is to build a general-purpose AI assistant that can do most tasks. Because we have a super broad objective, this often requires over 1M samples just for supervised fine-tuning.\n\t2\ufe0f\u20e3 Domain-specific fine-tuning targets domains like finance, law, medicine, specific languages, etc. The goal is slightly similar to general-purpose fine-tuning but within a defined category. This type of fine-tuning often benefits from continual pre-training (CPT) if your domain is poorly represented in the pre-training data.\n\tThis type of fine-tuning tends to disappear because base models are getting better and better. However, it is still useful for niche domains and rare languages. You can see an example of a pipeline with CPT and model merging to create a strong domain-specific LLM. \u2193\n\t3\ufe0f\u20e3 Task-specific fine-tuning is defined by its narrow scope, like summarization, information extraction, etc. This is quite popular with small models. With the right dataset, you reach the performance of frontier LLMs using cheap and fast models for your use case. It requires more tinkering but this can also build interesting technical moats. Because of the narrow focus, this also requires a lot fewer samples than the other types of fine-tuning.\n\tIf you use fine-tuning in your daily job, I'm super interested in knowing more about your use cases. This is not an influencer trick to get comments, I'm genuinely trying to update my view of the FT landscape. :))\n\t\n\t[Image](No URL)\n\t\n\t[Image](No URL)\n\t</child_page>\n\n# Tools\n\n\t[Link Preview](https://github.com/unslothai/unsloth)\n\t[Link Preview](https://github.com/axolotl-ai-cloud/axolotl)\n\t[Link Preview](https://github.com/huggingface/trl)\n\n---\n\n# Articles\n\n\t- Parameter efficient fine-tuning: [https://lightning.ai/pages/community/article/understanding-llama-adapters/](https://lightning.ai/pages/community/article/understanding-llama-adapters/)\n\t- [Fine-tune Llama 3 with ORPO](https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada)\n\t- [LLM Fine-Tuning and Model Selection Using Neptune and Transformers](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers)\n\n# Papers\n\n\t- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n\t- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n\n# Code & Repositories\n\n\t- [Templates for Chat Models [HuggingFace]](https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use)\n\t- [https://github.com/huggingface/trl](https://github.com/huggingface/trl)\n\n# Benchmarks\n\n\t- [MMLU (Massive Multitask Language Understanding)](/2481cc7a06434b84aebb95999eb77dff)\n\t- [Code Generation\u00a0on\u00a0HumanEval](https://paperswithcode.com/sota/code-generation-on-humaneval)\n\n# Videos\n\n\t- [SFT, Preference Alignment, Merging, MoE](https://www.youtube.com/live/R0X7mPagRiE?app=desktop&t=11716)",
    "answer": "```markdown\n# TL;DR Summary\n\nFine-tuning in AI includes three categories: \n1. **General-purpose** (1M+ samples, broad tasks).\n2. **Domain-specific** (niche areas, benefits from continual pre-training).\n3. **Task-specific** (narrow focus, fewer samples, efficient for small models).\n\nKey resources include tools, articles, papers, code, and benchmarks related to LLMs. \n```\n"
  },
  {
    "instruction": "[Skip to main content](https://www.databricks.com/product/unity-catalog/#main)\n[![](https://www.databricks.com/product/unity-catalog/)](https://www.databricks.com/)\n[Login](https://login.databricks.com/?dbx_source=www&itm=main-cta-login&l=en-EN)\n[![](https://www.databricks.com/product/unity-catalog/)](https://www.databricks.com/)\n  * Why Databricks \n    *       * Discover\n        * [For Executives](https://www.databricks.com/why-databricks/executives)\n        * [For Startups ](https://www.databricks.com/product/startups)\n        * [Lakehouse Architecture ](https://www.databricks.com/product/data-lakehouse)\n        * [Mosaic Research](https://www.databricks.com/research/mosaic)\n      * Customers\n        * [Featured Stories](https://www.databricks.com/customers)\n        * [See All Customers](https://www.databricks.com/customers/all)\n      * Partners\n        * [Cloud ProvidersDatabricks on AWS, Azure, and GCP](https://www.databricks.com/company/partners/cloud-partners)\n        * [Consulting & System IntegratorsExperts to build, deploy and migrate to Databricks](https://www.databricks.com/company/partners/consulting-and-si)\n        * [Technology PartnersConnect your existing tools to your Lakehouse](https://www.databricks.com/company/partners/technology-partner-program)\n        * [C&SI Partner ProgramBuild, deploy or migrate to the Lakehouse](https://www.databricks.com/company/partners/consulting-and-si/candsi-partner-program)\n        * [Data PartnersAccess the ecosystem of data consumers](https://www.databricks.com/company/partners/data-partner-program)\n        * [Partner SolutionsFind custom industry and migration solutions](https://www.databricks.com/company/partners/consulting-and-si/partner-solutions)\n        * [Built on DatabricksBuild, market and grow your business](https://www.databricks.com/company/partners/built-on-partner-program)\n  * Product \n    *       * Databricks Platform\n        * [Platform OverviewA unified platform for data, analytics and AI](https://www.databricks.com/product/data-intelligence-platform)\n        * [Data ManagementData reliability, security and performance](https://www.databricks.com/product/delta-lake-on-databricks)\n        * [SharingAn open, secure, zero-copy sharing for all data](https://www.databricks.com/product/delta-sharing)\n        * [Data WarehousingServerless data warehouse for SQL analytics](https://www.databricks.com/product/databricks-sql)\n        * [GovernanceUnified governance for all data, analytics and AI assets](https://www.databricks.com/product/unity-catalog)\n        * [Real-Time AnalyticsReal-time analytics, AI and applications made simple](https://www.databricks.com/product/data-streaming)\n        * [Artificial IntelligenceBuild and deploy ML and GenAI applications](https://www.databricks.com/product/artificial-intelligence)\n        * [Data EngineeringETL and orchestration for batch and streaming data](https://www.databricks.com/solutions/data-engineering)\n        * [Business IntelligenceIntelligent analytics for real-world data](https://www.databricks.com/product/business-intelligence)\n        * [Data ScienceCollaborative data science at scale](https://www.databricks.com/product/data-science)\n      * Integrations and Data\n        * [MarketplaceOpen marketplace for data, analytics and AI](https://www.databricks.com/product/marketplace)\n        * [IDE IntegrationsBuild on the Lakehouse in your favorite IDE](https://www.databricks.com/product/data-science/ide-integrations)\n        * [Partner ConnectDiscover and integrate with the Databricks ecosystem](https://www.databricks.com/partnerconnect)\n      * Pricing\n        * [Databricks PricingExplore product pricing, DBUs and more](https://www.databricks.com/product/pricing)\n        * [Cost CalculatorEstimate your compute costs on any cloud](https://www.databricks.com/product/pricing/product-pricing/instance-types)\n      * Open Source\n        * [Open Source TechnologiesLearn more about the innovations behind the platform](https://www.databricks.com/product/open-source)\n  * Solutions \n    *       * Databricks for Industries\n        * [Communications](https://www.databricks.com/solutions/industries/communications)\n        * [Media and Entertainment](https://www.databricks.com/solutions/industries/media-and-entertainment)\n        * [Financial Services](https://www.databricks.com/solutions/industries/financial-services)\n        * [Public Sector](https://www.databricks.com/solutions/industries/public-sector)\n        * [Healthcare & Life Sciences](https://www.databricks.com/solutions/industries/healthcare-and-life-sciences)\n        * [Retail](https://www.databricks.com/solutions/industries/retail-industry-solutions)\n        * [Manufacturing](https://www.databricks.com/solutions/industries/manufacturing-industry-solutions)\n        * [See All Industries](https://www.databricks.com/solutions)\n      * Cross Industry Solutions\n        * [Customer Data Platform](https://www.databricks.com/solutions/industries/customer-experience)\n        * [Cyber Security](https://www.databricks.com/solutions/industries/cybersecurity)\n      * Migration & Deployment\n        * [Data Migration](https://www.databricks.com/solutions/migration)\n        * [Professional Services](https://www.databricks.com/professional-services)\n      * Solution Accelerators\n        * [Explore AcceleratorsMove faster toward outcomes that matter](https://www.databricks.com/solutions/accelerators)\n  * Resources \n    *       * Training and Certification\n        * [Learning OverviewHub for training, certification, events and more](https://www.databricks.com/learn)\n        * [Training OverviewDiscover curriculum tailored to your needs](https://www.databricks.com/learn/training/home)\n        * [Databricks AcademySign in to the Databricks learning platform](https://www.databricks.com/learn/training/login)\n        * [CertificationGain recognition and differentiation](https://www.databricks.com/learn/training/certification)\n        * [University AllianceWant to teach Databricks? See how.](https://www.databricks.com/university)\n      * Events\n        * [Data + AI Summit](https://www.databricks.com/dataaisummit)\n        * [Data + AI World Tour](https://www.databricks.com/dataaisummit/worldtour)\n        * [Data Intelligence Days](https://www.databricks.com/lp/data-intelligence-days)\n        * [Event Calendar](https://www.databricks.com/events)\n      * Blog and Podcasts\n        * [Databricks BlogExplore news, product announcements, and more](https://www.databricks.com/blog)\n        * [Databricks Mosaic Research BlogDiscover the latest in our Gen AI research](https://www.databricks.com/blog/category/generative-ai/mosaic-research)\n        * [Data Brew PodcastLet\u2019s talk data!](https://www.databricks.com/discover/data-brew)\n        * [Champions of Data + AI PodcastInsights from data leaders powering innovation](https://www.databricks.com/discover/champions-of-data-and-ai)\n      * Get Help\n        * [Customer Support](https://www.databricks.com/support)\n        * [Documentation](https://docs.databricks.com/en/index.html)\n        * [Community](https://community.databricks.com/s/)\n      * Dive Deep\n        * [Resource Center](https://www.databricks.com/resources)\n        * [Demo Center](https://www.databricks.com/resources/demos)\n  * About \n    *       * Company\n        * [Who We Are](https://www.databricks.com/company/about-us)\n        * [Our Team](https://www.databricks.com/company/leadership-team)\n        * [Databricks Ventures](https://www.databricks.com/databricks-ventures)\n        * [Contact Us](https://www.databricks.com/company/contact)\n      * Careers\n        * [Working at Databricks](https://www.databricks.com/company/careers)\n        * [Open Jobs](https://www.databricks.com/company/careers/open-positions)\n      * Press\n        * [Awards and Recognition](https://www.databricks.com/company/awards-and-recognition)\n        * [Newsroom](https://www.databricks.com/company/newsroom)\n      * Security and Trust\n        * [Security and Trust](https://www.databricks.com/trust)\n\n\n  * Ready to get started?\n  * [Get a Demo](https://www.databricks.com/resources/demos)\n\n\n  * [Login](https://login.databricks.com/?dbx_source=www&itm=main-cta-login&l=en-EN)\n  * [Contact Us](https://www.databricks.com/company/contact)\n  * [Try Databricks](https://login.databricks.com/?dbx_source=www&itm_data=databricks-web-nav&intent=SIGN_UP&l=en-EN)\n\n\n# \nUnity Catalog\n### Unified and open governance for data and AI\n[Get started](https://login.databricks.com/?dbx_source=www&intent=SIGN_UP \"Get started\")[Watch demo](https://www.databricks.com/resources/demos/videos/data-governance/unity-catalog-overview \"Watch demo\")\n[![](https://www.databricks.com/sites/default/files/2025-02/2023-10-EB-Data-and-AI-Governance-PC-107x107-2x.png?v=1740752502) A Comprehensive Guide to Data and AI Governance Build a unified governance strategy for your data and AI estate. Get the eBook](https://www.databricks.com/resources/ebook/data-analytics-and-ai-governance?itm_data=product-unity-catalog-pc-feb25)\nDatabricks Unity Catalog is the industry\u2019s only unified and open governance solution for data and AI, built into the Databricks Data Intelligence Platform. With Unity Catalog, organizations can seamlessly govern both structured and unstructured data in any format, as well as machine learning models, notebooks, dashboards and files across any cloud or platform. Data scientists, analysts and engineers can securely discover, access and collaborate on trusted data and AI assets across platforms, leveraging AI to boost productivity and unlock the full potential of the lakehouse environment. This unified and open approach to governance promotes interoperability and accelerates data and AI initiatives while simplifying regulatory compliance.\n### \u201cDatabricks Unity Catalog is now an integral part of the PepsiCo Data Foundation, our centralized global system that consolidates over 6 petabytes of data worldwide. It streamlines the onboarding process for more than 1,500 active users and enables unified data discovery for our 30+ digital product teams across the globe, supporting both business intelligence and artificial intelligence applications.\u201d\n\u2014 Bhaskar Palit, Senior Director, Data and Analytics\n[Learn more](https://www.databricks.com/blog/how-pepsico-established-enterprise-grade-data-intelligence-platform-powered-databricks-unity)\n![pepsico](https://www.databricks.com/sites/default/files/2024-06/pepsico-customer-image.png?v=1717761445)\n### \u201cUnity Catalog distills permissioning into its important pieces by removing the complexity introduced by IAM policies and other data control platforms, allowing us to focus on what is important to our business use case.\u201d\n\u2014 Joseph Kesting, Software Engineer\n[Learn more](https://www.databricks.com/customers/block/unity-catalog)\n![Block](https://www.databricks.com/sites/default/files/2023-06/Block.png?v=1687639080)\n### \u201cUnity Catalog has enabled us to centralize our data governance, providing robust security features and streamlined compliance processes. This centralization reduced the complexities associated with managing permissions across disparate systems and helped ensure that only authorized personnel had access to sensitive data, which is crucial for adhering to stringent data protection laws, including GDPR.\u201d\n\u2014 Michael Ewins, Director of Engineering\n[Learn more](https://www.databricks.com/blog/data-destinations-how-skyscanner-optimizes-traveler-experiences-databricks-unity-catalog)\n![Skyscanner](https://www.databricks.com/sites/default/files/2024-06/skyscanner-customer-image.png?v=1717760942)\n### \u201cWith Unity Catalog, we are centralizing data catalog and access management across various teams and workspaces, which has simplified governance.\u201d\n\u2014 Venkatesh Sivasubramanian, Senior Director, Big Data\n[Learn more](https://www.databricks.com/customers/rivian)\n![Rivian](https://www.databricks.com/sites/default/files/2023-06/Rivian.png?v=1687639181)\n### \u201cDistributing data governance through Databricks Unity Catalog transformed Akamai\u2019s approach to managing and governing data. With Unity Catalog, we are now managing and governing over 6 petabytes of data with fine-grained access controls on rows and columns.\u201d\n\u2014 Gilad Asulin, Big Data Team Leader\n[Learn more](https://medium.com/@agilad_3118/how-akamai-leverages-databricks-unity-catalog-for-distributed-data-governance-d1eda1c5851)\n![Akamai](https://www.databricks.com/sites/default/files/2023-06/Akamai.png?v=1687639211)\n![PepsiCo logo](https://www.databricks.com/sites/default/files/2024-06/logo-color-pepsico.svg?v=1717720292)\n![block](https://www.databricks.com/sites/default/files/2023-09/logo-color-block-image.png?v=1724434337)\n![Skyscanner](https://www.databricks.com/sites/default/files/2024-06/logo-color-skyscanner.svg?v=1724667400)\n![Rivian logo](https://www.databricks.com/sites/default/files/2023-03/logo-color-rivian.svg?v=1678822598)\n![Akamai](https://www.databricks.com/sites/default/files/2023-03/akamai-logo.svg?v=1724432005)\n![UC](https://www.databricks.com/sites/default/files/2023-06/unity-catalog1.png?v=1687797543)\n### Unified visibility into data and AI\nEasily discover and classify both structured and unstructured data in any format, including machine learning models, notebooks, dashboards and files across all cloud platforms. Seamlessly manage, govern and query data from external databases and data warehouses like MySQL, PostgreSQL, Amazon Redshift, Snowflake, Azure SQL, Azure Synapse, Google BigQuery and catalogs such as HMS and AWS Glue in one place. Accelerate your data and AI initiatives with a single point of access for data exploration. Improve productivity with intelligent search, discovery and automatically generated data insights and documentation.\n![Single permission model for data and AI](https://www.databricks.com/sites/default/files/2024-06/uc-image-02.png?v=1717757695)\n### Single permission model for data and AI\nSimplify access management with a unified interface to define access policies on data and AI assets and consistently apply and audit these policies on any cloud or data platform. Securely access data from other computing platforms using open interfaces, with consistent permissions managed in one place. Enhance security with fine-grained control on rows and columns, while efficiently managing access through low-code attribute-based access policies that scale seamlessly.\n![A dashboard displaying data and graphs.](https://www.databricks.com/sites/default/files/2023-06/unity-catalog-3.png?v=1703004975)\n### AI-powered monitoring and observability\nHarness the power of AI to automate monitoring, diagnose errors and uphold data and ML model quality. Benefit from proactive alerts that automatically detect personally identifiable information (PII) data, track model drift, and effectively resolve issues within your data and AI pipelines to maintain accuracy and integrity. Streamline debugging, root cause analysis, and impact assessment with automated column-level data lineage. Gain comprehensive observability into your data and AI with operational intelligence utilizing built-in system tables for billing, auditing, lineage and more.\n![Open accessibility](https://www.databricks.com/sites/default/files/2024-06/uc-image-04.png?v=1717759484)\n### Open accessibility\nSecurely access your data and AI assets from any compute engine using open APIs and standard interfaces. Share data and AI assets across clouds, regions and platforms with open source Delta Sharing. Securely collaborate with anyone, anywhere to unlock new revenue streams and drive business value, without relying on proprietary formats, complex ETL processes or costly data replication.\n## Here's more to explore\n[![](https://www.databricks.com/sites/default/files/2024-03/2023-05-wb-introducing-generative-ai-ty-tn-362x190-2x.png?v=1711095832)TrainingGenerative AI FundamentalsStart now](https://www.databricks.com/resources/learn/training/generative-ai-fundamentals?itm_data=cro-fpd-target-360)[![](https://www.databricks.com/sites/default/files/2023-11/2023-04-lakehouse-fundamentals-build-your-skill-ty-tn-362x190-2x.png?v=1700688856)TrainingLakehouse FundamentalsStart now](https://www.databricks.com/resources/learn/training/lakehouse-fundamentals?itm_data=cro-fpd-target-360)[![](https://www.databricks.com/sites/default/files/2024-10/2022-09-Databricks-on-Azure-3-part-training-series-TY-TN-360x188-2x.jpg?v=1729099703)TrainingFree Azure Databricks TrainingWatch now](https://www.databricks.com/resources/webinar/azure-databricks-free-training-series?itm_data=cro-fpd-target-360)\n## Here's more to explore\n[![](https://www.databricks.com/sites/default/files/2024-03/2023-05-wb-introducing-generative-ai-ty-tn-362x190-2x.png?v=1711095832)TrainingGenerative AI FundamentalsStart now](https://www.databricks.com/resources/learn/training/generative-ai-fundamentals?itm_data=cro-fpd-target-360)[![](https://www.databricks.com/sites/default/files/2023-11/2023-04-lakehouse-fundamentals-build-your-skill-ty-tn-362x190-2x.png?v=1700688856)TrainingLakehouse FundamentalsStart now](https://www.databricks.com/resources/learn/training/lakehouse-fundamentals?itm_data=cro-fpd-target-360)[![](https://www.databricks.com/sites/default/files/2024-07/2024-07-eb-big-book-of-data-engineering-3rd-ed-ty-tn-360x188-2x.png?v=1721029189)eBookBig Book of Data EngineeringRead now](https://www.databricks.com/resources/ebook/big-book-of-data-engineering?itm_data=cro-fpd-target-360)\n## Resources\n[![](https://www.databricks.com/sites/default/files/2023-10/2023-10-eb-data-and-ai-governance-ad-1200x628.png?v=1702986485)eBookA Comprehensive Guide to Data and AI GovernanceRead now](https://www.databricks.com/resources/ebook/data-analytics-and-ai-governance)[![](https://www.databricks.com/sites/default/files/2023-06/2022-06-EB-New-Approach-to-Data-Sharing-Ad-1200x628.png?v=1687654866)eBookA New Approach to Data SharingRead now](https://www.databricks.com/resources/ebook/a-new-approach-to-data-sharing)[![](https://www.databricks.com/sites/default/files/2024-04/2024-02-wp-databricks-ai-security-framework-cover-ty-tn-360x188-2x.png?v=1712042992)WhitepaperDatabricks AI Security FrameworkRead now](https://www.databricks.com/resources/whitepaper/databricks-ai-security-framework-dasf)\n## Integrations\nUnity Catalog works with your existing data catalogs, data storage systems and governance solutions so you can leverage your existing investments and build a future-proof governance model without expensive migration costs.\n[![](https://www.databricks.com/sites/default/files/2025-02/LP-hero-image-2023-09-EB-A-New-Approach-to-Data-Sharing-460x460-2x.jpg?v=1738573853) Share data and AI securely Learn the easiest way to share across platforms. Download now](https://www.databricks.com/resources/ebook/a-new-approach-to-data-sharing?itm_data=product-unity-catalog-pc2-feb5)\n## Resources\n[![ebook: A Comprehensive Guide to Data and AI Governance](https://www.databricks.com/sites/default/files/2023-10/2023-10-eb-data-and-ai-governance-ad-1200x628.png)EbookA Comprehensive Guide to Data and AI Governance Read now](https://www.databricks.com/resources/ebook/data-analytics-and-ai-governance)\n[![eBook: A New Approach to Data Sharing](https://www.databricks.com/sites/default/files/2023-06/2022-06-EB-New-Approach-to-Data-Sharing-Ad-1200x628.png)eBookA New Approach to Data Sharing Read now](https://www.databricks.com/resources/ebook/a-new-approach-to-data-sharing)\n[![Databricks AI Security Framework](https://www.databricks.com/sites/default/files/2024-04/2024-02-wp-databricks-ai-security-framework-cover-ty-tn-360x188-2x.png)WhitepaperDatabricks AI Security Framework Read now](https://www.databricks.com/resources/whitepaper/databricks-ai-security-framework-dasf)\n### Featured Talks\n[[Keynote] Evolving Data Governance With Unity Catalog Presented by Matei Zaharia at Data + AI Summit 2024](https://www.youtube.com/watch?v=lZbmBAxa3O4&list=PLTPXxbhUt-YXh57uL3QUQiKLwTf39IoT1&index=17)\n[What\u2019s New in Unity Catalog - With Live Demos](https://youtu.be/vnznr18oGfI)\n[Technical Deep Dive for Practitioners: Databricks Unity Catalog from A-Z](https://www.youtube.com/watch?v=iMGG1Ng7sy4)\n[Introducing Unity Catalog Open Source: Unified Governance and Interoperability for Data + AI ](https://youtu.be/COnxxM0gJmw)\n[Attribute-Based Access Controls in Unity Catalog - Building a Scalable Access Management Framework](https://www.youtube.com/watch?v=KufwTp7MQD4)\n[Lakehouse Federation: Discover, Query and Govern Any Data with Unity Catalog](https://www.youtube.com/watch?v=xGImEGpw0uE)\n[Lakehouse Monitoring GA: Profiling, Diagnosing, and Enforcing Data Quality with Intelligence](https://www.youtube.com/watch?v=aDBPoKyA0DQ)\n[Upgrading to Unity Catalog With Ease, Using UCX](https://www.youtube.com/watch?v=pmW9jOFE0qI)\n### Blogs\n[What\u2019s New with Databricks Unity Catalog at Data + AI Summit 2024](https://www.databricks.com/blog/whats-new-databricks-unity-catalog-data-ai-summit-2024)\n[Open Sourcing Unity Catalog](https://www.databricks.com/blog/open-sourcing-unity-catalog)\n[What\u2019s New with Data Sharing and Collaboration](https://www.databricks.com/blog/whats-new-data-sharing-and-collaboration)\n[Unity Catalog Lakeguard: Industry-first and only data governance for multi-user Apache Spark\u2122 clusters](https://www.databricks.com/blog/unity-catalog-lakeguard-industry-first-and-only-data-governance-multi-user-apachetm-spark)\n[Introducing Lakehouse Federation Capabilities in Unity Catalog](https://www.databricks.com/blog/introducing-lakehouse-federation-capabilities-unity-catalog)\n[Built-In Governance for Your Databricks Workspace](https://www.databricks.com/blog/built-governance-your-databricks-workspace)\n### Getting Started\n[AWS Documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n[Azure Documentation](https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/)\n[GCP Documentation](https://docs.gcp.databricks.com/data-governance/unity-catalog/index.html)\n## Ready to get started?\n[Try Databricks for free](https://login.databricks.com/?dbx_source=www&intent=SIGN_UP)\n[![databricks logo](https://www.databricks.com/sites/default/files/2023-08/databricks-default.png?v=1712162038)](https://www.databricks.com/)\nWhy Databricks\nDiscover\n  * [For Executives](https://www.databricks.com/why-databricks/executives)\n  * [For Startups](https://www.databricks.com/product/startups)\n  * [Lakehouse Architecture](https://www.databricks.com/product/data-lakehouse)\n  * [Mosaic Research](https://www.databricks.com/research/mosaic)\n\n\nCustomers\n  * [Featured](https://www.databricks.com/customers)\n  * [See All](https://www.databricks.com/customers/all)\n\n\nPartners\n  * [Cloud Providers](https://www.databricks.com/company/partners/cloud-partners)\n  * [Technology Partners](https://www.databricks.com/company/partners/technology-partner-program)\n  * [Data Partners](https://www.databricks.com/company/partners/data-partner-program)\n  * [Built on Databricks](https://www.databricks.com/company/partners/built-on-partner-program)\n  * [Consulting & System Integrators](https://www.databricks.com/company/partners/consulting-and-si)\n  * [C&SI Partner Program](https://www.databricks.com/company/partners/consulting-and-si/candsi-partner-program)\n  * [Partner Solutions](https://www.databricks.com/company/partners/consulting-and-si/partner-solutions)\n\n\nWhy Databricks\nDiscover\n  * [For Executives](https://www.databricks.com/why-databricks/executives)\n  * [For Startups](https://www.databricks.com/product/startups)\n  * [Lakehouse Architecture](https://www.databricks.com/product/data-lakehouse)\n  * [Mosaic Research](https://www.databricks.com/research/mosaic)\n\n\nCustomers\n  * [Featured](https://www.databricks.com/customers)\n  * [See All](https://www.databricks.com/customers/all)\n\n\nPartners\n  * [Cloud Providers](https://www.databricks.com/company/partners/cloud-partners)\n  * [Technology Partners](https://www.databricks.com/company/partners/technology-partner-program)\n  * [Data Partners](https://www.databricks.com/company/partners/data-partner-program)\n  * [Built on Databricks](https://www.databricks.com/company/partners/built-on-partner-program)\n  * [Consulting & System Integrators](https://www.databricks.com/company/partners/consulting-and-si)\n  * [C&SI Partner Program](https://www.databricks.com/company/partners/consulting-and-si/candsi-partner-program)\n  * [Partner Solutions](https://www.databricks.com/company/partners/consulting-and-si/partner-solutions)\n\n\nProduct\nDatabricks Platform\n  * [Platform Overview](https://www.databricks.com/product/data-intelligence-platform)\n  * [Sharing](https://www.databricks.com/product/delta-sharing)\n  * [Governance](https://www.databricks.com/product/unity-catalog)\n  * [Artificial Intelligence](https://www.databricks.com/product/artificial-intelligence)\n  * [Business Intelligence](https://www.databricks.com/product/business-intelligence)\n  * [Data Management](https://www.databricks.com/product/delta-lake-on-databricks)\n  * [Data Warehousing](https://www.databricks.com/product/databricks-sql)\n  * [Real-Time Analytics](https://www.databricks.com/product/data-streaming)\n  * [Data Engineering](https://www.databricks.com/solutions/data-engineering)\n  * [Data Science](https://www.databricks.com/product/data-science)\n\n\nPricing\n  * [Pricing Overview](https://www.databricks.com/product/pricing)\n  * [Pricing Calculator](https://www.databricks.com/product/pricing/product-pricing/instance-types)\n\n\n[Open Source](https://www.databricks.com/product/open-source)\nIntegrations and Data\n  * [Marketplace](https://www.databricks.com/product/marketplace)\n  * [IDE Integrations](https://www.databricks.com/product/data-science/ide-integrations)\n  * [Partner Connect](https://www.databricks.com/partnerconnect)\n\n\nProduct\nDatabricks Platform\n  * [Platform Overview](https://www.databricks.com/product/data-intelligence-platform)\n  * [Sharing](https://www.databricks.com/product/delta-sharing)\n  * [Governance](https://www.databricks.com/product/unity-catalog)\n  * [Artificial Intelligence](https://www.databricks.com/product/artificial-intelligence)\n  * [Business Intelligence](https://www.databricks.com/product/business-intelligence)\n  * [Data Management](https://www.databricks.com/product/delta-lake-on-databricks)\n  * [Data Warehousing](https://www.databricks.com/product/databricks-sql)\n  * [Real-Time Analytics](https://www.databricks.com/product/data-streaming)\n  * [Data Engineering](https://www.databricks.com/solutions/data-engineering)\n  * [Data Science](https://www.databricks.com/product/data-science)\n\n\nPricing\n  * [Pricing Overview](https://www.databricks.com/product/pricing)\n  * [Pricing Calculator](https://www.databricks.com/product/pricing/product-pricing/instance-types)\n\n\nOpen Source\nIntegrations and Data\n  * [Marketplace](https://www.databricks.com/product/marketplace)\n  * [IDE Integrations](https://www.databricks.com/product/data-science/ide-integrations)\n  * [Partner Connect](https://www.databricks.com/partnerconnect)\n\n\nSolutions\nDatabricks For Industries\n  * [Communications](https://www.databricks.com/solutions/industries/communications)\n  * [Financial Services](https://www.databricks.com/solutions/industries/financial-services)\n  * [Healthcare and Life Sciences](https://www.databricks.com/solutions/industries/healthcare-and-life-sciences)\n  * [Manufacturing](https://www.databricks.com/solutions/industries/manufacturing-industry-solutions)\n  * [Media and Entertainment](https://www.databricks.com/solutions/industries/media-and-entertainment)\n  * [Public Sector](https://www.databricks.com/solutions/industries/federal-government)\n  * [Retail](https://www.databricks.com/solutions/industries/retail-industry-solutions)\n  * [View All](https://www.databricks.com/solutions)\n\n\nCross Industry Solutions\n  * [Customer Data Platform](https://www.databricks.com/solutions/industries/customer-experience)\n  * [Cyber Security](https://www.databricks.com/solutions/industries/cybersecurity)\n\n\n[Data Migration](https://www.databricks.com/solutions/migration)\n[Professional Services](https://www.databricks.com/professional-services)\n[Solution Accelerators](https://www.databricks.com/solutions/accelerators)\nSolutions\nDatabricks For Industries\n  * [Communications](https://www.databricks.com/solutions/industries/communications)\n  * [Financial Services](https://www.databricks.com/solutions/industries/financial-services)\n  * [Healthcare and Life Sciences](https://www.databricks.com/solutions/industries/healthcare-and-life-sciences)\n  * [Manufacturing](https://www.databricks.com/solutions/industries/manufacturing-industry-solutions)\n  * [Media and Entertainment](https://www.databricks.com/solutions/industries/media-and-entertainment)\n  * [Public Sector](https://www.databricks.com/solutions/industries/federal-government)\n  * [Retail](https://www.databricks.com/solutions/industries/retail-industry-solutions)\n  * [View All](https://www.databricks.com/solutions)\n\n\nCross Industry Solutions\n  * [Customer Data Platform](https://www.databricks.com/solutions/industries/customer-experience)\n  * [Cyber Security](https://www.databricks.com/solutions/industries/cybersecurity)\n\n\nData Migration\nProfessional Services\nSolution Accelerators\nResources\n[Documentation](https://docs.databricks.com/en/index.html)\n[Customer Support](https://www.databricks.com/support)\n[Community](https://community.databricks.com/)\nTraining and Certification\n  * [Learning Overview](https://www.databricks.com/learn)\n  * [Training Overview](https://www.databricks.com/learn/training/home)\n  * [Certification](https://www.databricks.com/learn/training/certification)\n  * [University Alliance](https://www.databricks.com/university)\n  * [Databricks Academy Login](https://www.databricks.com/learn/training/login)\n\n\nEvents\n  * [Data + AI Summit](https://www.databricks.com/dataaisummit)\n  * [Data + AI World Tour](https://www.databricks.com/dataaisummit/worldtour)\n  * [Data Intelligence Days](https://www.databricks.com/lp/data-intelligence-days)\n  * [Full Calendar](https://www.databricks.com/events)\n\n\nBlog and Podcasts\n  * [Databricks Blog](https://www.databricks.com/blog)\n  * [Databricks Mosaic Research Blog](https://www.databricks.com/blog/category/generative-ai/mosaic-research)\n  * [Data Brew Podcast](https://www.databricks.com/discover/data-brew)\n  * [Champions of Data & AI Podcast](https://www.databricks.com/discover/champions-of-data-and-ai)\n\n\nResources\nDocumentation\nCustomer Support\nCommunity\nTraining and Certification\n  * [Learning Overview](https://www.databricks.com/learn)\n  * [Training Overview](https://www.databricks.com/learn/training/home)\n  * [Certification](https://www.databricks.com/learn/training/certification)\n  * [University Alliance](https://www.databricks.com/university)\n  * [Databricks Academy Login](https://www.databricks.com/learn/training/login)\n\n\nEvents\n  * [Data + AI Summit](https://www.databricks.com/dataaisummit)\n  * [Data + AI World Tour](https://www.databricks.com/dataaisummit/worldtour)\n  * [Data Intelligence Days](https://www.databricks.com/lp/data-intelligence-days)\n  * [Full Calendar](https://www.databricks.com/events)\n\n\nBlog and Podcasts\n  * [Databricks Blog](https://www.databricks.com/blog)\n  * [Databricks Mosaic Research Blog](https://www.databricks.com/blog/category/generative-ai/mosaic-research)\n  * [Data Brew Podcast](https://www.databricks.com/discover/data-brew)\n  * [Champions of Data & AI Podcast](https://www.databricks.com/discover/champions-of-data-and-ai)\n\n\nAbout\nCompany\n  * [Who We Are](https://www.databricks.com/company/about-us)\n  * [Our Team](https://www.databricks.com/company/leadership-team)\n  * [Databricks Ventures](https://www.databricks.com/databricks-ventures)\n  * [Contact Us](https://www.databricks.com/company/contact)\n\n\nCareers\n  * [Open Jobs](https://www.databricks.com/company/careers/open-positions)\n  * [Working at Databricks](https://www.databricks.com/company/careers)\n\n\nPress\n  * [Awards and Recognition](https://www.databricks.com/company/awards-and-recognition)\n  * [Newsroom](https://www.databricks.com/company/newsroom)\n\n\n[Security and Trust](https://www.databricks.com/trust)\nAbout\nCompany\n  * [Who We Are](https://www.databricks.com/company/about-us)\n  * [Our Team](https://www.databricks.com/company/leadership-team)\n  * [Databricks Ventures](https://www.databricks.com/databricks-ventures)\n  * [Contact Us](https://www.databricks.com/company/contact)\n\n\nCareers\n  * [Open Jobs](https://www.databricks.com/company/careers/open-positions)\n  * [Working at Databricks](https://www.databricks.com/company/careers)\n\n\nPress\n  * [Awards and Recognition](https://www.databricks.com/company/awards-and-recognition)\n  * [Newsroom](https://www.databricks.com/company/newsroom)\n\n\nSecurity and Trust\n[![databricks logo](https://www.databricks.com/sites/default/files/2023-08/databricks-default.png?v=1712162038)](https://www.databricks.com/)\nDatabricks Inc. 160 Spear Street, 15th Floor San Francisco, CA 94105 1-866-330-0121\n  * [](https://www.linkedin.com/company/databricks)\n  * [](https://www.facebook.com/pages/Databricks/560203607379694)\n  * [](https://twitter.com/databricks)\n  * [](https://www.databricks.com/feed)\n  * [](https://www.glassdoor.com/Overview/Working-at-Databricks-EI_IE954734.11,21.htm)\n  * [](https://www.youtube.com/@Databricks)\n\n\n![](https://www.databricks.com/sites/default/files/2021/02/telco-icon-2.png?v=1715274112)\n[See Careers at Databricks](https://www.databricks.com/company/careers)\n  * [](https://www.linkedin.com/company/databricks)\n  * [](https://www.facebook.com/pages/Databricks/560203607379694)\n  * [](https://twitter.com/databricks)\n  * [](https://www.databricks.com/feed)\n  * [](https://www.glassdoor.com/Overview/Working-at-Databricks-EI_IE954734.11,21.htm)\n  * [](https://www.youtube.com/@Databricks)\n\n\n\u00a9 Databricks 2025. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the [Apache Software Foundation.](https://www.apache.org/)\n  * [Privacy Notice](https://www.databricks.com/legal/privacynotice)\n  * |[Terms of Use](https://www.databricks.com/terms-of-use)\n  * |[Modern Slavery Statement](https://www.databricks.com/legal/modern-slavery-policy-statement)\n  * |[California Privacy](https://www.databricks.com/legal/supplemental-privacy-notice-california-residents)\n  * |[Your Privacy Choices](https://www.databricks.com/product/unity-catalog/#yourprivacychoices)\n  * ![](https://www.databricks.com/sites/default/files/2022-12/gpcicon_small.png)\n\n\n",
    "answer": "```markdown\n# TL;DR Summary\n\n**Unity Catalog** is Databricks' unified governance solution for data and AI, enabling seamless management of structured and unstructured data across platforms. It enhances collaboration, simplifies compliance, and provides fine-grained access controls, boosting productivity and security for organizations like PepsiCo and Akamai.\n```"
  },
  {
    "instruction": "[](https://www.pinecone.io/)\nProduct\n[Docs](https://docs.pinecone.io)[Customers](https://www.pinecone.io/customers/)\nResources\n[Pricing](https://www.pinecone.io/pricing/)\n[Contact](https://www.pinecone.io/contact/)[Log in](https://app.pinecone.io/?sessionType=login)[Sign up](https://app.pinecone.io/?sessionType=signup)\n[\u2190 Learn](https://www.pinecone.io/learn/)\n# What is a Vector Database & How Does it Work? Use Cases + Examples\n![Roie Schwaber-Cohen](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&w=3840&q=75)\nRoie Schwaber-Cohen\nMay 3, 2023[Core Components](https://www.pinecone.io/learn/category/core-components/)\nShare: \n[](https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/vector-database)[](https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/vector-database)[](https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/vector-database)\nJump to section:\n  * [What is a Vector Database?](https://www.pinecone.io/learn/vector-database/#What-is-a-Vector-Database)\n  * [Serverless Vector Databases](https://www.pinecone.io/learn/vector-database/#Serverless-Vector-Databases)\n  * [Summary](https://www.pinecone.io/learn/vector-database/#Summary)\n\n\n## What is a Vector Database?\nA vector database indexes and stores vector embeddings for fast retrieval and similarity search, with capabilities like CRUD operations, metadata filtering, horizontal scaling, and serverless.\nWe\u2019re in the midst of the AI revolution. It\u2019s upending any industry it touches, promising great innovations - but it also introduces new challenges. Efficient data processing has become more crucial than ever for applications that involve large language models, generative AI, and semantic search.\nAll of these new applications rely on [vector embeddings](https://www.pinecone.io/learn/vector-embeddings-for-developers/), a type of vector data representation that carries within it semantic information that\u2019s critical for the AI to gain understanding and maintain a long-term memory they can draw upon when executing complex tasks.\nEmbeddings are generated by AI models (such as Large Language Models) and have many attributes or features, making their representation challenging to manage. In the context of AI and machine learning, these features represent different dimensions of the data that are essential for understanding patterns, relationships, and underlying structures.\nThat is why we need a specialized database designed specifically for handling this data type. Vector databases like [Pinecone](https://www.pinecone.io/) fulfill this requirement by offering optimized storage and querying capabilities for embeddings. Vector databases have the capabilities of a traditional database that are absent in standalone vector indexes and the specialization of dealing with vector embeddings, which traditional scalar-based databases lack.\nThe challenge of working with vector data is that traditional scalar-based databases can\u2019t keep up with the complexity and scale of such data, making it difficult to extract insights and perform real-time analysis. That\u2019s where vector databases come into play \u2013 they are intentionally designed to handle this type of data and offer the performance, scalability, and flexibility you need to make the most out of your data.\nWe are seeing the next generation of vector databases introduce more sophisticated architectures to handle the efficient cost and scaling of intelligence. This ability is handled by serverless vector databases, that can separate the cost of storage and compute to enable low-cost knowledge support for AI.\nWith a vector database, we can add knowledge to our AIs, like semantic information retrieval, long-term memory, and more. The diagram below gives us a better understanding of the role of vector databases in this type of application:\n![Vector Database](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fe88ebbacb848b09e477d11eedf4209d10ea4ac0a-1399x537.png&w=3840&q=75)\nLet\u2019s break this down:\n  1. First, we use the [embedding model](https://www.pinecone.io/learn/series/rag/embedding-models-rundown/) to create vector embeddings for the content we want to index.\n  2. The vector embedding is inserted into the vector database, with some reference to the original content the embedding was created from.\n  3. When the application issues a query, we use the same embedding model to create embeddings for the query and use those embeddings to query the database for _similar_ vector embeddings. As mentioned before, those similar embeddings are associated with the original content that was used to create them.\n\n\n### What\u2019s the difference between a vector index and a vector database?\nStandalone vector indices like [FAISS](https://www.pinecone.io/learn/faiss/) (Facebook AI Similarity Search) can significantly improve the search and retrieval of vector embeddings, but they lack capabilities that exist in any database. Vector databases, on the other hand, are purpose-built to _manage_ vector embeddings, providing several advantages over using standalone vector indices:\n  1. Data management: Vector databases offer well-known and easy-to-use features for data storage, like inserting, deleting, and updating data. This makes managing and maintaining vector data easier than using a standalone vector _index_ like FAISS, which requires additional work to integrate with a storage solution.\n  2. Metadata storage and filtering: Vector databases can store metadata associated with each vector entry. Users can then query the database using additional metadata filters for finer-grained queries.\n  3. Scalability: Vector databases are designed to scale with growing data volumes and user demands, providing better support for distributed and parallel processing. Standalone vector indices may require custom solutions to achieve similar levels of scalability (such as deploying and managing them on Kubernetes clusters or other similar systems). Modern vector databases also use serverless architectures to optimize cost at scale.\n  4. Real-time updates: Vector databases often support real-time data updates, allowing for dynamic changes to the data to keep results fresh, whereas standalone vector indexes may require a full re-indexing process to incorporate new data, which can be time-consuming and computationally expensive. Advanced vector databases can use performance upgrades available via index rebuilds while maintaining freshness.\n  5. Backups and collections: Vector databases handle the routine operation of backing up all the data stored in the database. Pinecone also allows users to selectively choose specific indexes that can be backed up in the form of \u201ccollections,\u201d which store the data in that index for later use.\n  6. Ecosystem integration: Vector databases can more easily integrate with other components of a data processing ecosystem, such as ETL pipelines (like Spark), analytics tools (like [Tableau](https://www.tableau.com/) and [Segment](https://segment.com/)), and visualization platforms (like [Grafana](https://grafana.com/)) \u2013 streamlining the data management workflow. It also enables easy integration with other AI related tooling like [LangChain](https://python.langchain.com/en/latest/index.html), [LlamaIndex](https://gpt-index.readthedocs.io/), [Cohere](https://www.pinecone.io/learn/series/rag/rerankers/), and many others..\n  7. Data security and access control: Vector databases typically offer built-in data security features and access control mechanisms to protect sensitive information, which may not be available in standalone vector index solutions. Multitenancy through namespaces allows users to partition their indexes fully and even create fully isolated partitions within their own index.\n\n\nIn short, a vector database provides a superior solution for handling vector embeddings by addressing the limitations of standalone vector indices, such as scalability challenges, cumbersome integration processes, and the absence of real-time updates and built-in security measures, ensuring a more effective and streamlined data management experience.\nStart using Pinecone for free\nPinecone is the developer-favorite [vector database](https://www.pinecone.io/learn/vector-database/) that's fast and easy to use at any scale.\n[Sign up free](https://app.pinecone.io)[View Examples](https://docs.pinecone.io/page/examples)\n### How does a vector database work?\nWe all know how traditional databases work (more or less)\u2014they store strings, numbers, and other types of scalar data in rows and columns. On the other hand, a vector database operates on vectors, so the way it\u2019s optimized and queried is quite different.\nIn traditional databases, we are usually querying for rows in the database where the value usually exactly matches our query. In vector databases, we apply a similarity metric to find a vector that is the most similar to our query.\nA vector database uses a combination of different algorithms that all participate in Approximate Nearest Neighbor (ANN) search. These algorithms optimize the search through hashing, quantization, or graph-based search.\nThese algorithms are assembled into a pipeline that provides fast and accurate retrieval of the neighbors of a queried vector. Since the vector database provides approximate results, the main trade-offs we consider are between accuracy and speed. The more accurate the result, the slower the query will be. However, a good system can provide ultra-fast search with near-perfect accuracy.\nHere\u2019s a common pipeline for a vector database:\n![Vector Database pipeline](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fff9ba425d0c78d696372e0a43ce57851b4f1d4b7-1307x233.png&w=3840&q=75)\n  1. **Indexing** : The vector database indexes vectors using an algorithm such as PQ, LSH, or HNSW (more on these below). This step maps the vectors to a data structure that will enable faster searching.\n  2. **Querying** : The vector database compares the indexed query vector to the indexed vectors in the dataset to find the nearest neighbors (applying a similarity metric used by that index)\n  3. **Post Processing** : In some cases, the vector database retrieves the final nearest neighbors from the dataset and post-processes them to return the final results. This step can include re-ranking the nearest neighbors using a different similarity measure.\n\n\n## Serverless Vector Databases\nServerless represents the next evolution of vector databases. The above architectures get us to a vector database architecture that is accurate, fast, scalable, _but expensive_. This architecture is what we see in first-generation vector DBs. With the rise of AI use cases where cost and elasticity are increasingly important, a second generation of serverless vector databases is needed.\nFirst-generation vector DBs have three critical pain points that a serverless vector database solves:\n  * **Separation of storage from compute** : To optimize costs, compute should only be used when needed. That means decoupling the index storage from queries and searching _only_ what is needed \u2014 this becomes increasingly difficult when involving latency.\n  * **Multitenancy** : Handling namespaces in indexes to ensure infrequently queried namespaces do not increase costs.\n  * **Freshness** : A vector DB needs to provide fresh data, meaning within a few seconds of inserting new data, it is queryable. _Note, for Pinecone Serverless freshness can be delayed when inserting large amounts of data._\n\n\nTo separate storage from compute highly sophisticated geometric partitioning algorithms can break an index into sub-indices, allowing us to focus the search on specific partitions:\n![Chart demonstrating Voronoi tessalations](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9323f434cbef5cc73e859888f6ad864b342ff654-1200x880.png&w=3840&q=75)\nPartitioning of the search space\nWith these partitions, the search space of a query can focus on just a few parts of a vector index rather than the full search space. Typical search behaviors will show that certain partitions are accessed more frequently than others, allowing us to dial between compute costs and cold startup times to find an optimal balance between cost and latency.\nWhen we do this partitioning, we solve the **separation of compute and storage** problem. However, geometric partitioning is a slower process at index build time. Meaning we can run into _freshness_ problems as we must wait for new data to be correctly stored in the index.\nTo solve this problem, a vector database needs another separate layer called a _freshness layer_. The freshness layer acts as a temporary \u201ccache\u201d of vectors that can be queried. At the same time, we wait for an index builder to place new vectors into the geometrically partitioned index.\n![Freshness layer diagram demonstrating the query being sent to both the freshness layer and the existing, parititioned index](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F599081cb02fc876b4f0b52ba56520ca70f86531b-2164x1290.png&w=3840&q=75)\nThe freshness layer keeps our data up to date so we can begin querying quickly.\nDuring this process, a query router can send queries to both the index _and_ the freshness layer \u2014 [solving the freshness problem](https://www.pinecone.io/blog/serverless-architecture/). However, it\u2019s worth noting that the freshness layer exists in compute instances, so we cannot store the full index there. Instead, we wait for the new vectors to be inserted into the index \u2014 once complete, they are removed from the freshness layer.\nFinally, there is the problem of **multi tenancy**. Many first-generation vector DBs handle multi tenancy and have done so for a long time. However, multitenancy in a serverless architecture is more complex.\nWe must avoid colocating different types of users on the same hardware to keep costs and latencies low. If we have user A, who makes 20 queries a second almost every day on the same hardware as user B who makes 20 queries _a month_ , user B will be stuck on compute hardware 24/7 as that is required for the constantly low latencies that user A needs.\nTo cover this problem, a vector database must be able to identify users with similar usage and colocate them while keeping full separation between them. Again, this can be done based on user usage metrics and automatic allocation of hot/cold infrastructure based on usage.\nTaking a first-generation vector database and adding separation of storage from computing, multitenancy, and freshness gives us a new generation of modern vector databases. This architecture (paired with vector DB fundamentals) is preferred for the modern AI stack.\nIn the following sections, we will discuss a few of the algorithms behind the fundamentals of vector DBs and explain how they contribute to the overall performance of our database.\n### Algorithms\nSeveral algorithms can facilitate the creation of a vector index. Their common goal is to enable fast querying by creating a data structure that can be traversed quickly. They will commonly transform the representation of the original vector into a compressed form to optimize the query process.\nHowever, as a user of Pinecone, you don\u2019t need to worry about the intricacies and selection of these various algorithms. Pinecone is designed to handle all the complexities and algorithmic decisions behind the scenes, ensuring you get the best performance and results without any hassle. By leveraging Pinecone\u2019s expertise, you can focus on what truly matters \u2013 extracting valuable insights and delivering powerful AI solutions.\nThe following sections will explore several algorithms and their unique approaches to handling vector embeddings. This knowledge will empower you to make informed decisions and appreciate the seamless performance Pinecone delivers as you unlock the full potential of your application.\n#### Random Projection\nThe basic idea behind random projection is to project the high-dimensional vectors to a lower-dimensional space using a **random projection matrix**. We create a matrix of random numbers. The size of the matrix is going to be the target low-dimension value we want. We then calculate the dot product of the input vectors and the matrix, which results in a **projected matrix** that has fewer dimensions than our original vectors but still preserves their similarity.\n![Random Projection](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F22dffa542410c1b542fcaeafcafc088ad278add5-1303x534.png&w=3840&q=75)\nWhen we query, we use the same projection matrix to project the query vector onto the lower-dimensional space. Then, we compare the projected query vector to the projected vectors in the database to find the nearest neighbors. Since the dimensionality of the data is reduced, the search process is significantly faster than searching the entire high-dimensional space.\nJust keep in mind that random projection is an approximate method, and the projection quality depends on the properties of the projection matrix. In general, the more random the projection matrix is, the better the quality of the projection will be. But generating a truly random projection matrix can be computationally expensive, especially for large datasets. [Learn more about random projection.](https://www.pinecone.io/learn/locality-sensitive-hashing-random-projection/)\n#### Product Quantization\nAnother way to build an index is product quantization (PQ), which is a _lossy_ compression technique for high-dimensional vectors (like vector embeddings). It takes the original vector, breaks it up into smaller chunks, simplifies the representation of each chunk by creating a representative \u201ccode\u201d for each chunk, and then puts all the chunks back together - without losing information that is vital for similarity operations. The process of PQ can be broken down into four steps: splitting, training, encoding, and querying.\n![Product Quantization](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F791910350d7d2140dbe684b405ef5ee761c8fc6a-1060x720.png&w=3840&q=75)\n  1. **Splitting** -The vectors are broken into segments.\n  2. **Training** - we build a \u201ccodebook\u201d for each segment. Simply put - the algorithm generates a pool of potential \u201ccodes\u201d that could be assigned to a vector. In practice - this \u201ccodebook\u201d is made up of the center points of clusters created by performing k-means clustering on each of the vector\u2019s segments. We would have the same number of values in the segment codebook as the value we use for the k-means clustering.\n  3. **Encoding** - The algorithm assigns a specific code to each segment. In practice, we find the nearest value in the codebook to each vector segment after the training is complete. Our PQ code for the segment will be the identifier for the corresponding value in the codebook. We could use as many PQ codes as we\u2019d like, meaning we can pick multiple values from the codebook to represent each segment.\n  4. **Querying** - When we query, the algorithm breaks down the vectors into sub-vectors and quantizes them using the same codebook. Then, it uses the indexed codes to find the nearest vectors to the query vector.\n\n\nThe number of representative vectors in the codebook is a trade-off between the accuracy of the representation and the computational cost of searching the codebook. The more representative vectors in the codebook, the more accurate the representation of the vectors in the subspace, but the higher the computational cost to search the codebook. By contrast, the fewer representative vectors in the codebook, the less accurate the representation, but the lower the computational cost. [Learn more about PQ](https://www.pinecone.io/learn/product-quantization/).\n#### Locality-sensitive hashing\nLocality-Sensitive Hashing (LSH) is a technique for indexing in the context of an approximate nearest-neighbor search. It is optimized for speed while still delivering an approximate, non-exhaustive result. LSH maps similar vectors into \u201cbuckets\u201d using a set of hashing functions, as seen below:\n![Locality-sensitive hashing](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F606382d0ca90a8d24f26780f5f9954123e37be91-575x603.png&w=1200&q=75)\nTo find the nearest neighbors for a given query vector, we use the same hashing functions used to \u201cbucket\u201d similar vectors into hash tables. The query vector is hashed to a particular table and then compared with the other vectors in that same table to find the closest matches. This method is much faster than searching through the entire dataset because there are far fewer vectors in each hash table than in the whole space.\nIt\u2019s important to remember that LSH is an approximate method, and the quality of the approximation depends on the properties of the hash functions. In general, the more hash functions used, the better the approximation quality will be. However, using a large number of hash functions can be computationally expensive and may not be feasible for large datasets. [Learn more about LSH](https://www.pinecone.io/learn/locality-sensitive-hashing/).\nStart using Pinecone\nPinecone is the developer-favorite [vector database](https://www.pinecone.io/learn/vector-database/) that's fast and easy to use at any scale.\nSubscribe\n#### Hierarchical Navigable Small World (HNSW)\nHNSW creates a hierarchical, tree-like structure where each node of the tree represents a set of vectors. The edges between the nodes represent the **similarity** between the vectors. The algorithm starts by creating a set of nodes, each with a small number of vectors. This could be done randomly or by clustering the vectors with algorithms like k-means, where each cluster becomes a node.\n![Hierarchical Navigable Small World \\(HNSW\\)](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fe06c4bb4c0095a1fb5b45b04acaa15663b6d6a96-734x696.png&w=1920&q=75)\nThe algorithm then examines the vectors of each node and draws an edge between that node and the nodes that have the most similar vectors to the one it has.\n![Hierarchical Navigable Small World \\(HNSW\\)](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F63110626f18b50b5a15c48a8eea3c852ff64196b-496x467.png&w=1080&q=75)\nWhen we query an HNSW index, it uses this graph to navigate through the tree, visiting the nodes that are most likely to contain the closest vectors to the query vector. [Learn more about HNSW](https://www.pinecone.io/learn/hnsw/).\n### Similarity Measures\nBuilding on the previously discussed algorithms, we need to understand the role of similarity measures in vector databases. These measures are the foundation of how a vector database compares and identifies the most relevant results for a given query.\nSimilarity measures are mathematical methods for determining how similar two vectors are in a vector space. Similarity measures are used in vector databases to compare the vectors stored in the database and find the ones that are most similar to a given query vector.\nSeveral similarity measures can be used, including:\n  * **Cosine similarity:** measures the cosine of the angle between two vectors in a vector space. It ranges from -1 to 1, where 1 represents identical vectors, 0 represents orthogonal vectors, and -1 represents vectors that are diametrically opposed.\n  * **Euclidean distance:** measures the straight-line distance between two vectors in a vector space. It ranges from 0 to infinity, where 0 represents identical vectors, and larger values represent increasingly dissimilar vectors.\n  * **Dot product:** measures the product of the magnitudes of two vectors and the cosine of the angle between them. It ranges from -\u221e to \u221e, where a positive value represents vectors that point in the same direction, 0 represents orthogonal vectors, and a negative value represents vectors that point in opposite directions.\n\n\nThe choice of similarity measure will have an effect on the results obtained from a vector database. It is also important to note that each similarity measure has its own advantages and disadvantages, and it is important to choose the right one depending on the use case and requirements. [Learn more about similarity measures](https://www.pinecone.io/learn/vector-similarity/).\n### Filtering\nEvery vector stored in the database also includes metadata. In addition to the ability to query for similar vectors, vector databases can also filter the results based on a metadata query. To do this, the vector database usually maintains two indexes: a vector index and a metadata index. It then performs the metadata filtering either before or after the vector search itself, but in either case, there are difficulties that cause the query process to slow down.\n![Post-filtering and Pre-filtering](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Ffede570123cbdabff288da8900330d472834ceb8-1920x1220.png&w=3840&q=75)\nThe filtering process can be performed either before or after the vector search itself, but each approach has its own challenges that may impact the query performance:\n  * **Pre-filtering:** In this approach, metadata filtering is done before the vector search. While this can help reduce the search space, it may also cause the system to overlook relevant results that don\u2019t match the metadata filter criteria. Additionally, extensive metadata filtering may slow down the query process due to the added computational overhead.\n  * **Post-filtering:** In this approach, the metadata filtering is done after the vector search. This can help ensure that all relevant results are considered, but it may also introduce additional overhead and slow down the query process as irrelevant results need to be filtered out after the search is complete.\n\n\nTo optimize the filtering process, vector databases use various techniques, such as leveraging advanced indexing methods for metadata or using parallel processing to speed up the filtering tasks. Balancing the trade-offs between search performance and filtering accuracy is essential for providing efficient and relevant query results in vector databases. [Learn more about vector search filtering](https://www.pinecone.io/learn/vector-search-filtering/).\n### Database Operations\nUnlike vector indexes, vector databases are equipped with a set of capabilities that makes them better qualified to be used in high scale production settings. Let\u2019s take a look at an overall overview of the components that are involved in operating the database.\n![Database Operations](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fd0adc44e56739a985cc198ea327fbb3d701f21e5-1094x984.png&w=3840&q=75)\n#### Performance and Fault tolerance\nPerformance and fault tolerance are tightly related. The more data we have, the more nodes that are required - and the bigger chance for errors and failures. As is the case with other types of databases, we want to ensure that queries are executed as quickly as possible even if some of the underlying nodes fail. This could be due to hardware failures, network failures, or other types of technical bugs. This kind of failure could result in downtime or even incorrect query results.\nTo ensure both high performance and fault tolerance, vector databases use sharding and replication apply the following:\n  1. **Sharding** - partitioning the data across multiple nodes. There are different methods for partitioning the data - for example, it can be partitioned by the similarity of different clusters of data so that similar vectors are stored in the same partition. When a query is made, it is sent to all the shards and the results are retrieved and combined. This is called the \u201cscatter-gather\u201d pattern.\n  2. **Replication** - creating multiple copies of the data across different nodes. This ensures that even if a particular node fails, other nodes will be able to replace it. There are two main consistency models: _eventual_ consistency and _strong_ consistency. Eventual consistency allows for temporary inconsistencies between different copies of the data which will improve availability and reduce latency but may result in conflicts and even data loss. On the other hand, strong consistency requires that all copies of the data are updated before a write operation is considered complete. This approach provides stronger consistency but may result in higher latency.\n\n\n#### Monitoring\nTo effectively manage and maintain a vector database, we need a robust monitoring system that tracks the important aspects of the database\u2019s performance, health, and overall status. Monitoring is critical for detecting potential problems, optimizing performance, and ensuring smooth production operations. Some aspects of monitoring a vector database include the following:\n  1. **Resource usage** - monitoring resource usage, such as CPU, memory, disk space, and network activity, enables the identification of potential issues or resource constraints that could affect the performance of the database.\n  2. **Query performance** - query latency, throughput, and error rates may indicate potential systemic issues that need to be addressed.\n  3. **System health** - overall system health monitoring includes the status of individual nodes, the replication process, and other critical components.\n\n\n#### Access-control\nAccess control is the process of managing and regulating user access to data and resources. It is a vital component of data security, ensuring that only authorized users have the ability to view, modify, or interact with sensitive data stored within the vector database.\nAccess control is important for several reasons:\n  1. **Data protection:** As AI applications often deal with sensitive and confidential information, implementing strict access control mechanisms helps safeguard data from unauthorized access and potential breaches.\n  2. **Compliance:** Many industries, such as healthcare and finance, are subject to strict data privacy regulations. Implementing proper access control helps organizations comply with these regulations, protecting them from legal and financial repercussions.\n  3. **Accountability and auditing:** Access control mechanisms enable organizations to maintain a record of user activities within the vector database. This information is crucial for auditing purposes, and when security breaches happen, it helps trace back any unauthorized access or modifications.\n  4. **Scalability and flexibility:** As organizations grow and evolve, their access control needs may change. A robust access control system allows for seamless modification and expansion of user permissions, ensuring that data security remains intact throughout the organization\u2019s growth.\n\n\n#### Backups and collections\nWhen all else fails, vector databases offer the ability to rely on regularly created backups. These backups can be stored on external storage systems or cloud-based storage services, ensuring the safety and recoverability of the data. In case of data loss or corruption, these backups can be used to restore the database to a previous state, minimizing downtime and impact on the overall system. With Pinecone, users can choose to back up specific indexes as well and save them as \u201ccollections,\u201d which can later be used to populate new indexes.\n#### API and SDKs\nThis is where the rubber meets the road: Developers who interact with the database want to do so with an easy-to-use API, using a toolset that is familiar and comfortable. By providing a user-friendly interface, the vector database API layer simplifies the development of high-performance vector search applications.\nIn addition to the API, vector databases would often provide programming language specific SDKs that wrap the API. The SDKs make it even easier for developers to interact with the database in their applications. This allows developers to concentrate on their specific use cases, such as semantic text search, generative question-answering, hybrid search, image similarity search, or product recommendations, without having to worry about the underlying infrastructure complexities.\n## Summary\nThe exponential growth of vector embeddings in fields such as NLP, computer vision, and other AI applications has resulted in the emergence of vector databases as the computation engine that allows us to interact effectively with vector embeddings in our applications.\nVector databases are purpose-built databases that are specialized to tackle the problems that arise when managing vector embeddings in production scenarios. For that reason, they offer significant advantages over traditional scalar-based databases and standalone vector indexes.\nIn this post, we reviewed the key aspects of a vector database, including how it works, what algorithms it uses, and the additional features that make it operationally ready for production scenarios. We hope this helps you understand the inner workings of vector databases. Luckily, this isn\u2019t something you must know to use Pinecone. Pinecone takes care of all of these considerations (and then some) and frees you to focus on the rest of your application.\nShare: \n[](https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/vector-database)[](https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/vector-database)[](https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/vector-database)\nRecommended for you\nFurther Reading\n[Mar 5, 2025Don\u2019t be dense: Launching sparse indexes in Pinecone9 min read](https://www.pinecone.io/learn/sparse-retrieval/)[Mar 5, 2025Unlock High-Precision Keyword Search with pinecone-sparse-english-v018 min read](https://www.pinecone.io/learn/learn-pinecone-sparse/)[Feb 24, 2025Pinpoint references faster with citation highlights in Pinecone Assistant5 min read](https://www.pinecone.io/learn/pinecone-assistant-citation-highlights/)\n[](https://x.com/pinecone)[](https://www.linkedin.com/company/pinecone-io)[](https://www.youtube.com/@pinecone-io)[](https://github.com/pinecone-io)\nProduct\n[Vector Database](https://www.pinecone.io/product/)[Assistant](https://www.pinecone.io/product/assistant/)[Documentation](https://docs.pinecone.io/)[Pricing](https://www.pinecone.io/pricing/)[Security](https://www.pinecone.io/security/)[Integrations](https://www.pinecone.io/integrations/)\nResources\n[Community Forum](https://community.pinecone.io/)[Learning Center](https://www.pinecone.io/learn/)[Blog](https://www.pinecone.io/blog/)[Customer Case Studies](https://www.pinecone.io/customers/)[Status](https://status.pinecone.io/)[What is a Vector DB?](https://www.pinecone.io/learn/vector-database/)[What is RAG?](https://www.pinecone.io/learn/retrieval-augmented-generation/)\nCompany\n[About](https://www.pinecone.io/company/)[Partners](https://www.pinecone.io/partners/)[Careers](https://www.pinecone.io/careers/)[Newsroom](https://www.pinecone.io/newsroom/)[Contact](https://www.pinecone.io/contact/)\nLegal\n[Customer Terms](https://www.pinecone.io/legal/)[Website Terms](https://www.pinecone.io/terms/)[Privacy](https://www.pinecone.io/privacy/)[Cookies](https://www.pinecone.io/cookies/)[Cookie Preferences](https://www.pinecone.io/learn/vector-database/)\n\u00a9 Pinecone Systems, Inc. | San Francisco, CA\nPinecone is a registered trademark of Pinecone Systems, Inc.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nVector databases, like Pinecone, are specialized for managing vector embeddings, crucial for AI applications. They offer advantages over traditional databases, including scalability, real-time updates, and metadata filtering. Serverless architectures enhance cost efficiency and performance.\n```"
  },
  {
    "instruction": "[![logo](https://qdrant.tech/img/qdrant-logo.svg)](https://qdrant.tech/)\n  * [Qdrant](https://qdrant.tech/documentation/)\n  * [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n  * [Build](https://qdrant.tech/documentation/build/)\n  * [Learn](https://qdrant.tech/articles/)\n  * [API Reference](https://api.qdrant.tech/api-reference)\n\n\nSearch\n[Log in](https://cloud.qdrant.io/login) [Start Free](https://cloud.qdrant.io/signup)\n![logo](https://qdrant.tech/img/qdrant-logo.svg)\nSearch\n  * [ Qdrant](https://qdrant.tech/documentation/)\n  * [ Cloud](https://qdrant.tech/documentation/cloud-intro/)\n  * [ Build](https://qdrant.tech/documentation/build/)\n  * [ Learn](https://qdrant.tech/articles/)\n  * [ API Reference](https://api.qdrant.tech/api-reference)\n\n\n### Learn\n[Vector Search Manuals](https://qdrant.tech/articles/vector-search-manuals/)\n[Qdrant Internals](https://qdrant.tech/articles/qdrant-internals/)\n[Data Exploration](https://qdrant.tech/articles/data-exploration/)\n[Machine Learning](https://qdrant.tech/articles/machine-learning/)\n[RAG & GenAI](https://qdrant.tech/articles/rag-and-genai/)\n[Practical Examples](https://qdrant.tech/articles/practicle-examples/)\n[Ecosystem](https://qdrant.tech/articles/ecosystem/)\n### Learn\n[Vector Search Manuals](https://qdrant.tech/articles/vector-search-manuals/)\n[Qdrant Internals](https://qdrant.tech/articles/qdrant-internals/)\n[Data Exploration](https://qdrant.tech/articles/data-exploration/)\n[Machine Learning](https://qdrant.tech/articles/machine-learning/)\n[RAG & GenAI](https://qdrant.tech/articles/rag-and-genai/)\n[Practical Examples](https://qdrant.tech/articles/practicle-examples/)\n[Ecosystem](https://qdrant.tech/articles/ecosystem/)\n  * [Articles](https://qdrant.tech/articles/)\n  * What is a Sparse Vector? How to Achieve Vector-based Hybrid Search\n\n\n[ Back to Vector Search Manuals](https://qdrant.tech/articles/vector-search-manuals/)\n# What is a Sparse Vector? How to Achieve Vector-based Hybrid Search\nNirant Kasliwal\n\u00b7\nDecember 09, 2023\n![What is a Sparse Vector? How to Achieve Vector-based Hybrid Search](https://qdrant.tech/articles_data/sparse-vectors/preview/title.jpg)\nThink of a library with a vast index card system. Each index card only has a few keywords marked out (sparse vector) of a large possible set for each book (document). This is what sparse vectors enable for text.\n## What are sparse and dense vectors?\nSparse vectors are like the Marie Kondo of data\u2014keeping only what sparks joy (or relevance, in this case).\nConsider a simplified example of 2 documents, each with 200 words. A dense vector would have several hundred non-zero values, whereas a sparse vector could have, much fewer, say only 20 non-zero values.\nIn this example: We assume it selects only 2 words or tokens from each document. The rest of the values are zero. This is why it\u2019s called a sparse vector.\n```\ndense = [0.2, 0.3, 0.5, 0.7, ...] # several hundred floats\nsparse = [{331: 0.5}, {14136: 0.7}] # 20 key value pairs\n\n```\n\nThe numbers 331 and 14136 map to specific tokens in the vocabulary e.g. `['chocolate', 'icecream']`. The rest of the values are zero. This is why it\u2019s called a sparse vector.\nThe tokens aren\u2019t always words though, sometimes they can be sub-words: `['ch', 'ocolate']` too.\nThey\u2019re pivotal in information retrieval, especially in ranking and search systems. BM25, a standard ranking function used by search engines like [Elasticsearch](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors), exemplifies this. BM25 calculates the relevance of documents to a given search query.\nBM25\u2019s capabilities are well-established, yet it has its limitations.\nBM25 relies solely on the frequency of words in a document and does not attempt to comprehend the meaning or the contextual importance of the words. Additionally, it requires the computation of the entire corpus\u2019s statistics in advance, posing a challenge for large datasets.\nSparse vectors harness the power of neural networks to surmount these limitations while retaining the ability to query exact words and phrases. They excel in handling large text data, making them crucial in modern data processing a and marking an advancement over traditional methods such as BM25.\n## Understanding sparse vectors\nSparse Vectors are a representation where each dimension corresponds to a word or subword, greatly aiding in interpreting document rankings. This clarity is why sparse vectors are essential in modern search and recommendation systems, complimenting the meaning-rich embedding or dense vectors.\nDense vectors from models like OpenAI Ada-002 or Sentence Transformers contain non-zero values for every element. In contrast, sparse vectors focus on relative word weights per document, with most values being zero. This results in a more efficient and interpretable system, especially in text-heavy applications like search.\nSparse Vectors shine in domains and scenarios where many rare keywords or specialized terms are present. For example, in the medical domain, many rare terms are not present in the general vocabulary, so general-purpose dense vectors cannot capture the nuances of the domain.\nFeature| Sparse Vectors| Dense Vectors  \n---|---|---  \n**Data Representation**|  Majority of elements are zero| All elements are non-zero  \n**Computational Efficiency**|  Generally higher, especially in operations involving zero elements| Lower, as operations are performed on all elements  \n**Information Density**|  Less dense, focuses on key features| Highly dense, capturing nuanced relationships  \n**Example Applications**|  Text search, Hybrid search| [RAG](https://qdrant.tech/articles/what-is-rag-in-ai/), many general machine learning tasks  \nWhere do sparse vectors fail though? They\u2019re not great at capturing nuanced relationships between words. For example, they can\u2019t capture the relationship between \u201cking\u201d and \u201cqueen\u201d as well as dense vectors.\n## SPLADE\nLet\u2019s check out [SPLADE](https://europe.naverlabs.com/research/computer-science/splade-a-sparse-bi-encoder-bert-based-model-achieves-effective-and-efficient-full-text-document-ranking/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors), an excellent way to make sparse vectors. Let\u2019s look at some numbers first. Higher is better:\nModel| MRR@10 (MS MARCO Dev)| Type  \n---|---|---  \nBM25| 0.184| Sparse  \nTCT-ColBERT| 0.359| Dense  \ndoc2query-T5 [link](https://github.com/castorini/docTTTTTquery)| 0.277| Sparse  \nSPLADE| 0.322| Sparse  \nSPLADE-max| 0.340| Sparse  \nSPLADE-doc| 0.322| Sparse  \nDistilSPLADE-max| 0.368| Sparse  \nAll numbers are from [SPLADEv2](https://arxiv.org/abs/2109.10086). MRR is [Mean Reciprocal Rank](https://www.wikiwand.com/en/Mean_reciprocal_rank#References), a standard metric for ranking. [MS MARCO](https://microsoft.github.io/MSMARCO-Passage-Ranking/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) is a dataset for evaluating ranking and retrieval for passages.\nSPLADE is quite flexible as a method, with regularization knobs that can be tuned to obtain [different models](https://github.com/naver/splade) as well:\n> SPLADE is more a class of models rather than a model per se: depending on the regularization magnitude, we can obtain different models (from very sparse to models doing intense query/doc expansion) with different properties and performance.\nFirst, let\u2019s look at how to create a sparse vector. Then, we\u2019ll look at the concepts behind SPLADE.\n## Creating a sparse vector\nWe\u2019ll explore two different ways to create a sparse vector. The higher performance way to create a sparse vector from dedicated document and query encoders. We\u2019ll look at a simpler approach \u2013 here we will use the same model for both document and query. We will get a dictionary of token ids and their corresponding weights for a sample text - representing a document.\nIf you\u2019d like to follow along, here\u2019s a [Colab Notebook](https://colab.research.google.com/gist/NirantK/ad658be3abefc09b17ce29f45255e14e/splade-single-encoder.ipynb), [alternate link](https://gist.github.com/NirantK/ad658be3abefc09b17ce29f45255e14e) with all the code.\n### Setting Up\n```\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\nmodel_id = \"naver/splade-cocondenser-ensembledistil\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForMaskedLM.from_pretrained(model_id)\ntext = \"\"\"Arthur Robert Ashe Jr. (July 10, 1943 \u2013 February 6, 1993) was an American professional tennis player. He won three Grand Slam titles in singles and two in doubles.\"\"\"\n\n```\n\n### Computing the sparse vector\n```\nimport torch\ndef compute_vector(text):\n  \"\"\"\n  Computes a vector from logits and attention mask using ReLU, log, and max operations.\n  \"\"\"\n  tokens = tokenizer(text, return_tensors=\"pt\")\n  output = model(**tokens)\n  logits, attention_mask = output.logits, tokens.attention_mask\n  relu_log = torch.log(1 + torch.relu(logits))\n  weighted_log = relu_log * attention_mask.unsqueeze(-1)\n  max_val, _ = torch.max(weighted_log, dim=1)\n  vec = max_val.squeeze()\n  return vec, tokens\nvec, tokens = compute_vector(text)\nprint(vec.shape)\n\n```\n\nYou\u2019ll notice that there are 38 tokens in the text based on this tokenizer. This will be different from the number of tokens in the vector. In a TF-IDF, we\u2019d assign weights only to these tokens or words. In SPLADE, we assign weights to all the tokens in the vocabulary using this vector using our learned model.\n## Term expansion and weights\n```\ndef extract_and_map_sparse_vector(vector, tokenizer):\n  \"\"\"\n  Extracts non-zero elements from a given vector and maps these elements to their human-readable tokens using a tokenizer. The function creates and returns a sorted dictionary where keys are the tokens corresponding to non-zero elements in the vector, and values are the weights of these elements, sorted in descending order of weights.\n  This function is useful in NLP tasks where you need to understand the significance of different tokens based on a model's output vector. It first identifies non-zero values in the vector, maps them to tokens, and sorts them by weight for better interpretability.\n  Args:\n  vector (torch.Tensor): A PyTorch tensor from which to extract non-zero elements.\n  tokenizer: The tokenizer used for tokenization in the model, providing the mapping from tokens to indices.\n  Returns:\n  dict: A sorted dictionary mapping human-readable tokens to their corresponding non-zero weights.\n  \"\"\"\n  # Extract indices and values of non-zero elements in the vector\n  cols = vector.nonzero().squeeze().cpu().tolist()\n  weights = vector[cols].cpu().tolist()\n  # Map indices to tokens and create a dictionary\n  idx2token = {idx: token for token, idx in tokenizer.get_vocab().items()}\n  token_weight_dict = {\n    idx2token[idx]: round(weight, 2) for idx, weight in zip(cols, weights)\n  }\n  # Sort the dictionary by weights in descending order\n  sorted_token_weight_dict = {\n    k: v\n    for k, v in sorted(\n      token_weight_dict.items(), key=lambda item: item[1], reverse=True\n    )\n  }\n  return sorted_token_weight_dict\n# Usage example\nsorted_tokens = extract_and_map_sparse_vector(vec, tokenizer)\nsorted_tokens\n\n```\n\nThere will be 102 sorted tokens in total. This has expanded to include tokens that weren\u2019t in the original text. This is the term expansion we will talk about next.\nHere are some terms that are added: \u201cBerlin\u201d, and \u201cfounder\u201d - despite having no mention of Arthur\u2019s race (which leads to Owen\u2019s Berlin win) and his work as the founder of Arthur Ashe Institute for Urban Health. Here are the top few `sorted_tokens` with a weight of more than 1:\n```\n{\n  \"ashe\": 2.95,\n  \"arthur\": 2.61,\n  \"tennis\": 2.22,\n  \"robert\": 1.74,\n  \"jr\": 1.55,\n  \"he\": 1.39,\n  \"founder\": 1.36,\n  \"doubles\": 1.24,\n  \"won\": 1.22,\n  \"slam\": 1.22,\n  \"died\": 1.19,\n  \"singles\": 1.1,\n  \"was\": 1.07,\n  \"player\": 1.06,\n  \"titles\": 0.99, \n  ...\n}\n\n```\n\nIf you\u2019re interested in using the higher-performance approach, check out the following models:\n  1. [naver/efficient-splade-VI-BT-large-doc](https://huggingface.co/naver/efficient-splade-vi-bt-large-doc)\n  2. [naver/efficient-splade-VI-BT-large-query](https://huggingface.co/naver/efficient-splade-vi-bt-large-doc)\n\n\n## Why SPLADE works: term expansion\nConsider a query \u201csolar energy advantages\u201d. SPLADE might expand this to include terms like \u201crenewable,\u201d \u201csustainable,\u201d and \u201cphotovoltaic,\u201d which are contextually relevant but not explicitly mentioned. This process is called term expansion, and it\u2019s a key component of SPLADE.\nSPLADE learns the query/document expansion to include other relevant terms. This is a crucial advantage over other sparse methods which include the exact word, but completely miss the contextually relevant ones.\nThis expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.\nFor example, assume a 1M document corpus. Say, we use 100 sparse token ids + weights per document. Correspondingly, dense BERT vector would be 768M floats, the OpenAI Embedding would be 1.536B floats, and the sparse vector would be a maximum of 100M integers + 100M floats. This could mean a **10x reduction in memory usage** , which is a huge win for large-scale systems:\nVector Type| Memory (GB)  \n---|---  \nDense BERT Vector| 6.144  \nOpenAI Embedding| 12.288  \nSparse Vector| 1.12  \n### How SPLADE works: leveraging BERT\nSPLADE leverages a transformer architecture to generate sparse representations of documents and queries, enabling efficient retrieval. Let\u2019s dive into the process.\nThe output logits from the transformer backbone are inputs upon which SPLADE builds. The transformer architecture can be something familiar like BERT. Rather than producing dense probability distributions, SPLADE utilizes these logits to construct sparse vectors\u2014think of them as a distilled essence of tokens, where each dimension corresponds to a term from the vocabulary and its associated weight in the context of the given document or query.\nThis sparsity is critical; it mirrors the probability distributions from a typical [Masked Language Modeling](http://jalammar.github.io/illustrated-bert/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) task but is tuned for retrieval effectiveness, emphasizing terms that are both:\n  1. Contextually relevant: Terms that represent a document well should be given more weight.\n  2. Discriminative across documents: Terms that a document has, and other documents don\u2019t, should be given more weight.\n\n\nThe token-level distributions that you\u2019d expect in a standard transformer model are now transformed into token-level importance scores in SPLADE. These scores reflect the significance of each term in the context of the document or query, guiding the model to allocate more weight to terms that are likely to be more meaningful for retrieval purposes.\nThe resulting sparse vectors are not only memory-efficient but also tailored for precise matching in the high-dimensional space of a search engine like Qdrant.\n### Interpreting SPLADE\nA downside of dense vectors is that they are not interpretable, making it difficult to understand why a document is relevant to a query.\nSPLADE importance estimation can provide insights into the \u2018why\u2019 behind a document\u2019s relevance to a query. By shedding light on which tokens contribute most to the retrieval score, SPLADE offers some degree of interpretability alongside performance, a rare feat in the realm of neural IR systems. For engineers working on search, this transparency is invaluable.\n## Known limitations of SPLADE\n### Pooling strategy\nThe switch to max pooling in SPLADE improved its performance on the MS MARCO and TREC datasets. However, this indicates a potential limitation of the baseline SPLADE pooling method, suggesting that SPLADE\u2019s performance is sensitive to the choice of pooling strategy\u200b\u200b.\n### Document and query Eecoder\nThe SPLADE model variant that uses a document encoder with max pooling but no query encoder reaches the same performance level as the prior SPLADE model. This suggests a limitation in the necessity of a query encoder, potentially affecting the efficiency of the model\u200b\u200b.\n### Other sparse vector methods\nSPLADE is not the only method to create sparse vectors.\nEssentially, sparse vectors are a superset of TF-IDF and BM25, which are the most popular text retrieval methods. In other words, you can create a sparse vector using the term frequency and inverse document frequency (TF-IDF) to reproduce the BM25 score exactly.\nAdditionally, attention weights from Sentence Transformers can be used to create sparse vectors. This method preserves the ability to query exact words and phrases but avoids the computational overhead of query expansion used in SPLADE.\nWe will cover these methods in detail in a future article.\n## Leveraging sparse vectors in Qdrant for hybrid search\nQdrant supports a separate index for Sparse Vectors. This enables you to use the same collection for both dense and sparse vectors. Each \u201cPoint\u201d in Qdrant can have both dense and sparse vectors.\nBut let\u2019s first take a look at how you can work with sparse vectors in Qdrant.\n## Practical implementation in Python\nLet\u2019s dive into how Qdrant handles sparse vectors with an example. Here is what we will cover:\n  1. Setting Up Qdrant Client: Initially, we establish a connection with Qdrant using the QdrantClient. This setup is crucial for subsequent operations.\n  2. Creating a Collection with Sparse Vector Support: In Qdrant, a collection is a container for your vectors. Here, we create a collection specifically designed to support sparse vectors. This is done using the create_collection method where we define the parameters for sparse vectors, such as setting the index configuration.\n  3. Inserting Sparse Vectors: Once the collection is set up, we can insert sparse vectors into it. This involves defining the sparse vector with its indices and values, and then upserting this point into the collection.\n  4. Querying with Sparse Vectors: To perform a search, we first prepare a query vector. This involves computing the vector from a query text and extracting its indices and values. We then use these details to construct a query against our collection.\n  5. Retrieving and Interpreting Results: The search operation returns results that include the id of the matching document, its score, and other relevant details. The score is a crucial aspect, reflecting the similarity between the query and the documents in the collection.\n\n\n### 1. Set up\n```\n# Qdrant client setup\nclient = QdrantClient(\":memory:\")\n# Define collection name\nCOLLECTION_NAME = \"example_collection\"\n# Insert sparse vector into Qdrant collection\npoint_id = 1 # Assign a unique ID for the point\n\n```\n\n### 2. Create a collection with sparse vector support\n```\nclient.create_collection(\n  collection_name=COLLECTION_NAME,\n  vectors_config={},\n  sparse_vectors_config={\n    \"text\": models.SparseVectorParams(\n      index=models.SparseIndexParams(\n        on_disk=False,\n      )\n    )\n  },\n)\n\n```\n\n### 3. Insert sparse vectors\nHere, we see the process of inserting a sparse vector into the Qdrant collection. This step is key to building a dataset that can be quickly retrieved in the first stage of the retrieval process, utilizing the efficiency of sparse vectors. Since this is for demonstration purposes, we insert only one point with Sparse Vector and no dense vector.\n```\nclient.upsert(\n  collection_name=COLLECTION_NAME,\n  points=[\n    models.PointStruct(\n      id=point_id,\n      payload={}, # Add any additional payload if necessary\n      vector={\n        \"text\": models.SparseVector(\n          indices=indices.tolist(), values=values.tolist()\n        )\n      },\n    )\n  ],\n)\n\n```\n\nBy upserting points with sparse vectors, we prepare our dataset for rapid first-stage retrieval, laying the groundwork for subsequent detailed analysis using dense vectors. Notice that we use \u201ctext\u201d to denote the name of the sparse vector.\nThose familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API \u2013 this is to make it easier to use sparse vectors in existing codebases. As always, you\u2019re able to **apply payload filters** , shard keys, and other advanced features you\u2019ve come to expect from Qdrant. To make things easier for you, the indices and values don\u2019t have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.\n### 4. Query with sparse vectors\nWe use the same process to prepare a query vector as well. This involves computing the vector from a query text and extracting its indices and values. We then use these details to construct a query against our collection.\n```\n# Preparing a query vector\nquery_text = \"Who was Arthur Ashe?\"\nquery_vec, query_tokens = compute_vector(query_text)\nquery_vec.shape\nquery_indices = query_vec.nonzero().numpy().flatten()\nquery_values = query_vec.detach().numpy()[query_indices]\n\n```\n\nIn this example, we use the same model for both document and query. This is not a requirement, but it\u2019s a simpler approach.\n### 5. Retrieve and interpret results\nAfter setting up the collection and inserting sparse vectors, the next critical step is retrieving and interpreting the results. This process involves executing a search query and then analyzing the returned results.\n```\n# Searching for similar documents\nresult = client.search(\n  collection_name=COLLECTION_NAME,\n  query_vector=models.NamedSparseVector(\n    name=\"text\",\n    vector=models.SparseVector(\n      indices=query_indices,\n      values=query_values,\n    ),\n  ),\n  with_vectors=True,\n)\nresult\n\n```\n\nIn the above code, we execute a search against our collection using the prepared sparse vector query. The `client.search` method takes the collection name and the query vector as inputs. The query vector is constructed using the `models.NamedSparseVector`, which includes the indices and values derived from the query text. This is a crucial step in efficiently retrieving relevant documents.\n```\nScoredPoint(\n  id=1,\n  version=0,\n  score=3.4292831420898438,\n  payload={},\n  vector={\n    \"text\": SparseVector(\n      indices=[2001, 2002, 2010, 2018, 2032, ...],\n      values=[\n        1.0660614967346191,\n        1.391068458557129,\n        0.8903818726539612,\n        0.2502821087837219,\n        ...,\n      ],\n    )\n  },\n)\n\n```\n\nThe result, as shown above, is a `ScoredPoint` object containing the ID of the retrieved document, its version, a similarity score, and the sparse vector. The score is a key element as it quantifies the similarity between the query and the document, based on their respective vectors.\nTo understand how this scoring works, we use the familiar dot product method:\n$$\\text{Similarity}(\\text{Query}, \\text{Document}) = \\sum_{i \\in I} \\text{Query}_i \\times \\text{Document}_i$$\nThis formula calculates the similarity score by multiplying corresponding elements of the query and document vectors and summing these products. This method is particularly effective with sparse vectors, where many elements are zero, leading to a computationally efficient process. The higher the score, the greater the similarity between the query and the document, making it a valuable metric for assessing the relevance of the retrieved documents.\n## Hybrid search: combining sparse and dense vectors\nBy combining search results from both dense and sparse vectors, you can achieve a hybrid search that is both efficient and accurate. Results from sparse vectors will guarantee, that all results with the required keywords are returned, while dense vectors will cover the semantically similar results.\nThe mixture of dense and sparse results can be presented directly to the user, or used as a first stage of a two-stage retrieval process.\nLet\u2019s see how you can make a hybrid search query in Qdrant.\nFirst, you need to create a collection with both dense and sparse vectors:\n```\nclient.create_collection(\n  collection_name=COLLECTION_NAME,\n  vectors_config={\n    \"text-dense\": models.VectorParams(\n      size=1536, # OpenAI Embeddings\n      distance=models.Distance.COSINE,\n    )\n  },\n  sparse_vectors_config={\n    \"text-sparse\": models.SparseVectorParams(\n      index=models.SparseIndexParams(\n        on_disk=False,\n      )\n    )\n  },\n)\n\n```\n\nThen, assuming you have upserted both dense and sparse vectors, you can query them together:\n```\nquery_text = \"Who was Arthur Ashe?\"\n# Compute sparse and dense vectors\nquery_indices, query_values = compute_sparse_vector(query_text)\nquery_dense_vector = compute_dense_vector(query_text)\nclient.search_batch(\n  collection_name=COLLECTION_NAME,\n  requests=[\n    models.SearchRequest(\n      vector=models.NamedVector(\n        name=\"text-dense\",\n        vector=query_dense_vector,\n      ),\n      limit=10,\n    ),\n    models.SearchRequest(\n      vector=models.NamedSparseVector(\n        name=\"text-sparse\",\n        vector=models.SparseVector(\n          indices=query_indices,\n          values=query_values,\n        ),\n      ),\n      limit=10,\n    ),\n  ],\n)\n\n```\n\nThe result will be a pair of result lists, one for dense and one for sparse vectors.\nHaving those results, there are several ways to combine them:\n### Mixing or fusion\nYou can mix the results from both dense and sparse vectors, based purely on their relative scores. This is a simple and effective approach, but it doesn\u2019t take into account the semantic similarity between the results. Among the [popular mixing methods](https://medium.com/plain-simple-software/distribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18) are:\n```\n- Reciprocal Ranked Fusion (RRF)\n- Relative Score Fusion (RSF)\n- Distribution-Based Score Fusion (DBSF)\n\n```\n![Relative Score Fusion](https://qdrant.tech/articles_data/sparse-vectors/mixture.png)\nRelative Score Fusion\n[Ranx](https://github.com/AmenRa/ranx) is a great library for mixing results from different sources.\n### Re-ranking\nYou can use obtained results as a first stage of a two-stage retrieval process. In the second stage, you can re-rank the results from the first stage using a more complex model, such as [Cross-Encoders](https://www.sbert.net/examples/applications/cross-encoder/README.html) or services like [Cohere Rerank](https://txt.cohere.com/rerank/).\nAnd that\u2019s it! You\u2019ve successfully achieved hybrid search with Qdrant!\n## Additional resources\nFor those who want to dive deeper, here are the top papers on the topic most of which have code available:\n  1. Problem Motivation: [Sparse Overcomplete Word Vector Representations](https://ar5iv.org/abs/1506.02004?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n  2. [SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval](https://ar5iv.org/abs/2109.10086?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n  3. [SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking](https://ar5iv.org/abs/2107.05720?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n  4. Late Interaction - [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](https://ar5iv.org/abs/2112.01488?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n  5. [SparseEmbed: Learning Sparse Lexical Representations with Contextual Embeddings for Retrieval](https://research.google/pubs/pub52289/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n\n\n**Why just read when you can try it out?**\nWe\u2019ve packed an easy-to-use Colab for you on how to make a Sparse Vector: [Sparse Vectors Single Encoder Demo](https://colab.research.google.com/drive/1wa2Yr5BCOgV0MTOFFTude99BOXCLHXky?usp=sharing). Run it, tinker with it, and start seeing the magic unfold in your projects. We can\u2019t wait to hear how you use it!\n## Conclusion\nAlright, folks, let\u2019s wrap it up. Better search isn\u2019t a \u2019nice-to-have,\u2019 it\u2019s a game-changer, and Qdrant can get you there.\nGot questions? Our [Discord community](https://qdrant.to/discord?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) is teeming with answers.\nIf you enjoyed reading this, why not sign up for our [newsletter](https://qdrant.tech/subscribe/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) to stay ahead of the curve.\nAnd, of course, a big thanks to you, our readers, for pushing us to make ranking better for everyone.\n##### Was this page useful?\n![Thumb up icon](https://qdrant.tech/icons/outline/thumb-up.svg) Yes  ![Thumb down icon](https://qdrant.tech/icons/outline/thumb-down.svg) No\nThank you for your feedback! \ud83d\ude4f\nWe are sorry to hear that. \ud83d\ude14 You can [edit](https://qdrant.tech/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/sparse-vectors.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue.\nOn this page:\n  * [What are sparse and dense vectors?](https://qdrant.tech/articles/sparse-vectors/#what-are-sparse-and-dense-vectors)\n  * [Understanding sparse vectors](https://qdrant.tech/articles/sparse-vectors/#understanding-sparse-vectors)\n  * [SPLADE](https://qdrant.tech/articles/sparse-vectors/#splade)\n  * [Creating a sparse vector](https://qdrant.tech/articles/sparse-vectors/#creating-a-sparse-vector)\n    * [Setting Up](https://qdrant.tech/articles/sparse-vectors/#setting-up)\n    * [Computing the sparse vector](https://qdrant.tech/articles/sparse-vectors/#computing-the-sparse-vector)\n  * [Term expansion and weights](https://qdrant.tech/articles/sparse-vectors/#term-expansion-and-weights)\n  * [Why SPLADE works: term expansion](https://qdrant.tech/articles/sparse-vectors/#why-splade-works-term-expansion)\n    * [How SPLADE works: leveraging BERT](https://qdrant.tech/articles/sparse-vectors/#how-splade-works-leveraging-bert)\n    * [Interpreting SPLADE](https://qdrant.tech/articles/sparse-vectors/#interpreting-splade)\n  * [Known limitations of SPLADE](https://qdrant.tech/articles/sparse-vectors/#known-limitations-of-splade)\n    * [Pooling strategy](https://qdrant.tech/articles/sparse-vectors/#pooling-strategy)\n    * [Document and query Eecoder](https://qdrant.tech/articles/sparse-vectors/#document-and-query-eecoder)\n    * [Other sparse vector methods](https://qdrant.tech/articles/sparse-vectors/#other-sparse-vector-methods)\n  * [Leveraging sparse vectors in Qdrant for hybrid search](https://qdrant.tech/articles/sparse-vectors/#leveraging-sparse-vectors-in-qdrant-for-hybrid-search)\n  * [Practical implementation in Python](https://qdrant.tech/articles/sparse-vectors/#practical-implementation-in-python)\n    * [1. Set up](https://qdrant.tech/articles/sparse-vectors/#1-set-up)\n    * [2. Create a collection with sparse vector support](https://qdrant.tech/articles/sparse-vectors/#2-create-a-collection-with-sparse-vector-support)\n    * [3. Insert sparse vectors](https://qdrant.tech/articles/sparse-vectors/#3-insert-sparse-vectors)\n    * [4. Query with sparse vectors](https://qdrant.tech/articles/sparse-vectors/#4-query-with-sparse-vectors)\n    * [5. Retrieve and interpret results](https://qdrant.tech/articles/sparse-vectors/#5-retrieve-and-interpret-results)\n  * [Hybrid search: combining sparse and dense vectors](https://qdrant.tech/articles/sparse-vectors/#hybrid-search-combining-sparse-and-dense-vectors)\n    * [Mixing or fusion](https://qdrant.tech/articles/sparse-vectors/#mixing-or-fusion)\n    * [Re-ranking](https://qdrant.tech/articles/sparse-vectors/#re-ranking)\n  * [Additional resources](https://qdrant.tech/articles/sparse-vectors/#additional-resources)\n  * [Conclusion](https://qdrant.tech/articles/sparse-vectors/#conclusion)\n\n\n  * [ Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/sparse-vectors.md)\n  * [ Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)\n\n\n#### Ready to get started with Qdrant?\n[Start Free](https://qdrant.to/cloud/)\n\u00a9 2025 Qdrant. All Rights Reserved\n[Terms](https://qdrant.tech/legal/terms_and_conditions/) [Privacy Policy](https://qdrant.tech/legal/privacy-policy/) [Impressum](https://qdrant.tech/legal/impressum/)\n\u00d7\n[ Powered by ](https://qdrant.tech/)\nWe use cookies to learn more about you. At any time you can delete or block cookies through your browser settings.\n[Learn more](https://qdrant.tech/legal/privacy-policy/)[I accept](https://qdrant.tech/articles/sparse-vectors/)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nSparse vectors enhance text retrieval by focusing on key terms, improving efficiency over dense vectors. SPLADE, a model for creating sparse vectors, enables term expansion and interpretable results. Qdrant supports hybrid search, combining sparse and dense vectors for optimal performance.\n```"
  },
  {
    "instruction": "[\ud83d\ude80 Open-source RAG evaluation and testing with Evidently. New release](https://www.evidentlyai.com/blog/open-source-rag-evaluation-tool)![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc2747cf_vector.svg)\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66180fbf4f40e9ed73ca2d39_evidently_ai_logo_fi.png)](https://www.evidentlyai.com/)\nProduct\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d2624dc76fe4c62697aff8_chats-teardrop-duotone%20\\(1\\).svg)LLM Testing PlatformEvaluate LLM quality and safety](https://www.evidentlyai.com/llm-testing)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d3354a454c59d5f7affee2_list-magnifying-glass-duotone%20\\(3\\).svg)RAG TestingImprove retrieval, cut hallucinations](https://www.evidentlyai.com/rag-testing)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d2624e2132e631148cfd1a_warning-duotone%20\\(2\\).svg)AI Risk AssessmentIdentify AI risks and get a plan](https://www.evidentlyai.com/llm-readiness)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d33514b0f63efed5f253e9_crosshair-simple-duotone%20\\(1\\).svg)Adversarial TestingTest AI for threats and edge cases](https://www.evidentlyai.com/llm-red-teaming)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6650a6bcbdd0a756edf0d829_chart-bar-duotone%20\\(1\\).svg)ML MonitoringTrack data drift and predictive quality](https://www.evidentlyai.com/ml-monitoring)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9a94702f76fd53c7983b6_tree-structure-duotone%20\\(1\\).svg)AI Agent TestingValidate multi-step workflows](https://www.evidentlyai.com/ai-agent-testing)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7ee755c4dc43cefc2_users-three-duotone%20\\(1\\).svg)Open-SourceOpen-source Evidently Python library](https://www.evidentlyai.com/evidently-oss)\n##### [See Evidently in action![Evidently AI Testing for LLM](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/663511956e05f300480f19fd_llm_2.png)](https://www.evidentlyai.com/get-demo)[Get demo now![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/get-demo)\n[Pricing](https://www.evidentlyai.com/pricing)[Docs](https://docs.evidentlyai.com/)\nResources\n[![book](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abba265e01ed75b39988c_book-duotone.svg)BlogInsights on building AI products](https://www.evidentlyai.com/blog)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/675a45076643cf7042075aac_star-duotone%20\\(1\\).svg)LLM benchmarks100+ LLM benchmarks and datasets](https://www.evidentlyai.com/llm-evaluation-benchmarks-datasets)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7fbdd19d6d3f1875c_code-duotone%20\\(1\\).svg)TutorialsAI observability and MLOps tutorials](https://www.evidentlyai.com/mlops-tutorials)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb798a210c5d2640337_lightbulb-duotone%20\\(1\\).svg)ML and LLM system design500 ML and LLM use cases](https://www.evidentlyai.com/ml-system-design)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb798a210c5d2640368_check-duotone%20\\(1\\).svg)GuidesIn-depth AI quality and MLOps guides ](https://www.evidentlyai.com/mlops-guides)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdbd5bfc045c59b3467f_gear-duotone%20\\(1\\).svg)ML and AI platforms45+ internal ML and AI platforms](https://www.evidentlyai.com/ml-platforms)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7ee755c4dc43cefc2_users-three-duotone%20\\(1\\).svg)CommunityGet support and chat about AI products](https://www.evidentlyai.com/community)\n##### [Course on LLM evaluations for AI product teams![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/673f571568470a0a8d49aa43_three-cheerful-robot-students-take-exam%20\\(4\\).jpg)](https://www.evidentlyai.com/llm-evaluations-course)[Sign up now![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/llm-evaluations-course)\n[Sign up](https://www.evidentlyai.com/register)[Get demo](https://www.evidentlyai.com/get-demo)\n[Sign up](https://www.evidentlyai.com/register)[Get demo](https://www.evidentlyai.com/get-demo)\n![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abe8f5bd7dda2d12a1e6d_list-bold.svg)\n###### Ranking and Recommendation Metrics Guide\n# Normalized Discounted Cumulative Gain (NDCG) explained\nLast updated:\nFebruary 14, 2025\ncontents**\u200d**\n[ How to compute NDCG](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/#how-to-compute-ndcg)\n[NDCG formula](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/#ndcg-formula)\n[Relevance score](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/#relevance-score)\n[K parameter](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/#k-parameter)\n[Cumulative gain](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/#cumulative-gain)\n[Discounted gain (DCG)](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/#discounted-gain-dcg)\n[Normalized DCG (NDCG)](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/#normalized-dcg-ndcg)\n[Interpretation](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/#interpretation)\n[Pros and cons](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/#pros-and-cons)\n[NDCG vs MAP](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/#ndcg-vs-map)\n[NDCG vs MRR](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/#ndcg-vs-mrr)\n[Evaluating ranking with Evidently](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/#evaluating-ranking-with-evidently)\nNormalized Discounted Cumulative Gain (NDCG) is a metric that evaluates the quality of recommendation and information retrieval systems. NDCG helps measure a machine learning algorithm's ability to sort items based on relevance. \nIn this article, we explain it step by step.\nWe also introduce [Evidently](https://github.com/evidentlyai/evidently), an open-source Python library for ML model evaluation and monitoring. \n## TL;DR\n  * **Normalized Discounted Cumulative Gain (NDCG)** is a ranking quality metric. It compares rankings to an ideal order where all relevant items are at the top of the list.\n  * NDCG at K is determined by dividing the **Discounted Cumulative Gain** (DCG) by the**ideal DCG** representing a perfect ranking. \n  * DCG measures the **total item relevance in a list** with a discount that helps address the diminishing value of items further down the list.\n  * You can aggregate the NDCG results across all users to get an overall measure of the ranking quality in the dataset. \n  * NDCG **can take values from 0 to 1** , where 1 indicates a match with the ideal order, and lower values represent a lower quality of ranking.\n\n\n![Evidently Classification Performance Report](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/662c488bf7fc70afcba17ffa_cropped_00_main_01_hero-min-p-2000.png)\nStart with AI observability\nWant to keep tabs on your ranking and recommendation models? Automate the quality checks with Evidently Cloud. Powered by the leading open-source Evidently library with 20m+ downloads.\n[Start free \u27f6](https://www.evidentlyai.com/register)[Or try open source \u27f6](https://github.com/evidentlyai/evidently)\n![Evidently Classification Performance Report](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/662c488bf7fc70afcba17ffa_cropped_00_main_01_hero-min-p-2000.png)\nStart with AI observability\nWant to keep tabs on your ranking and recommendation models? Automate the quality checks with Evidently Cloud. Powered by the leading open-source Evidently library with 20m+ downloads.\n[Start free \u27f6](https://www.evidentlyai.com/register)[Or try open source \u27f6](https://github.com/evidentlyai/evidently)\nConsider an example: you are looking for a movie to watch, and there are many options! Some are a perfect match for your taste. Some are less so, but still are interesting and entertaining. Preferences, relevance, and tastes are not always as simple as binary \u201cyes\u201d or \u201cno.\u201d If you watch all the movies under consideration, you can probably assign a score to each and make an orderly list, sorting them from the best to worst.\nNow, let\u2019s turn the table and imagine that you are developing a recommender system for movie recommendations. Knowing what you know \u2013 your ground truth about movies you liked \u2013 how can the ideal output of such a system look? Intuitively, it should look the same: if you create a long list of movies to watch, you should sort it based on relevance: the best movies on the top, the fine choices after it, and so on.\nNDCG is a metric that helps evaluate the ranking quality with a similar principle. We assume that there is some ideal ranking with all the items sorted in decreasing order of relevance. NDCG helps measure how close the algorithm output comes to this perfect order. \n![Normalized Discounted Cumulative Gain \\(NDCG\\)](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9d9_657789f98238bac999396657_ndcg1.png)\nYou can also use NDCG in [LLM evaluations](https://www.evidentlyai.com/llm-guide/llm-evaluation), for example to evaluate RAG quality.\n## How to compute NDCG\nNDCG stands for **Normalized Discounted Cumulative Gain.** As usual with ranking, you often compute NDCG at **K** , defining a cutoff point for the top-ranked items you consider.\n### NDCG formula\nTo calculate NDCG, you must divide a given list's discounted cumulative gain (DCG) by the ideal DCG representing a perfect ranking order.\n![NDCG formula](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9bc_65778a1b0605ecaf394b136d_ndcg2.png)\nThe formula might not be easy to grasp initially, so let\u2019s unpack it. We will explain this metric step by step by covering:\n  * What is the **relevance** in recommendations and ranking?\n  * What is **K** (the @K part of the formula)?\n  * What is gain and cumulative gain (the **CG** part of the formula? \n  * What is a discount, and how do you compute discounted cumulative gain (the **DCG**)?\n  * Finally, how do we normalize the DCG to get the **NDCG?**\n\n\n### Relevance score\n> **TL;DR**. A relevance score is a ground truth or target value necessary for evaluating ranking system quality. It can be a binary label (relevant or not) or a numerical score (e.g., 1 to 5).\nA ranking or recommender system typically returns a **sorted list** of the relevant items. If you deal with recommendations, this might be a list of items (products, articles, songs, etc.) for a specific user, ranked by their likely match. If you deal with search and retrieval, this can be a list of items or answers most relevant to a specific query.\n![Recommender system sorted list ](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9ed_65774504c9aaf12aca69c337_ranking_evalutions2.png)\nTo evaluate the ranking quality, you need some **ground truth** to this. The ground truth helps assess whether the recommended items matched the user\u2019s preferences or query. When a recommender system is in production, you can capture the relevant user actions and interactions. Say, you can track if the user clicks on the item shown in the recommendation block. \nOften, this relevance is **binary** : say, the user clicked on, watched, or purchased an item (or not). Sometimes, this can be a **numerical score** , such as a rating where different weights are assigned to specific user actions, reflecting a relative degree of match. \nThe way you get the relevance score depends on the case. For example, if you collect data about user actions in the app, you can develop a custom formula to define the relevance based on a particular user action. Say, a click is good (score 1), but adding the item to favorites is better (score 2). Putting a product in the basket is even more valuable (score 3), and finally, completing the purchase (score 5) best reflects the item's relevance.\nHowever, this granularity can be pretty complex to implement in production, so having a binary \u201crelevant or not\u201d label based on actions like clicks is a popular approach. \nOnce you have the ground truth, you can define how good your system is by comparing the ranked list (what the recommender system predicted to be relevant) and the actual relevance score. You can use various metrics in this context, with NDCG being one of them. Unlike some other metrics, NDCG can handle binary and numerical relevance scores.\n> \u200d**Want to understand which other metrics exist?** Check out this introduction to [recommendations metrics](https://www.evidentlyai.com/ranking-metrics/evaluating-recommender-systems).\n### K parameter\n> **TL;DR**. **K** is a user-assigned parameter that defines the cutoff point (list length) while evaluating ranking quality. For example, you can only look at top-10 items.\nWhen evaluating the quality of recommendations or ranking, you must also define the depth of the ranked list you will look at. \nThe **K parameter** represents the cutoff point in the list where you\u2019d look for relevant items. \nFor example, you might consider top-10 search results or top-3 recommendations since this is the app\u2019s recommendation block size. Or, you might look at the top 30 recommendations if you expect the user behavior to be explorative and know this to be the average number of items to view.\n![K parameter example](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9c3_65778b48c5b384ec853d8365_ndcg4.png)\nThis K is a use-case-specific parameter that you can set. You might also evaluate metrics at different levels of K, treating each as an additional quality measure: for example, look at top-3 and top-30 results in parallel. \n### Cumulative gain\n> **TL;DR**. Cumulative gain is a measure of the total relevance of the ranked list. It sums up the relevance scores of the individual items in the list.\nBefore explaining the **discount** , let\u2019s introduce the idea of cumulative gain. \nA **gain** of a single recommended item is its relevance score, be it binary or graded. The **cumulative gain** is a sum of scores of all relevant items among the top-K results in the list.\n![Cumulative gain formula](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9c6_65778cf257e250a8b547fee8_ndcg6.png)\nFor example, if you have a binary notion of relevance, and out of 5 recommended items, 3 got a click, the cumulative gain of a given list will be 1+1+1=3. If you have graded scores from 1 to 5, and all your top 5 recommendations got a 5-star, the total gain will be 25.\nHowever, this way of computing gain has a limitation \u2013 it does not take into account the ordering. If you change the position of the relevant items, the outcome will be the same.\nLet\u2019s consider two situations with binary relevance:\n  * Relevant items take positions 3 to 5. The items in positions 1 and 2 are not relevant.\n  * Relevant items take positions 1 to 3. The items in positions 4 and 5 are not relevant.\n\n\n![Cumulative gain example](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9e9_65778d21c47ce3a6884d0fe4_ndcg7.png)\nAs we swap positions, the cumulative gain stays the same.\nHowever, intuitively, the second result seems like a much better outcome. The ranking often matters: having the most relevant results at the top might be just what we expect. For example, the best search result should be the first, not the last, on the page. \nWe should reflect it in our evaluation: give credit to the system when it can place more relevant items higher and penalize it for placing relevant results lower on the list. To do that, we can introduce a **discount.**\n### Discounted gain (DCG)\n> **TL;DR:** A logarithmic**discount** helps assign lower gain when relevant items appear further down in the ranked list.\nTo compute the DCG - discounted cumulative gain \u2013 we introduce the discount that gives less weight to relevant items that appear lower. This way, instead of simply summing up the item\u2019s relevance, we adjust the contribution of each item based on its position. \nBut how exactly do you come up with the discount?\nA common way to introduce this discount in the industry is a logarithmic penalty. As you move down the list, you divide each item\u2019s gain by a growing number, computed as an inverse logarithm of the position number. This discount helps factor in the diminishing value of relevant items further down. This is quite intuitive: a good match at rank 10 is less valuable than at rank 1. \n\u200d**DCG formula.** For each item in the top K positions, we calculate the discounted gain using the formula:\n![Discounted gain \\(DCG\\) formula](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9cf_65778cdc6b7b673e1fc61310_ndcg5.png)\nWhere **rel (i)** is the relevance score of the item at position i.\n> **Note:** there is also an [alternative](https://en.wikipedia.org/wiki/Discounted_cumulative_gain#cite_note-4) DCG formula that gives an even higher penalty.\nUsing the logarithmic discount, you can ensure that the difference between positions 1 and 2 is more substantial than between positions 2 and 3, and so on. This mirrors the intuition that highest-ranked items have a more significant impact on the user experience, and there is often a substantial drop-off as one moves down the list. Consider search engine results where users are less likely to explore the results beyond the first page.\nIn essence, each position in the fixed list has an assigned value factor. The second place is divided by 1.58, the 3rd place is divided by 2, and so on. \n![Alternative DCG formula](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9d5_657869f6a5568b2b9beea08b_ndcg8.png)\n**Let\u2019s take an example of DCG computation.** Suppose we have a ranked list with binary scores [1, 0, 1, 1, 0], and we want to calculate DCG@3.\nWe can sum the relevance scores of all items up to position K = 3, weighted by the corresponding logarithmic discount.\n  * The item at rank 1 is relevant and contributes 1/1=1. There is no discount in rank 1.\n  * The item at rank 2 is irrelevant, so it contributes 0.\n  * The item at rank 3 is relevant but is divided by 2: 1/2 = 0.5.\n\n\nThe resulting sum is 1 + 0 + 0.5 = 1.5. This is the discounted gain of the list at K = 3.\n![DCG example computation](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9d2_65786a23436c09a6ec96b069_ndcg9.png)\nThe cumulative gain of the same list would have equaled 1+0+1 = 2 without the discount. \nWhat if we swap the order and put the 3rd relevant item in the second position? Our ranked list looks like a ranked list: [1, 1, 0, 1, 0].\n  * The item at rank 1 contributes 1 like before.\n  * The item at rank 2 now contributes 1/1.585 \u2248 0.63.\n  * The item at rank 3 is irrelevant, so it contributes 0.\n\n\nThe resulting sum equals 1 + 0.63 + 0 = 1.63. This is the new discounted gain of the list at K = 3. We improved the DCG @3 by moving the relevant item higher up (from position 3 to position 2).\n![DCG example computation](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9e3_65786a51cdbb10517ddad96d_ndcg10.png)\nThe Discounted Cumulative Gain (DCG) helps address the ranking order. However, there is still a limitation. The absolute values of DCG depend on the number of items in the list and the relevance scores assigned. This makes it hard to compare the DCG scores across use cases and lists.\nLonger lists have higher potential DCG values since more items can contribute to the score. DCG at 10 can be higher than DCG at 3 simply due to the length of the list rather than the inherently better ranking. Using binary or graded relevance will also lead to different gain values.\nFinally, even an \u201cideal\u201d recommendation algorithm won\u2019t always be able to get DCG@k = 1 due to the applied discount.\nTo address this, you can normalize the DCG.\n### Normalized DCG (NDCG)\n> **TLDR:** To normalize DCG, you can divide it by the ideal ranking (IDCG@k), where every item appears in the order of relevance. \nTo get the normalized DCG (NDCG), you must divide the computed DCG by the ideal DCG (IDCG) for the given list. IDCG represents the maximum achievable DCG with the same set of relevance scores but in the perfect ranking order. \nWhat exactly is ideal? In the case of binary relevance scores, all relevant items should be at the start of the list. In the case of graded relevance, you should place all items in a descending order of relevance.\nHere is the final NDCG formula: \n![Normalized DCG \\(NDCG\\) formula](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9bc_65778a1b0605ecaf394b136d_ndcg2.png)\nNormalized DCG enables fair comparisons between different lists, regardless of their length and the scale of relevance scores. As the final step, you typically compute an average NDCG for all users or queries to get a single score for the overall dataset. \n## Interpretation\nNDCG can take values from 0 to 1. \n  * NDCG equals 1 in the case of ideal ranking when items are perfectly sorted by relevance. \n  * NDCG equals 0 when there are no relevant objects in top-K.\n  * NDCG can be between 0 and 1 in all other cases. The higher the NDCG, the better. \n\n\nIn essence, NDCG@k is a ranking metric that helps consider both the relevance of items and their positions in the list. A higher NDCG@k value indicates that the system is doing a better job of presenting relevant items at the top of the list.\n### Pros and cons\nLike any metric, NDCG has its pros and cons.\nPros:\n  * **Rank-aware metric**. NDCG considers the entire ranking order and assigns higher weights to relevant items at higher positions. This is useful in many scenarios where you expect the items to be sorted by relevance. This differentiates NDCG from purely predictive quality metrics like [Precision and Recall](https://www.evidentlyai.com/ranking-metrics/precision-recall-at-k). \n  * **Handles both binary and numeral scores.** You can use the NDCG metric with different relevance scales, making it versatile across various applications. This differs from [MAP@K](https://www.evidentlyai.com/ranking-metrics/mean-average-precision-map), which is also rank-aware but can only handle binary inputs.\n  * **Normalized metric.** The normalization aspect of NDCG allows for fair comparisons between lists of varying lengths and relevance distributions. A metric on a scale of 0 to 1 is convenient to deal with. \n\n\nCons: \n  * **Limited interpretability**. NDCG might be harder to interpret since the log discount might appear arbitrary.\n\n\n### NDCG vs MAP\n![NDCG vs MAP](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9e0_65786a8f76d31121ac899381_ndcg11.png)\nNDCG might appear similar to Mean Average Precision (MAP), which is another rank-aware metric that considers the positions of relevant items up to a K. Both NDCG and MAP reflect the quality of ranking, but with some distinctions. \n> **How to compute MAP?** You must calculate and average the Precision (the share of relevant items up to a given position) at each relevant position in top K. Read the [complete guide to MAP](https://www.evidentlyai.com/ranking-metrics/mean-average-precision-map) for a step-by-step explanation.\n**First, MAP works for binary relevance**. MAP is designed to evaluate a ranking or recommender system with a binary relevance function. NDCG can handle both binary and graded relevance. This is useful when you deal with use cases like search: the documents are usually not wholly relevant or irrelevant but exist on a particular scale. \n**Second, NDCG and MAP account for diminishing ranks differently.** Both NDCG and MAP at K consider the order of items and thus are suitable when you care about ranking quality. However, they treat decreasing ranks differently. \nMAP gives more weight to the relevant items at the top of the list. Since the metric is based on Precision values at each relevant position in the ranked list, it is more sensitive to changes in the early positions. Any non-relevant items at the beginning of the list influence the aggregation at each subsequent Precision calculation and contribute to the overall MAP score. MAP drops more rapidly if there are non-relevant items at the top.\nDCG assigns diminishing weights to items as you move down the list, but they are logarithmic. The contribution of items decreases, but not extremely rapidly. Check out [this video](https://www.youtube.com/watch?v=qm1In7NH8WE) for a walkthrough explanation: DCG has a sharper elbow and a heavier tail compared to the more rapid descent of MAP as you move down the K. \nUltimately, the choice between MAP and NDCG depends on the nature of the data and the importance of considering accounting for graded relevance, where NDCG excels. Due to MAP bias towards the top ranks, it is often suitable when the emphasis is on Precision at higher ranks.\n### NDCG vs MRR\n![NDCG vs MRR](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9ca_6578787444d41844e4e93fe6_ndcg12.png)\n**MRR (Mean Reciprocal Rank)** is another ranking metric used in information retrieval and recommendation systems. It focuses on the position of the first relevant item in the ranked list. \n> **How to compute MRR?** MRR is calculated by taking the reciprocal of the rank of the first relevant item and averaging these reciprocal ranks across multiple queries or instances. Read the [complete guide to MRR](https://www.evidentlyai.com/ranking-metrics/mean-reciprocal-rank-mrr) for a step-by-step explanation.\nLet\u2019s compare MRR to NDCG. \n**MRR is very simple to explain.** It provides a straightforward measure of how quickly you find a relevant item in the list.\n**MRR disregards the ranks after the first relevant item**. However, MRR considers only the position of the first relevant item in the list. It does not care what happens after it. This is different from NDCG, which focuses on the entire ranking.\n**MRR handles only binary relevance.** MRR can only handle binary relevance scores and treats the items as either relevant or not. NDCG can work with both binary and numerical relevance scores.\nDepending on the scenario, you might prefer NDCG to MRR or vice versa. MRR aligns well with scenarios where the primary goal is to present the most relevant item as early as possible or when there is a single most relevant and correct answer. This is common in information retrieval. On the other hand, NDCG fits well with scenarios where you care about the order of many relevant items, such as recommendation systems.\nYou can also look at both metrics to evaluate different aspects of the system simultaneously: how quickly you hit the first correct answer (MRR) and how well-ranked the rest of the list is (NDCG).\n## Evaluating ranking with Evidently\n[Evidently](https://github.com/evidentlyai/evidently) is an open-source Python library that helps evaluate, test and monitor machine learning models, including ranking and recommendations. Evidently computes and visualizes 15+ different ranking metrics, from NDCG, MAP and MRR to behavioral metrics like serendipity and diversity. \nBy passing your dataset, you can quickly generate a comprehensive report with multiple metrics and interactive visualizations out of the box. \n![NDCG metric with Evidently AI](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c43276eacc9d38bcbe9e6_657878bbe786a1d088d8de34_ndcg13.png)\nYou can also use Evidently to run CI/CD tests, for example, to evaluate the model quality after retraining. You can also deploy a live monitoring dashboard to keep track of the model metrics and test results over time.\nWould you like to learn more? Check out the open-source [Getting Started tutorials](https://docs.evidentlyai.com).\n##### Get started with AI observability\n> Try our open-source library with over 25 million downloads, or sign up to Evidently Cloud to run no-code checks and bring all the team to a single workspace to collaborate on AI quality.[Sign up free \u27f6](https://www.evidentlyai.com/register)[Or try open source \u27f6](https://github.com/evidentlyai/evidently)\n[RANKING QUALITY GUIDE](https://www.evidentlyai.com/ranking-metrics)\n[Metrics overview](https://www.evidentlyai.com/ranking-metrics/evaluating-recommender-systems)\n[Precision and Recall at K](https://www.evidentlyai.com/ranking-metrics/precision-recall-at-k)\n[Mean Average Precision (MAP)](https://www.evidentlyai.com/ranking-metrics/mean-average-precision-map)\n[Normalized Discounted Cumulative Gain (NDCG)](https://www.evidentlyai.com/ranking-metrics/ndcg-metric)\n[Mean Reciprocal Rank (MRR)](https://www.evidentlyai.com/ranking-metrics/mean-reciprocal-rank-mrr)\nStart testing your AI systems today\n[Get demo](https://www.evidentlyai.com/get-demo)[Sign up](https://www.evidentlyai.com/register)\n[Try open source![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://github.com/evidentlyai/evidently)\nWRITTEN BY\n![Evidently AI logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/663e5a2bef9e93f84fbb5ef1_66266589274806473c5ff4e1_64405645c79bf610ee86e32d_evidently_ai_logo_red_square%2520\\(1\\).png)\n#### Evidently AI Team\nshare on\n[![LinkedIn logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27475f_Group%2068.svg)](https://www.linkedin.com/)[![Twitter logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/665498e176df469709a54190_x-logo%20\\(1\\).svg)](http://twitter.com/)[![Facebook logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc274774_Group%2065.svg)](http://facebook.com)\n## Read next\n[![Mean Average Precision \\(MAP\\) in ranking and recommendations](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c432b08687cce06cf3d01_657787707dbfa09e6cf02810_map_guide-min.png)Mean Average Precision (MAP)Mean Average Precision (MAP) at K reflects both the share of relevant recommendations and how good the system is at placing more relevant items at the top of the list. You can compute it as the mean of Average Precision (AP) across all users or queries.](https://www.evidentlyai.com/ranking-metrics/mean-average-precision-map)\n[![Mean Reciprocal Rank \\(MRR\\) explained](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c4327893f6579e99bc3bb_65787c6e4d178108c353cf02_mrr_guide-min.png)Mean Reciprocal Rank (MRR)The Mean Reciprocal Rank (MRR) at K helps assess the ranking quality by considering the position of the first relevant item in the list. You can calculate it as the mean of the reciprocal ranks across all users or queries.](https://www.evidentlyai.com/ranking-metrics/mean-reciprocal-rank-mrr)\n\ud83c\udf93 Free course on LLM evaluations for AI product teams.[ Sign up **\u27f6**](https://www.evidentlyai.com/llm-evaluations-course)\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66180fbf4f40e9ed73ca2d39_evidently_ai_logo_fi.png)](https://www.evidentlyai.com/)\nProduct\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d2624dc76fe4c62697aff8_chats-teardrop-duotone%20\\(1\\).svg)LLM Testing PlatformEvaluate LLM quality and safety](https://www.evidentlyai.com/llm-testing)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d3354a454c59d5f7affee2_list-magnifying-glass-duotone%20\\(3\\).svg)RAG TestingImprove retrieval, cut hallucinations](https://www.evidentlyai.com/rag-testing)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d2624e2132e631148cfd1a_warning-duotone%20\\(2\\).svg)AI Risk AssessmentIdentify AI risks and get a plan](https://www.evidentlyai.com/llm-readiness)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d33514b0f63efed5f253e9_crosshair-simple-duotone%20\\(1\\).svg)Adversarial TestingTest AI for threats and edge cases](https://www.evidentlyai.com/llm-red-teaming)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6650a6bcbdd0a756edf0d829_chart-bar-duotone%20\\(1\\).svg)ML MonitoringTrack data drift and predictive quality](https://www.evidentlyai.com/ml-monitoring)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9a94702f76fd53c7983b6_tree-structure-duotone%20\\(1\\).svg)AI Agent TestingValidate multi-step workflows](https://www.evidentlyai.com/ai-agent-testing)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7ee755c4dc43cefc2_users-three-duotone%20\\(1\\).svg)Open-SourceOpen-source Evidently Python library](https://www.evidentlyai.com/evidently-oss)\n##### [See Evidently in action![Evidently AI Testing for LLM](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/663511956e05f300480f19fd_llm_2.png)](https://www.evidentlyai.com/llm-evaluations-course)[Get demo now![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/llm-evaluations-course)\n[Pricing](https://www.evidentlyai.com/pricing)[Docs](https://docs.evidentlyai.com/)\nResources\n[![book](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abba265e01ed75b39988c_book-duotone.svg)BlogInsights on building AI products](https://www.evidentlyai.com/blog)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/675a45076643cf7042075aac_star-duotone%20\\(1\\).svg)LLM benchmarks100+ LLM benchmarks and datasets](https://www.evidentlyai.com/llm-evaluation-benchmarks-datasets)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7fbdd19d6d3f1875c_code-duotone%20\\(1\\).svg)TutorialsAI observability and MLOps tutorials](https://www.evidentlyai.com/mlops-tutorials)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb798a210c5d2640337_lightbulb-duotone%20\\(1\\).svg)ML and LLM system design500 ML and LLM use cases](https://www.evidentlyai.com/ml-system-design)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb798a210c5d2640368_check-duotone%20\\(1\\).svg)GuidesIn-depth AI quality and MLOps guides](https://www.evidentlyai.com/mlops-guides)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdbd5bfc045c59b3467f_gear-duotone%20\\(1\\).svg)ML and AI platforms45+ internal ML and AI platforms](https://www.evidentlyai.com/ml-platforms)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7ee755c4dc43cefc2_users-three-duotone%20\\(1\\).svg)CommunityGet support and chat about AI products](https://www.evidentlyai.com/community)\n##### [Course on LLM evaluations for AI product teams![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/673f571568470a0a8d49aa43_three-cheerful-robot-students-take-exam%20\\(4\\).jpg)](https://www.evidentlyai.com/llm-evaluations-course)[Sign up now![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/llm-evaluations-course)\n[Sign up](https://www.evidentlyai.com/register)[Get demo](https://www.evidentlyai.com/get-demo)\n[Sign up](https://www.evidentlyai.com/register)[Get demo](https://www.evidentlyai.com/get-demo)\n![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abe8f5bd7dda2d12a1e6d_list-bold.svg)\n## Start testing your AI systems today\nBook a personalized 1:1 demo with our team or sign up for a free account.\n[Get demo](https://www.evidentlyai.com/get-demo)[Start free](https://www.evidentlyai.com/register)\n![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc274771_Group%2069.svg)\nNo credit card required\n[![Evidently AI logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664ac309d9d1086b0e8309f9_evidently%20logo_white.png)](https://www.evidentlyai.com/)\nEvaluate, test and monitor your AI-powered products.\nSubscribe to our monthly newsletter\nThank you! Your submission has been received!\nOops! Something went wrong while submitting the form.\n[LLM testing platform](https://www.evidentlyai.com/llm-testing)[AI risk assessment](https://www.evidentlyai.com/llm-readiness)[RAG testing](https://www.evidentlyai.com/rag-testing)[Adversarial testing](https://www.evidentlyai.com/llm-red-teaming)[AI Agent testing](https://www.evidentlyai.com/ai-agent-testing)[ML monitoring](https://www.evidentlyai.com/ml-monitoring)[Open-source](https://www.evidentlyai.com/evidently-oss)\n[Blog](https://www.evidentlyai.com/blog)[Tutorials](https://www.evidentlyai.com/mlops-tutorials)[Guides](https://www.evidentlyai.com/mlops-guides)[ML platforms](https://www.evidentlyai.com/ml-platforms)[ML use cases](https://www.evidentlyai.com/ml-system-design)[LLM evaluations course](https://www.evidentlyai.com/llm-evaluations-course)\n[Pricing](https://www.evidentlyai.com/pricing)[Docs](https://docs.evidentlyai.com/)[GitHub](https://github.com/evidentlyai/evidently)[Community](https://www.evidentlyai.com/community)\n[Privacy policy](https://www.evidentlyai.com/privacy)[Terms of service](https://www.evidentlyai.com/terms)\n\u00a9 2025, Evidently AI. All rights reserved\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc274742_Group%2027.svg)](https://www.linkedin.com/company/evidently-ai/)[![Twitter logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664f62d9e2ffd7e31ffae6c8_x-logo-duotone%20\\(1\\).svg)](http://twitter.com/EvidentlyAI)[![Discord logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664f6316d09cc4b5e975db27_discord-logo-duotone%20\\(2\\).svg)](https://discord.com/invite/PyAJuUD5mB)[![YouTube logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664f634afd92ff37de706ab9_youtube-logo-duotone.svg)](https://www.youtube.com/c/evidentlyai)\nBy clicking \u201cAccept\u201d, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts. View our [Privacy Policy](https://www.evidentlyai.com/privacy) for more information.\n[Deny](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/)[Accept](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/)\nPrivacy Preferences\nEssential cookies\nRequired\nMarketing cookies\nEssential\nPersonalization cookies\nEssential\nAnalytics cookies\nEssential\n[Reject all cookies](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/)[Allow all cookies](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/)[Save preferences](https://www.evidentlyai.com/ranking-metrics/ndcg-metric/)\n",
    "answer": "```markdown\n## TL;DR\nNormalized Discounted Cumulative Gain (NDCG) is a ranking metric that evaluates recommendation systems by comparing actual rankings to ideal ones. It accounts for item relevance and position, with values ranging from 0 to 1. NDCG is versatile, handling both binary and graded relevance scores, making it useful for various applications in AI and ML.\n```"
  },
  {
    "instruction": "[![Temporal Logo](https://temporal.io/images/logos/logo-temporal-with-copy-white-text.svg)](https://temporal.io/)\n  * Platform \n    * [Overview](https://temporal.io/product)\n    * [How Temporal Works](https://temporal.io/how-it-works)\n    * [Temporal Cloud](https://temporal.io/cloud)\n  * Platform \n  * [Docs](https://docs.temporal.io/)\n  * [Pricing](https://temporal.io/pricing)\n  * Use Cases \n    * [Financial Services](https://temporal.io/solutions/financial-services)\n    * [Artificial Intelligence](https://temporal.io/solutions/ai)\n    * [Customer Stories](https://temporal.io/in-use)\n  * Use Cases \n  * Resources \n    * [Resource Library](https://temporal.io/resources)\n    * [Learn Temporal](https://learn.temporal.io)\n    * [Community](https://temporal.io/community)\n    * [Code Exchange](https://temporal.io/code-exchange)\n    * [Blog](https://temporal.io/blog)\n    * [For Startups](https://temporal.io/startup)\n    * [Partners](https://temporal.io/partners)\n    * [Change Log](https://temporal.io/change-log)\n  * Resources \n  * [Replay 2025](https://temporal.io/replay/2025)\n[](https://github.com/temporalio)\n[ Try Free ](https://temporal.io/get-cloud)[ Log In ](https://cloud.temporal.io/)\n\n\n#### Durable Execution Platform \n# Build invincible applications\nManage failures, network outages, flaky endpoints, long-running processes and more, ensuring your workflows never fail.\n[ Try Cloud for Free ](https://temporal.io/get-cloud)[ Get Started with OSS ](https://learn.temporal.io/getting_started/)\nWORKFLOW.PSEUDO\ndef process_order(order):\ncheck_fraud(order.order_id, order.payment_info)\nprepare_shipment(order)\ncharge_confirm = charge(order.order_id, order.payment_info)\nshipment_confirmation = ship(order)\nCONSOLE\n# Represented above is a simple order processing program\n# With DURABLE EXECUTION, you code for the single positive result you want to execute\n![Roblox Logo](https://images.ctfassets.net/0uuz8ydxyd9p/3zGEMVpUpbE1fidHeIVE1/b7b50b4f5f12653f4b4bce4cb1f11982/d82801b936c26e174c4e782510a72d78-roblox_logo_dark_08292022.svg)![Qualtrics logo - light](https://images.ctfassets.net/0uuz8ydxyd9p/6KZIJ1pQc69qo9o3bDzKni/58f2725df8b11b5f5c8dfd97b7d5fe90/Qualtrics_logo_-_light.svg)![REA Group Logo](https://images.ctfassets.net/0uuz8ydxyd9p/4Nb7RDUkk1NqP0txn7aNAq/efa0b817fa07d52b2911b911027816a4/Group.svg)![Snap logomark - light](https://images.ctfassets.net/0uuz8ydxyd9p/5gs6C1JatS1f04dKDqqOdd/d33d8b2c9bf114b03833507f165661a0/Snap_logomark_-_light.svg)![Macquarie Logo](https://images.ctfassets.net/0uuz8ydxyd9p/3VToqFWm1KL9yTWqJPFMi1/506650011cdaac612134a1d2bf4b31a6/MQG.AX_BIG.D.svg) ![ANZ White Logo](https://images.ctfassets.net/0uuz8ydxyd9p/5VM05HeGpG9o34Y7XXYFDs/4de3a22236afe8e35e3db9f3f4586700/anzW.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/5VM05HeGpG9o34Y7XXYFDs/4de3a22236afe8e35e3db9f3f4586700/anzW.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/5VM05HeGpG9o34Y7XXYFDs/4de3a22236afe8e35e3db9f3f4586700/anzW.png?h=48&fm=jpg&q=80 2x)![Yum! Brand Logo](https://images.ctfassets.net/0uuz8ydxyd9p/76t50FAZNldtoeNOxrszrx/205e7f935074091f32043ed919bc9fd2/g2503.svg) ![Kotak logo](https://images.ctfassets.net/0uuz8ydxyd9p/4QOY0W93CBtLK0HbffHljE/5bb5fb35159014ce5008f34d84662b2a/kotak.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/4QOY0W93CBtLK0HbffHljE/5bb5fb35159014ce5008f34d84662b2a/kotak.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/4QOY0W93CBtLK0HbffHljE/5bb5fb35159014ce5008f34d84662b2a/kotak.png?h=48&fm=jpg&q=80 2x)![Vodafone Logo](https://images.ctfassets.net/0uuz8ydxyd9p/71dlhPtA8tJxVW6xfmW5TO/e59f54f0cb7aa88e991b34a6e57c7d24/Vodafone_2017_logo_1.svg)![Cloudflare Logo](https://images.ctfassets.net/0uuz8ydxyd9p/2Kw3v1c4ExNMaferYM6Wbn/7ecc3a519ed1206f6cd44d781536b6a8/NET_BIG.W.svg)![comcast logo png white](https://images.ctfassets.net/0uuz8ydxyd9p/6jEM9XT6xKcPoI3L9PGxm3/9a9c20a2396413f5988dff1c99a413c3/Group__1_.svg)![GitLab Logo](https://images.ctfassets.net/0uuz8ydxyd9p/t6vElMorqJb728juRdtWG/b21edde0e2ba3f374818a312751bc78e/LOGO__1_.svg)![Remitly Logo](https://images.ctfassets.net/0uuz8ydxyd9p/4GDIW1m1EuLnPnRVo7ZU8n/99a6af8482d8e686d35cfd6debd6fea5/Remitly_logo_-_light.svg) ![Nvidia Logo White png](https://images.ctfassets.net/0uuz8ydxyd9p/3jb3ILwfcpE0al3YEuTgoa/b14723dfc4e3feab527ff91fc1930fc6/nvidiaw.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/3jb3ILwfcpE0al3YEuTgoa/b14723dfc4e3feab527ff91fc1930fc6/nvidiaw.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/3jb3ILwfcpE0al3YEuTgoa/b14723dfc4e3feab527ff91fc1930fc6/nvidiaw.png?h=48&fm=jpg&q=80 2x) ![Alaska Airlines logo](https://images.ctfassets.net/0uuz8ydxyd9p/2hOL3iDASXRPZcrpgEOO9K/79f86136df38a4b40d095d4ee315ff79/alaskaAirlinesW.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/2hOL3iDASXRPZcrpgEOO9K/79f86136df38a4b40d095d4ee315ff79/alaskaAirlinesW.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/2hOL3iDASXRPZcrpgEOO9K/79f86136df38a4b40d095d4ee315ff79/alaskaAirlinesW.png?h=48&fm=jpg&q=80 2x)![Bentley Systems Logo](https://images.ctfassets.net/0uuz8ydxyd9p/55CawRvmiIwzfh3dfMpIbs/0f5ef6fb3263fc906b07611be9b33d32/bentley-logo-white-400x100-1.svg)![Bestseller Logo](https://images.ctfassets.net/0uuz8ydxyd9p/5BnK9hlwx1j7pqnb7tgK2a/3021eb16cde9887c1e17c1b30783e807/path3384.svg)![Deloitte](https://images.ctfassets.net/0uuz8ydxyd9p/3gJ8sBPuYbrK1h3qxIfdZx/33cf10c9f035ce0ebd8489c871c12507/Deloitte.svg) ![GoDaddy Logo](https://images.ctfassets.net/0uuz8ydxyd9p/tHMcuiabjlHCsPDPN1nVb/ae0b75ce528ed89de8120d8e425dd6a7/godaddy-logo-2048x515.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/tHMcuiabjlHCsPDPN1nVb/ae0b75ce528ed89de8120d8e425dd6a7/godaddy-logo-2048x515.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/tHMcuiabjlHCsPDPN1nVb/ae0b75ce528ed89de8120d8e425dd6a7/godaddy-logo-2048x515.png?h=48&fm=jpg&q=80 2x)![Doordash Logo](https://images.ctfassets.net/0uuz8ydxyd9p/4fLTm2EneQaAy01Dolb70T/f93405cfc9b46705f96ea53db3e88cae/DASH_BIG.D.svg) ![Retool Logo white png](https://images.ctfassets.net/0uuz8ydxyd9p/1kgD0Yz7rFq5f8LAD1FSl6/18a5c8a7f2971d6e83cad362ab0c8fd7/retoolw.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/1kgD0Yz7rFq5f8LAD1FSl6/18a5c8a7f2971d6e83cad362ab0c8fd7/retoolw.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/1kgD0Yz7rFq5f8LAD1FSl6/18a5c8a7f2971d6e83cad362ab0c8fd7/retoolw.png?h=48&fm=jpg&q=80 2x)![Checkr Logo](https://images.ctfassets.net/0uuz8ydxyd9p/4hjC77Y1Wv8BshJxBKYWdZ/00c62f50c0b6635a2a34b7247278d981/checkr_BIG.D.svg)![Turo logo - light](https://images.ctfassets.net/0uuz8ydxyd9p/34vCpXmfPW6VxvojJtpNtX/2748544c6f679b2f4566efdeaadfe3f5/Turo_logo_-_light.svg)![Mollie Logo](https://images.ctfassets.net/0uuz8ydxyd9p/2ADYihv1lhL9pfq1wvMHDW/bac315dbf13f4a540cf9a5e4ce4d83e2/Mollie-Logo-White-2023.svg)![Kyte Logo](https://images.ctfassets.net/0uuz8ydxyd9p/6WJBpglwt32wVndGRZyk9r/74b8b9ac9f38827d5566a21b34822d32/kyte-logo-white_2.svg) ![GoDaddy Logo](https://images.ctfassets.net/0uuz8ydxyd9p/tHMcuiabjlHCsPDPN1nVb/ae0b75ce528ed89de8120d8e425dd6a7/godaddy-logo-2048x515.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/tHMcuiabjlHCsPDPN1nVb/ae0b75ce528ed89de8120d8e425dd6a7/godaddy-logo-2048x515.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/tHMcuiabjlHCsPDPN1nVb/ae0b75ce528ed89de8120d8e425dd6a7/godaddy-logo-2048x515.png?h=48&fm=jpg&q=80 2x)\n![Roblox Logo](https://images.ctfassets.net/0uuz8ydxyd9p/3zGEMVpUpbE1fidHeIVE1/b7b50b4f5f12653f4b4bce4cb1f11982/d82801b936c26e174c4e782510a72d78-roblox_logo_dark_08292022.svg)![Qualtrics logo - light](https://images.ctfassets.net/0uuz8ydxyd9p/6KZIJ1pQc69qo9o3bDzKni/58f2725df8b11b5f5c8dfd97b7d5fe90/Qualtrics_logo_-_light.svg)![REA Group Logo](https://images.ctfassets.net/0uuz8ydxyd9p/4Nb7RDUkk1NqP0txn7aNAq/efa0b817fa07d52b2911b911027816a4/Group.svg)![Snap logomark - light](https://images.ctfassets.net/0uuz8ydxyd9p/5gs6C1JatS1f04dKDqqOdd/d33d8b2c9bf114b03833507f165661a0/Snap_logomark_-_light.svg)![Macquarie Logo](https://images.ctfassets.net/0uuz8ydxyd9p/3VToqFWm1KL9yTWqJPFMi1/506650011cdaac612134a1d2bf4b31a6/MQG.AX_BIG.D.svg) ![ANZ White Logo](https://images.ctfassets.net/0uuz8ydxyd9p/5VM05HeGpG9o34Y7XXYFDs/4de3a22236afe8e35e3db9f3f4586700/anzW.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/5VM05HeGpG9o34Y7XXYFDs/4de3a22236afe8e35e3db9f3f4586700/anzW.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/5VM05HeGpG9o34Y7XXYFDs/4de3a22236afe8e35e3db9f3f4586700/anzW.png?h=48&fm=jpg&q=80 2x)![Yum! Brand Logo](https://images.ctfassets.net/0uuz8ydxyd9p/76t50FAZNldtoeNOxrszrx/205e7f935074091f32043ed919bc9fd2/g2503.svg) ![Kotak logo](https://images.ctfassets.net/0uuz8ydxyd9p/4QOY0W93CBtLK0HbffHljE/5bb5fb35159014ce5008f34d84662b2a/kotak.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/4QOY0W93CBtLK0HbffHljE/5bb5fb35159014ce5008f34d84662b2a/kotak.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/4QOY0W93CBtLK0HbffHljE/5bb5fb35159014ce5008f34d84662b2a/kotak.png?h=48&fm=jpg&q=80 2x)![Vodafone Logo](https://images.ctfassets.net/0uuz8ydxyd9p/71dlhPtA8tJxVW6xfmW5TO/e59f54f0cb7aa88e991b34a6e57c7d24/Vodafone_2017_logo_1.svg)![Cloudflare Logo](https://images.ctfassets.net/0uuz8ydxyd9p/2Kw3v1c4ExNMaferYM6Wbn/7ecc3a519ed1206f6cd44d781536b6a8/NET_BIG.W.svg)![comcast logo png white](https://images.ctfassets.net/0uuz8ydxyd9p/6jEM9XT6xKcPoI3L9PGxm3/9a9c20a2396413f5988dff1c99a413c3/Group__1_.svg)![GitLab Logo](https://images.ctfassets.net/0uuz8ydxyd9p/t6vElMorqJb728juRdtWG/b21edde0e2ba3f374818a312751bc78e/LOGO__1_.svg)![Remitly Logo](https://images.ctfassets.net/0uuz8ydxyd9p/4GDIW1m1EuLnPnRVo7ZU8n/99a6af8482d8e686d35cfd6debd6fea5/Remitly_logo_-_light.svg) ![Nvidia Logo White png](https://images.ctfassets.net/0uuz8ydxyd9p/3jb3ILwfcpE0al3YEuTgoa/b14723dfc4e3feab527ff91fc1930fc6/nvidiaw.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/3jb3ILwfcpE0al3YEuTgoa/b14723dfc4e3feab527ff91fc1930fc6/nvidiaw.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/3jb3ILwfcpE0al3YEuTgoa/b14723dfc4e3feab527ff91fc1930fc6/nvidiaw.png?h=48&fm=jpg&q=80 2x) ![Alaska Airlines logo](https://images.ctfassets.net/0uuz8ydxyd9p/2hOL3iDASXRPZcrpgEOO9K/79f86136df38a4b40d095d4ee315ff79/alaskaAirlinesW.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/2hOL3iDASXRPZcrpgEOO9K/79f86136df38a4b40d095d4ee315ff79/alaskaAirlinesW.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/2hOL3iDASXRPZcrpgEOO9K/79f86136df38a4b40d095d4ee315ff79/alaskaAirlinesW.png?h=48&fm=jpg&q=80 2x)![Bentley Systems Logo](https://images.ctfassets.net/0uuz8ydxyd9p/55CawRvmiIwzfh3dfMpIbs/0f5ef6fb3263fc906b07611be9b33d32/bentley-logo-white-400x100-1.svg)![Bestseller Logo](https://images.ctfassets.net/0uuz8ydxyd9p/5BnK9hlwx1j7pqnb7tgK2a/3021eb16cde9887c1e17c1b30783e807/path3384.svg)![Deloitte](https://images.ctfassets.net/0uuz8ydxyd9p/3gJ8sBPuYbrK1h3qxIfdZx/33cf10c9f035ce0ebd8489c871c12507/Deloitte.svg) ![GoDaddy Logo](https://images.ctfassets.net/0uuz8ydxyd9p/tHMcuiabjlHCsPDPN1nVb/ae0b75ce528ed89de8120d8e425dd6a7/godaddy-logo-2048x515.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/tHMcuiabjlHCsPDPN1nVb/ae0b75ce528ed89de8120d8e425dd6a7/godaddy-logo-2048x515.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/tHMcuiabjlHCsPDPN1nVb/ae0b75ce528ed89de8120d8e425dd6a7/godaddy-logo-2048x515.png?h=48&fm=jpg&q=80 2x)![Doordash Logo](https://images.ctfassets.net/0uuz8ydxyd9p/4fLTm2EneQaAy01Dolb70T/f93405cfc9b46705f96ea53db3e88cae/DASH_BIG.D.svg) ![Retool Logo white png](https://images.ctfassets.net/0uuz8ydxyd9p/1kgD0Yz7rFq5f8LAD1FSl6/18a5c8a7f2971d6e83cad362ab0c8fd7/retoolw.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/1kgD0Yz7rFq5f8LAD1FSl6/18a5c8a7f2971d6e83cad362ab0c8fd7/retoolw.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/1kgD0Yz7rFq5f8LAD1FSl6/18a5c8a7f2971d6e83cad362ab0c8fd7/retoolw.png?h=48&fm=jpg&q=80 2x)![Checkr Logo](https://images.ctfassets.net/0uuz8ydxyd9p/4hjC77Y1Wv8BshJxBKYWdZ/00c62f50c0b6635a2a34b7247278d981/checkr_BIG.D.svg)![Turo logo - light](https://images.ctfassets.net/0uuz8ydxyd9p/34vCpXmfPW6VxvojJtpNtX/2748544c6f679b2f4566efdeaadfe3f5/Turo_logo_-_light.svg)![Mollie Logo](https://images.ctfassets.net/0uuz8ydxyd9p/2ADYihv1lhL9pfq1wvMHDW/bac315dbf13f4a540cf9a5e4ce4d83e2/Mollie-Logo-White-2023.svg)![Kyte Logo](https://images.ctfassets.net/0uuz8ydxyd9p/6WJBpglwt32wVndGRZyk9r/74b8b9ac9f38827d5566a21b34822d32/kyte-logo-white_2.svg) ![GoDaddy Logo](https://images.ctfassets.net/0uuz8ydxyd9p/tHMcuiabjlHCsPDPN1nVb/ae0b75ce528ed89de8120d8e425dd6a7/godaddy-logo-2048x515.png?h=24&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/tHMcuiabjlHCsPDPN1nVb/ae0b75ce528ed89de8120d8e425dd6a7/godaddy-logo-2048x515.png?h=36&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/tHMcuiabjlHCsPDPN1nVb/ae0b75ce528ed89de8120d8e425dd6a7/godaddy-logo-2048x515.png?h=48&fm=jpg&q=80 2x)\n### Code business logic, not plumbing\nTemporal delivers durable execution. It abstracts away the complexity of building scalable distributed systems and lets you keep focus on what matters \u2013 delivering reliable systems, faster.\nIt allows you to avoid coding for infrastructure nuances and their inevitable failures.\n![Open Source Initiative Logo](https://temporal.io/images/open-source-initiative.svg) [ 13,459 Stars ](https://github.com/temporalio/temporal)\n![Home Page Protection Graphic](https://images.ctfassets.net/0uuz8ydxyd9p/1zFSzjG64ODcEU7GkXMRcQ/7977f23e1cd03f49634a6e0d7174c5ad/home-page-protection-graphic.png?fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/1zFSzjG64ODcEU7GkXMRcQ/7977f23e1cd03f49634a6e0d7174c5ad/home-page-protection-graphic.png?fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/1zFSzjG64ODcEU7GkXMRcQ/7977f23e1cd03f49634a6e0d7174c5ad/home-page-protection-graphic.png?fm=jpg&q=80 2x)\nWhat if you could avoid the complexity of failure and focus on only what matters?\n[ Explore Temporal ](https://temporal.io/product)\nShip more features, faster\nTemporal eliminates recovery logic, callbacks, and timers from your code so you can spend more time building features.\nCode more reliable systems\nTemporal makes your software durable and fault tolerant by default, reducing failures by 10-100X.\nGain visibility into execution\nTemporal records every execution, its progress and state, providing new insights into errors and app performance.\n### 100% open, 100% open source\nTemporal is built in the open and released under the MIT license. It has been endorsed by some of the world's best companies and is loved by a growing, vibrant community. \n[ Set Up Your Local Environment ](https://temporal.io/setup/install-temporal-cli)[ View the Repo ](https://github.com/temporalio/temporal)\nDon't fight development patterns, ship code\nService development patterns try to hide complexity with pictures and concepts, but leave developers to do the dirty work. Temporal abstracts away the complexity of infrastructure and delivers the value you want from these development patterns, without the heavy implementation burden.\nEvent-driven Architecture SAGA & Distributed Transactions State Machines Batch Processing Scheduled Jobs & Cron\nEvent-driven Architecture\n### Event-Driven Architectures\nEvent-driven architectures and queue-driven design promise ease of deployment and scale, but are a development nightmare.\n[Read now: Event-Driven Architectures - A better way forward \u203a](https://pages.temporal.io/whitepaper-event-driven-architectures)**Without Temporal** Event-driven applications are loosely coupled at runtime, but highly coupled at build time. This creates great complexity for error handling and propagation of state across disparate services.**With Temporal** Application state, retries, and error handling are abstracted away so that you no longer have to code for them. System testing is a breeze because Temporal eliminates common failure scenarios.\n![EDA Diagram](https://images.ctfassets.net/0uuz8ydxyd9p/4pPIlt0w80BbF6mqGrEjO/6ef14535ea80f8b686d1fe731c2d49a6/EDA_Diagram_3x.png?h=400&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/4pPIlt0w80BbF6mqGrEjO/6ef14535ea80f8b686d1fe731c2d49a6/EDA_Diagram_3x.png?h=600&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/4pPIlt0w80BbF6mqGrEjO/6ef14535ea80f8b686d1fe731c2d49a6/EDA_Diagram_3x.png?h=800&fm=jpg&q=80 2x)\nSAGA & Distributed Transactions\n### SAGA & Distributed Transactions\nThe saga pattern ensures safe and consistent state across distributed services so that the failure of one operation within a sequence results in prior operations being reversed using compensating transactions.[Read now: Automating the Saga Pattern with Temporal \u203a](https://pages.temporal.io/download-saga-pattern-made-easy)**Without Temporal** Saga typically requires significant forethought as there is no central system to manage coordination and visibility. Debugging unexpected failure modes can be treacherous.**With Temporal** Workflow definitions allow you to more easily understand, debug, and modify sagas. Tighter coupling for compensations increases and your application code remains clean.\n![Saga Diagram](https://images.ctfassets.net/0uuz8ydxyd9p/2mMzBU1t83AK26SqNpLni7/132764fafb8533bf617c5c701560fa13/Saga_Diagram_3x.png?h=400&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/2mMzBU1t83AK26SqNpLni7/132764fafb8533bf617c5c701560fa13/Saga_Diagram_3x.png?h=600&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/2mMzBU1t83AK26SqNpLni7/132764fafb8533bf617c5c701560fa13/Saga_Diagram_3x.png?h=800&fm=jpg&q=80 2x)\nState Machines\n### State Machines\nState machines are often used to define and manage valid state and transitions for entities within your application and depending on your application, they can be quite complex.\n[Read now: State Machines Simplified \u203a](https://pages.temporal.io/download-state-machines-simplified)**Without Temporal** State machine code grows in complexity and length with the addition of each new state and maintenance and testing of them can be a challenge.**With Temporal** Complete state of your function and workflow is captured, so you no longer need to automate, track and validate state so you can eliminate or avoid state machines.\n![State Machine Diagram](https://images.ctfassets.net/0uuz8ydxyd9p/64YIKRN1Dype1XpU6OyeU0/6c6e2d21b3d7d87fb0de5aae8216d45d/State_Machine_Diagram_3x.png?h=400&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/64YIKRN1Dype1XpU6OyeU0/6c6e2d21b3d7d87fb0de5aae8216d45d/State_Machine_Diagram_3x.png?h=600&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/64YIKRN1Dype1XpU6OyeU0/6c6e2d21b3d7d87fb0de5aae8216d45d/State_Machine_Diagram_3x.png?h=800&fm=jpg&q=80 2x)\nBatch Processing\n### Batch Processing\nBatch Processes are created to execute a well-defined function across large or even small sets of data.**Without Temporal** Often, batch processes can be quite large and when they fail, you have limited or no insight into where they failed, what completed and what hasn't.**With Temporal** Each execution within the batch process becomes a Workflow Execution with state captured, so that upon failure, you have insight into what completed and where to restart the process.\n![Batch Processing Diagram](https://images.ctfassets.net/0uuz8ydxyd9p/3EqTUAO0CEdjOYNi4LsYlp/db362e4433a478ca3c6a6ae03e44654c/Batch_Processing_Diagram_3x.png?h=400&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/3EqTUAO0CEdjOYNi4LsYlp/db362e4433a478ca3c6a6ae03e44654c/Batch_Processing_Diagram_3x.png?h=600&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/3EqTUAO0CEdjOYNi4LsYlp/db362e4433a478ca3c6a6ae03e44654c/Batch_Processing_Diagram_3x.png?h=800&fm=jpg&q=80 2x)\nScheduled Jobs & Cron\n### Scheduled Jobs & Cron\nFor years, we have relied on Cron to schedule jobs to be executed at a certain time or regular interval.**Without Temporal** Cron is a largely manual process that can be unreliable and provides limited to no controls over execution.**With Temporal** You can replace Cron with Scheduled Workflows to be reliably executed. You can start and pause them and even set up signals to start them on command for a Workflow.\n![Schedules Diagram](https://images.ctfassets.net/0uuz8ydxyd9p/6m8IbVkFH3P4apCpY1aWkL/d4f49a164ed9a15a30bc68d2b2f92bc1/Schedules_Diagram_3x.png?h=400&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/6m8IbVkFH3P4apCpY1aWkL/d4f49a164ed9a15a30bc68d2b2f92bc1/Schedules_Diagram_3x.png?h=600&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/6m8IbVkFH3P4apCpY1aWkL/d4f49a164ed9a15a30bc68d2b2f92bc1/Schedules_Diagram_3x.png?h=800&fm=jpg&q=80 2x)\nOne of the most interesting pieces of tech I've seen in years\u2026 Temporal does to backend and infra, what React did to frontend\u2026 the surface exposed to the developer is a beautiful \"render()\" function to organize your backend workflows.\n![Guillermo Rauch, Founder & CEO at Vercel](https://images.ctfassets.net/0uuz8ydxyd9p/35XpuMxd7XM0cxwHPs7hnw/f2e999228beb25a18bfdcf3969f91689/guillermo-rauch.png?w=104&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/35XpuMxd7XM0cxwHPs7hnw/f2e999228beb25a18bfdcf3969f91689/guillermo-rauch.png?w=156&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/35XpuMxd7XM0cxwHPs7hnw/f2e999228beb25a18bfdcf3969f91689/guillermo-rauch.png?w=208&fm=jpg&q=80 2x)\nGuillermo Rauch\nFounder & CEO, Vercel\n![Vercel](https://images.ctfassets.net/0uuz8ydxyd9p/4MDoOIS8XDGmajdIvnygpf/5c7f28033a8677c9ed60e5f8185c1ec7/Vercel_logo_-_dark.svg)\nTemporal's technology satisfied all of these requirements out of the box and allowed our developers to focus on business logic.\nWithout Temporal's technology, we would've spent a significant amount of time rebuilding Temporal and would've very likely done a worse job.\n![Mitchell Hashimoto, Co-founder at Hashicorp](https://images.ctfassets.net/0uuz8ydxyd9p/5DKr4Le66oqIonKUzZC0cA/5774165e77d157ad777141476f782e28/mitchell-hashimoto.png?w=104&fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/5DKr4Le66oqIonKUzZC0cA/5774165e77d157ad777141476f782e28/mitchell-hashimoto.png?w=156&fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/5DKr4Le66oqIonKUzZC0cA/5774165e77d157ad777141476f782e28/mitchell-hashimoto.png?w=208&fm=jpg&q=80 2x)\nMitchell Hashimoto\nCo-founder, Hashicorp\n![Hashicorp](https://images.ctfassets.net/0uuz8ydxyd9p/7Bk8auMRdp1TkfTLCh3G8J/3f8ea1401439e4b1b5015e86c60dd2f5/Hashicorp_logo_-_dark.svg)\n### Temporal works the way you work\nTemporal works with your preexisting choices for runtime, test framework, deployment, continuous integration, web frameworks, and more.\n[ Contact Us ](https://pages.temporal.io/contact-us)\n![Temporal works the way you work](https://images.ctfassets.net/0uuz8ydxyd9p/3TOOv2DUCgMhCJB1joBF1E/381abc9e00cceb067a855be583f84e48/temporal-works-the-way-you-work.png?fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/3TOOv2DUCgMhCJB1joBF1E/381abc9e00cceb067a855be583f84e48/temporal-works-the-way-you-work.png?fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/3TOOv2DUCgMhCJB1joBF1E/381abc9e00cceb067a855be583f84e48/temporal-works-the-way-you-work.png?fm=jpg&q=80 2x)\n### ...in your language\nTemporal allows you to develop with durable execution in different languages and multiple languages can be used to build single services, enabling polyglot development.\n[ View SDKs ](https://docs.temporal.io/dev-guide)\n![Code languages](https://images.ctfassets.net/0uuz8ydxyd9p/6bvrcHKmc7T739at7UMbM/99fa460df42531c7e1a43692979f2ed7/code_languages.png?fm=jpg&q=80, https://images.ctfassets.net/0uuz8ydxyd9p/6bvrcHKmc7T739at7UMbM/99fa460df42531c7e1a43692979f2ed7/code_languages.png?fm=jpg&q=80 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/6bvrcHKmc7T739at7UMbM/99fa460df42531c7e1a43692979f2ed7/code_languages.png?fm=jpg&q=80 2x)\n...and in your applications.\nTemporal is used for a wide variety of applications from transaction processing to control planes to applied AI to content pipelines and more. It's useful anywhere you want to guarantee reliability and correctness.\nPayment Processing Artificial Intelligence Customer Onboarding Order Management & Bookings Infrastructure Management Identity Verification\nPayment Processing\nTechnical Guide\n[ ![saga-image](https://images.ctfassets.net/0uuz8ydxyd9p/3rCJMWpnUkTTWW7OZvtH8Z/5fe079f365777bb393a0ec9d8f3b9bdb/saga-image.png?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/3rCJMWpnUkTTWW7OZvtH8Z/5fe079f365777bb393a0ec9d8f3b9bdb/saga-image.png?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/3rCJMWpnUkTTWW7OZvtH8Z/5fe079f365777bb393a0ec9d8f3b9bdb/saga-image.png?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://learn.temporal.io/tutorials/typescript/subscriptions/)\nTypeScript\n[Build a recurring billing subscription workflow with TypeScript](https://learn.temporal.io/tutorials/typescript/subscriptions/)\n[View Technical Guide ](https://learn.temporal.io/tutorials/typescript/subscriptions/)\nReplay 2023\n[ ![hero generic](https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/building-scalable-reliable-bank-payments-on-temporal)\nFinancial Services\n[Building Scalable, Reliable Bank Payments on Temporal at SoFi](https://temporal.io/resources/on-demand/building-scalable-reliable-bank-payments-on-temporal)\n[View Talk ](https://temporal.io/resources/on-demand/building-scalable-reliable-bank-payments-on-temporal)\nReplay 2023\n[ ![hero generic](https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/real-time-payments-with-temporal)\nFinancial Services\n[Afterpay: Real-time Payments with Temporal](https://temporal.io/resources/on-demand/real-time-payments-with-temporal)\n[View Talk ](https://temporal.io/resources/on-demand/real-time-payments-with-temporal)\nReplay 2023\n[ ![hero generic](https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/how-will-bank-leverages-temporal-to-handle-2-million-customers)\nFinancial Services\n[How Will Bank leverages Temporal to handle 2 million customers](https://temporal.io/resources/on-demand/how-will-bank-leverages-temporal-to-handle-2-million-customers)\n[View Talk ](https://temporal.io/resources/on-demand/how-will-bank-leverages-temporal-to-handle-2-million-customers)\nArtificial Intelligence\nWebinar\n[ ![Build an AI Agent with Temporal Screenshot](https://images.ctfassets.net/0uuz8ydxyd9p/4W169n6mKZxHtY1GmwCmDb/9cb4caff1581fe7b597ad7ef56863a07/Build_an_AI_Agent_with_Temporal_2-12_screenshot.png?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/4W169n6mKZxHtY1GmwCmDb/9cb4caff1581fe7b597ad7ef56863a07/Build_an_AI_Agent_with_Temporal_2-12_screenshot.png?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/4W169n6mKZxHtY1GmwCmDb/9cb4caff1581fe7b597ad7ef56863a07/Build_an_AI_Agent_with_Temporal_2-12_screenshot.png?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/demo-ai-agent)\nPython\n[Demo: Build an AI Agent with Temporal](https://temporal.io/resources/on-demand/demo-ai-agent)\n[View Webinar ](https://temporal.io/resources/on-demand/demo-ai-agent)\nCase Study\n[ ![dubber png white](https://images.ctfassets.net/0uuz8ydxyd9p/492bukBPX96Fip0NJ7jo1X/2738a29becb1f199c7cb919b99db07f7/dubberW.png?w=480&fm=jpg&q=80&fit=pad, https://images.ctfassets.net/0uuz8ydxyd9p/492bukBPX96Fip0NJ7jo1X/2738a29becb1f199c7cb919b99db07f7/dubberW.png?w=720&fm=jpg&q=80&fit=pad 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/492bukBPX96Fip0NJ7jo1X/2738a29becb1f199c7cb919b99db07f7/dubberW.png?w=960&fm=jpg&q=80&fit=pad 2x)](https://temporal.io/resources/case-studies/dubber)\nHigh Tech\nJava\n[Dubber, Temporal, and Conversational AI at Scale](https://temporal.io/resources/case-studies/dubber)\n[View Case Study ](https://temporal.io/resources/case-studies/dubber)\nReplay 2024\n[ ![Replay 2024 Day 2 456 Anton Tsitou 1](https://images.ctfassets.net/0uuz8ydxyd9p/5oMya5krlsz9O8XyPBLALZ/efefcde7d8f1b6b6fdf089a08fce6e39/Replay_2024_Day_2_456__Anton_Tsitou_1.png?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/5oMya5krlsz9O8XyPBLALZ/efefcde7d8f1b6b6fdf089a08fce6e39/Replay_2024_Day_2_456__Anton_Tsitou_1.png?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/5oMya5krlsz9O8XyPBLALZ/efefcde7d8f1b6b6fdf089a08fce6e39/Replay_2024_Day_2_456__Anton_Tsitou_1.png?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/practical-tactical-approach-temporal-ai)\n[Spiral Scout: A Practical and Tactical Approach to Temporal and AI](https://temporal.io/resources/on-demand/practical-tactical-approach-temporal-ai)\n[View Talk ](https://temporal.io/resources/on-demand/practical-tactical-approach-temporal-ai)\nReplay 2024\n[ ![hands-pink](https://images.ctfassets.net/0uuz8ydxyd9p/wFfgclawP3VyCdXAmdkQD/33e1515fcc08b01ce09d4484df8b6c42/hands-pink.png?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/wFfgclawP3VyCdXAmdkQD/33e1515fcc08b01ce09d4484df8b6c42/hands-pink.png?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/wFfgclawP3VyCdXAmdkQD/33e1515fcc08b01ce09d4484df8b6c42/hands-pink.png?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/account-summaries-gen-ai)\n[ZoomInfo: Using Temporal to Create Account Summaries utilizing GenAI](https://temporal.io/resources/on-demand/account-summaries-gen-ai)\n[View Talk ](https://temporal.io/resources/on-demand/account-summaries-gen-ai)\nCustomer Onboarding\nReplay 2023\n[ ![hero generic](https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/re-building-async-processing-at-rippling-using-temporal)\nHigh Tech\n[Re-Building Async Processing Using Temporal at Rippling](https://temporal.io/resources/on-demand/re-building-async-processing-at-rippling-using-temporal)\n[View Talk ](https://temporal.io/resources/on-demand/re-building-async-processing-at-rippling-using-temporal)\nReplay 2023\n[ ![Retool Hero](https://images.ctfassets.net/0uuz8ydxyd9p/6TD5MI7Hum8YPVJDXlPo2H/c71b86f611e4295b6e88f7656ea4d0c1/hero_retool.jpg?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/6TD5MI7Hum8YPVJDXlPo2H/c71b86f611e4295b6e88f7656ea4d0c1/hero_retool.jpg?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/6TD5MI7Hum8YPVJDXlPo2H/c71b86f611e4295b6e88f7656ea4d0c1/hero_retool.jpg?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/orchestrating-complex-customer-defined-dags-with-temporal)\n[Retool: Orchestrating complex customer-defined DAGS with Temporal](https://temporal.io/resources/on-demand/orchestrating-complex-customer-defined-dags-with-temporal)\n[View Talk ](https://temporal.io/resources/on-demand/orchestrating-complex-customer-defined-dags-with-temporal)\nOrder Management & Bookings\nWebinar\n[ ![Video thumbnail of Live Demo](https://images.ctfassets.net/0uuz8ydxyd9p/4anEuuWEqb2kmSCSQTjspQ/41f039fe7464d30f4b2abec87b315f97/Screenshot_2024-11-15_at_10.37.04_AM.png?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/4anEuuWEqb2kmSCSQTjspQ/41f039fe7464d30f4b2abec87b315f97/Screenshot_2024-11-15_at_10.37.04_AM.png?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/4anEuuWEqb2kmSCSQTjspQ/41f039fe7464d30f4b2abec87b315f97/Screenshot_2024-11-15_at_10.37.04_AM.png?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/demo-failure-handling)\n[Temporal Live Coding Demo: Handling Failures & Human Interaction](https://temporal.io/resources/on-demand/demo-failure-handling)\n[View Webinar ](https://temporal.io/resources/on-demand/demo-failure-handling)\nReplay 2023\n[ ![Yum Brands Hero](https://images.ctfassets.net/0uuz8ydxyd9p/201ol8a1ynNTwKjHBN6Gvq/00871e68a93ab122ced8419b85dbc1fb/hero_matt_yum.jpg?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/201ol8a1ynNTwKjHBN6Gvq/00871e68a93ab122ced8419b85dbc1fb/hero_matt_yum.jpg?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/201ol8a1ynNTwKjHBN6Gvq/00871e68a93ab122ced8419b85dbc1fb/hero_matt_yum.jpg?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/temporal-at-yum-brands-a-year-later)\n[Temporal at Yum! Brands: A Year Later](https://temporal.io/resources/on-demand/temporal-at-yum-brands-a-year-later)\n[View Talk ](https://temporal.io/resources/on-demand/temporal-at-yum-brands-a-year-later)\nReplay 2023\n[ ![hero generic](https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/building-a-time-machine-for-the-logistics-industry)\nLogistics\n[Maersk: Building a Time Machine for the Logistics Industry](https://temporal.io/resources/on-demand/building-a-time-machine-for-the-logistics-industry)\n[View Talk ](https://temporal.io/resources/on-demand/building-a-time-machine-for-the-logistics-industry)\nReplay 2022\n[ ![Yum Brands Hero](https://images.ctfassets.net/0uuz8ydxyd9p/201ol8a1ynNTwKjHBN6Gvq/00871e68a93ab122ced8419b85dbc1fb/hero_matt_yum.jpg?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/201ol8a1ynNTwKjHBN6Gvq/00871e68a93ab122ced8419b85dbc1fb/hero_matt_yum.jpg?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/201ol8a1ynNTwKjHBN6Gvq/00871e68a93ab122ced8419b85dbc1fb/hero_matt_yum.jpg?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/temporal-at-yum-brands)\nRetail\n[Chicken, Pizza, and Tacos, Oh My! Temporal at Yum! Brands](https://temporal.io/resources/on-demand/temporal-at-yum-brands)\n[View Talk ](https://temporal.io/resources/on-demand/temporal-at-yum-brands)\nInfrastructure Management\nWebinar\n[![netflix](https://images.ctfassets.net/0uuz8ydxyd9p/2pUduKFFebpttU1j9GG550/45704a2049fd62e658e79421447d4b80/netflix.svg)](https://temporal.io/resources/on-demand/netflix)\nHigh Tech\nGo\n[How Netflix uses Temporal](https://temporal.io/resources/on-demand/netflix)\n[View Webinar ](https://temporal.io/resources/on-demand/netflix)\nCase Study\n[ ![Maersk Logo white png](https://images.ctfassets.net/0uuz8ydxyd9p/7lpINsVnWbQA6vu2eaO2eE/2ff951d800144fd1d6a4707b69ee13a7/Maerskw.png?w=480&fm=jpg&q=80&fit=pad, https://images.ctfassets.net/0uuz8ydxyd9p/7lpINsVnWbQA6vu2eaO2eE/2ff951d800144fd1d6a4707b69ee13a7/Maerskw.png?w=720&fm=jpg&q=80&fit=pad 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/7lpINsVnWbQA6vu2eaO2eE/2ff951d800144fd1d6a4707b69ee13a7/Maerskw.png?w=960&fm=jpg&q=80&fit=pad 2x)](https://temporal.io/resources/case-studies/maersk)\nLogistics\nJava\n[How Maersk Built a \u201cTime Machine\u201d for the Logistics Industry with Temporal](https://temporal.io/resources/case-studies/maersk)\n[View Case Study ](https://temporal.io/resources/case-studies/maersk)\nReplay 2023\n[ ![hero generic](https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=480&h=270&fm=jpg&q=80&fit=fill, https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=720&h=405&fm=jpg&q=80&fit=fill 1.5x, https://images.ctfassets.net/0uuz8ydxyd9p/6KqCaPS10y0K8mvLJzCV0U/cd5dcc25054598e3d1c9c39f47fbce5f/hero_generic.jpg?w=960&h=540&fm=jpg&q=80&fit=fill 2x)](https://temporal.io/resources/on-demand/building-a-time-machine-for-the-logistics-industry)\nLogistics\n[Maersk: Building a Time Machine for the Logistics Industry](https://temporal.io/resources/on-demand/building-a-time-machine-for-the-logistics-industry)\n[View Talk ](https://temporal.io/resources/on-demand/building-a-time-machine-for-the-logistics-industry)\nIdentity Verification\nCase Study\n[![logo checkr light](https://images.ctfassets.net/0uuz8ydxyd9p/3aBos7ZVobLVUtvjJGKlJ/28f340c8c4d48a2c71bc180a004a92ce/logo_checkr_light.svg)](https://temporal.io/resources/case-studies/checkr)\nHigh Tech\nGo\n[How Temporal Simplified Checkr Workflows](https://temporal.io/resources/case-studies/checkr)\n[View Case Study ](https://temporal.io/resources/case-studies/checkr)\n![Globe with pixelated continents and orbitting dashed lines](https://temporal.io/_app/immutable/assets/globe.BwqgSt4J.png)\nTemporal Cloud\nReliable, scalable, serverless Temporal in 11+ regions\nRun the Temporal Service today, without hassle, and with peace of mind. Pay for only what you use with our managed service, built, managed, and supported by the creators of the Temporal project. \n[ Learn more ](https://temporal.io/cloud)[ Sign up for Cloud ](https://temporal.io/get-cloud)\nTemporal Cloud\nReliable, scalable, serverless Temporal in 11+ regions\nRun the Temporal Service today, without hassle, and with peace of mind. Pay for only what you use with our managed service, built, managed, and supported by the creators of the Temporal project. \n[ Learn more ](https://temporal.io/cloud)[ Sign up for Cloud ](https://temporal.io/get-cloud)\n![Globe with pixelated continents and orbitting dashed lines](https://temporal.io/_app/immutable/assets/globe.BwqgSt4J.png)\nTemporal Cloud\nReliable, scalable, serverless Temporal in 11+ regions\nRun the Temporal Service today, without hassle, and with peace of mind. Pay for only what you use with our managed service, built, managed, and supported by the creators of the Temporal project. \n[ Learn more ](https://temporal.io/cloud)[ Sign up for Cloud ](https://temporal.io/get-cloud)\n![Globe with pixelated continents and orbitting dashed lines](https://temporal.io/_app/immutable/assets/globe.BwqgSt4J.png)\nBuild Invincible Apps\nGive your apps and services durable execution.\n[ Documentation ](https://docs.temporal.io/)[ Code Base ](https://github.com/temporalio/temporal)[ Samples ](https://learn.temporal.io/examples/)\n[ALL SYSTEMS OPERATIONAL](https://status.temporal.io/)\n[](https://www.youtube.com/temporalio) [](https://twitter.com/temporalio) [](https://github.com/temporalio) [](https://www.linkedin.com/company/temporal-technologies) [](https://t.mp/slack)\nDiscover\n[Overview](https://temporal.io/product)[How Temporal Works](https://temporal.io/how-it-works)[Temporal Cloud](https://temporal.io/cloud)[Pricing](https://temporal.io/pricing)[Security](https://temporal.io/security)[Startups](https://temporal.io/startup)\n[ Try Cloud for Free ](https://temporal.io/get-cloud)[ Log In ](https://cloud.temporal.io/login)\nExplore\n[Customer Stories](https://temporal.io/in-use)[Project-based tutorials](https://learn.temporal.io/tutorials/)[Example applications](https://learn.temporal.io/examples/)[Code Exchange](https://temporal.io/code-exchange)[Replay 2024 Rewind](https://replay.temporal.io/2024)[Ask an expert](https://pages.temporal.io/ask-an-expert)[Blog](https://temporal.io/blog)\nDevelopers\n[Getting Started with Temporal](https://learn.temporal.io/getting_started/)[Start building your next app](https://docs.temporal.io/dev-guide/)[Temporal Cloud docs](https://docs.temporal.io/cloud)[Production deployments](https://docs.temporal.io/production-deployment)[Temporal 101](https://learn.temporal.io/courses/temporal_101/)[Temporal 102](https://learn.temporal.io/courses/temporal_102/)[Introduction to Temporal Cloud](https://learn.temporal.io/courses/intro_to_temporal_cloud/)\nCommunity\n[Join our Slack group](https://t.mp/slack)[Find a meetup near you](https://temporal.io/community)[Community forum](https://community.temporal.io/)[Replay conference](https://replay.temporal.io/)\nCompany\n[About](https://temporal.io/about)[Careers](https://temporal.io/careers)[News](https://temporal.io/news)[Contact us](https://pages.temporal.io/contact-us)[Partners](https://temporal.io/partners)\nSign up for our newsletter\nEmail Address\n2025 \u00a9 Temporal Technologies. All Rights Reserved.\n[Privacy Policy](https://docs.temporal.io/privacy-policy) [Candidate Privacy Policy](https://temporal.io/candidate-privacy-policy) [Terms of Service](https://temporal.io/terms-of-service) [Code of Conduct](https://temporal.io/code-of-conduct)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nTemporal is a durable execution platform that simplifies building reliable applications by abstracting infrastructure complexities. It enhances fault tolerance, reduces failures, and supports various architectures like SAGA and event-driven systems. Temporal is open-source and supports polyglot development, making it suitable for diverse applications, including AI and financial services.\n```"
  },
  {
    "instruction": "# Notes\n\n\n\n<child_page>\n# Agents Memory\n\n\ud83d\udd17\u00a0Source: [https://www.linkedin.com/posts/aurimas-griciunas_llm-ai-machinelearning-activity-7284520203705610240-gkSj?utm_source=share&utm_medium=member_desktop](https://www.linkedin.com/posts/aurimas-griciunas_llm-ai-machinelearning-activity-7284520203705610240-gkSj?utm_source=share&utm_medium=member_desktop)\n---\n\nA simple way to explain \ud835\uddd4\ud835\udddc \ud835\uddd4\ud835\uddf4\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\udde0\ud835\uddf2\ud835\uddfa\ud835\uddfc\ud835\uddff\ud835\ude06.\nIn general, the memory for an agent is something that we provide via context in the prompt passed to LLM that helps the agent to better plan and react given past interactions or data not immediately available.\nIt is useful to group the memory into four types:\n\ud835\udfed. Episodic - This type of memory contains past interactions and actions performed by the agent. After an action is taken, the application controlling the agent would store the action in some kind of persistent storage so that it can be retrieved later if needed. A good example would be using a vector Database to store semantic meaning of the interactions.\n\ud835\udfee. Semantic - Any external information that is available to the agent and any knowledge the agent should have about itself. You can think of this as a context similar to one used in RAG applications. It can be internal knowledge only available to the agent or a grounding context to isolate part of the internet scale data for more accurate answers.\n\ud835\udfef. Procedural - This is systemic information like the structure of the System Prompt, available tools, guardrails etc. It will usually be stored in Git, Prompt and Tool Registries.\n\ud835\udff0. Occasionally, the agent application would pull information from long-term memory and store it locally if it is needed for the task at hand.\n\ud835\udff1. All of the information pulled together from the long-term or stored in local memory is called short-term or working memory. Compiling all of it into a prompt will produce the prompt to be passed to the LLM and it will provide further actions to be taken by the system.\nWe usually label 1. - 3. as Long-Term memory and 5. as Short-Term memory.\nA visual explanation of potential implementation details \ud83d\udc47\nAnd that is it! The rest is all about how you architect the flow of your Agentic systems.\n[Image](No URL)\n</child_page>\n\n\n\n\n---\n\n# Community\n\n\t[https://github.com/neural-maze/agentic_patterns](https://github.com/neural-maze/agentic_patterns)\n\t[Prompt Engineering Guide](https://www.promptingguide.ai/)\n\t[LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent)\n\t[LLM Agents](https://www.promptingguide.ai/research/llm-agents)\n\t[https://huyenchip.com//2025/01/07/agents.html](https://huyenchip.com//2025/01/07/agents.html)\n\t[https://github.com/coleam00/ai-agents-masterclass/tree/main](https://github.com/coleam00/ai-agents-masterclass/tree/main)\n\n# Science\n\n\t[Chain of Thought](https://arxiv.org/abs/2201.11903?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN)\u00a0prompting, which asks LLMs to think step by step\n\t[Self-consistency](https://arxiv.org/abs/2203.11171?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN), which prompts a model to generate several responses and pick the one that\u2019s most consistent with the others\n\t[ReAc](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN)t, which interleaves reasoning and action steps to accomplish a goal\n\t[Self-Refine](https://arxiv.org/abs/2303.17651?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN), which enables an agent to reflect on its own output\n\t[Reflexion](https://arxiv.org/abs/2303.11366?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN), which enables a model to act, evaluate, reflect, and repeat.\n\t[Test-time compute](https://arxiv.org/abs/2408.03314?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN), which increases the amount of processing power allotted to inference\n\n# Tools\n\n\t- [AgentGPT](https://github.com/reworkd/AgentGPT)\n\t- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)\n\t- [babyagi](https://github.com/yoheinakajima/babyagi)\n\t- [https://github.com/gpt-engineer-org/gpt-engineer](https://github.com/gpt-engineer-org/gpt-engineer)\n\t\n\t[https://github.com/huggingface/smolagents](https://github.com/huggingface/smolagents)\n\t[https://github.com/pydantic/pydantic-ai](https://github.com/pydantic/pydantic-ai)\n\t[https://docs.llamaindex.ai/en/stable/module_guides/workflow/](https://docs.llamaindex.ai/en/stable/module_guides/workflow/)\n\t[https://www.langchain.com/langgraph](https://www.langchain.com/langgraph)\n\t[https://github.com/crewAIInc/crewAI](https://github.com/crewAIInc/crewAI)",
    "answer": "```markdown\n# TL;DR Summary\n\n**Agents Memory**: AI agents utilize memory categorized into four types: \n1. **Episodic** - past interactions stored in databases.\n2. **Semantic** - external and internal knowledge.\n3. **Procedural** - systemic information on prompts and tools.\n4. **Short-term** - local, task-specific memory.\n\nThis architecture enhances agent planning and responses.\n\n**Tags**: Generative AI, LLMs\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/GokuMohandas/Made-With-ML/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FGokuMohandas%2FMade-With-ML%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FGokuMohandas%2FMade-With-ML%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=GokuMohandas%2FMade-With-ML) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/GokuMohandas/Made-With-ML/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/GokuMohandas/Made-With-ML/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/GokuMohandas/Made-With-ML/) to refresh your session. Dismiss alert\n{{ message }}\n[ GokuMohandas ](https://github.com/GokuMohandas) / **[Made-With-ML](https://github.com/GokuMohandas/Made-With-ML) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2FGokuMohandas%2FMade-With-ML) You must be signed in to change notification settings\n  * [ Fork 6.1k ](https://github.com/login?return_to=%2FGokuMohandas%2FMade-With-ML)\n  * [ Star  38.4k ](https://github.com/login?return_to=%2FGokuMohandas%2FMade-With-ML)\n\n\nLearn how to design, develop, deploy and iterate on production-grade ML applications. \n[madewithml.com](https://madewithml.com \"https://madewithml.com\")\n### License\n[ MIT license ](https://github.com/GokuMohandas/Made-With-ML/blob/main/LICENSE)\n[ 38.4k stars ](https://github.com/GokuMohandas/Made-With-ML/stargazers) [ 6.1k forks ](https://github.com/GokuMohandas/Made-With-ML/forks) [ Branches ](https://github.com/GokuMohandas/Made-With-ML/branches) [ Tags ](https://github.com/GokuMohandas/Made-With-ML/tags) [ Activity ](https://github.com/GokuMohandas/Made-With-ML/activity)\n[ Star  ](https://github.com/login?return_to=%2FGokuMohandas%2FMade-With-ML)\n[ Notifications ](https://github.com/login?return_to=%2FGokuMohandas%2FMade-With-ML) You must be signed in to change notification settings\n  * [ Code ](https://github.com/GokuMohandas/Made-With-ML)\n  * [ Issues 17 ](https://github.com/GokuMohandas/Made-With-ML/issues)\n  * [ Pull requests 5 ](https://github.com/GokuMohandas/Made-With-ML/pulls)\n  * [ Actions ](https://github.com/GokuMohandas/Made-With-ML/actions)\n  * [ Projects 0 ](https://github.com/GokuMohandas/Made-With-ML/projects)\n  * [ Security ](https://github.com/GokuMohandas/Made-With-ML/security)\n  * [ Insights ](https://github.com/GokuMohandas/Made-With-ML/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/GokuMohandas/Made-With-ML)\n  * [ Issues  ](https://github.com/GokuMohandas/Made-With-ML/issues)\n  * [ Pull requests  ](https://github.com/GokuMohandas/Made-With-ML/pulls)\n  * [ Actions  ](https://github.com/GokuMohandas/Made-With-ML/actions)\n  * [ Projects  ](https://github.com/GokuMohandas/Made-With-ML/projects)\n  * [ Security  ](https://github.com/GokuMohandas/Made-With-ML/security)\n  * [ Insights  ](https://github.com/GokuMohandas/Made-With-ML/pulse)\n\n\n# GokuMohandas/Made-With-ML\nmain\n[**3** Branches](https://github.com/GokuMohandas/Made-With-ML/branches)[**0** Tags](https://github.com/GokuMohandas/Made-With-ML/tags)\n[](https://github.com/GokuMohandas/Made-With-ML/branches)[](https://github.com/GokuMohandas/Made-With-ML/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[18 Commits](https://github.com/GokuMohandas/Made-With-ML/commits/main/)[](https://github.com/GokuMohandas/Made-With-ML/commits/main/)  \n[.github/workflows](https://github.com/GokuMohandas/Made-With-ML/tree/main/.github/workflows \"This path skips through empty directories\")| [.github/workflows](https://github.com/GokuMohandas/Made-With-ML/tree/main/.github/workflows \"This path skips through empty directories\")  \n[datasets](https://github.com/GokuMohandas/Made-With-ML/tree/main/datasets \"datasets\")| [datasets](https://github.com/GokuMohandas/Made-With-ML/tree/main/datasets \"datasets\")  \n[deploy](https://github.com/GokuMohandas/Made-With-ML/tree/main/deploy \"deploy\")| [deploy](https://github.com/GokuMohandas/Made-With-ML/tree/main/deploy \"deploy\")  \n[docs](https://github.com/GokuMohandas/Made-With-ML/tree/main/docs \"docs\")| [docs](https://github.com/GokuMohandas/Made-With-ML/tree/main/docs \"docs\")  \n[madewithml](https://github.com/GokuMohandas/Made-With-ML/tree/main/madewithml \"madewithml\")| [madewithml](https://github.com/GokuMohandas/Made-With-ML/tree/main/madewithml \"madewithml\")  \n[notebooks](https://github.com/GokuMohandas/Made-With-ML/tree/main/notebooks \"notebooks\")| [notebooks](https://github.com/GokuMohandas/Made-With-ML/tree/main/notebooks \"notebooks\")  \n[tests](https://github.com/GokuMohandas/Made-With-ML/tree/main/tests \"tests\")| [tests](https://github.com/GokuMohandas/Made-With-ML/tree/main/tests \"tests\")  \n[.gitignore](https://github.com/GokuMohandas/Made-With-ML/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/GokuMohandas/Made-With-ML/blob/main/.gitignore \".gitignore\")  \n[.pre-commit-config.yaml](https://github.com/GokuMohandas/Made-With-ML/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/GokuMohandas/Made-With-ML/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")  \n[LICENSE](https://github.com/GokuMohandas/Made-With-ML/blob/main/LICENSE \"LICENSE\")| [LICENSE](https://github.com/GokuMohandas/Made-With-ML/blob/main/LICENSE \"LICENSE\")  \n[Makefile](https://github.com/GokuMohandas/Made-With-ML/blob/main/Makefile \"Makefile\")| [Makefile](https://github.com/GokuMohandas/Made-With-ML/blob/main/Makefile \"Makefile\")  \n[README.md](https://github.com/GokuMohandas/Made-With-ML/blob/main/README.md \"README.md\")| [README.md](https://github.com/GokuMohandas/Made-With-ML/blob/main/README.md \"README.md\")  \n[mkdocs.yml](https://github.com/GokuMohandas/Made-With-ML/blob/main/mkdocs.yml \"mkdocs.yml\")| [mkdocs.yml](https://github.com/GokuMohandas/Made-With-ML/blob/main/mkdocs.yml \"mkdocs.yml\")  \n[pyproject.toml](https://github.com/GokuMohandas/Made-With-ML/blob/main/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/GokuMohandas/Made-With-ML/blob/main/pyproject.toml \"pyproject.toml\")  \n[requirements.txt](https://github.com/GokuMohandas/Made-With-ML/blob/main/requirements.txt \"requirements.txt\")| [requirements.txt](https://github.com/GokuMohandas/Made-With-ML/blob/main/requirements.txt \"requirements.txt\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/GokuMohandas/Made-With-ML/)\n  * [MIT license](https://github.com/GokuMohandas/Made-With-ML/)\n\n\n# [![](https://camo.githubusercontent.com/bc306f689a14d3d6dc9782a9d6207b1a72bb753d02e24812bc658d334fa57b7e/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f726f756e6465645f6c6f676f2e706e67)](https://camo.githubusercontent.com/bc306f689a14d3d6dc9782a9d6207b1a72bb753d02e24812bc658d334fa57b7e/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f726f756e6465645f6c6f676f2e706e67) [Made With ML](https://madewithml.com/)\n[](https://github.com/GokuMohandas/Made-With-ML/#made-with-ml)\nDesign \u00b7 Develop \u00b7 Deploy \u00b7 Iterate Join 40K+ developers in learning how to responsibly deliver value with ML. \n[![](https://camo.githubusercontent.com/1c8877d013d3bce832b494b16e92a58bb0b56d0756071d8d20da475c5ea4d95b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5375627363726962652d34304b2d627269676874677265656e)](https://madewithml.com/) [![](https://camo.githubusercontent.com/a9dccbaf0395471a9de4f70c2f883806ec1b335a30ea3c43aa0c62e514e01497/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f476f6b754d6f68616e6461732f4d6164652d576974682d4d4c2e7376673f7374796c653d736f6369616c266c6162656c3d53746172)](https://github.com/GokuMohandas/Made-With-ML) [![](https://camo.githubusercontent.com/fc554d6bdc946c70ad1a2b66fd77c3dad48cb4d7e5d6049ef463738c0aaaf4da/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374796c652d2d3565626130302e7376673f6c6162656c3d4c696e6b6564496e266c6f676f3d6c696e6b6564696e267374796c653d736f6369616c)](https://www.linkedin.com/in/goku) [![](https://camo.githubusercontent.com/1928f043a6430a553dd6b3d8ecf4faea06dbeea5c40660d6de57a9c56edb909a/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f476f6b754d6f68616e6461732e7376673f6c6162656c3d466f6c6c6f77267374796c653d736f6369616c)](https://twitter.com/GokuMohandas) \ud83d\udd25 Among the [top ML repositories](https://github.com/GokuMohandas/Made-With-ML) on GitHub \n## Lessons\n[](https://github.com/GokuMohandas/Made-With-ML/#lessons)\nLearn how to combine machine learning with software engineering to design, develop, deploy and iterate on production-grade ML applications.\n  * Lessons: <https://madewithml.com/>\n  * Code: [GokuMohandas/Made-With-ML](https://github.com/GokuMohandas/Made-With-ML)\n\n[ ![lessons](https://camo.githubusercontent.com/f459b72e73251f4b7792dfe1048867766a8b8d024dae50541b0c16b1b0d0c854/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6c6573736f6e732e706e67) ](https://madewithml.com/#course)\n## Overview\n[](https://github.com/GokuMohandas/Made-With-ML/#overview)\nIn this course, we'll go from experimentation (design + development) to production (deployment + iteration). We'll do this iteratively by motivating the components that will enable us to build a _reliable_ production system.\n> [![](https://camo.githubusercontent.com/986178ecaedccd395a9f79a6e622d5c2594a85781d34df0cc765c0ad22d42e0f/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f302f30392f596f75547562655f66756c6c2d636f6c6f725f69636f6e5f253238323031372532392e7376672f36343070782d596f75547562655f66756c6c2d636f6c6f725f69636f6e5f253238323031372532392e7376672e706e67)](https://camo.githubusercontent.com/986178ecaedccd395a9f79a6e622d5c2594a85781d34df0cc765c0ad22d42e0f/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f302f30392f596f75547562655f66756c6c2d636f6c6f725f69636f6e5f253238323031372532392e7376672f36343070782d596f75547562655f66756c6c2d636f6c6f725f69636f6e5f253238323031372532392e7376672e706e67) Be sure to watch the video below for a quick overview of what we'll be building. \n[![Course overview video](https://camo.githubusercontent.com/bcd05c283ea01abfbe99af338ff8c8818f73b983a606cafea6386c96935618e1/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f4157676b7438483879566f2f302e6a7067)](https://youtu.be/AWgkt8H8yVo)\n  * **\ud83d\udca1 First principles** : before we jump straight into the code, we develop a first principles understanding for every machine learning concept.\n  * **\ud83d\udcbb Best practices** : implement software engineering best practices as we develop and deploy our machine learning models.\n  * **\ud83d\udcc8 Scale** : easily scale ML workloads (data, train, tune, serve) in Python without having to learn completely new languages.\n  * **\u2699\ufe0f MLOps** : connect MLOps components (tracking, testing, serving, orchestration, etc.) as we build an end-to-end machine learning system.\n  * **\ud83d\ude80 Dev to Prod** : learn how to quickly and reliably go from development to production without any changes to our code or infra management.\n  * **\ud83d\udc19 CI/CD** : learn how to create mature CI/CD workflows to continuously train and deploy better models in a modular way that integrates with any stack.\n\n\n## Audience\n[](https://github.com/GokuMohandas/Made-With-ML/#audience)\nMachine learning is not a separate industry, instead, it's a powerful way of thinking about data that's not reserved for any one type of person.\n  * **\ud83d\udc69\u200d\ud83d\udcbb All developers** : whether software/infra engineer or data scientist, ML is increasingly becoming a key part of the products that you'll be developing.\n  * **\ud83d\udc69\u200d\ud83c\udf93 College graduates** : learn the practical skills required for industry and bridge gap between the university curriculum and what industry expects.\n  * **\ud83d\udc69\u200d\ud83d\udcbc Product/Leadership** : who want to develop a technical foundation so that they can build amazing (and reliable) products powered by machine learning.\n\n\n## Set up\n[](https://github.com/GokuMohandas/Made-With-ML/#set-up)\nBe sure to go through the [course](https://madewithml/#course) for a much more detailed walkthrough of the content on this repository. We will have instructions for both local laptop and Anyscale clusters for the sections below, so be sure to toggle the \u25ba dropdown based on what you're using (Anyscale instructions will be toggled on by default). If you do want to run this course with Anyscale, where we'll provide the **structure** , **compute (GPUs)** and **community** to learn everything in one day, join our next upcoming live cohort \u2192 [sign up here](https://4190urw86oh.typeform.com/madewithml)!\n### Cluster\n[](https://github.com/GokuMohandas/Made-With-ML/#cluster)\nWe'll start by setting up our cluster with the environment and compute configurations.\nLocal Your personal laptop (single machine) will act as the cluster, where one CPU will be the head node and some of the remaining CPU will be the worker nodes. All of the code in this course will work in any personal laptop though it will be slower than executing the same workloads on a larger cluster.  Anyscale\nWe can create an [Anyscale Workspace](https://docs.anyscale.com/develop/workspaces/get-started) using the [webpage UI](https://console.anyscale.com/o/madewithml/workspaces/add/blank).\n```\n- Workspace name: `madewithml`\n- Project: `madewithml`\n- Cluster environment name: `madewithml-cluster-env`\n# Toggle `Select from saved configurations`\n- Compute config: `madewithml-cluster-compute-g5.4xlarge`\n```\n\n> Alternatively, we can use the [CLI](https://docs.anyscale.com/reference/anyscale-cli) to create the workspace via `anyscale workspace create ...`\nOther (cloud platforms, K8s, on-prem)\nIf you don't want to do this course locally or via Anyscale, you have the following options:\n  * On [AWS and GCP](https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index). Community-supported Azure and Aliyun integrations also exist.\n  * On [Kubernetes](https://docs.ray.io/en/latest/cluster/kubernetes/index.html#kuberay-index), via the officially supported KubeRay project.\n  * Deploy Ray manually [on-prem](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/on-premises.html#on-prem) or onto platforms [not listed here](https://docs.ray.io/en/latest/cluster/vms/user-guides/community/index.html#ref-cluster-setup).\n\n\n### Git setup\n[](https://github.com/GokuMohandas/Made-With-ML/#git-setup)\nCreate a repository by following these instructions: [Create a new repository](https://github.com/new) \u2192 name it `Made-With-ML` \u2192 Toggle `Add a README file` (**very important** as this creates a `main` branch) \u2192 Click `Create repository` (scroll down)\nNow we're ready to clone the repository that has all of our code:\n```\ngit clone https://github.com/GokuMohandas/Made-With-ML.git .\n```\n\n### Credentials\n[](https://github.com/GokuMohandas/Made-With-ML/#credentials)\n```\ntouch .env\n```\n\n```\n# Inside .env\nGITHUB_USERNAME=\"CHANGE_THIS_TO_YOUR_USERNAME\" # \u2190 CHANGE THIS\n```\n\n```\nsource .env\n```\n\n### Virtual environment\n[](https://github.com/GokuMohandas/Made-With-ML/#virtual-environment)\nLocal\n```\nexport PYTHONPATH=$PYTHONPATH:$PWD\npython3 -m venv venv # recommend using Python 3.10\nsource venv/bin/activate # on Windows: venv\\Scripts\\activate\npython3 -m pip install --upgrade pip setuptools wheel\npython3 -m pip install -r requirements.txt\npre-commit install\npre-commit autoupdate\n```\n\n> Highly recommend using Python `3.10` and using [pyenv](https://github.com/pyenv/pyenv) (mac) or [pyenv-win](https://github.com/pyenv-win/pyenv-win) (windows).\nAnyscale\nOur environment with the appropriate Python version and libraries is already all set for us through the cluster environment we used when setting up our Anyscale Workspace. So we just need to run these commands:\n```\nexport PYTHONPATH=$PYTHONPATH:$PWD\npre-commit install\npre-commit autoupdate\n```\n\n## Notebook\n[](https://github.com/GokuMohandas/Made-With-ML/#notebook)\nStart by exploring the [jupyter notebook](https://github.com/GokuMohandas/Made-With-ML/blob/main/notebooks/madewithml.ipynb) to interactively walkthrough the core machine learning workloads.\n[![](https://camo.githubusercontent.com/102df90a3988494a936aeef7b28b9f21cd4cd804fdffe52dec4aca17cd76b1db/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6d6c6f70732f73797374656d732d64657369676e2f776f726b6c6f6164732e706e67)](https://camo.githubusercontent.com/102df90a3988494a936aeef7b28b9f21cd4cd804fdffe52dec4aca17cd76b1db/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6d6c6f70732f73797374656d732d64657369676e2f776f726b6c6f6164732e706e67)\nLocal\n```\n# Start notebook\njupyter lab notebooks/madewithml.ipynb\n```\n\nAnyscale\nClick on the Jupyter icon [![](https://camo.githubusercontent.com/fb6238b3b65a06550fd5caee8cc71b544011fc5b20e748610514ade292f67d7e/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f332f33382f4a7570797465725f6c6f676f2e7376672f3132303070782d4a7570797465725f6c6f676f2e7376672e706e67)](https://camo.githubusercontent.com/fb6238b3b65a06550fd5caee8cc71b544011fc5b20e748610514ade292f67d7e/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f332f33382f4a7570797465725f6c6f676f2e7376672f3132303070782d4a7570797465725f6c6f676f2e7376672e706e67) at the top right corner of our Anyscale Workspace page and this will open up our JupyterLab instance in a new tab. Then navigate to the `notebooks` directory and open up the `madewithml.ipynb` notebook.\n## Scripts\n[](https://github.com/GokuMohandas/Made-With-ML/#scripts)\nNow we'll execute the same workloads using the clean Python scripts following software engineering best practices (testing, documentation, logging, serving, versioning, etc.) The code we've implemented in our notebook will be refactored into the following scripts:\n```\nmadewithml\n\u251c\u2500\u2500 config.py\n\u251c\u2500\u2500 data.py\n\u251c\u2500\u2500 evaluate.py\n\u251c\u2500\u2500 models.py\n\u251c\u2500\u2500 predict.py\n\u251c\u2500\u2500 serve.py\n\u251c\u2500\u2500 train.py\n\u251c\u2500\u2500 tune.py\n\u2514\u2500\u2500 utils.py\n```\n\n**Note** : Change the `--num-workers`, `--cpu-per-worker`, and `--gpu-per-worker` input argument values below based on your system's resources. For example, if you're on a local laptop, a reasonable configuration would be `--num-workers 6 --cpu-per-worker 1 --gpu-per-worker 0`.\n### Training\n[](https://github.com/GokuMohandas/Made-With-ML/#training)\n```\nexport EXPERIMENT_NAME=\"llm\"\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\nexport TRAIN_LOOP_CONFIG='{\"dropout_p\": 0.5, \"lr\": 1e-4, \"lr_factor\": 0.8, \"lr_patience\": 3}'\npython madewithml/train.py \\\n  --experiment-name \"$EXPERIMENT_NAME\" \\\n  --dataset-loc \"$DATASET_LOC\" \\\n  --train-loop-config \"$TRAIN_LOOP_CONFIG\" \\\n  --num-workers 1 \\\n  --cpu-per-worker 3 \\\n  --gpu-per-worker 1 \\\n  --num-epochs 10 \\\n  --batch-size 256 \\\n  --results-fp results/training_results.json\n```\n\n### Tuning\n[](https://github.com/GokuMohandas/Made-With-ML/#tuning)\n```\nexport EXPERIMENT_NAME=\"llm\"\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\nexport TRAIN_LOOP_CONFIG='{\"dropout_p\": 0.5, \"lr\": 1e-4, \"lr_factor\": 0.8, \"lr_patience\": 3}'\nexport INITIAL_PARAMS=\"[{\\\"train_loop_config\\\": $TRAIN_LOOP_CONFIG}]\"\npython madewithml/tune.py \\\n  --experiment-name \"$EXPERIMENT_NAME\" \\\n  --dataset-loc \"$DATASET_LOC\" \\\n  --initial-params \"$INITIAL_PARAMS\" \\\n  --num-runs 2 \\\n  --num-workers 1 \\\n  --cpu-per-worker 3 \\\n  --gpu-per-worker 1 \\\n  --num-epochs 10 \\\n  --batch-size 256 \\\n  --results-fp results/tuning_results.json\n```\n\n### Experiment tracking\n[](https://github.com/GokuMohandas/Made-With-ML/#experiment-tracking)\nWe'll use [MLflow](https://mlflow.org/) to track our experiments and store our models and the [MLflow Tracking UI](https://www.mlflow.org/docs/latest/tracking.html#tracking-ui) to view our experiments. We have been saving our experiments to a local directory but note that in an actual production setting, we would have a central location to store all of our experiments. It's easy/inexpensive to spin up your own MLflow server for all of your team members to track their experiments on or use a managed solution like [Weights & Biases](https://wandb.ai/site), [Comet](https://www.comet.ml/), etc.\n```\nexport MODEL_REGISTRY=$(python -c \"from madewithml import config; print(config.MODEL_REGISTRY)\")\nmlflow server -h 0.0.0.0 -p 8080 --backend-store-uri $MODEL_REGISTRY\n```\n\nLocal\nIf you're running this notebook on your local laptop then head on over to [](http://localhost:8080/)<http://localhost:8080/> to view your MLflow dashboard.\nAnyscale\nIf you're on [Anyscale Workspaces](https://docs.anyscale.com/develop/workspaces/get-started), then we need to first expose the port of the MLflow server. Run the following command on your Anyscale Workspace terminal to generate the public URL to your MLflow server.\n```\nAPP_PORT=8080\necho https://$APP_PORT-port-$ANYSCALE_SESSION_DOMAIN\n```\n\n### Evaluation\n[](https://github.com/GokuMohandas/Made-With-ML/#evaluation)\n```\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\nexport HOLDOUT_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/holdout.csv\"\npython madewithml/evaluate.py \\\n  --run-id $RUN_ID \\\n  --dataset-loc $HOLDOUT_LOC \\\n  --results-fp results/evaluation_results.json\n```\n\n```\n{\n \"timestamp\": \"June 09, 2023 09:26:18 AM\",\n \"run_id\": \"6149e3fec8d24f1492d4a4cabd5c06f6\",\n \"overall\": {\n  \"precision\": 0.9076136428670714,\n  \"recall\": 0.9057591623036649,\n  \"f1\": 0.9046792827719773,\n  \"num_samples\": 191.0\n },\n...\n```\n\n### Inference\n[](https://github.com/GokuMohandas/Made-With-ML/#inference)\n```\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npython madewithml/predict.py predict \\\n  --run-id $RUN_ID \\\n  --title \"Transfer learning with transformers\" \\\n  --description \"Using transformers for transfer learning on text classification tasks.\"\n```\n\n```\n[{\n \"prediction\": [\n  \"natural-language-processing\"\n ],\n \"probabilities\": {\n  \"computer-vision\": 0.0009767753,\n  \"mlops\": 0.0008223939,\n  \"natural-language-processing\": 0.99762577,\n  \"other\": 0.000575123\n }\n}]\n```\n\n### Serving\n[](https://github.com/GokuMohandas/Made-With-ML/#serving)\nLocal\n```\n# Start\nray start --head\n```\n\n```\n# Set up\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npython madewithml/serve.py --run_id $RUN_ID\n```\n\nOnce the application is running, we can use it via cURL, Python, etc.:\n```\n# via Python\nimport json\nimport requests\ntitle = \"Transfer learning with transformers\"\ndescription = \"Using transformers for transfer learning on text classification tasks.\"\njson_data = json.dumps({\"title\": title, \"description\": description})\nrequests.post(\"http://127.0.0.1:8000/predict\", data=json_data).json()\n```\n\n```\nray stop # shutdown\n```\n\nAnyscale\nIn Anyscale Workspaces, Ray is already running so we don't have to manually start/shutdown like we have to do locally.\n```\n# Set up\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npython madewithml/serve.py --run_id $RUN_ID\n```\n\nOnce the application is running, we can use it via cURL, Python, etc.:\n```\n# via Python\nimport json\nimport requests\ntitle = \"Transfer learning with transformers\"\ndescription = \"Using transformers for transfer learning on text classification tasks.\"\njson_data = json.dumps({\"title\": title, \"description\": description})\nrequests.post(\"http://127.0.0.1:8000/predict\", data=json_data).json()\n```\n\n### Testing\n[](https://github.com/GokuMohandas/Made-With-ML/#testing)\n```\n# Code\npython3 -m pytest tests/code --verbose --disable-warnings\n# Data\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\npytest --dataset-loc=$DATASET_LOC tests/data --verbose --disable-warnings\n# Model\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npytest --run-id=$RUN_ID tests/model --verbose --disable-warnings\n# Coverage\npython3 -m pytest tests/code --cov madewithml --cov-report html --disable-warnings # html report\npython3 -m pytest tests/code --cov madewithml --cov-report term --disable-warnings # terminal report\n```\n\n## Production\n[](https://github.com/GokuMohandas/Made-With-ML/#production)\nFrom this point onwards, in order to deploy our application into production, we'll need to either be on Anyscale or on a [cloud VM](https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index) / [on-prem](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/on-premises.html#on-prem) cluster you manage yourself (w/ Ray). If not on Anyscale, the commands will be [slightly different](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html) but the concepts will be the same.\n> If you don't want to set up all of this yourself, we highly recommend joining our [upcoming live cohort](https://4190urw86oh.typeform.com/madewithml){:target=\"_blank\"} where we'll provide an environment with all of this infrastructure already set up for you so that you just focused on the machine learning.\n[![](https://camo.githubusercontent.com/8b57e924766b8fb69d3d10c001c3562a75be929854772c5a1ddba9c280f42153/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6d6c6f70732f6a6f62735f616e645f73657276696365732f6d616e75616c2e706e67)](https://camo.githubusercontent.com/8b57e924766b8fb69d3d10c001c3562a75be929854772c5a1ddba9c280f42153/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6d6c6f70732f6a6f62735f616e645f73657276696365732f6d616e75616c2e706e67)\n### Authentication\n[](https://github.com/GokuMohandas/Made-With-ML/#authentication)\nThese credentials below are **automatically** set for us if we're using Anyscale Workspaces. We **do not** need to set these credentials explicitly on Workspaces but we do if we're running this locally or on a cluster outside of where our Anyscale Jobs and Services are configured to run.\n```\nexport ANYSCALE_HOST=https://console.anyscale.com\nexport ANYSCALE_CLI_TOKEN=$YOUR_CLI_TOKEN # retrieved from Anyscale credentials page\n```\n\n### Cluster environment\n[](https://github.com/GokuMohandas/Made-With-ML/#cluster-environment)\nThe cluster environment determines **where** our workloads will be executed (OS, dependencies, etc.) We've already created this [cluster environment](https://github.com/GokuMohandas/Made-With-ML/blob/main/deploy/cluster_env.yaml) for us but this is how we can create/update one ourselves.\n```\nexport CLUSTER_ENV_NAME=\"madewithml-cluster-env\"\nanyscale cluster-env build deploy/cluster_env.yaml --name $CLUSTER_ENV_NAME\n```\n\n### Compute configuration\n[](https://github.com/GokuMohandas/Made-With-ML/#compute-configuration)\nThe compute configuration determines **what** resources our workloads will be executes on. We've already created this [compute configuration](https://github.com/GokuMohandas/Made-With-ML/blob/main/deploy/cluster_compute.yaml) for us but this is how we can create it ourselves.\n```\nexport CLUSTER_COMPUTE_NAME=\"madewithml-cluster-compute-g5.4xlarge\"\nanyscale cluster-compute create deploy/cluster_compute.yaml --name $CLUSTER_COMPUTE_NAME\n```\n\n### Anyscale jobs\n[](https://github.com/GokuMohandas/Made-With-ML/#anyscale-jobs)\nNow we're ready to execute our ML workloads. We've decided to combine them all together into one [job](https://github.com/GokuMohandas/Made-With-ML/blob/main/deploy/jobs/workloads.yaml) but we could have also created separate jobs for each workload (train, evaluate, etc.) We'll start by editing the `$GITHUB_USERNAME` slots inside our [`workloads.yaml`](https://github.com/GokuMohandas/Made-With-ML/blob/main/deploy/jobs/workloads.yaml) file:\n```\nruntime_env:\n working_dir: .\n upload_path: s3://madewithml/$GITHUB_USERNAME/jobs # <--- CHANGE USERNAME (case-sensitive)\n env_vars:\n  GITHUB_USERNAME: $GITHUB_USERNAME # <--- CHANGE USERNAME (case-sensitive)\n```\n\nThe `runtime_env` here specifies that we should upload our current `working_dir` to an S3 bucket so that all of our workers when we execute an Anyscale Job have access to the code to use. The `GITHUB_USERNAME` is used later to save results from our workloads to S3 so that we can retrieve them later (ex. for serving).\nNow we're ready to submit our job to execute our ML workloads:\n```\nanyscale job submit deploy/jobs/workloads.yaml\n```\n\n### Anyscale Services\n[](https://github.com/GokuMohandas/Made-With-ML/#anyscale-services)\nAnd after our ML workloads have been executed, we're ready to launch our serve our model to production. Similar to our Anyscale Jobs configs, be sure to change the `$GITHUB_USERNAME` in [`serve_model.yaml`](https://github.com/GokuMohandas/Made-With-ML/blob/main/deploy/services/serve_model.yaml).\n```\nray_serve_config:\n import_path: deploy.services.serve_model:entrypoint\n runtime_env:\n  working_dir: .\n  upload_path: s3://madewithml/$GITHUB_USERNAME/services # <--- CHANGE USERNAME (case-sensitive)\n  env_vars:\n   GITHUB_USERNAME: $GITHUB_USERNAME # <--- CHANGE USERNAME (case-sensitive)\n```\n\nNow we're ready to launch our service:\n```\n# Rollout service\nanyscale service rollout -f deploy/services/serve_model.yaml\n# Query\ncurl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $SECRET_TOKEN\" -d '{\n \"title\": \"Transfer learning with transformers\",\n \"description\": \"Using transformers for transfer learning on text classification tasks.\"\n}' $SERVICE_ENDPOINT/predict/\n# Rollback (to previous version of the Service)\nanyscale service rollback -f $SERVICE_CONFIG --name $SERVICE_NAME\n# Terminate\nanyscale service terminate --name $SERVICE_NAME\n```\n\n### CI/CD\n[](https://github.com/GokuMohandas/Made-With-ML/#cicd)\nWe're not going to manually deploy our application every time we make a change. Instead, we'll automate this process using GitHub Actions!\n[![](https://camo.githubusercontent.com/cf6e7ce2720bf03a1c9703584f7b84227ee90eb116f208b9925faea6a43d4cda/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6d6c6f70732f636963642f636963642e706e67)](https://camo.githubusercontent.com/cf6e7ce2720bf03a1c9703584f7b84227ee90eb116f208b9925faea6a43d4cda/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6d6c6f70732f636963642f636963642e706e67)\n  1. Create a new github branch to save our changes to and execute CI/CD workloads:\n\n\n```\ngit remote set-url origin https://github.com/$GITHUB_USERNAME/Made-With-ML.git # <-- CHANGE THIS to your username\ngit checkout -b dev\n```\n\n  1. We'll start by adding the necessary credentials to the [`/settings/secrets/actions`](https://github.com/GokuMohandas/Made-With-ML/settings/secrets/actions) page of our GitHub repository.\n\n\n```\nexport ANYSCALE_HOST=https://console.anyscale.com\nexport ANYSCALE_CLI_TOKEN=$YOUR_CLI_TOKEN # retrieved from https://console.anyscale.com/o/madewithml/credentials\n```\n\n  1. Now we can make changes to our code (not on `main` branch) and push them to GitHub. But in order to push our code to GitHub, we'll need to first authenticate with our credentials before pushing to our repository:\n\n\n```\ngit config --global user.name $GITHUB_USERNAME # <-- CHANGE THIS to your username\ngit config --global user.email you@example.com # <-- CHANGE THIS to your email\ngit add .\ngit commit -m \"\" # <-- CHANGE THIS to your message\ngit push origin dev\n```\n\nNow you will be prompted to enter your username and password (personal access token). Follow these steps to get personal access token: [New GitHub personal access token](https://github.com/settings/tokens/new) \u2192 Add a name \u2192 Toggle `repo` and `workflow` \u2192 Click `Generate token` (scroll down) \u2192 Copy the token and paste it when prompted for your password.\n  1. Now we can start a PR from this branch to our `main` branch and this will trigger the [workloads workflow](https://github.com/GokuMohandas/Made-With-ML/blob/main/.github/workflows/workloads.yaml). If the workflow (Anyscale Jobs) succeeds, this will produce comments with the training and evaluation results directly on the PR.\n\n\n[![](https://camo.githubusercontent.com/da3c36d291da9d55e1e62dffb4440632672e07f952d4874f9406aafcb819d064/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6d6c6f70732f636963642f636f6d6d656e74732e706e67)](https://camo.githubusercontent.com/da3c36d291da9d55e1e62dffb4440632672e07f952d4874f9406aafcb819d064/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6d6c6f70732f636963642f636f6d6d656e74732e706e67)\n  1. If we like the results, we can merge the PR into the `main` branch. This will trigger the [serve workflow](https://github.com/GokuMohandas/Made-With-ML/blob/main/.github/workflows/serve.yaml) which will rollout our new service to production!\n\n\n### Continual learning\n[](https://github.com/GokuMohandas/Made-With-ML/#continual-learning)\nWith our CI/CD workflow in place to deploy our application, we can now focus on continually improving our model. It becomes really easy to extend on this foundation to connect to scheduled runs (cron), [data pipelines](https://madewithml.com/courses/mlops/data-engineering/), drift detected through [monitoring](https://madewithml.com/courses/mlops/monitoring/), [online evaluation](https://madewithml.com/courses/mlops/evaluation/#online-evaluation), etc. And we can easily add additional context such as comparing any experiment with what's currently in production (directly in the PR even), etc.\n[![](https://camo.githubusercontent.com/0c6bb2cbfd969b1e053fc3d626fa240409e5c8d723f43fa07b5c3c67640c296e/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6d6c6f70732f636963642f636f6e74696e75616c2e706e67)](https://camo.githubusercontent.com/0c6bb2cbfd969b1e053fc3d626fa240409e5c8d723f43fa07b5c3c67640c296e/68747470733a2f2f6d616465776974686d6c2e636f6d2f7374617469632f696d616765732f6d6c6f70732f636963642f636f6e74696e75616c2e706e67)\n## FAQ\n[](https://github.com/GokuMohandas/Made-With-ML/#faq)\n### Jupyter notebook kernels\n[](https://github.com/GokuMohandas/Made-With-ML/#jupyter-notebook-kernels)\nIssues with configuring the notebooks with jupyter? By default, jupyter will use the kernel with our virtual environment but we can also manually add it to jupyter:\n```\npython3 -m ipykernel install --user --name=venv\n```\n\nNow we can open up a notebook \u2192 Kernel (top menu bar) \u2192 Change Kernel \u2192 `venv`. To ever delete this kernel, we can do the following:\n```\njupyter kernelspec list\njupyter kernelspec uninstall venv\n```\n\n## About\nLearn how to design, develop, deploy and iterate on production-grade ML applications. \n[madewithml.com](https://madewithml.com \"https://madewithml.com\")\n### Topics\n[ python ](https://github.com/topics/python \"Topic: python\") [ data-science ](https://github.com/topics/data-science \"Topic: data-science\") [ machine-learning ](https://github.com/topics/machine-learning \"Topic: machine-learning\") [ natural-language-processing ](https://github.com/topics/natural-language-processing \"Topic: natural-language-processing\") [ deep-learning ](https://github.com/topics/deep-learning \"Topic: deep-learning\") [ pytorch ](https://github.com/topics/pytorch \"Topic: pytorch\") [ data-engineering ](https://github.com/topics/data-engineering \"Topic: data-engineering\") [ ray ](https://github.com/topics/ray \"Topic: ray\") [ data-quality ](https://github.com/topics/data-quality \"Topic: data-quality\") [ distributed-training ](https://github.com/topics/distributed-training \"Topic: distributed-training\") [ mlops ](https://github.com/topics/mlops \"Topic: mlops\") [ distributed-ml ](https://github.com/topics/distributed-ml \"Topic: distributed-ml\") [ llms ](https://github.com/topics/llms \"Topic: llms\")\n### Resources\n[ Readme ](https://github.com/GokuMohandas/Made-With-ML/#readme-ov-file)\n### License\n[ MIT license ](https://github.com/GokuMohandas/Made-With-ML/#MIT-1-ov-file)\n[ Activity](https://github.com/GokuMohandas/Made-With-ML/activity)\n### Stars\n[ **38.4k** stars](https://github.com/GokuMohandas/Made-With-ML/stargazers)\n### Watchers\n[ **1.2k** watching](https://github.com/GokuMohandas/Made-With-ML/watchers)\n### Forks\n[ **6.1k** forks](https://github.com/GokuMohandas/Made-With-ML/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FGokuMohandas%2FMade-With-ML&report=GokuMohandas+%28user%29)\n## Languages\n  * [ Jupyter Notebook 97.8% ](https://github.com/GokuMohandas/Made-With-ML/search?l=jupyter-notebook)\n  * [ Python 2.1% ](https://github.com/GokuMohandas/Made-With-ML/search?l=python)\n  * Other 0.1%\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary\n\n**Made With ML** is a comprehensive resource for learning to design, develop, deploy, and iterate on production-grade machine learning applications. It emphasizes best practices in software engineering, MLOps, and CI/CD workflows. The course is suitable for developers, graduates, and product leaders, providing hands-on experience with tools like MLflow and Anyscale for managing ML workloads.\n```"
  },
  {
    "instruction": "[Skip to main content](https://www.datacamp.com/tutorial/speculative-decoding/#main)\n[](https://www.datacamp.com/)[**Write for us**](https://datacamp-1.gitbook.io/community-submissions)\nEN\n[Log In](https://www.datacamp.com/users/sign_in?redirect=%2Ftutorial%2Fspeculative-decoding)[Get Started](https://www.datacamp.com/users/sign_up?redirect=%2Ftutorial%2Fspeculative-decoding)\nTutorials\n[Blogs](https://www.datacamp.com/blog)\n[Tutorials](https://www.datacamp.com/tutorial)\n[docs](https://www.datacamp.com/doc)\n[Podcasts](https://www.datacamp.com/podcast)\n[Cheat Sheets](https://www.datacamp.com/cheat-sheet)\n[code-alongs](https://www.datacamp.com/code-along)\nCategory\nCategory\nTechnologies \nDiscover content by tools and technology\n[Artificial Intelligence](https://www.datacamp.com/tutorial/category/ai)[AWS](https://www.datacamp.com/tutorial/category/aws)[Azure](https://www.datacamp.com/tutorial/category/microsoft-azure)[Business Intelligence](https://www.datacamp.com/tutorial/category/learn-business-intelligence)[ChatGPT](https://www.datacamp.com/tutorial/category/chatgpt)[Databricks](https://www.datacamp.com/tutorial/category/databricks)[dbt](https://www.datacamp.com/tutorial/category/dbt)[Docker](https://www.datacamp.com/tutorial/category/docker)[Excel](https://www.datacamp.com/tutorial/category/excel)[Generative AI](https://www.datacamp.com/tutorial/category/generative-ai)[Git](https://www.datacamp.com/tutorial/category/git)[Google Cloud Platform](https://www.datacamp.com/tutorial/category/google-cloud-platform)[Hugging Face](https://www.datacamp.com/tutorial/category/Hugging-Face)[Java](https://www.datacamp.com/tutorial/category/java)[Julia](https://www.datacamp.com/tutorial/category/julia)[Kubernetes](https://www.datacamp.com/tutorial/category/kubernetes)[Large Language Models](https://www.datacamp.com/tutorial/category/large-language-models)[OpenAI](https://www.datacamp.com/tutorial/category/OpenAI)[PostgreSQL](https://www.datacamp.com/tutorial/category/postgresql)[Power BI](https://www.datacamp.com/tutorial/category/power-bi)[Python](https://www.datacamp.com/tutorial/category/python)[R](https://www.datacamp.com/tutorial/category/r-programming)[Scala](https://www.datacamp.com/tutorial/category/scala)[Snowflake](https://www.datacamp.com/tutorial/category/snowflake)[Spreadsheets](https://www.datacamp.com/tutorial/category/spreadsheets)[SQL](https://www.datacamp.com/tutorial/category/sql)[SQLite](https://www.datacamp.com/tutorial/category/sqlite)[Tableau](https://www.datacamp.com/tutorial/category/tableau)\nCategory\nTopics \nDiscover content by data science topics\n[AI for Business](https://www.datacamp.com/tutorial/category/ai-for-business)[Big Data](https://www.datacamp.com/tutorial/category/big-data)[Career Services](https://www.datacamp.com/tutorial/category/career-services)[Cloud](https://www.datacamp.com/tutorial/category/cloud)[Data Analysis](https://www.datacamp.com/tutorial/category/data-analysis)[Data Engineering](https://www.datacamp.com/tutorial/category/data-engineering)[Data Literacy](https://www.datacamp.com/tutorial/category/data-literacy)[Data Science](https://www.datacamp.com/tutorial/category/data-science)[Data Visualization](https://www.datacamp.com/tutorial/category/data-visualization)[DataLab](https://www.datacamp.com/tutorial/category/datalab)[Deep Learning](https://www.datacamp.com/tutorial/category/deep-learning)[Machine Learning](https://www.datacamp.com/tutorial/category/machine-learning)[MLOps](https://www.datacamp.com/tutorial/category/mlops)[Natural Language Processing](https://www.datacamp.com/tutorial/category/natural-language-processing)\n[Request a Demo](https://www.datacamp.com/business)[](https://www.datacamp.com/search-resources)\ncategory\n  1. [Home](https://www.datacamp.com)\n  2. [Tutorials](https://www.datacamp.com/tutorial)\n  3. [Artificial Intelligence](https://www.datacamp.com/tutorial/category/ai)\n\n\n# Speculative Decoding: A Guide With Implementation Examples\nLearn what speculative decoding is, how it works, when to use it, and how to implement it using Gemma2 models.\nList Contents\nNov 8, 2024 \u00b7 12 min read\nContents\n  * [What Is Speculative Decoding?](https://www.datacamp.com/tutorial/speculative-decoding/#what-is-speculative-decoding?-<span)\n\n\n  * [Traditional Decoding vs. Speculative Decoding](https://www.datacamp.com/tutorial/speculative-decoding/#traditional-decoding-vs.-speculative-decoding-<span)\n\n\n  * [Hands-On Example: Speculative Decoding With Gemma2 Models](https://www.datacamp.com/tutorial/speculative-decoding/#hands-on-example:-speculative-decoding-with-gemma2-models-<span)\n  * [Step 1: Model and Tokenizer setup](https://www.datacamp.com/tutorial/speculative-decoding/#step-1:-model-and-tokenizer-setup-<span)\n  * [Step 2: Autoregressive (normal) inference ](https://www.datacamp.com/tutorial/speculative-decoding/#step-2:-autoregressive-\\(normal\\)-inference\u00a0-<span)\n  * [Step 3: Speculative decoding ](https://www.datacamp.com/tutorial/speculative-decoding/#step-3:-speculative-decoding\u00a0-next,)\n  * [Step 4: Measuring latency](https://www.datacamp.com/tutorial/speculative-decoding/#step-4:-measuring-latency-after)\n\n\n  * [Testing Speculative Decoding on Five Prompts](https://www.datacamp.com/tutorial/speculative-decoding/#testing-speculative-decoding-on-five-prompts-<span)\n\n\n  * [Optimized Speculative Decoding With Quantization ](https://www.datacamp.com/tutorial/speculative-decoding/#optimized-speculative-decoding-with-quantization\u00a0\u00a0-theab)\n  * [4-bit quantization (weight compression)](https://www.datacamp.com/tutorial/speculative-decoding/#4-bit-quantization-\\(weight-compression\\)-<span)\n  * [bfloat16 for computation (efficient use of Tensor Cores)](https://www.datacamp.com/tutorial/speculative-decoding/#bfloat16-for-computation-\\(efficient-use-of-tensor-cores\\)-<span)\n  * [NF4 quantization type (optimized precision)](https://www.datacamp.com/tutorial/speculative-decoding/#nf4-quantization-type-\\(optimized-precision\\)-<span)\n  * [Double quantization disabled ](https://www.datacamp.com/tutorial/speculative-decoding/#double-quantization-disabled\u00a0-<span)\n\n\n  * [Applications of Speculative Decoding](https://www.datacamp.com/tutorial/speculative-decoding/#applications-of-speculative-decoding-<span)\n\n\n  * [Challenges of Speculative Decoding](https://www.datacamp.com/tutorial/speculative-decoding/#challenges-of-speculative-decoding-<span)\n\n\n  * [Conclusion](https://www.datacamp.com/tutorial/speculative-decoding/#conclusion-<span)\n\n\n[LLMs](https://www.datacamp.com/courses/large-language-models-llms-concepts) are so powerful, but they can often be a bit slow, and this is not ideal in scenarios where we need speed. Speculative decoding is a technique designed to speed LLMs by generating responses faster without compromising quality.\nIn essence, it\u2019s a way to \u201cguess ahead\u201d in the process of text generation, making predictions about the words that might come next while still allowing for the accuracy and depth we expect from LLMs.\nIn this blog, I\u2019ll explain what speculative decoding is, how it works, and how to implement it with Gemma 2 models.\n## What Is Speculative Decoding?\nSpeculative decoding accelerates LLMs by incorporating a smaller, faster model that generates preliminary predictions. This smaller model, often called the \u201cdraft\u201d model, generates a batch of tokens that the main, more powerful LLM can either confirm or refine. The draft model acts as a first pass, producing multiple tokens that speed up the generation process.\nInstead of the main LLM generating tokens sequentially, the draft model provides a set of likely candidates, and the main model evaluates them in parallel. This method reduces the computational load on the main LLM by offloading initial predictions, allowing it to focus only on corrections or validations.\n![Speculative Decoding pipeline](https://media.datacamp.com/cms/google/ad_4nxe9skexjvdpmgmrvqelgqhpfalhztym3paxvicwvo3znaeemswzztbayouvgg-laionqivqzlyj1aapawwbmg0vwu5uk9q-hj0sezmf9wyachjhgqbzrtyd2zgg1dmocddpkjn7rbuawnhei-vxbogguixu.png)\nThink of it like a writer with an editor. The main LLM is the writer, capable of producing high-quality text but at a slower pace. A smaller, faster \"draft\" model acts as the editor, quickly generating potential continuations of the text. The main LLM then evaluates these suggestions, incorporating the accurate ones and discarding the rest. This allows the LLM to process multiple tokens concurrently, speeding up text generation.\nLet's break down the process of speculative decoding into simple steps:\n  1. \u200bDraft generation: The smaller model (e.g., Gemma2-2B-it) generates multiple token suggestions based on the input prompt. These tokens are generated speculatively, meaning the model is not certain they are correct but provides them as \u201cdraft\u201d tokens.\n  2. Parallel \u200bverification: The larger model (e.g., Gemma2-9B-it) verifies these tokens in parallel, checking their probability against the model\u2019s learned distribution. If the tokens are deemed acceptable, they are used in the final output. If not, the larger model corrects them.\n  3. \u200bFinal output: Once the tokens are verified (or corrected), they are passed on to the user as the final output. This entire process happens much faster than traditional one-token-at-a-time decoding.\n\n\n## Traditional Decoding vs. Speculative Decoding\nTraditional decoding processes tokens one at a time, leading to high latency, but speculative decoding allows a smaller model to generate tokens in bulk, with the larger model verifying them. This can reduce response time by 30-40%, cutting latency from 25-30 seconds to as little as 15-18 seconds.\n![Traditional decoding vs speculative decoding](https://media.datacamp.com/cms/google/ad_4nxfg3vmjzp91nr8ry9x5_1y20vp_ly0osb_i5c_hrgm6ad-kwolpzc2iyq_c8ce1uwzxf_yerqqejsg-ptsocrsmb9zm2i-uc-5wp81j3-1vcf8jklmmegfgwthw_3elilwy_giitg.png)\nAdditionally, speculative decoding optimizes memory usage by shifting most of the token generation to the smaller model, reducing memory requirements from 26 GB to around 14 GB and making [on-device](https://www.datacamp.com/blog/edge-ai) inference more accessible.\nFinally, it lowers compute demands by 50%, as the larger model only verifies rather than generates tokens, enabling smoother performance on mobile devices with limited power and preventing overheating.\n## Hands-On Example: Speculative Decoding With Gemma2 Models\nTo implement a hands-on example of speculative decoding using the Gemma2 models. We\u2019ll explore how speculative decoding compares with standard inference in terms of both latency and performance.\n### Step 1: Model and Tokenizer setup\nTo get started, import the dependencies and set the seed.\nNext, check if GPU is available on the machine you are operating on. This is mainly required for big models such as Gemma 2-9B-it or LLama2-13B.\nFinally, we load both small and big model along with their tokenizers. Here, we are using the Gemma2-2b-it (instruct) model for the draft model and the Gemma2-9b-it model for verification.\nThere are a few other models that can be used alternatively as well. For example:\n  * Gemma 7B (main) & Gemma 2B (draft)\n  * Mixtral-8x7B (main) & Mistral 7B (draft)\n  * Pythia 12B (main) & Pythia 70M (draft)\n  * Llama 2 13B (main) & TinyLlama 1.1B (draft)\n\n```\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n# Set Seed\nset_seed(42)\n# Check if GPU is available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Load the smaller Gemma2 model (draft generation)\nsmall_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\")\nsmall_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n# Load the larger Gemma2 model (verification)\nbig_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\", device_map=\"auto\")\nbig_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\n### Step 2: Autoregressive (normal) inference \nFirst, we perform inference on just the large model (Gemma2-9b-it) and generate output. Start by [tokenizing](https://www.datacamp.com/tutorial/tiktoken-library-python) the input prompt and moving the tokens to the correct device (GPU if available). The `generate` method produces the model's output, generating up to `max_new_tokens`. The result is then decoded from token IDs back into human-readable text.\n```\n\ndef normal_inference(big_model, big_tokenizer, prompt, max_new_tokens=50):\n  inputs = big_tokenizer(prompt, return_tensors='pt').to(device)\n  outputs = big_model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens)\n  return big_tokenizer.decode(outputs[0], skip_special_tokens=True)\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\n### Step 3: Speculative decoding \nNext, we try the speculative decoding method, where we take the following steps:\n  1. Draft generation: The smaller model generates a draft of the text from the given prompt.\n  2. Verification: The larger model then verifies the draft by computing the log-likelihood for each token in the draft.\n  3. Log-likelihood calculation: We compute an average log-likelihood to determine how likely the large model considers the small model\u2019s draft to be correct.\n\n```\n\ndef speculative_decoding(small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens=50):\n  # Step 1: Use the small model to generate the draft\n  inputs = small_tokenizer(prompt, return_tensors='pt').to(device)\n  small_outputs = small_model.generate(inputs['input_ids'], max_new_tokens=max_new_tokens)\n  draft = small_tokenizer.decode(small_outputs[0], skip_special_tokens=True)\n  # Step 2: Verify the draft with the big model\n  big_inputs = big_tokenizer(draft, return_tensors='pt').to(device)\n  # Step 3: Calculate log-likelihood of the draft tokens under the large model\n  with torch.no_grad():\n    outputs = big_model(big_inputs['input_ids'])\n    log_probs = torch.log_softmax(outputs.logits, dim=-1)\n  draft_token_ids = big_inputs['input_ids']\n  log_likelihood = 0\n  for i in range(draft_token_ids.size(1) - 1):\n    token_id = draft_token_ids[0, i + 1]\n    log_likelihood += log_probs[0, i, token_id].item()\n  avg_log_likelihood = log_likelihood / (draft_token_ids.size(1) - 1)\n  # Return the draft and its log-likelihood score\n  return draft, avg_log_likelihood\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\nNote: Log-likelihood is the logarithm of the probability that a model assigns to a specific sequence of tokens. Here, it reflects how likely or \"confident\" the model is that the sequence of tokens (the generated text) is valid given the previous tokens.\n### Step 4: Measuring latency\nAfter implementing both techniques, we can measure their respective latencies. For speculative decoding, we assess performance by examining the log-likelihood value. A log-likelihood value approaching zero, particularly in the negative range, indicates that the generated text is accurate.\n```\n\ndef measure_latency(small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens=50):\n  # Measure latency for normal inference (big model only)\n  start_time = time.time()\n  normal_output = normal_inference(big_model, big_tokenizer, prompt, max_new_tokens)\n  normal_inference_latency = time.time() - start_time\n  print(f\"Normal Inference Output: {normal_output}\")\n  print(f\"Normal Inference Latency: {normal_inference_latency:.4f} seconds\")\n  print(\"\\n\\n\")\n  # Measure latency for speculative decoding\n  start_time = time.time()\n  speculative_output, log_likelihood = speculative_decoding(\n    small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens\n  )\n  speculative_decoding_latency = time.time() - start_time\n  print(f\"Speculative Decoding Output: {speculative_output}\")\n  print(f\"Speculative Decoding Latency: {speculative_decoding_latency:.4f} seconds\")\n  print(f\"Log Likelihood (Verification Score): {log_likelihood:.4f}\")\n  return normal_inference_latency, speculative_decoding_latency\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\nThis returns:\n  * Log-likelihood (verification score): -0.5242\n  * Normal inference latency: 17.8899 seconds\n  * Speculative decoding latency: 10.5291 seconds (about 70% faster)\n\n\nThe lower latency is thanks to less time taken by the smaller model for text generation and less time taken by the larger model for just verifying the generated text.\n## Testing Speculative Decoding on Five Prompts\nLet's compare speculative decoding with autoregressive inference by using five prompts:\n```\n\n# List of 5 prompts\nprompts = [\n  \"The future of artificial intelligence is \",\n  \"Machine learning is transforming the world by \",\n  \"Natural language processing enables computers to understand \",\n  \"Generative models like GPT-3 can create \",\n  \"AI ethics and fairness are important considerations for \"\n]\n# Inference settings\nmax_new_tokens = 200\n# Initialize accumulators for latency, memory, and tokens per second\ntotal_tokens_per_sec_normal = 0\ntotal_tokens_per_sec_speculative = 0\ntotal_normal_latency = 0\ntotal_speculative_latency = 0\n# Perform inference on each prompt and accumulate the results\nfor prompt in prompts:\n  normal_latency, speculative_latency, _, _, tokens_per_sec_normal, tokens_per_sec_speculative = measure_latency_and_memory(\n    small_model, big_model, small_tokenizer, big_tokenizer, prompt, max_new_tokens\n  )\n  total_tokens_per_sec_normal += tokens_per_sec_normal\n  total_tokens_per_sec_speculative += tokens_per_sec_speculative\n  total_normal_latency += normal_latency\n  total_speculative_latency += speculative_latency\n# Calculate averages\naverage_tokens_per_sec_normal = total_tokens_per_sec_normal / len(prompts)\naverage_tokens_per_sec_speculative = total_tokens_per_sec_speculative / len(prompts)\naverage_normal_latency = total_normal_latency / len(prompts)\naverage_speculative_latency = total_speculative_latency / len(prompts)\n# Output the averages\nprint(f\"Average Normal Inference Latency: {average_normal_latency:.4f} seconds\")\nprint(f\"Average Speculative Decoding Latency: {average_speculative_latency:.4f} seconds\")\nprint(f\"Average Normal Inference Tokens per second: {average_tokens_per_sec_normal:.2f} tokens/second\")\nprint(f\"Average Speculative Decoding Tokens per second: {average_tokens_per_sec_speculative:.2f} tokens/second\")\nPowered By [](https://www.datacamp.com/datalab)\n\n```\n```\n\nAverage Normal Inference Latency: 25.0876 seconds\nAverage Speculative Decoding Latency: 15.7802 seconds\nAverage Normal Inference Tokens per second: 7.97 tokens/second\nAverage Speculative Decoding Tokens per second: 12.68 tokens/second\nPowered By [](https://www.datacamp.com/datalab)\n\n```\n\nThis shows that speculative decoding is more efficient, generating more tokens per second than normal inference. This improvement is because the smaller model handles the majority of the text generation, while the larger model's role is limited to verification, reducing the overall computational load in terms of latency and memory. \nWith these memory requirements, we can easily deploy speculative decoding techniques on edge devices and gain speed-ups on our on-device applications such as chatbots, language translators, games, and more.\n## Optimized Speculative Decoding With Quantization \nThe above approach is efficient, but there is a trade-off between latency and memory optimization for on-device inference. To address this, let\u2019s apply [quantization](https://www.datacamp.com/tutorial/quantization-for-large-language-models) to both small and big models. You can experiment and try applying quantization to only the large model since the small model already takes the least space.\nQuantification is applied to smaller and larger models using the `BitsAndBytesConfig` configuration from the Hugging Face `transformers` library. Quantization allows us to significantly reduce memory usage and, in many cases, improve inference speed by converting the model\u2019s weights to a more compact form.\nAdd the following code snippet to the above code to witness the effects of quantization.\n```\n\nbnb_config = BitsAndBytesConfig(\n  load_in_4bit=True, # Enables 4-bit quantization\n  bnb_4bit_quant_type=\"nf4\", # Specifies the quantization type (nf4)\n  bnb_4bit_compute_dtype=torch.bfloat16, # Uses bfloat16 for computation\n  bnb_4bit_use_double_quant=False, # Disables double quantization to avoid additional overhead\n)\n# Load the smaller and larger Gemma2 models with quantization applied\nsmall_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\", quantization_config=bnb_config)\nbig_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b-it\", device_map=\"auto\", quantization_config=bnb_config)\nPowered By [](https://www.datacamp.com/datalab)\nWas this helpful? Yes No\n\n```\n\nHere is a quick comparison of outputs to showcase the effects of speculative decoding with and without quantization:\n![Comparisions with and without quantization](https://media.datacamp.com/cms/google/ad_4nxfup6giqvls9r9_fdmzkn_z7_1cuegd1ewdsrgbcipvva4xj5nlcgnpmtsadf1ehznithnkv4etzd6md0wk8zuipyk4nyp6wvlqgxhlpuwumx68rilmg8dq14m_punl35svrzz9t8hyuhoqx9ek2ptxzfm.png)\n### 4-bit quantization (weight compression)\nThe configuration specifies `load_in_4bit=True`, which means the model\u2019s weights are quantized from their original 32-bit or 16-bit floating-point format to 4-bit integers. This reduces the model's memory footprint. Quantization compresses the model\u2019s weights, allowing us to store and operate on them more efficiently. These are the concrete memory savings:\n  * By reducing the precision from 32-bit or 16-bit floats to 4-bit integers, each weight now takes up 1/4th or 1/8th of the original space, significantly reducing memory usage.\n  * This is reflected in the memory usage as:\n    * Normal inference memory usage: 26,458 MB\n    * Speculative decoding memory usage: 8,993 MB.\n\n\n### bfloat16 for computation (efficient use of Tensor Cores)\nThe configuration `bnb_4bit_compute_dtype=torch.bfloat16` specifies that the computation is performed in bfloat16 (BF16), a more efficient floating-point format. BF16 has a wider dynamic range than FP16 but takes up half the memory compared to FP32, making it a good balance between precision and performance.\nUsing BF16, especially on NVIDIA GPUs (like A100), uses Tensor Cores, which are optimized for BF16 operations. This results in faster matrix multiplications and other computations during inference.\nFor speculative decoding, we see improved latency:\n  * Normal inference latency: 27.65 seconds\n  * Speculative decoding latency: 15.56 seconds\n\n\nThe smaller memory footprint means faster memory access and more efficient use of GPU resources, leading to faster generation.\n### NF4 quantization type (optimized precision)\nThe `bnb_4bit_quant_type=\"nf4\"` option specifies Norm-Four Quantization (NF4), which is optimized for neural networks. NF4 quantization helps retain the precision of important parts of the model, even though the weights are represented in 4 bits. This minimizes the degradation in model performance compared to simple 4-bit quantization.\nNF4 helps achieve a balance between the compactness of 4-bit quantization and the accuracy of the model's predictions, ensuring that performance remains close to the original while drastically reducing memory usage.\n### Double quantization disabled \nDouble quantization (`bnb_4bit_use_double_quant=False`) introduces an additional layer of quantization on top of the 4-bit weights, which can further reduce memory usage but also add computation overhead. In this case, double quantization is disabled to avoid slowing down the inference.\n## Applications of Speculative Decoding\nThe potential applications of speculative decoding are vast and exciting. Here are a few examples:\n  * Chatbots and virtual assistants: To make those conversations with AI feel more natural and fluid with faster response times.\n  * Real-time translation: Speculative decoding reduces latency in real-time translation.\n  * Content generation: Speculative decoding speeds up content creation.\n  * Gaming and interactive applications: To improve the responsiveness of AI-powered characters or game elements for a more immersive experience, speculative decoding can help us attain almost real-time responses.\n\n\n![Applications of Speculative Decoding](https://media.datacamp.com/cms/google/ad_4nxdxjtdtb_3gotq7mu_ye_cb1wuva_dopcu3qjjrnkcwlbtchnxk1qssdq8oox6lgjuf0om_n86ffucxli8soawyypivtfxwltwaafyuuwdjc3uk2jsxysinerxwjsap-wb4yfjnl0v-a30-jog49fwy5xs.png)\n## Challenges of Speculative Decoding\nWhile speculative decoding holds immense promise, it's not without its challenges:\n  * Memory overhead: Maintaining multiple model states (for both the draft and main LLM) can increase memory usage, especially when larger models are used for verification.\n  * Tuning the draft model: Choosing the right draft model and tuning its parameters is crucial to strike the right balance between speed and accuracy, as an overly simplistic model may lead to frequent verification failures.\n  * Implementation complexity: Implementing speculative decoding is more complex than traditional methods, requiring careful synchronization between the small draft model and the larger verification model, as well as efficient error handling.\n  * Compatibility with decoding strategies: Speculative decoding currently supports only greedy search and sampling, limiting its use to more sophisticated decoding strategies such as beam search or diverse sampling.\n  * Verification overhead: If the smaller draft model generates tokens that frequently fail verification, the efficiency gains can be diminished, as the larger model will need to regenerate parts of the output, potentially negating the speed advantages.\n  * Limited support for batch processing: Speculative decoding typically doesn't support batched inputs, which can reduce its effectiveness in high-throughput systems requiring parallel processing of multiple requests.\n\n\n## Conclusion\nSpeculative decoding is a powerful technique that has the potential to revolutionize the way we interact with large language models. It can significantly speed up LLM inference without compromising the quality of the generated text. While there are challenges to overcome, the benefits of speculative decoding are undeniable, and we can expect to see its adoption grow in the coming years, enabling a new generation of faster, more responsive, and more efficient AI applications.\n![Aashi Dutt's photo](https://media.datacamp.com/legacy/v1725454401/aashi_pic_746310ff15.jpg?w=128)\nAuthor\n[Aashi Dutt](https://www.datacamp.com/portfolio/aashidutt3)\n[LinkedIn](https://www.linkedin.com/in/aashi-dutt)[Twitter](https://twitter.com/aashidutt?lang=en)\nI am a Google Developers Expert in ML(Gen AI), a Kaggle 3x Expert, and a Women Techmakers Ambassador with 3+ years of experience in tech. I co-founded a health-tech startup in 2020 and am pursuing a master's in computer science at Georgia Tech, specializing in machine learning.\nTopics\n[Artificial Intelligence](https://www.datacamp.com/tutorial/category/ai)[Large Language Models](https://www.datacamp.com/tutorial/category/large-language-models)\n![Aashi Dutt's photo](https://media.datacamp.com/legacy/v1725454401/aashi_pic_746310ff15.jpg?w=128)\n[Aashi Dutt](https://www.datacamp.com/portfolio/aashidutt3) Google Developers Expert in ML | Speaker | Kaggle 3x Expert \nTopics\n[Artificial Intelligence](https://www.datacamp.com/tutorial/category/ai)[Large Language Models](https://www.datacamp.com/tutorial/category/large-language-models)\n### [Gemini 2.0 Flash Thinking Experimental: A Guide With Examples](https://www.datacamp.com/blog/gemini-2-0-flash-experimental)\n### [Small Language Models: A Guide With Examples](https://www.datacamp.com/blog/small-language-models)\n### [AI Hallucination: A Guide With Examples](https://www.datacamp.com/blog/ai-hallucination)\n### [Speculative RAG Implementation With Transformers](https://www.datacamp.com/tutorial/speculative-rag)\n### [Fine Tuning Google Gemma: Enhancing LLMs with Customized Instructions](https://www.datacamp.com/tutorial/fine-tuning-google-gemma)\n### [Introduction to Large Language Models with GPT & LangChain](https://www.datacamp.com/code-along/introduction-to-large-language-models-gpt-langchain)\nLearn AI with these courses!\nTrack\n### [Developing AI Applications](https://www.datacamp.com/tracks/developing-ai-applications)\n23hrs hr\nLearn to create AI-powered applications with the latest AI developer tools, including the OpenAI API, Hugging Face, and LangChain.\n[See DetailsRight Arrow](https://www.datacamp.com/tracks/developing-ai-applications)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Ftracks%2Fdeveloping-ai-applications%2Fcontinue)\nCourse\n### [Developing AI Systems with the OpenAI API](https://www.datacamp.com/courses/developing-ai-systems-with-the-openai-api)\n3 hr\n5.2K\nLeverage the OpenAI API to get your AI applications ready for production.\n[See DetailsRight Arrow](https://www.datacamp.com/courses/developing-ai-systems-with-the-openai-api)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Fcourses%2Fdeveloping-ai-systems-with-the-openai-api%2Fcontinue)\nCourse\n### [AI Security and Risk Management](https://www.datacamp.com/courses/ai-security-and-risk-management)\n2 hr\n2.5K\nLearn the fundamentals of AI security to protect systems from threats, align security with business goals, and mitigate key risks. \n[See DetailsRight Arrow](https://www.datacamp.com/courses/ai-security-and-risk-management)[Start Course](https://www.datacamp.com/users/sign_up?redirect=%2Fcourses%2Fai-security-and-risk-management%2Fcontinue)\n[See MoreRight Arrow](https://www.datacamp.com/category/artificial-intelligence)\nRelated\n[blogGemini 2.0 Flash Thinking Experimental: A Guide With Examples](https://www.datacamp.com/blog/gemini-2-0-flash-experimental)\nLearn about Gemini 2.0 Flash Thinking Experimental, including its features, benchmarks, limitations, and how it compares to other reasoning models.\n![Alex Olteanu's photo](https://media.datacamp.com/legacy/v1718983935/DSC_00941_5_square_42e8868173.jpg?w=48)\nAlex Olteanu \n8 min\n[blogSmall Language Models: A Guide With Examples](https://www.datacamp.com/blog/small-language-models)\nLearn about small language models (SLMs), their benefits and applications, and how they compare to large language models (LLMs).\n![Dr Ana Rojo-Echebur\u00faa's photo](https://media.datacamp.com/cms/ana_rojo.png?w=48)\nDr Ana Rojo-Echebur\u00faa \n8 min\n[blogAI Hallucination: A Guide With Examples](https://www.datacamp.com/blog/ai-hallucination)\nLearn about AI hallucinations, their types, why they occur, their potential negative impacts, and how to mitigate them.\n[![Tom Farnschl\u00e4der's photo](https://media.datacamp.com/legacy/v1719243659/1698334986991_adfcb9390d.jpg?w=48)](https://www.datacamp.com/portfolio/tomfarnschlaeder)\nTom Farnschl\u00e4der \n8 min\n[TutorialSpeculative RAG Implementation With Transformers](https://www.datacamp.com/tutorial/speculative-rag)\nLearn Speculative RAG, a technique that improves RAG through a two-step drafting and verification process, and apply your skills with a hands-on implementation using Hugging Face Transformers.\n![Bhavishya Pandit's photo](https://media.datacamp.com/legacy/v1725454467/Unknown_f0f804ba73.jpg?w=48)\nBhavishya Pandit \n8 min\n[TutorialFine Tuning Google Gemma: Enhancing LLMs with Customized Instructions](https://www.datacamp.com/tutorial/fine-tuning-google-gemma)\nLearn how to run inference on GPUs/TPUs and fine-tune the latest Gemma 7b-it model on a role-play dataset.\n[![Abid Ali Awan's photo](https://media.datacamp.com/legacy/v1658155691/Abid_Ali_Awan_415cc44670.jpg?w=48)](https://www.datacamp.com/portfolio/kingabzpro)\nAbid Ali Awan \n12 min\n[code-alongIntroduction to Large Language Models with GPT & LangChain](https://www.datacamp.com/code-along/introduction-to-large-language-models-gpt-langchain)\nLearn the fundamentals of working with large language models and build a bot that analyzes data.\n[![Richie Cotton's photo](https://media.datacamp.com/legacy/v1658155296/richie_new_barnet_be7b436d1f.jpg?w=48)](https://www.datacamp.com/portfolio/richie)\nRichie Cotton \n[See More](https://www.datacamp.com/tutorial/category/ai)[See More](https://www.datacamp.com/tutorial/category/ai)\n## Grow your data skills with DataCamp for Mobile\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\n[Download on the App Store](https://datacamp.onelink.me/xztQ/45dozwue?deep_link_sub1=%7B%22src_url%22%3A%22https%3A%2F%2Fwww.datacamp.com%2Ftutorial%2Fspeculative-decoding%22%7D)[Get it on Google Play](https://datacamp.onelink.me/xztQ/go2f19ij?deep_link_sub1=%7B%22src_url%22%3A%22https%3A%2F%2Fwww.datacamp.com%2Ftutorial%2Fspeculative-decoding%22%7D)\n**Learn**\n[Learn Python](https://www.datacamp.com/blog/how-to-learn-python-expert-guide)[Learn AI](https://www.datacamp.com/blog/how-to-learn-ai)[Learn Power BI](https://www.datacamp.com/learn/power-bi)[Learn Data Engineering](https://www.datacamp.com/category/data-engineering)[Assessments](https://www.datacamp.com/signal)[Career Tracks](https://www.datacamp.com/tracks/career)[Skill Tracks](https://www.datacamp.com/tracks/skill)[Courses](https://www.datacamp.com/courses-all)[Data Science Roadmap](https://www.datacamp.com/blog/data-science-roadmap)\n**Data Courses**\n[Python Courses](https://www.datacamp.com/category/python)[R Courses](https://www.datacamp.com/category/r)[SQL Courses](https://www.datacamp.com/category/sql)[Power BI Courses](https://www.datacamp.com/category/power-bi)[Tableau Courses](https://www.datacamp.com/category/tableau)[Alteryx Courses](https://www.datacamp.com/category/alteryx)[Azure Courses](https://www.datacamp.com/category/azure)[AWS Courses](https://www.datacamp.com/category/aws)[Google Sheets Courses](https://www.datacamp.com/category/google-sheets)[Excel Courses](https://www.datacamp.com/category/excel)[AI Courses](https://www.datacamp.com/category/artificial-intelligence)[Data Analysis Courses](https://www.datacamp.com/category/data-analysis)[Data Visualization Courses](https://www.datacamp.com/category/data-visualization)[Machine Learning Courses](https://www.datacamp.com/category/machine-learning)[Data Engineering Courses](https://www.datacamp.com/category/data-engineering)[Probability & Statistics Courses](https://www.datacamp.com/category/probability-and-statistics)\n**DataLab**\n[Get Started](https://www.datacamp.com/datalab)[Pricing](https://www.datacamp.com/datalab/pricing)[Security](https://www.datacamp.com/datalab/security)[Documentation](https://datalab-docs.datacamp.com)\n**Certification**\n[Certifications](https://www.datacamp.com/certification)[Data Scientist](https://www.datacamp.com/certification/data-scientist)[Data Analyst](https://www.datacamp.com/certification/data-analyst)[Data Engineer](https://www.datacamp.com/certification/data-engineer)[SQL Associate](https://www.datacamp.com/certification/sql-associate)[Power BI Data Analyst](https://www.datacamp.com/certification/data-analyst-in-power-bi)[Tableau Certified Data Analyst](https://www.datacamp.com/certification/data-analyst-in-tableau)[Azure Fundamentals](https://www.datacamp.com/certification/azure-fundamentals)[AI Fundamentals](https://www.datacamp.com/certification/ai-fundamentals)\n**Resources**\n[Resource Center](https://www.datacamp.com/resources)[Upcoming Events](https://www.datacamp.com/webinars)[Blog](https://www.datacamp.com/blog)[Code-Alongs](https://www.datacamp.com/code-along)[Tutorials](https://www.datacamp.com/tutorial)[Docs](https://www.datacamp.com/doc)[Open Source](https://www.datacamp.com/open-source)[RDocumentation](https://www.rdocumentation.org)[Book a Demo with DataCamp for Business](https://www.datacamp.com/business/demo)[Data Portfolio](https://www.datacamp.com/data-portfolio)\n**Plans**\n[Pricing](https://www.datacamp.com/pricing)[For Students](https://www.datacamp.com/pricing/student)[For Business](https://www.datacamp.com/business)[For Universities](https://www.datacamp.com/universities)[Discounts, Promos & Sales](https://www.datacamp.com/promo)[DataCamp Donates](https://www.datacamp.com/donates)\n**For Business**\n[Business Pricing](https://www.datacamp.com/business/compare-plans)[Teams Plan](https://www.datacamp.com/business/learn-teams)[Data & AI Unlimited Plan](https://www.datacamp.com/business/data-unlimited)[Customer Stories](https://www.datacamp.com/business/customer-stories)[Partner Program](https://www.datacamp.com/business/partner-program)\n**About**\n[About Us](https://www.datacamp.com/about)[Learner Stories](https://www.datacamp.com/stories)[Careers](https://www.datacamp.com/careers)[Become an Instructor](https://www.datacamp.com/learn/create)[Press](https://www.datacamp.com/press)[Leadership](https://www.datacamp.com/about/leadership)[Contact Us](https://support.datacamp.com/hc/en-us/articles/360021185634)[DataCamp Espa\u00f1ol](https://www.datacamp.com/es)[DataCamp Portugu\u00eas](https://www.datacamp.com/pt)[DataCamp Deutsch](https://www.datacamp.com/de)[DataCamp Fran\u00e7ais](https://www.datacamp.com/fr)\n**Support**\n[Help Center](https://support.datacamp.com/hc/en-us)[Become an Affiliate](https://www.datacamp.com/affiliates)\n[Facebook](https://www.facebook.com/datacampinc/)[Twitter](https://twitter.com/datacamp)[LinkedIn](https://www.linkedin.com/school/datacampinc/)[YouTube](https://www.youtube.com/channel/UC79Gv3mYp6zKiSwYemEik9A)[Instagram](https://www.instagram.com/datacamp/)\n[Privacy Policy](https://www.datacamp.com/privacy-policy)[Cookie Notice](https://www.datacamp.com/cookie-notice)[Do Not Sell My Personal Information](https://www.datacamp.com/do-not-sell-my-personal-information)[Accessibility](https://www.datacamp.com/accessibility)[Security](https://www.datacamp.com/security)[Terms of Use](https://www.datacamp.com/terms-of-use)\n\u00a9 2025 DataCamp, Inc. All Rights Reserved.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nSpeculative decoding accelerates LLMs by using a smaller draft model to generate token predictions, which a larger model verifies. This method reduces latency by 30-40%, optimizing memory usage and compute demands, making it suitable for applications like chatbots and real-time translation.\n```"
  },
  {
    "instruction": "[\ud83d\ude80 Open-source RAG evaluation and testing with Evidently. New release](https://www.evidentlyai.com/blog/open-source-rag-evaluation-tool)![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc2747cf_vector.svg)\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66180fbf4f40e9ed73ca2d39_evidently_ai_logo_fi.png)](https://www.evidentlyai.com/)\nProduct\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d2624dc76fe4c62697aff8_chats-teardrop-duotone%20\\(1\\).svg)LLM Testing PlatformEvaluate LLM quality and safety](https://www.evidentlyai.com/llm-testing)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d3354a454c59d5f7affee2_list-magnifying-glass-duotone%20\\(3\\).svg)RAG TestingImprove retrieval, cut hallucinations](https://www.evidentlyai.com/rag-testing)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d2624e2132e631148cfd1a_warning-duotone%20\\(2\\).svg)AI Risk AssessmentIdentify AI risks and get a plan](https://www.evidentlyai.com/llm-readiness)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d33514b0f63efed5f253e9_crosshair-simple-duotone%20\\(1\\).svg)Adversarial TestingTest AI for threats and edge cases](https://www.evidentlyai.com/llm-red-teaming)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6650a6bcbdd0a756edf0d829_chart-bar-duotone%20\\(1\\).svg)ML MonitoringTrack data drift and predictive quality](https://www.evidentlyai.com/ml-monitoring)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9a94702f76fd53c7983b6_tree-structure-duotone%20\\(1\\).svg)AI Agent TestingValidate multi-step workflows](https://www.evidentlyai.com/ai-agent-testing)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7ee755c4dc43cefc2_users-three-duotone%20\\(1\\).svg)Open-SourceOpen-source Evidently Python library](https://www.evidentlyai.com/evidently-oss)\n##### [See Evidently in action![Evidently AI Testing for LLM](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/663511956e05f300480f19fd_llm_2.png)](https://www.evidentlyai.com/get-demo)[Get demo now![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/get-demo)\n[Pricing](https://www.evidentlyai.com/pricing)[Docs](https://docs.evidentlyai.com/)\nResources\n[![book](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abba265e01ed75b39988c_book-duotone.svg)BlogInsights on building AI products](https://www.evidentlyai.com/blog)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/675a45076643cf7042075aac_star-duotone%20\\(1\\).svg)LLM benchmarks100+ LLM benchmarks and datasets](https://www.evidentlyai.com/llm-evaluation-benchmarks-datasets)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7fbdd19d6d3f1875c_code-duotone%20\\(1\\).svg)TutorialsAI observability and MLOps tutorials](https://www.evidentlyai.com/mlops-tutorials)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb798a210c5d2640337_lightbulb-duotone%20\\(1\\).svg)ML and LLM system design500 ML and LLM use cases](https://www.evidentlyai.com/ml-system-design)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb798a210c5d2640368_check-duotone%20\\(1\\).svg)GuidesIn-depth AI quality and MLOps guides ](https://www.evidentlyai.com/mlops-guides)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdbd5bfc045c59b3467f_gear-duotone%20\\(1\\).svg)ML and AI platforms45+ internal ML and AI platforms](https://www.evidentlyai.com/ml-platforms)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7ee755c4dc43cefc2_users-three-duotone%20\\(1\\).svg)CommunityGet support and chat about AI products](https://www.evidentlyai.com/community)\n##### [Course on LLM evaluations for AI product teams![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/673f571568470a0a8d49aa43_three-cheerful-robot-students-take-exam%20\\(4\\).jpg)](https://www.evidentlyai.com/llm-evaluations-course)[Sign up now![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/llm-evaluations-course)\n[Sign up](https://www.evidentlyai.com/register)[Get demo](https://www.evidentlyai.com/get-demo)\n[GitHub](https://github.com/evidentlyai/evidently)\n[Sign up](https://www.evidentlyai.com/register)[Get demo](https://www.evidentlyai.com/get-demo)\n![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abe8f5bd7dda2d12a1e6d_list-bold.svg)\n###### [ML Monitoring](https://www.evidentlyai.com/blog-category/ml-monitoring)\n# Which test is the best? We compared 5 methods to detect data drift on large datasets\nLast updated:\nJanuary 9, 2025\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb7c4_6449cee400dfb78fe60dab0b_1_methods_data_drift_detection_blog_main-min.png)\n[Back to all blogs \u27f6](https://www.evidentlyai.com/blog)\ncontents**\u200d**\n[ Too much data, too much drift](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/#too-much-data-too-much-drift)\n[Picking the drift metric](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/#picking-the-drift-metric)\n[Experiment design](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/#experiment-design)\n[What did the experiments show?](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/#what-did-the-experiments-show)\n[Kolmogorov-Smirnov (KS) test](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/#kolmogorov-smirnov-ks-test)\n[Population Stability Index (PSI)](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/#population-stability-index-psi)\n[Kullback-Leibler divergence (KL)\u200d](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/#kullback-leibler-divergence-kl)\n[Jensen-Shannon divergence](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/#jensen-shannon-divergence)\n[Wasserstein distance (Earth-Mover Distance)](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/#wasserstein-distance-earth-mover-distance)\n[How do tests perform on real-world data?](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/#how-do-tests-perform-on-real-world-data)\n[Summing up](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/#summing-up)\nStart testing your AI systems today\n[Get demo](https://www.evidentlyai.com/get-demo)[Sign up free](https://www.evidentlyai.com/register)\n** _TL;DR:_**_We compared five different statistical tests for drift detection on large datasets. Our goal was to build intuition on how the tests react to data changes of varying magnitude. We also share the code so you can run the experiments on your data._ \u200d\nFrom this blog, you will get:\n  * **Key takeaways from the experiment.** You'll learn a few heuristics to help you choose between different statistical tests.\n  * **Jupyter notebook with all the code.** You'll be able to re-run the comparison to see how different statistical tests behave on your dataset.\n  * **Examples of the data distribution shift on different datasets.** You'll be able to compare the visual perception of drift against the outcomes of different statistical tests._\u200d_\n\n\n _Sounds interesting? Read on then!_\n_\u200d_ When ML models are in production, one often needs to keep tabs on the [data drift](https://www.evidentlyai.com/ml-in-production/data-drift) as a component of [ML model monitoring](https://www.evidentlyai.com/ml-in-production/model-monitoring). The goal is to detect changes in the input data distributions to make sure the model still operates in a familiar environment. Applying statistical tests to compare the new data with the old is one way to do it.\nHowever, the test outcomes may differ for \"small\" and \"large\" datasets.\n##### Get started with AI observability\n> Try our open-source library with over 25 million downloads, or sign up to Evidently Cloud to run no-code checks and bring all the team to a single workspace to collaborate on AI quality.[Sign up free \u27f6](https://www.evidentlyai.com/register)[Or try open source \u27f6](https://github.com/evidentlyai/evidently)\n## Too much data, too much drift\nEach statistical test has particular properties and in-built assumptions.Let's take a two-sample [Kolmogorov-Smirnov (KS) test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test). It is often a default choice for detecting a distributional change in numerical features. While it does the job in many cases, the test can be \"too sensitive\" for larger datasets. It would fire alarms for many real-world use cases all the time, just because you have a lot of data and small changes add up. You need to account for such test behavior when picking your drift metric.\n![data drift](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb824_6449cc03414eea1cb0445563_2_drift_detection_methods-min.png)\n## Picking the drift metric\nOK, so measuring data drift for large datasets is hard. The good news is choosing the proper statistical test for the occasion is half the battle.But there are dozens of tests out there! How do you choose the best one?We've run some experiments and compared five popular statistical tests to answer this. In particular, we wanted to explore how the test results differ based on the data volume and magnitude of change.We hope it will help shape your intuition on how different tests behave on large datasets and how to choose the right one for your scenario. We also included the [experimental notebook](https://colab.research.google.com/drive/1EFFcs0wDzToxSR6nw1umXDgPyeoP_Uk6) so you can reproduce the experiment on your dataset.\n> **And a few disclaimers:** \u200d1. We will only focus on numerical features to keep things simple.2. We will look at individual feature drift rather than multivariate (a whole different story!).3. This is not an academic exercise. Our goal is to remain practical and develop a few heuristics helpful for ML practitioners.\n## Experiment design\n**We picked five popular statistical tests and metrics for detecting data drift.** Then, we tested how they work on artificially shifted features and a few real datasets.If you are impatient, you can look at the results in the [experimental notebook](https://colab.research.google.com/drive/1EFFcs0wDzToxSR6nw1umXDgPyeoP_Uk6).**Here is the list of statistical tests we've played with:**\n  * [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)\n  * [Population Stability Index (PSI)](https://mwburke.github.io/data%20science/2018/04/29/population-stability-index.html)\n  * [Wasserstein distance](https://en.wikipedia.org/wiki/Earth_mover%27s_distance) (Earth-Mover distance)\n  * [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n  * [Jensen-Shannon distance](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)\n\n\n**First, we chose three features with different characteristics:**\n  * **Feature 1** \u2014 a continuous feature with non-normal distribution. For convenience, let's call it the \"Multimodal feature.\"\n  * **Feature 2** \u2014 a variable with a heavy right tail. We'll call it the \"Right-tailed feature.\"\n  * **Feature 3** \u2014 hereafter referred to as \"Feature with outliers.\"\n\n\nWe picked these variables from three different publicly available datasets. The goal was to have features with various distribution shapes. For each variable, we had from 500,000 to 1 million observations.\n![Feature distribution](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb828_62bddea362227a58c8508432_3_feature_distribution.png)\n_Feature distributions_\n**Then, we implemented a function to imitate data drift.** We wanted to introduce an artificial change that would resemble real-world drift. To do that, we decided to shift the whole distribution for each feature by a fixed value. Here is the formula we used to create drift:\n> _(alpha + mean(feature)) * perc_\n**By using the mean value of each feature, we ensured that:**\n  1. the shift was relative to the feature value range,\n  2. it was possible to compare how tests behave for different features with a specific \"drift size.\"\n\n\nWe also added a small \"alpha\" value. This created the shift even if the mean feature value is 0.Here is an example of the artificial 50%-drift:\n![How to imitate data drift](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb81d_62bddef544901454692b193b_4_artificial_drift.png)\n_Artificial 50-% drift_\nSomething like this can happen in the real world. Imagine that after a faulty update, the website becomes slower. This would look like an increase in the average load time for all users by some delta.\n\u200d** _Disclaimer:_**_this is not the only way to imitate data drift. You can experiment with another method of simulating drift\u2014or create your own\u2014in the_[ _example notebook_](https://colab.research.google.com/drive/1EFFcs0wDzToxSR6nw1umXDgPyeoP_Uk6) _._\n_\u200d_ Once we came up with this artificial drift function, we applied this change to all or some points in the \"current\" dataset. We used two sets of parameters to imitate various degrees of drift:\n  * **drift_size:** the percent by which we increase the initial values,\n  * **drift_ratio:** the share of data points to which we apply the defined increase to imitate drift in a segment of data.**\u200d**\n\n\n**Next, we started the experiments!** In each evaluation, we sampled two equally-sized groups from the distributions of each feature as if we had a \"reference\" and \"current\" data we could compare. We used samples of different sizes and created artificial drifts of varying magnitude.\nWe looked to answer three questions for each statistical test we tried.\n### How does the sample size influence the test results?\n![Drift detection ratio depending on the size of a dataset](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb804_62bddf84b242b32c9d3fc2c2_5_sample_size_example_ks.png)\n_Rate of the data drift detection depending on the sample size, Kolmogorov-Smirnov (KS) test_\nIn other words, will the test give a different outcome if we compare datasets of different sizes?Yes, this is statistics! The results will be different when comparing the \"same\" distributions but taking a sample small or large.**To imitate the drift, we shifted each feature's \"current\" distribution.** We increased the values of each \"current\" data point by 0.5% as if the change was tiny.**Then, we played with the sample size, going from 1,000 objects to 1,000,000.** We applied the selected drift test for each combination and recorded the result. To safeguard against random fluctuations, we applied sampling and artificial shifts a hundred times. Then we looked at the \"drift_detected_ratio\" to identify how often drift is detected during these experiments for each sample size.Rinse and repeat, for all five tests.\n> Our goal was to demonstrate how sensitive each test is to sample size. We kept the magnitude of change small and fixed and only varied the size of the datasets we compared against each other.\n\u200d\n### How does the magnitude of data change influence the test results?\nIn other words, will the test detect even a \"small\" change in the data, or does it only respond to a \"large\" one?**In this experiment, we again artificially shifted the feature distribution in the current group.** We sequentially applied different changes to imitate small or large drifts, shifting data by 1%, 5%, 7%, 10%, and 20%. We applied this change to the whole dataset.**We fixed the sample size and compared samples with 100,000 observations.** We talk about large datasets, after all!For each combination, we applied our chosen drift tests. The goal was to evaluate how sensitive the test is to the \"size of drift.\"To illustrate the idea, here is what 5%-change looks like in practice. You can still notice it, but the shift doesn't look that vivid.\n![Data drift for features with different distribution types](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb7f5_62bddffa2799318fa62a9a7f_6_five_percent_drift.png)\n_5%-change in data for various feature types_\nAnd here, the 20%-shift is seen with a naked eye:\n![Data drift for features with different distribution types](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb807_62bde0162ea3c0848b6e9167_7_twenty_percent_drift.png)\n_20%-change in data for various feature types_\n### Is the test sensitive to a change in the data segment?\nWe also evaluated whether the statistical tests would react to the drift in one dataset segment.**In this experiment, we shifted the data by 5%, 10%, 30%, 70%, and 100%.** But this time, we made this perturbation **only to the 20% of the observations** , imitating a change in one data segment. This is relevant for many applied use cases when the change only happens in some part of the population, for example, a specific geographic area.As an illustration, here is what a 50%-shift in data looks like if only 20% of observations are drifted.\n![Data drift in a segment of data](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb838_62bde05151585b567d5c6d0b_8_fifty_percent_drift_segment.png)\n_50%-change in a 20%-segment of data for various feature types_\nOnce again, we checked how each test responds to this type of change. We kept the size of the sample fixed at 100,000 observations.\nAfter running the experiments on our artificial dataset, we applied them to several real-life datasets to complement our findings.\nIf you want to see the whole output, check out the [notebook](https://colab.research.google.com/drive/1EFFcs0wDzToxSR6nw1umXDgPyeoP_Uk6). Otherwise, read on for the most curious take-aways!\n## What did the experiments show?\nLet's look at the results we got for each drift detection method.We'll show three groups of plots based on the experiment design.\n> **How to read the plots belowHow does the sample size influence test results?\u200d****Drift_detected_ratio** against the **sample_size**. It will show how often drift is detected in the case of the minor (0.5%) shift following the change in the sample size. In this case, drift detection is a binary outcome. For example, the statistical test result at a 0.95 confidence level.**Mean_drift_score** against the **sample_size**. It gives more information about particular drift test outcomes, such as p-value in the case of statistical tests or statistical distance in the case of distance metrics.**How does the magnitude of data change influence the test results?****Mean_drift_score** against **drift_size** (dataset drift). This shows how sensitive the test is to the magnitude of the artificial drift in the whole dataset at a fixed sample size of 100,000.**Is the test sensitive to a change in the data segment?****Mean_drift_score** against **drift_size** (segment drift). This shows how sensitive the test is to the magnitude of the artificial drift in the 20%-data segment at a fixed sample size of 100,000.\n### Kolmogorov-Smirnov (KS) test\nThe [KS test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) is a nonparametric statistical test. This means it does not make any assumptions about the underlying distribution, e.g., whether it's normal or not.The null hypothesis is that the two samples come from the same distribution. The KS-test is applied to reject or accept it.**It returns the p-value.** If the **p-value is less than 0.05** , you can usually declare that there is strong evidence to reject the null hypothesis and consider the two samples **different**.You can also set a different significance level and, for example, react only to p-values less than 0.01. It is good to remember that the p-value is not a \"measurement\" of drift size but a declaration of the statistical test significance.**In our experiment, we use the default 0.95 significance.If the p-value is < 0.05, we'll alert on the drift.**The graphs below show that the KS test tends to be pretty sensitive in larger datasets. It raises flags even for a minor change of 0.5%, as soon as we have more than 100,000 objects in a dataset.\n![Mean drift score and Drift detected ratio, depending on the sample size, Kolmogorov-Smirnov test](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb841_62bde113edb3f74ba9499be5_9_ks_sample_size.png)\n_Mean drift score and Drift detected ratio, depending on the sample size_\nBut does it stand true as the drift size changes? It looks so. For a fixed sample size of 100,000 observations, the p-value is close to zero for the dataset drift as small as 1%. Meaning it detects the drift early and often.\nKS is sensitive to a change in the 20%-data segment as well. The p-value approaches zero for the data drift of 5% and above.\n![Mean drift score, depending on the drift size, Kolmogorov-Smirnov test](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb820_62bde1345fb3ae46e37b2ce5_10_ks_drift_size.png)\n_Mean drift score, depending on the drift size. Sample size = 100,000_\n\u200d\nKolmogorov-Smirnov test is often used to detect drift in numerical features by default. It does the job, but you should keep in mind its sensitivity in the case of large datasets. The larger the dataset, the bigger the test's statistical power, aka sensitivity. If there is no need to spot the changes with pinpoint accuracy, the KS test might not be your best option.We'd recommend using the KS test if you have fewer observations (e.g., under 1000). It is also a good fit when you expect data to be stable and want to react even to a slight deviation inside a particular data segment.If you work with larger datasets, you might want to take a sample before applying the KS test.\n> **Interpreting the \"sensitivity\" of different tests to the drift size** To do this, you can look at the slope of the curve.If the increase/decrease is gradual, the test results for the drift of different sizes are similar. There is not much difference between individual points, and it is difficult to pick the threshold to distinguish \"drift\" from \"not drift.\" Meaning the test is not very sensitive.If the curve is steep, the test gives different results as the drift becomes larger. There is a sharp increase/decrease and a visible difference between individual points. Defining a moment when the test starts confidently detecting drift is easier. The test is more sensitive.In other words, if one of the graphs has a steeper curve, this test is more sensitive to the magnitude of change.**Note:** In the case of statistical tests like KS, the curve goes down: a smaller p-value means more confident drift detection.In the case of a distance metric like WD, the curve goes up: a larger value means larger drift.\n### Population Stability Index (PSI)\n[Population Stability Index](https://mwburke.github.io/data%20science/2018/04/29/population-stability-index.html) is another popular drift metric for numerical and categorical features. It is often used in domains like finance.Instead of a p-value, you get a number that can take **any value from 0 and above**. It also reflects the relative \"size\" of the drift: the larger the PSI value, the more different the distributions are. Practitioners commonly interpret the results as the following:\n  * PSI < 0.1: no significant population change\n  * 0.1 \u2264 PSI < 0.2: moderate population change\n  * PSI \u2265 0.2: significant population change\n\n\n**In our experiment, we declare feature drift if the PSI is > 0.1.**\n**\u200d**\n![Mean drift score and Drift detected ratio, depending on the sample size, PSI](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb855_62bde1b8b7f69d75b30175ed_11_psi_sample_size.png)\n _Mean drift score and Drift detected ratio, depending on the sample size_\nFor a minor change of 0.5%, the sample size has a low effect on the PSI value. The PSI test simply does not detect drift of this size. This is true for both small and large samples.As we increase the magnitude of drift, the PSI remains \"silent\" for a while. For a fixed sample size of 100,000, PSI only starts detecting significant change for a drift size larger than 10%. When it comes to the segment drift, PSI has the chance of detecting only major changes, such as the 100%-shift in the data segment.\n![Mean drift score, depending on the drift size, PSI](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb7f8_6449cc66ae44b8916c0f2bad_12_psi_drift_size-min.png)\n_Mean drift score, depending on the drift size. Sample size = 100,000_\nIn other words, PSI has low sensitivity. Unlike KS, it returns the same result regardless of the sample size. This makes it more \"predictable.\"The benefit of PSI is its interpretability. We'd recommend using the PSI in industries already familiar with the approach. It might be helpful to rely on existing rules of thumb to define \"drift size,\" especially for business stakeholders.You can also consider this test if you have a lot of data and want to react only to \"major changes.\"\u200d\n### Kullback-Leibler divergence (KL)[\u200d](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n[Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), aka relative entropy, is another statistical measure that quantifies the drift for numerical and categorical features.Just like with PSI, you have to first define a number of bins to use this metric for numerical features. Due to this, the KL metric value **does not depend on the size of a sample** : you are comparing histograms in the end. However, the choice of the binning strategy itself can impact the results.Like PSI, it returns a score that can serve as the measure of the drift. A KL score can range **from 0 to infinity**. A score of 0 tells us that the distributions are identical. The higher the score, the more different the distributions are. Unlike PSI, it is not symmetric. In other words, you get different values when you swap reference and sample distributions.In practical terms, KL behavior is very much like PSI. In the case of minor 0.5%-data change, the test has low sensitivity for both small and large datasets.\n![Mean drift score and Drift detected ratio, depending on the sample size, Kullback-Leibler divergence](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb7fb_62bde2179caef266e86ee3b7_13_kl_sample_size.png)\n_Mean drift score and Drift detected ratio, depending on the sample size_\nThe picture is almost identical to the PSI if we compare results for different drift sizes in the whole dataset and a 20%-segment of data.\n![Mean drift score, depending on the drift size, Kullback-Leibler divergence](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb801_6449ccb6823e4440e87a623e_14_kl_drift_size-min.png)\n_Mean drift score, depending on the drift size. Sample size = 100,000_\nLike a twin brother, Kullback-Leibler behaves similarly to PSI. The drift results are consistent regardless of the sample size. KL divergence is also not very sensitive to smaller changes in the whole dataset or one segment.To sum up, KL is a good default test to detect drift in larger datasets. However, it's best to keep in mind its asymmetry when interpreting the drift score. It is not a distance metric. You can generally use it as an estimate of the \"degree of drift,\" but cannot compare the \"drift sizes\" between each other.\n### Jensen-Shannon divergence\n[Jensen-Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) can be applied to numerical and categorical features. It is another way to calculate the difference between two probability distributions.JS is based on Kullback-Leibler divergence with two major differences: it is always finite and symmetric. The square root of the JS divergence is a metric often referred to as Jensen-Shannon distance. This is the metric we'll use!JS distance returns **a score between 0 and 1**. \"0\" corresponds to identical distributions and \"1\" to absolutely different. This makes this metric fairly interpretable.**In our experiment, 0.1 means drift.**\n![Mean drift score and Drift detected ratio, depending on the sample size, Jensen-Shannon divergence](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb84c_62bde29951585be0615c83e7_15_js_sample_size.png)\n_Mean drift score and Drift detected ratio, depending on the sample size_\nFor a minor 0.5%-data change, the test shows low sensitivity following the variation in the sample size. The test becomes more sensitive when the drift exceeds 10% for a fixed sample size of 100,000 observations. However, just like PSI, it barely detects drift of larger magnitudes if only the 20%-segment of data is drifted.\n![Mean drift score, depending on the drift size, Jensen-Shannon divergence](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb7e7_6449ccfec679c66e09d7baf6_16_js_drift_size-min.png)\n_Mean drift score, depending on the drift size. Sample size = 100,000_\n\u200d\nIn other words, the Jensen-Shannon test shows stable behavior for large datasets. It tends to be slightly more sensitive than Kullback-Leibler divergence and PSI. It is a good measure to detect significant changes in the whole dataset. Since you have to define a binning strategy first, it does not depend on the sample size.\n### Wasserstein distance (Earth-Mover Distance)\n[Wasserstein distance (WD)](https://en.wikipedia.org/wiki/Earth_mover%27s_distance) is applied only for numerical features. By default, it shows the absolute value of data drift. Roughly speaking, it measures how much effort it takes to turn one distribution into another. Here is an explanation of the intuition behind WD: if drift happens in one direction (e.g., all values increase), the absolute value of the WD metric often equals the difference of means.Even if the changes happen in both directions (some values increase, some values decrease), the WD metric will sum them up to reflect the change. Had we used the difference of means, these changes would \"cancel\" each other. This makes WD a more informative metric.However, if you have two different features\u2014say \"grams\" and \"years\"\u2014you'll need to interpret each distance separately. Imagine having a hundred features instead of two. That doesn't look very practical, right?**One solution is to turn the absolute values into relative.** Let's do so by dividing the absolute metric by standard deviation. The normed WD metric shows the number of standard deviations, on average, you should move each object of the current group to match the reference group.This normed WD metric is pretty interpretable. When setting the drift detection threshold, you can rely on the intuition of what a \"standard deviation\" is. In a simplified way, when you set the WD threshold to 0.1, you define that the change in the size of \"0.1 standard deviations\" is something you want to notice.Now back to our experiment. The normed WD metric returns **a value from 0 to infinity** , making the degree of drift comparable between features.**Once again, we consider the 0.1 value as drift.**\n![Mean drift score and Drift detected ratio, depending on the sample size, Wasserstein distance](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb84f_62bde2f3920727ddb59ff5fa_17_wd_sample_size.png)\n_Mean drift score and Drift detected ratio, depending on the sample size_\nWhen the sample size is \"small,\" WD tends to overestimate the effect, though way less than Kolmogorov-Smirnov (KS) does. It becomes sustainable for a sample size of more than 100,000.\n![Mean drift score, depending on the drift size, Wasserstein distance](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb80a_6449cd40a51b974f62cda858_18_wd_drift_size-min.png)\nMean drift score, depending on the drift size. Sample size = 100,000\n\u200d\nThe WD metric tends to be more sensitive than PSI. Overall, it can be a good compromise between \"way-too-sensitive\" KS and \"notice-only-big-changes\" PSI and JS.\n## How do tests perform on real-world data?\nOK, we've tested our metrics on sample data with the artificial data drift. In reality, however, there is no such thing as a predefined \"drift size.\" Instead, drift just happens and comes in different shapes and forms. Let's take a look at real-world data!**Here is what we'll do:**\n  * pick a feature from a dataset\n  * sort its values by time or index\n  * take two consecutive intervals as \"reference\" and \"current\"\n  * and see how our statistical tests perform!\n\n\nYou'll find six examples below. Each time we applied different statistical tests to see whether they detected data drift or not. Yes, it is not pure science. But isn't learning by example something we swear by in machine learning?\n> Our goal was to build intuition on how test results relate to the visual changes in data distribution. Keep in mind, though:1. visualization is always a simplification2. there is no correct answer to the \"drift/no drift\" question, as it all depends on the task.\n\u200d\n**Example 1. The change is \"huge,\" and all tests are in agreement.**\nThat should be an easy one!The size of the current and reference dataset is 100,000.\n![Evidently data drift plots example 1](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb81a_62bde36a18930cd9121e8310_19_example1_plots.png)\n![Evidently data drift table example 1](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb82b_6449cd94a255aeaa5a207ba3_20_example1_metrics-min.png)\n**Example 2. The change is \"minor,\" but enough for KS to react.**\nThis is a good example of why KS tends to be \"too sensitive.\" Still, in some domains with generally stable data, you might be interested even in a minor pattern change.The current and reference datasets have 200,000 observations each.\n![Evidently data drift plots example 2](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb7fe_62bde39146ffd5dc3a06c831_21_example2_plots.png)\n![Evidently data drift table example 2](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb83e_6449cdcca255ae5eb42083bb_22_example2_metrics-min.png)\n**Example 3. A change in the trend. Results differ!**\nThis is an interesting example due to sudden change at the end of the observed period. The outcomes would also vary had we picked a different comparison window.You might notice that PSI reacted to drift even though WD did not. PSI is sensitive to new values, while WD only compares the distance between the two distributions and does not \"care\" about particular spikes. Another interesting trait to keep in mind!The current dataset has 90,000 observations, and the reference has 70,000.\n![Evidently data drift plots example 3](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb82e_62bde3af2ea3c01ebf6eadbe_22_example3_plots.png)\n![Evidently data drift table example 3](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb814_6449cdfb7651e24603224dd2_23_example3_metrics-min.png)\n**Example 4. An \"obvious\" drift, once again.**\nThe example is somewhat similar to the above one. But this time, the data split between the comparison windows is more clear-cut. All tests are in agreement.The size of the current and reference dataset is 120,000.\n![Evidently data drift plots example 4](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb863_62bde3cd0c128f268cb1ebb1_24_example4_plots.png)\n![Evidently data drift table example 4](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb811_6449ce2b4b1e63504a187d66_25_example4_metrics-min.png)\n**Example 5. A sudden spike. Only KL missed it!**\nA different type of change, and most tests catch it, except for KL.The number of observations in the current and reference datasets is 10,000 each.\n![Evidently data drift plots example 5](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb852_62bde3edb242b3f5583fe0ee_26_example5_plots.png)\n![Evidently data drift table example 5](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb832_6449ce5963779b630c782d11_27_example5_metrics-min.png)\n**Example 6. Could be drift, could be not.**\nIs this the drift you'd want to detect? This should inform your choice of tests.The size of the current and reference dataset is 100,000.\n![Evidently data drift plots example 6](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb80e_62bde410904e7c0ebd104bef_28_example6_plots.png)\n![Evidently data drift table example 6](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb83b_6449ce84fca28242d250d275_29_example6_metrics-min.png)\n## Summing up\nThere is no such thing as \"objective\" data drift and a \"perfect\" test for it. It depends on the use case and data.**To shape the approach, you can consider:**\n**The size of drift you want to detect.** In some cases, you want to react only to large changes. In others, even to a minor one. The definition of \"large\" and \"small\" would vary based on your data.\n**The size of samples you will compare.** Since different tests give different results depending on the sample size, you should factor it in. In healthcare use cases, you might have hundreds of objects, in payments\u2014millions.\n**The cost of the model performance drop.** If every mistake is expensive, you might want to pick a more sensitive test, even at the cost of false-positive alerts.\u200d\n**You can still use some heuristics! Start with a problem you're trying to solve:**\n**If the accuracy is key, pick KS.** If you deal with cases where even a 1%-drift is critical, you can choose the most sensitive test. Just control the sample size to make sure it is not too large. You can use statistical power analysis to pick the optimal sample size depending on the size of data drift.\n**If you want to detect reasonable drift, pick WD.** You can set a threshold according to the number of standard deviations you assume the distribution should change to qualify for \"drift.\"\n**If you are used to PSI, go for it.** It can work just fine for features where fluctuation is a norm, and you only want to be alerted on significant changes. While there is no quantitative intuition, you can rely on historical data to set the threshold. As a rule of thumb, you can interpret the results as follows:\n  * PSI < 0.1: no significant population change\n  * 0.1 \u2264 PSI < 0.2: moderate population change\n  * PSI \u2265 0.2: significant population change\n\n\n**If you care about data segments, monitor them separately.** No test is perfect here. The smaller the segment, the harder it is to detect drift. If even a minor change is critical, it makes sense to monitor segments separately from the rest of the dataset.It is always best to experiment! You can tweak your initial approach by looking at the [past drift patterns](https://evidentlyai.com/blog/tutorial-3-historical-data-drift). You can also start with reasonable defaults and then iterate based on the results of the production model monitoring.\n## Can I run these tests on my own?\nSure! Here's a sample [notebook](https://colab.research.google.com/drive/1EFFcs0wDzToxSR6nw1umXDgPyeoP_Uk6) to repeat the experiment or run all these tests on your dataset.\n## How to implement drift detection in production?\nIf you want to implement these tests to detect data drift as part of your ML pipeline, you can use [Evidently](https://github.com/evidentlyai/evidently), an open-source Python library. It helps evaluate, test, and monitor the performance of ML models in production. It also has an in-built algorithm that selects a suitable drift test based on the feature type, number of observations, and unique values.In other words, we added reasonable defaults so that you don't have to manually pick statistical tests to detect data drift\u2014unless you want to!\n![Evidently data drift dashboard](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0d159e0238ac5bcb7f1_6449ceafae44b8f0dd0f63e8_30_evidently_example-min.png)\n**__________________**\n**Acknowledgments** \u200d\n\u200d _Thanks to_[ _Elena Samuylova_](https://www.linkedin.com/in/elenasamuylova/) _and_[ _Emeli Dral_](https://www.linkedin.com/in/emelidral/) _for a thorough fact-checking, feedback on the experiment design, and enormous support while preparing this blog._\nwritten by\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/66266589fe4b9b284471ac0b_62bcd7d0d21b0e576c9de3fe_olga%2520filipova_round.jpeg)\n#### [Olga FilippovaLead Data ScientistEvidently AI](https://www.evidentlyai.com/authors/olga-fillipova)\n![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/66266589dd958874dc9ded66_62bcd717c16504f4b3ff1053_unnamed-3-2.jpeg)\n#### [Dasha MaliuginaCommunity ManagerEvidently AI](https://www.evidentlyai.com/authors/dasha-maliugina)\n[#data-drift](https://www.evidentlyai.com/blog-tag/data-drift)\n[#code-example](https://www.evidentlyai.com/blog-tag/code-example)\nshare on\n[![LinkedIn logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27475f_Group%2068.svg)](https://www.linkedin.com/)[![Twitter logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/665498e176df469709a54190_x-logo%20\\(1\\).svg)](http://twitter.com/)[![Facebook logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc274774_Group%2065.svg)](http://facebook.com)\n## You might also like\n[![To retrain, or not to retrain? Let's get analytical about ML model updates](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0e1ce5af6ba15764ba0_62e1bd5f785b4a8dafb00231_blog_retrain_or_not_.png)ML MonitoringTo retrain, or not to retrain? Let's get analytical about ML model updatesIs it time to retrain your machine learning model? Even though data science is all about\u2026 data, the answer to this question is surprisingly often based on a gut feeling. Can we do better?](https://www.evidentlyai.com/blog/retrain-or-not-retrain)\n[![\"My data drifted. What's next?\" How to handle ML model drift in production.](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/6625e0de1524d2ef8cdcf9f7_644964daa13aa654c39e067b_evidently_data_drift-7-min.png)ML Monitoring\"My data drifted. What's next?\" How to handle ML model drift in production.What can you do once you detect data drift for a production ML model? Here is an introductory overview of the possible steps.](https://www.evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\ud83c\udf93 Free course on LLM evaluations for AI product teams.[ Sign up **\u27f6**](https://www.evidentlyai.com/llm-evaluations-course)\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/66180fbf4f40e9ed73ca2d39_evidently_ai_logo_fi.png)](https://www.evidentlyai.com/)\nProduct\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d2624dc76fe4c62697aff8_chats-teardrop-duotone%20\\(1\\).svg)LLM Testing PlatformEvaluate LLM quality and safety](https://www.evidentlyai.com/llm-testing)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d3354a454c59d5f7affee2_list-magnifying-glass-duotone%20\\(3\\).svg)RAG TestingImprove retrieval, cut hallucinations](https://www.evidentlyai.com/rag-testing)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d2624e2132e631148cfd1a_warning-duotone%20\\(2\\).svg)AI Risk AssessmentIdentify AI risks and get a plan](https://www.evidentlyai.com/llm-readiness)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67d33514b0f63efed5f253e9_crosshair-simple-duotone%20\\(1\\).svg)Adversarial TestingTest AI for threats and edge cases](https://www.evidentlyai.com/llm-red-teaming)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/6650a6bcbdd0a756edf0d829_chart-bar-duotone%20\\(1\\).svg)ML MonitoringTrack data drift and predictive quality](https://www.evidentlyai.com/ml-monitoring)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/67c9a94702f76fd53c7983b6_tree-structure-duotone%20\\(1\\).svg)AI Agent TestingValidate multi-step workflows](https://www.evidentlyai.com/ai-agent-testing)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7ee755c4dc43cefc2_users-three-duotone%20\\(1\\).svg)Open-SourceOpen-source Evidently Python library](https://www.evidentlyai.com/evidently-oss)\n##### [See Evidently in action![Evidently AI Testing for LLM](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/663511956e05f300480f19fd_llm_2.png)](https://www.evidentlyai.com/llm-evaluations-course)[Get demo now![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/llm-evaluations-course)\n[Pricing](https://www.evidentlyai.com/pricing)[Docs](https://docs.evidentlyai.com/)\nResources\n[![book](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abba265e01ed75b39988c_book-duotone.svg)BlogInsights on building AI products](https://www.evidentlyai.com/blog)[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/675a45076643cf7042075aac_star-duotone%20\\(1\\).svg)LLM benchmarks100+ LLM benchmarks and datasets](https://www.evidentlyai.com/llm-evaluation-benchmarks-datasets)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7fbdd19d6d3f1875c_code-duotone%20\\(1\\).svg)TutorialsAI observability and MLOps tutorials](https://www.evidentlyai.com/mlops-tutorials)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb798a210c5d2640337_lightbulb-duotone%20\\(1\\).svg)ML and LLM system design500 ML and LLM use cases](https://www.evidentlyai.com/ml-system-design)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb798a210c5d2640368_check-duotone%20\\(1\\).svg)GuidesIn-depth AI quality and MLOps guides](https://www.evidentlyai.com/mlops-guides)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdbd5bfc045c59b3467f_gear-duotone%20\\(1\\).svg)ML and AI platforms45+ internal ML and AI platforms](https://www.evidentlyai.com/ml-platforms)[![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abdb7ee755c4dc43cefc2_users-three-duotone%20\\(1\\).svg)CommunityGet support and chat about AI products](https://www.evidentlyai.com/community)\n##### [Course on LLM evaluations for AI product teams![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/673f571568470a0a8d49aa43_three-cheerful-robot-students-take-exam%20\\(4\\).jpg)](https://www.evidentlyai.com/llm-evaluations-course)[Sign up now![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc27473e_Group%209.svg)](https://www.evidentlyai.com/llm-evaluations-course)\n[Sign up](https://www.evidentlyai.com/register)[Get demo](https://www.evidentlyai.com/get-demo)\n[GitHub](https://github.com/evidentlyai/evidently)\n[Sign up](https://www.evidentlyai.com/register)[Get demo](https://www.evidentlyai.com/get-demo)\n![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664abe8f5bd7dda2d12a1e6d_list-bold.svg)\n## Start testing your AI systems today\nBook a personalized 1:1 demo with our team or sign up for a free account.\n[Get demo](https://www.evidentlyai.com/get-demo)[Start free](https://www.evidentlyai.com/register)\n![Icon](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc274771_Group%2069.svg)\nNo credit card required\n[![Evidently AI logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664ac309d9d1086b0e8309f9_evidently%20logo_white.png)](https://www.evidentlyai.com/)\nEvaluate, test and monitor your AI-powered products.\nSubscribe to our monthly newsletter\nThank you! Your submission has been received!\nOops! Something went wrong while submitting the form.\n[LLM testing platform](https://www.evidentlyai.com/llm-testing)[AI risk assessment](https://www.evidentlyai.com/llm-readiness)[RAG testing](https://www.evidentlyai.com/rag-testing)[Adversarial testing](https://www.evidentlyai.com/llm-red-teaming)[AI Agent testing](https://www.evidentlyai.com/ai-agent-testing)[ML monitoring](https://www.evidentlyai.com/ml-monitoring)[Open-source](https://www.evidentlyai.com/evidently-oss)\n[Blog](https://www.evidentlyai.com/blog)[Tutorials](https://www.evidentlyai.com/mlops-tutorials)[Guides](https://www.evidentlyai.com/mlops-guides)[ML platforms](https://www.evidentlyai.com/ml-platforms)[ML use cases](https://www.evidentlyai.com/ml-system-design)[LLM evaluations course](https://www.evidentlyai.com/llm-evaluations-course)\n[Pricing](https://www.evidentlyai.com/pricing)[Docs](https://docs.evidentlyai.com/)[GitHub](https://github.com/evidentlyai/evidently)[Community](https://www.evidentlyai.com/community)\n[Privacy policy](https://www.evidentlyai.com/privacy)[Terms of service](https://www.evidentlyai.com/terms)\n\u00a9 2025, Evidently AI. All rights reserved\n[![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/660ef16a9e0687d9cc274742_Group%2027.svg)](https://www.linkedin.com/company/evidently-ai/)[![Twitter logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664f62d9e2ffd7e31ffae6c8_x-logo-duotone%20\\(1\\).svg)](http://twitter.com/EvidentlyAI)[![Discord logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664f6316d09cc4b5e975db27_discord-logo-duotone%20\\(2\\).svg)](https://discord.com/invite/PyAJuUD5mB)[![YouTube logo](https://cdn.prod.website-files.com/660ef16a9e0687d9cc2746d7/664f634afd92ff37de706ab9_youtube-logo-duotone.svg)](https://www.youtube.com/c/evidentlyai)\nBy clicking \u201cAccept\u201d, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts. View our [Privacy Policy](https://www.evidentlyai.com/privacy) for more information.\n[Deny](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/)[Accept](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/)\nPrivacy Preferences\nEssential cookies\nRequired\nMarketing cookies\nEssential\nPersonalization cookies\nEssential\nAnalytics cookies\nEssential\n[Reject all cookies](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/)[Allow all cookies](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/)[Save preferences](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets/)\n",
    "answer": "```markdown\n## TL;DR\n\nThis document compares five statistical tests for detecting data drift in large datasets, highlighting their sensitivity to sample size and drift magnitude. Key insights include the Kolmogorov-Smirnov test's high sensitivity, while PSI and KL divergence show lower sensitivity. Recommendations for test selection based on use case are provided.\n\n# Tags\n#data-drift #code-example #Generative AI #LLMs\n```"
  },
  {
    "instruction": "[![VectorHub by Superlinked](https://superlinked.com/vectorhub/_next/static/media/vectorhub-logo.d71e47bd.svg)](https://superlinked.com/vectorhub/)\n[Building Blocks](https://superlinked.com/vectorhub/building-blocks)[Articles](https://superlinked.com/vectorhub/all-articles)[Contributing](https://superlinked.com/vectorhub/contributing/contributing)[VDB Comparison](https://superlinked.com/vector-db-comparison/)\n[---](https://github.com/superlinked/superlinked)\nSearch\nSubscribe\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nMenu\nImprove your vector search[![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow-primary.2ff1db1a.svg)Try Superlinked](https://github.com/superlinked/superlinked)\nHome\nManifesto\nMost Recent\nCompare Vector DBs\nContributeSubscribe\nSearch\nTable of Contents\nWhy evaluate RAG?\nWhy do we need RAG?\nEvaluating RAG\nStrategies for Evaluation\nModel Evaluation\nData Ingestion Evaluation\nSemantic Retrieval Evaluation\nEnd-to-End Evaluation\nIn sum\nWhat's next\nContributors\n  1. [Articles](https://superlinked.com/vectorhub/all-articles)\n/\n  2. [Evaluating retrieval augmented...](https://superlinked.com/vectorhub/evaluating-retrieval-augmented-generation-a-framework-for-assessment?utm_medium=email&_hsmi=295274462&utm_content=295274462&utm_source=hs_email/)\n\n\nPublication Date: February 16, 2024|\n#Quality And Evaluation#RAG\n|Update on Github\n# Evaluating Retrieval Augmented Generation - a framework for assessment\n## Takeaways\n[Watch Summary](https://youtu.be/clTMsP9jJCE)\n  * Multi-level evaluation needed: model, ingestion, retrieval, end-to-end\n\n\n  * MTEB benchmark helpful for evaluating embedding models\n\n\n  * Chunk size and overlap impact retrieval quality\n\n\n  * Golden test sets needed for semantic retrieval evaluation\n\n\n  * BLEU and ROUGE metrics useful for evaluating output quality\n\n\n  * No single metric sufficient - need comprehensive approach\n\n\n_In this first article of a three-part (monthly) series, we introduce RAG evaluation, outline its challenges, propose an effective evaluation framework, and provide a rough overview of the various tools and approaches you can use to evaluate your RAG application._\n## Why evaluate RAG?\nRetrieval Augmented Generation (RAG) is probably the most useful application of large language models today. RAG enhances content generation by leveraging existing information effectively. It can amalgamate specific, relevant details from multiple sources to generate more accurate and relevant query results. This makes RAG potentially invaluable in various domains, including content creation, question & answer applications, and information synthesis. RAG does this by combining the strengths of retrieval, usually using dense vector search, and text generation models, like GPT. For a more in-depth introduction to RAG, read [here](https://superlinked.com/vectorhub/retrieval-augmented-generation).\n![Implementation of RAG using Qdrant as a vector database](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/retrieval_augmented_generation_eval/rag_qdrant.jpg)\n_RAG system (above) using_ [_Qdrant_](https://qdrant.tech) _as the knowledge store. To determine which Vector Database fits your specific use case, refer to the_ [_Vector DB feature matrix_](https://vdbs.superlinked.com/).\n**But to see what is and isn't working in your RAG system, to refine and optimize, you have to evaluate it**. Evaluation is essential to validate and make sure your application does what users expect it to. In this article (the first of three, one per month), we go over the broad strokes of our proposed evaluation framework, which includes separate assessments of the model itself, data ingestion, semantic retrieval, and, finally, the RAG application end-to-end, providing a high level discussion of what's involved in each.\nIn article 2, we'll look at RAGAS ([RAG Assessment](https://github.com/explodinggradients/ragas)), learn how to set it up with an example, calculate some of the supported metrics, and compare that with our proposed framework. We'll also examine some examples of our proposed framework. Then, in article 3, we will look at [Arize AI](https://arize.com/)'s way of evaluating RAG applications, using Phoenix Evals to focus on Retrieval evaluation and Response evaluation.\n## Why do we need RAG?\nRAG significantly enhances [vector search](https://superlinked.com/building-blocks/vector-search/introduction) with the power of Large Language Models (LLM), by enabling dynamic content generation based on retrieved knowledge. RAG is indispensable when users seek to generate new content rather than interact with documents or search results directly. It excels in providing contextually rich, informative, and human-like responses. For tasks requiring detailed, coherent explanations, summaries, or responses that transcend the explicit data stored in vectors, RAG is invaluable. _Before setting up a RAG system, you should consider conducting feasibility studies to determine how and whether RAG aligns with your specific needs and value expectations._\nWhile vector search efficiently retrieves relevant similar documents/chunks from a document corpus, RAG permits content synthesis and a deeper level of understanding, providing essential context to queries and results generation. In this way, RAG can ensure that answers are unique and tailored to each query, in essence personalized to the user.\nThese are the promises of RAG. But how do we make sure that our RAG system is achieving its potential?\n## Evaluating RAG\nIn the process of scaling RAG from Proof-of-concept (POC) to production for clients in academia and finance, we've learned many things - including various data processing and enrichment techniques, how to choose a model to understand the 'domain specific' jargon, the combined impact of the former techniques and model on retrieval, how to bolster with creative prompts, built with guardrails, to generate trustworthy and informative responses, and so on. But our chief takeaway was the **importance of evaluation in enhancing and refining our RAG system**.\n> If you can't quantify it, you can't improve it. - Peter Drucker\nIn the case of RAG, not only is it important to have good metrics, but that you measure things separately. That is:\n> If you can't retrieve it, you can't generate it.\nTo see where things are going well, can be improved, and also where errors may originate, it's important to evaluate each component in isolation. In the following visual, we've classified RAG's components - Information Retrieval, Context Augmentation, and Response Generation - along with what needs evaluation in each:\n![Classification of Challenges of RAG Evaluation](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/retrieval_augmented_generation_eval/rag_challenges.jpg)\n_The challenges of RAG evaluation (above), including the_ [_'Lost in the Middle'_](https://arxiv.org/abs/2307.03172) _problem_.\nThe evaluation framework we propose is meant to ensure granular and thorough measurement, addressing the challenges faced in all three components. Broadly, we want to assess:\n  * Retrieval effectiveness - the degree to which the information retrieved from the underlying vector database is semantically relevant to the intention of the user's query.\n  * Relevance of responses to retrieved information - how meaningful and aligned the generated responses are with the content and context of retrieved information.\n  * Coherence of generated responses - how logically connected, fluent, and contextually consistent generated responses are when compared to the user's query.\n  * Up-to-date data - how reflective of real-world changes your RAG application's database is, to meet user expectations. Commonly, your RAG database will be dynamic, and include not just new queries but also continuously revised data from real-time data streams, economic fluctuations, business metrics, or research data, etc.\n\n\n## Strategies for Evaluation\nTo meet these evaluation challenges systematically, it's best practice to break down our evaluation into different levels, so that we understand what specifically is working well, and what needs improvement.\n![Granular Levels of Evaluation of RAG](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/retrieval_augmented_generation_eval/rag_granular.png)\nLet's take a closer look to see what's involved in each of these levels individually.\n### Model Evaluation\nWe want to ensure that the model can understand the data that we encode. The [Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard) leverages different public/private datasets to evaluate and report on the different capabilities of individual models. We can use the MTEB to evaluate any model in its list. If, on the other hand, you're working with specialized domains, you may want to put together a specialized dataset to train the model. Another option is to run relevant 'tasks' for your custom model, using instructions available [here](https://github.com/embeddings-benchmark/mteb#leaderboard).\nFor a custom SentenceTransformer-based model, we can set up evaluation tasks as in the following code. We import, configure, initialize, and then evaluate our model:\n```\n\nimport logging\n\nfrom mteb import MTEB\nfrom sentence_transformers import SentenceTransformer\n\nlogging.basicConfig(level=logging.INFO)\nmodel_name =\"<<CUSTOM_MODEL_NAME>>\"model = SentenceTransformer(model_name)evaluation = MTEB(task_langs=[\"en\"])evaluation.run(model, output_folder=f\"results/{model_name}\", eval_splits=[\"test\"])\nprint(\"--DONE--\")\n\n```\n\n### Data Ingestion Evaluation\nAfter we evaluate our model\u2019s performance using benchmarks, and (optionally) fine-tune it on the language of our domain, we can then configure data ingestion into our semantic retrieval store (vector store). Various [vector databases](https://vdbs.superlinked.com/) offer index configurations (of whatever index types the database supports) to influence and enhance the retrieval quality. Common index types include Flat (Brute Force), LSH (Locality Sensitive Hashing), HNSW (Hierarchical Navigable Small World), and IVF (Inverted File Index). Here's one such [example, based on HNSW retrieval-quality](https://qdrant.tech/documentation/tutorials/retrieval-quality/).\nTo evaluate data ingestion, we need to observe and measure how changes in related variables affect ingestion outcomes. For example:\n  * **Chunk size** - the size of each data segment, which depends on the token limit of our embedding model. Chunk size substantially determines data granularity and the contextual understanding of the data, which impacts the precision, recall, and relevancy of our results.\n  * **Chunk overlap** - the extent to which data points of events are shared by adjacent data chunks. Overlap helps with retention of context information contained in chunks, but should be paired with strategies like deduplication and content normalization to eradicate adverse effects (redundancy or inconsistency).\n  * **Chunking/Text splitting strategy** - the process of data splitting and further treatment, based on both data type (e.g. html, markdown, code, or pdf, etc.) and nuances of your use-case. For example, summarization use-cases might split segments based on chapters or paragraphs; a legal document assistant may section docs based on headings and subsections; a medical literature assistant might split docs based on sentence boundaries or key concepts.\n\n\nUtilities like [ChunkViz](https://chunkviz.up.railway.app/) help visualize different chunk splitting strategies on different chunk sizes and chunk overlaps.\n### Semantic Retrieval Evaluation\nSemantic retrieval evaluation puts the work you've done so far in evaluating your model and ingestion to a litmus test, and can be approached like a classic information retrieval evaluation problem - that is, using metrics such as precision, recall, accuracy, and F1-score to determine if retrieved documents are relevant.\nFirst, we establish what we expect the returned results to look like in terms of important parameters. This guides our choice of reference metrics. We have several existing metrics to inform and define our baseline:\n  * Precision (accuracy), Recall (completeness), or their combination F1 Score\n  * DCG (Discounted Cumulative Gain) and nDCG (normalized DCG), which measure the relevance quality of ranked lists of documents\n\n\nBut semantic retrieval, because it goes beyond mere keyword matching, synonyms, and token-level enrichment, and encompasses broader semantic understanding and context, typically requires a \"ground truth\" evaluation set - a [Golden Set](https://www.luigisbox.com/search-glossary/golden-set/). A Golden Set assesses the correctness and faithfulness of generated answers to a human annotated reference set.\nTo generate a starter pack for evaluation using a Golden Set, we can leverage the [T5 Model](https://huggingface.co/docs/transformers/model_doc/t5).\n### End-to-End Evaluation\nAn end-to-end evaluation of a RAG application assesses the final outputs generated by LLMs in response to given inputs. It requires addressing issues discussed above related to data heterogeneity, domain specificity, and user query and preference diversity. It's impossible to devise a fixed metric or methodology that fits all domains and use-cases.\nTo address these difficulties, you can assess the fidelity and quality of generated text using established metrics like [BLEU](https://huggingface.co/spaces/evaluate-metric/bleu) and [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge), and combine these scores with LLM-based or human evaluation methods. This [Quality Criteria paper](https://scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/c6a53998-09e3-4d17-91fd-c7416d51b250/content) provides some excellent ideas for creating such a solution.\n## In sum\nTo summarize, a robust RAG evaluation strategy requires you to establish methods to automatically evaluate similarity and content overlap between generated response and reference summaries. You can also leverage human evaluation to evaluate subjective information, such as context-relevance, novelty, and fluency. In addition, you can build a classified 'domain - questions' set based on question complexity (easy, medium, hard). Doing this will provide you with an overall sense of the RAG application's performance.\nWhile our proposed evaluation strategy is meant to improve RAG evaluation, we should acknowledge that no evaluation is perfect, and it remains complicated and challenging to formulate a comprehensive set of evaluation metrics.\n## What's next\nWe've laid a general foundation for discussing RAG evaluation. In the next article (next month), we'll demystify an existing evaluation framework (RAGAS), and see how well it does at covering all the layers of evaluation we've discussed. Looking forward to seeing you in the next part!\n## Contributors\n  * [Atita Arora, Author](https://www.linkedin.com/in/atitaarora/)\n  * [M\u00f3r Kapronczay, Editor](https://www.linkedin.com/in/m%C3%B3r-kapronczay-49447692)\n  * [Robert Turner, Editor](https://robertturner.co/copyedit)\n\n\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nStay updated with VectorHub\nSubscribe\n![arrow](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nContinue Reading\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/research_agent_3a22caf3b8.png)\n#Superlinked\nSimplifying Complex Research with AI\nLearn how to build an AI research assistant that finds, summarizes, and answers questions about scie...\nMarch 24, 2025\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/vector_indexing_8463f7f6de.png)\n#RAG\n#Vector Search\nVector Indexes\nVector indexes are crucial to efficient, relevant, and accurate search in various common application...\nAugust 14, 2024\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/rag_feedback_878ba93f75.png)\n#RAG\nYour RAG application is a communication system\nEffective RAG can take the best features of language models and moderate their shortcomings. In this...\nJuly 29, 2024\n[![Superlinked](https://superlinked.com/vectorhub/_next/static/media/superlinked-logo.9ad3911a.svg)](https://superlinked.com/)\nSubscribe\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nBy subscribing, you agree to our [Terms and Conditions](https://www.superlinked.com/policies/terms-and-conditions).\nAbout\n[Company](https://superlinked.com/)[Careers](https://superlinked.notion.site/Work-at-Superlinked-541f2d74ce714c889a1c6dba6dd0dbf1)[Index](https://superlinked.com/vectorhub/siteindex)\nSupport\nContact Us[Terms of Use](https://superlinked.com/policies/terms-and-conditions)[Privacy Policy](https://superlinked.com/policies/privacy-policy)[Cookie Policy](https://superlinked.com/policies/cookie-policy)\nSocial\n[Github](https://github.com/superlinked/superlinked)[X (Twitter)](https://twitter.com/superlinked)[LinkedIn](https://www.linkedin.com/company/superlinked/)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nEvaluating Retrieval Augmented Generation (RAG) is crucial for optimizing LLM applications. Key insights include the need for multi-level evaluations (model, ingestion, retrieval, end-to-end), using MTEB benchmarks, and the importance of chunk size and overlap. A comprehensive approach with multiple metrics (BLEU, ROUGE) is essential for accurate assessment.\n```"
  },
  {
    "instruction": "Back to Top\n[Distilled AI](https://aman.ai/primers/ai/) [Back to aman.ai](https://aman.ai)\n# Primers \u2022 Generative Pre-trained Transformer (GPT)\n  * [Introduction](https://aman.ai/primers/ai/gpt/#introduction)\n  * [GPT-1: Improving Language Understanding by Generative Pre-Training](https://aman.ai/primers/ai/gpt/#gpt-1-improving-language-understanding-by-generative-pre-training)\n  * [GPT-2: Language Models are Unsupervised Multitask Learners](https://aman.ai/primers/ai/gpt/#gpt-2-language-models-are-unsupervised-multitask-learners)\n  * [GPT-3: Language Models are Few-Shot Learners](https://aman.ai/primers/ai/gpt/#gpt-3-language-models-are-few-shot-learners)\n  * [GPT-4](https://aman.ai/primers/ai/gpt/#gpt-4)\n  * [References](https://aman.ai/primers/ai/gpt/#references)\n\n\n## Introduction\n  * The [Generative Pre-trained Transformer (GPT)](https://s3-us-west-2.amazonaws.com/openai-assets/gpt/research-covers/language-unsupervised/language_understanding_paper.pdf) by OpenAI is a family of autoregressive language models.\n  * GPT utilizes the decoder architecture from the standard [Transformer](https://aman.ai/primers/ai/transformers) network (with a few engineering tweaks) as a independent unit. This is coupled with an unprecedented size of 2048 as the number of tokens as input and 175 billion parameters (requiring ~800 GB of storage).\n  * The training method is \u201cgenerative pretraining\u201d, meaning that it is trained to predict what the next token is. The model demonstrated strong few-shot learning on many text-based tasks.\n  * The end result is the ability to generate human-like text with swift response time and great accuracy. Owing to the GPT family of models having been exposed to a reasonably large dataset and number of parameters (175B), these language models require few or in some cases no examples to fine-tune the model (a process that is called \u201cprompt-based\u201d fine-tuning) to fit the downstream task. The quality of the text generated by GPT-3 is so high that it can be difficult to determine whether or not it was written by a human, which has both benefits and risks ([source](https://analyticsindiamag.com/open-ai-gpt-3-language-model/)).\n  * Before GPT, language models (LMs) were typically trained on a large amount of accurately labelled data, which was hard to come by. These LMs offered great performance on the particular supervised task that they were trained to do, but were unable to be domain-adapted to other tasks easily.\n  * Microsoft announced on September 22, 2020, that it had licensed \u201cexclusive\u201d use of GPT-3; others can still use the public API to receive output, but only Microsoft has access to GPT-3\u2019s underlying model.\n  * Let\u2019s look below at each GPT1, GPT2, and GPT3 (with emphasis on GPT-3 as its more widely used today) and how they were able to make a dent in Natural Language Processing tasks.\n\n\n![](https://aman.ai/primers/ai/assets/gpt/1.jpg)\n## GPT-1: [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/gpt/research-covers/language-unsupervised/language_understanding_paper.pdf)\n  * GPT-1 was released in 2018 by OpenAI. It contained 117 million parameters.\n  * Trained on an enormous BooksCorpus dataset, this generative language model was able to learn large range dependencies and acquire vast knowledge on a diverse corpus of contiguous text and long stretches.\n  * GPT-1 uses the 12-layer decoder from the original transformer architecture that contains self attention.\n  * GPT was able to use transfer learning and thus, was able to perform many NLP tasks with very little fine-tuning.\n  * The right half of the image below, from the original transformer paper, [\u201cAttention Is All You Need\u201d](https://arxiv.org/abs/1706.03762), represents the decoder model:\n\n\n![](https://aman.ai/primers/ai/assets/gpt/0.jpg)\n## GPT-2: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n  * GPT-2 was released in February 2019 by OpenAI and it used a larger dataset while also adding additional parameters to build a more robust language model.\n  * GPT-2 became 10 times larger than GPT-1 with 1.5 billion parameters and had 10 times the data compared to GPT-1.\n  * [Write With Transformer](https://transformer.huggingface.co/doc/gpt2-large) is a webapp created and hosted by Hugging Face showcasing the generative capabilities of several models. GPT-2 is one of them and is available in five different sizes: small, medium, large, XL and a distilled version of the small checkpoint: distilgpt-2.\n  * GPT-2 is an unsupervised deep learning transformer-based language model created by OpenAI to help in predicting the next word or words in a sentence.\n  * Language tasks such as reading, summarizing, and translation can be learned by GPT-2 from raw text without using domain specific training data.\n\n\n## GPT-3: [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)\n  * GPT-3 is a massive language prediction and generation model developed by OpenAI capable of generating long sequences of the original text. GPT-3 became the OpenAI\u2019s breakthrough AI language program.\n  * GPT-3 is able to generate paragraphs and texts to almost sound like a person has generated them instead.\n  * GPT-3 contains 175 billion parameters and is 100 times larger than GPT-2. Its trained on 500 billion word data set known as \u201cCommon Crawl\u201d.\n  * GPT-3 is also able to write code snippets, like SQL queries, and perform other intelligent tasks. However, it is expensive and inconvenient to perform inference owing to its 175B-parameter size.\n  * GPT-3 eliminates the finetuning step that was needed for its predecessors as well as for encoder models such as BERT.\n  * GPT-3 is capable of responding to any text by generating a new piece of text that is both creative and appropriate to its context.\n  * Here is a working use case of GPT-3 you can try out: [debuild.io](https://debuild.app) where GPT will give you the code to build the application you define.\n  * Just like its predecessors, GPT-3 is an API served by OpenAI, lets look at the image below ([source](https://www.youtube.com/watch?v=MQnJZuBGmSQ&t=655s)) to see a visual representation of how this black box works:\n\n\n![](https://aman.ai/primers/ai/assets/gpt/2.jpg)\n  * The output here is generated by what GPT has learned during the training phase. GPT-3\u2019s dataset was quite massive with 300 billion tokens or words that it was trained on.\n  * It was trained on only one specific task which was predicting the next word, and thus, is an unsupervised pre-trained model.\n  * In the image below ([source](https://www.youtube.com/watch?v=MQnJZuBGmSQ&t=655s)), we can see what the training process looks like for GPT-3. \n    * We feed words into our model.\n    * We expect an output.\n    * We then check if the output matches the expected output.\n    * If not, we calculate the error or loss and update the model and ask for a new prediction.\n    * Thus, the next time it comes across this example, it knows what the output should look like.\n    * This is the general loop used in supervised training, nothing out of the ordinary, lets look below.\n\n\n![](https://aman.ai/primers/ai/assets/gpt/3.jpg)\n  * Let\u2019s visualize another way of looking at the model in the figure below ([source](https://www.youtube.com/watch?v=MQnJZuBGmSQ&t=655s)).\n\n\n![](https://aman.ai/primers/ai/assets/gpt/4.jpg)\n  * Each word goes through its own track and the model has a context window of say 2048 tokens. Thus the input and output have to fit within this range. Note, there are ways to expand and adjust this number but we will use this number for now.\n  * Each token is only processed in its own parallel \u201ctrack\u201d. Looking at it even deeper. Each token/word is a vector on its own, so when we process a word, we process a vector.\n  * This vector will then go through many layers of Transformer Decoders (GPT-3 has 96 layers), which are all stacked one on top of another as shown in the figure below ([source](https://www.youtube.com/watch?v=MQnJZuBGmSQ&t=655s)).\n\n\n![](https://aman.ai/primers/ai/assets/gpt/5.jpg)\n  * The last token is the response or the next word prediction from the model.\n\n\n## GPT-4\n  * Please refer the [primer on GPT-4](https://aman.ai/primers/ai/gpt-4).\n\n\n## References\n  * [Jay Alammar: How GPT3 Works - Easily Explained with Animations](https://www.youtube.com/watch?v=MQnJZuBGmSQ&t=655s)\n  * [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/gpt/research-covers/language-unsupervised/language_understanding_paper.pdf)\n  * [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n  * [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)\n  * [Priya Shree: The Journey of Open AI GPT models](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2)\n  * [360digitmg: GPT-1, GPT-2 and GPT-3](https://360digitmg.com/types-of-gpt-in-artificial-intelligence)\n  * [dzone: GPT-2 (GPT2) vs. GPT-3 (GPT3): The OpenAI Showdown](https://dzone.com/articles/gpt-2-gpt2-vs-gpt-3-gpt3-the-openai-showdown)\n\n\n  * [ ](https://github.com/amanchadha) | [ ](https://citations.amanchadha.com/) |  [ ](https://twitter.com/i_amanchadha) |  | \n\n\n[www.amanchadha.com](https://www.amanchadha.com/)\n",
    "answer": "```markdown\n# TL;DR Summary of Generative Pre-trained Transformer (GPT)\n\nThe GPT family by OpenAI includes models GPT-1, GPT-2, GPT-3, and GPT-4, utilizing autoregressive language modeling. GPT-3, with 175B parameters, excels in few-shot learning and generates human-like text, revolutionizing NLP tasks. Microsoft holds exclusive access to GPT-3's model.\n```"
  },
  {
    "instruction": "[Skip to main content](https://arxiv.org/abs/2201.11903?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN/#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/) > [cs](https://arxiv.org/list/cs/recent) > arXiv:2201.11903 \n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[ ![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg) ](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n## quick links\n  * [Login](https://arxiv.org/login)\n  * [Help Pages](https://info.arxiv.org/help)\n  * [About](https://info.arxiv.org/about)\n\n\n# Computer Science > Computation and Language\n**arXiv:2201.11903** (cs) \n[Submitted on 28 Jan 2022 ([v1](https://arxiv.org/abs/2201.11903v1)), last revised 10 Jan 2023 (this version, v6)]\n# Title:Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\nAuthors:[Jason Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei,+J), [Xuezhi Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+X), [Dale Schuurmans](https://arxiv.org/search/cs?searchtype=author&query=Schuurmans,+D), [Maarten Bosma](https://arxiv.org/search/cs?searchtype=author&query=Bosma,+M), [Brian Ichter](https://arxiv.org/search/cs?searchtype=author&query=Ichter,+B), [Fei Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia,+F), [Ed Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi,+E), [Quoc Le](https://arxiv.org/search/cs?searchtype=author&query=Le,+Q), [Denny Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou,+D)\nView a PDF of the paper titled Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, by Jason Wei and 8 other authors\n[View PDF](https://arxiv.org/pdf/2201.11903)\n> Abstract:We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier. \nSubjects: |  Computation and Language (cs.CL); Artificial Intelligence (cs.AI)  \n---|---  \nCite as: | [arXiv:2201.11903](https://arxiv.org/abs/2201.11903) [cs.CL]  \n(or  [arXiv:2201.11903v6](https://arxiv.org/abs/2201.11903v6) [cs.CL] for this version)   \n<https://doi.org/10.48550/arXiv.2201.11903> Focus to learn more arXiv-issued DOI via DataCite  \n## Submission history\nFrom: Jason Wei [[view email](https://arxiv.org/show-email/9f27af3f/2201.11903)] **[[v1]](https://arxiv.org/abs/2201.11903v1)** Fri, 28 Jan 2022 02:33:07 UTC (944 KB) **[[v2]](https://arxiv.org/abs/2201.11903v2)** Wed, 6 Apr 2022 03:51:50 UTC (933 KB) **[[v3]](https://arxiv.org/abs/2201.11903v3)** Wed, 1 Jun 2022 00:10:30 UTC (303 KB) **[[v4]](https://arxiv.org/abs/2201.11903v4)** Mon, 13 Jun 2022 21:44:34 UTC (283 KB) **[[v5]](https://arxiv.org/abs/2201.11903v5)** Mon, 10 Oct 2022 20:21:17 UTC (285 KB) **[v6]** Tue, 10 Jan 2023 23:07:57 UTC (306 KB) \nFull-text links:\n## Access Paper:\nView a PDF of the paper titled Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, by Jason Wei and 8 other authors\n  * [View PDF](https://arxiv.org/pdf/2201.11903)\n  * [TeX Source](https://arxiv.org/src/2201.11903)\n  * [Other Formats](https://arxiv.org/format/2201.11903)\n\n\n[ ![license icon](https://arxiv.org/icons/licenses/by-4.0.png) view license ](http://creativecommons.org/licenses/by/4.0/ \"Rights to this article\")\nCurrent browse context: \ncs.CL\n[< prev](https://arxiv.org/prevnext?id=2201.11903&function=prev&context=cs.CL \"previous in cs.CL \\(accesskey p\\)\") |  [next >](https://arxiv.org/prevnext?id=2201.11903&function=next&context=cs.CL \"next in cs.CL \\(accesskey n\\)\")\n[new](https://arxiv.org/list/cs.CL/new) |  [recent](https://arxiv.org/list/cs.CL/recent) | [2022-01](https://arxiv.org/list/cs.CL/2022-01)\nChange to browse by: \n[cs](https://arxiv.org/abs/2201.11903?context=cs) [cs.AI](https://arxiv.org/abs/2201.11903?context=cs.AI)\n### References & Citations\n  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2201.11903)\n  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2201.11903)\n  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:2201.11903)\n\n\n### [ 18 blog links](https://arxiv.org/tb/2201.11903)\n([what is this?](https://info.arxiv.org/help/trackback.html)) \n### [DBLP](https://dblp.uni-trier.de) - CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2201.html#abs-2201-11903 \"listing on DBLP\") | [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2201-11903 \"DBLP bibtex record\")\n[Jason Wei](https://dblp.uni-trier.de/search/author?author=Jason%20Wei \"DBLP author search\")[Xuezhi Wang](https://dblp.uni-trier.de/search/author?author=Xuezhi%20Wang \"DBLP author search\")[Dale Schuurmans](https://dblp.uni-trier.de/search/author?author=Dale%20Schuurmans \"DBLP author search\")[Quoc Le](https://dblp.uni-trier.de/search/author?author=Quoc%20Le \"DBLP author search\")[Denny Zhou](https://dblp.uni-trier.de/search/author?author=Denny%20Zhou \"DBLP author search\")\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citation Loading...\n## BibTeX formatted citation\n\u00d7\nloading...\nData provided by: \n### Bookmark\n[ ![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png) ](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2201.11903&description=Chain-of-Thought Prompting Elicits Reasoning in Large Language Models \"Bookmark on BibSonomy\") [ ![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png) ](https://reddit.com/submit?url=https://arxiv.org/abs/2201.11903&title=Chain-of-Thought Prompting Elicits Reasoning in Large Language Models \"Bookmark on Reddit\")\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\nConnected Papers Toggle\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\nLitmaps Toggle\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\nscite.ai Toggle\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_\nDagsHub Toggle\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\nGotitPub Toggle\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\nHuggingface Toggle\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\nLinks to Code Toggle\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\nScienceCast Toggle\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\nDemos\n# Demos\nReplicate Toggle\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\nSpaces Toggle\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\nSpaces Toggle\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\nCore recommender toggle\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n\nAbout arXivLabs \n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2201.11903) | [Disable MathJax](javascript:setMathjaxCookie\\(\\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n  * [About](https://info.arxiv.org/about)\n  * [Help](https://info.arxiv.org/help)\n\n\n  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)\n  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)\n\n\n  * [Copyright](https://info.arxiv.org/help/license/index.html)\n  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n\n  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n  * [arXiv Operational Status ](https://status.arxiv.org) Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n\n\n",
    "answer": "```markdown\n# TL;DR: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n\nThis paper by Jason Wei et al. explores how \"chain-of-thought\" prompting enhances reasoning in large language models (LLMs). By providing intermediate reasoning steps as examples, LLMs show significant performance improvements in arithmetic, commonsense, and symbolic reasoning tasks, achieving state-of-the-art results on benchmarks like GSM8K.\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/FrancescoSaverioZuppichini/python-template/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FFrancescoSaverioZuppichini%2Fpython-template%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FFrancescoSaverioZuppichini%2Fpython-template%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=FrancescoSaverioZuppichini%2Fpython-template) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/FrancescoSaverioZuppichini/python-template/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/FrancescoSaverioZuppichini/python-template/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/FrancescoSaverioZuppichini/python-template/) to refresh your session. Dismiss alert\n{{ message }}\n[ FrancescoSaverioZuppichini ](https://github.com/FrancescoSaverioZuppichini) / **[python-template](https://github.com/FrancescoSaverioZuppichini/python-template) ** Public template\n  * [ Notifications ](https://github.com/login?return_to=%2FFrancescoSaverioZuppichini%2Fpython-template) You must be signed in to change notification settings\n  * [ Fork 15 ](https://github.com/login?return_to=%2FFrancescoSaverioZuppichini%2Fpython-template)\n  * [ Star  182 ](https://github.com/login?return_to=%2FFrancescoSaverioZuppichini%2Fpython-template)\n\n\n### License\n[ MIT license ](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/LICENSE)\n[ 182 stars ](https://github.com/FrancescoSaverioZuppichini/python-template/stargazers) [ 15 forks ](https://github.com/FrancescoSaverioZuppichini/python-template/forks) [ Branches ](https://github.com/FrancescoSaverioZuppichini/python-template/branches) [ Tags ](https://github.com/FrancescoSaverioZuppichini/python-template/tags) [ Activity ](https://github.com/FrancescoSaverioZuppichini/python-template/activity)\n[ Star  ](https://github.com/login?return_to=%2FFrancescoSaverioZuppichini%2Fpython-template)\n[ Notifications ](https://github.com/login?return_to=%2FFrancescoSaverioZuppichini%2Fpython-template) You must be signed in to change notification settings\n  * [ Code ](https://github.com/FrancescoSaverioZuppichini/python-template)\n  * [ Issues 1 ](https://github.com/FrancescoSaverioZuppichini/python-template/issues)\n  * [ Pull requests 0 ](https://github.com/FrancescoSaverioZuppichini/python-template/pulls)\n  * [ Actions ](https://github.com/FrancescoSaverioZuppichini/python-template/actions)\n  * [ Projects 0 ](https://github.com/FrancescoSaverioZuppichini/python-template/projects)\n  * [ Security ](https://github.com/FrancescoSaverioZuppichini/python-template/security)\n  * [ Insights ](https://github.com/FrancescoSaverioZuppichini/python-template/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/FrancescoSaverioZuppichini/python-template)\n  * [ Issues  ](https://github.com/FrancescoSaverioZuppichini/python-template/issues)\n  * [ Pull requests  ](https://github.com/FrancescoSaverioZuppichini/python-template/pulls)\n  * [ Actions  ](https://github.com/FrancescoSaverioZuppichini/python-template/actions)\n  * [ Projects  ](https://github.com/FrancescoSaverioZuppichini/python-template/projects)\n  * [ Security  ](https://github.com/FrancescoSaverioZuppichini/python-template/security)\n  * [ Insights  ](https://github.com/FrancescoSaverioZuppichini/python-template/pulse)\n\n\n# FrancescoSaverioZuppichini/python-template\nmain\n[**3** Branches](https://github.com/FrancescoSaverioZuppichini/python-template/branches)[**1** Tag](https://github.com/FrancescoSaverioZuppichini/python-template/tags)\n[](https://github.com/FrancescoSaverioZuppichini/python-template/branches)[](https://github.com/FrancescoSaverioZuppichini/python-template/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n[![FrancescoSaverioZuppichini](https://avatars.githubusercontent.com/u/15908060?v=4&size=40)](https://github.com/FrancescoSaverioZuppichini)[FrancescoSaverioZuppichini](https://github.com/FrancescoSaverioZuppichini/python-template/commits?author=FrancescoSaverioZuppichini)[Update pyproject.toml](https://github.com/FrancescoSaverioZuppichini/python-template/commit/06459b247e3ab191c2d3da094f447f6b05e30be2)Jan 16, 2025[06459b2](https://github.com/FrancescoSaverioZuppichini/python-template/commit/06459b247e3ab191c2d3da094f447f6b05e30be2) \u00b7 Jan 16, 2025\n## History\n[38 Commits](https://github.com/FrancescoSaverioZuppichini/python-template/commits/main/)[](https://github.com/FrancescoSaverioZuppichini/python-template/commits/main/)  \n[.github/workflows](https://github.com/FrancescoSaverioZuppichini/python-template/tree/main/.github/workflows \"This path skips through empty directories\")| [.github/workflows](https://github.com/FrancescoSaverioZuppichini/python-template/tree/main/.github/workflows \"This path skips through empty directories\")  \n[examples](https://github.com/FrancescoSaverioZuppichini/python-template/tree/main/examples \"examples\")| [examples](https://github.com/FrancescoSaverioZuppichini/python-template/tree/main/examples \"examples\")  \n[src/jokes](https://github.com/FrancescoSaverioZuppichini/python-template/tree/main/src/jokes \"This path skips through empty directories\")| [src/jokes](https://github.com/FrancescoSaverioZuppichini/python-template/tree/main/src/jokes \"This path skips through empty directories\")  \n[tests](https://github.com/FrancescoSaverioZuppichini/python-template/tree/main/tests \"tests\")| [tests](https://github.com/FrancescoSaverioZuppichini/python-template/tree/main/tests \"tests\")  \n[.gitignore](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/.gitignore \".gitignore\")  \n[.python-version](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/.python-version \".python-version\")| [.python-version](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/.python-version \".python-version\")  \n[Dockerfile](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/Dockerfile \"Dockerfile\")| [Dockerfile](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/Dockerfile \"Dockerfile\")  \n[LICENSE](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/LICENSE \"LICENSE\")| [LICENSE](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/LICENSE \"LICENSE\")  \n[Makefile](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/Makefile \"Makefile\")| [Makefile](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/Makefile \"Makefile\")  \n[README.md](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/README.md \"README.md\")| [README.md](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/README.md \"README.md\")  \n[pyproject.toml](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/pyproject.toml \"pyproject.toml\")  \n[uv.lock](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/uv.lock \"uv.lock\")| [uv.lock](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/uv.lock \"uv.lock\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/FrancescoSaverioZuppichini/python-template/)\n  * [MIT license](https://github.com/FrancescoSaverioZuppichini/python-template/)\n\n\n# Python Template \ud83d\udc0d\n[](https://github.com/FrancescoSaverioZuppichini/python-template/#python-template-)\n[![PyPI version](https://camo.githubusercontent.com/881ee4dfd62a0edbf71a73d8424d982377eec679a033aadbe26434dfcdc4aeb2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f707974686f6e2d74656d706c6174652d7a7570706966)](https://pypi.org/project/python-template-zuppif/)\nA template for a python project for 2025\nFeatures:\n  * \ud83d\udee0\ufe0f configuration in a single file [`pyproject.toml`](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/pyproject.toml)\n  * \ud83d\udce6 [`uv`](https://docs.astral.sh/uv/) as package manager\n  * \ud83d\udc85 [`ruff`](https://docs.astral.sh/ruff/) for linting and formatting\n  * \ud83e\uddea [`pytest`](https://docs.pytest.org/en/stable/)\n  * \ud83e\uddf9 [`Makefile`](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/Makefile) with code quality checks\n  * \ud83d\udcda auto doc generation\n  * \ud83d\udc33 CI/CD Optimized Docker Image runs when a new _release_ is created pushing to gh registry\n  * \ud83e\uddbe GitHub actions: \n    * auto publish to [`pypi`](https://pypi.org/) on push on `main`\n    * auto creating a new tag on push on `main`, sync versions\n    * run `tests` and `lint` on `dev` and `main` when a PR is open\n\n\n## Getting started\n[](https://github.com/FrancescoSaverioZuppichini/python-template/#getting-started)\n### Installation\n[](https://github.com/FrancescoSaverioZuppichini/python-template/#installation)\nTo set it up and run\n```\nuv sync\n```\n\nThen\n```\npython main.py\n```\n\nWill output a random joke\n```\nWhy did the cow in the pasture get promoted at work? ... Because he is OUT-STANDING in his field!\n\n```\n\nYou can now run, for example, a function defined as `scripts` in the [`pyproject.toml`](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/pyproject.toml)\n```\nmake_me_laugh\n```\n\n### Linting\n[](https://github.com/FrancescoSaverioZuppichini/python-template/#linting)\n```\nruff check\n\n```\n\n### Formatting\n[](https://github.com/FrancescoSaverioZuppichini/python-template/#formatting)\n```\nruff format\n\n```\n\n## CI/CD\n[](https://github.com/FrancescoSaverioZuppichini/python-template/#cicd)\n### Tests\n[](https://github.com/FrancescoSaverioZuppichini/python-template/#tests)\nTests inside `/tests` are run using [`pytest`](https://docs.pytest.org/en/stable/) on PR both on `dev` and `main`\n### Publish Package\n[](https://github.com/FrancescoSaverioZuppichini/python-template/#publish-package)\nIn order to publish to [pypi](https://pypi.org/) you need to create a publisher on pypi.\nThis is explained [here](https://packaging.python.org/en/latest/guides/publishing-package-distribution-releases-using-github-actions-ci-cd-workflows/) and [here](https://docs.pypi.org/trusted-publishers/)\nIn practice go your pypi project -> Manage Project -> Publishing, scroll and \"add a new publisher\"\n### Docker\n[](https://github.com/FrancescoSaverioZuppichini/python-template/#docker)\n[`Dockerfile`](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/Dockerfile) contains a multi stage build that uses `--compile-bytecode` to compite your package. For this example, the resulting image is just\n```\ndocker build -t python-template .\n```\n\n```\nREPOSITORY    TAG    IMAGE ID    CREATED     SIZE\npython-template  latest  1ded7d260b1c  58 seconds ago  55.4MB\n\n```\n\nThe image is build using the [`build`](https://github.com/FrancescoSaverioZuppichini/python-template/blob/main/.github/workflows/build.yml) workflow when a new _release_ is created\n## About\nNo description, website, or topics provided. \n### Resources\n[ Readme ](https://github.com/FrancescoSaverioZuppichini/python-template/#readme-ov-file)\n### License\n[ MIT license ](https://github.com/FrancescoSaverioZuppichini/python-template/#MIT-1-ov-file)\n[ Activity](https://github.com/FrancescoSaverioZuppichini/python-template/activity)\n### Stars\n[ **182** stars](https://github.com/FrancescoSaverioZuppichini/python-template/stargazers)\n### Watchers\n[ **2** watching](https://github.com/FrancescoSaverioZuppichini/python-template/watchers)\n### Forks\n[ **15** forks](https://github.com/FrancescoSaverioZuppichini/python-template/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FFrancescoSaverioZuppichini%2Fpython-template&report=FrancescoSaverioZuppichini+%28user%29)\n##  [Releases 1](https://github.com/FrancescoSaverioZuppichini/python-template/releases)\n[ v0.1.1 Latest  Dec 22, 2024 ](https://github.com/FrancescoSaverioZuppichini/python-template/releases/tag/v0.1.1)\n##  [Packages 1](https://github.com/users/FrancescoSaverioZuppichini/packages?repo_name=python-template)\n  * [ python-template ](https://github.com/users/FrancescoSaverioZuppichini/packages/container/package/python-template)\n\n\n##  [Contributors 3](https://github.com/FrancescoSaverioZuppichini/python-template/graphs/contributors)\n  * [ ![@FrancescoSaverioZuppichini](https://avatars.githubusercontent.com/u/15908060?s=64&v=4) ](https://github.com/FrancescoSaverioZuppichini) [ **FrancescoSaverioZuppichini** Francesco Saverio Zuppichini ](https://github.com/FrancescoSaverioZuppichini)\n  * [ ![@gmacario](https://avatars.githubusercontent.com/u/75182?s=64&v=4) ](https://github.com/gmacario) [ **gmacario** Gianpaolo Macario ](https://github.com/gmacario)\n  * [ ![@pablojim](https://avatars.githubusercontent.com/u/683731?s=64&v=4) ](https://github.com/pablojim) [ **pablojim** Barry Fitzgerald ](https://github.com/pablojim)\n\n\n## Languages\n  * [ Dockerfile 48.9% ](https://github.com/FrancescoSaverioZuppichini/python-template/search?l=dockerfile)\n  * [ Python 35.4% ](https://github.com/FrancescoSaverioZuppichini/python-template/search?l=python)\n  * [ Makefile 15.7% ](https://github.com/FrancescoSaverioZuppichini/python-template/search?l=makefile)\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary\n\n**Python Template \ud83d\udc0d**: A project template for Python in 2025 featuring:\n- Single-file configuration (`pyproject.toml`)\n- Package management with `uv`\n- Linting via `ruff`\n- Testing with `pytest`\n- CI/CD with GitHub Actions for auto-publishing to PyPI and Docker image creation.\n\n**Key Features**:\n- Easy setup and joke output via `python main.py`\n- Code quality checks with `Makefile`\n- Multi-stage Docker build for optimized images.\n\n**License**: MIT\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/mendableai/firecrawl/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fmendableai%2Ffirecrawl%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fmendableai%2Ffirecrawl%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=mendableai%2Ffirecrawl) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/mendableai/firecrawl/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/mendableai/firecrawl/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/mendableai/firecrawl/) to refresh your session. Dismiss alert\n{{ message }}\n[ mendableai ](https://github.com/mendableai) / **[firecrawl](https://github.com/mendableai/firecrawl) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Fmendableai%2Ffirecrawl) You must be signed in to change notification settings\n  * [ Fork 2.9k ](https://github.com/login?return_to=%2Fmendableai%2Ffirecrawl)\n  * [ Star  33.5k ](https://github.com/login?return_to=%2Fmendableai%2Ffirecrawl)\n\n\n\ud83d\udd25 Turn entire websites into LLM-ready markdown or structured data. Scrape, crawl and extract with a single API. \n[firecrawl.dev](https://firecrawl.dev \"https://firecrawl.dev\")\n### License\n[ AGPL-3.0 license ](https://github.com/mendableai/firecrawl/blob/main/LICENSE)\n[ 33.5k stars ](https://github.com/mendableai/firecrawl/stargazers) [ 2.9k forks ](https://github.com/mendableai/firecrawl/forks) [ Branches ](https://github.com/mendableai/firecrawl/branches) [ Tags ](https://github.com/mendableai/firecrawl/tags) [ Activity ](https://github.com/mendableai/firecrawl/activity)\n[ Star  ](https://github.com/login?return_to=%2Fmendableai%2Ffirecrawl)\n[ Notifications ](https://github.com/login?return_to=%2Fmendableai%2Ffirecrawl) You must be signed in to change notification settings\n  * [ Code ](https://github.com/mendableai/firecrawl)\n  * [ Issues 135 ](https://github.com/mendableai/firecrawl/issues)\n  * [ Pull requests 42 ](https://github.com/mendableai/firecrawl/pulls)\n  * [ Discussions ](https://github.com/mendableai/firecrawl/discussions)\n  * [ Actions ](https://github.com/mendableai/firecrawl/actions)\n  * [ Projects 0 ](https://github.com/mendableai/firecrawl/projects)\n  * [ Security 1 ](https://github.com/mendableai/firecrawl/security)\n  * [ Insights ](https://github.com/mendableai/firecrawl/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/mendableai/firecrawl)\n  * [ Issues  ](https://github.com/mendableai/firecrawl/issues)\n  * [ Pull requests  ](https://github.com/mendableai/firecrawl/pulls)\n  * [ Discussions  ](https://github.com/mendableai/firecrawl/discussions)\n  * [ Actions  ](https://github.com/mendableai/firecrawl/actions)\n  * [ Projects  ](https://github.com/mendableai/firecrawl/projects)\n  * [ Security  ](https://github.com/mendableai/firecrawl/security)\n  * [ Insights  ](https://github.com/mendableai/firecrawl/pulse)\n\n\n# mendableai/firecrawl\nmain\n[**388** Branches](https://github.com/mendableai/firecrawl/branches)[**13** Tags](https://github.com/mendableai/firecrawl/tags)\n[](https://github.com/mendableai/firecrawl/branches)[](https://github.com/mendableai/firecrawl/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n[![mogery](https://avatars.githubusercontent.com/u/66118807?v=4&size=40)](https://github.com/mogery)[mogery](https://github.com/mendableai/firecrawl/commits?author=mogery)[new acuc](https://github.com/mendableai/firecrawl/commit/e0a3c54967bb1a7d470f8059927131c73d1cc64f)Mar 30, 2025[e0a3c54](https://github.com/mendableai/firecrawl/commit/e0a3c54967bb1a7d470f8059927131c73d1cc64f) \u00b7 Mar 30, 2025\n## History\n[3,113 Commits](https://github.com/mendableai/firecrawl/commits/main/)[](https://github.com/mendableai/firecrawl/commits/main/)  \n[.github](https://github.com/mendableai/firecrawl/tree/main/.github \".github\")| [.github](https://github.com/mendableai/firecrawl/tree/main/.github \".github\")| [tally rework api switchover (](https://github.com/mendableai/firecrawl/commit/71b6b83ec2760d2900613d7d287c5a9e364286d3 \"tally rework api switchover \\(#1328\\)\n* tally rework api switchover\n* fix and send logs\n* temp: force main instance while RPCs propagate\n* Revert \"temp: force main instance while RPCs propagate\"\nThis reverts commit 4c93379cfa64efd60eb4767dd8eced1bdd302531.\")[#1328](https://github.com/mendableai/firecrawl/pull/1328)[)](https://github.com/mendableai/firecrawl/commit/71b6b83ec2760d2900613d7d287c5a9e364286d3 \"tally rework api switchover \\(#1328\\)\n* tally rework api switchover\n* fix and send logs\n* temp: force main instance while RPCs propagate\n* Revert \"temp: force main instance while RPCs propagate\"\nThis reverts commit 4c93379cfa64efd60eb4767dd8eced1bdd302531.\")| Mar 12, 2025  \n[apps](https://github.com/mendableai/firecrawl/tree/main/apps \"apps\")| [apps](https://github.com/mendableai/firecrawl/tree/main/apps \"apps\")| [new acuc](https://github.com/mendableai/firecrawl/commit/e0a3c54967bb1a7d470f8059927131c73d1cc64f \"new acuc\")| Mar 30, 2025  \n[examples](https://github.com/mendableai/firecrawl/tree/main/examples \"examples\")| [examples](https://github.com/mendableai/firecrawl/tree/main/examples \"examples\")| [Merge pull request](https://github.com/mendableai/firecrawl/commit/830d15f2f64d6437812f0dc84137586cfa4d42b6 \"Merge pull request #1384 from aparupganguly/feature/v3-extractor\nAdd examples/ Deepseek V3 Company Researcher\") [#1384](https://github.com/mendableai/firecrawl/pull/1384) [from aparupganguly/feature/v3-extractor](https://github.com/mendableai/firecrawl/commit/830d15f2f64d6437812f0dc84137586cfa4d42b6 \"Merge pull request #1384 from aparupganguly/feature/v3-extractor\nAdd examples/ Deepseek V3 Company Researcher\")| Mar 28, 2025  \n[img](https://github.com/mendableai/firecrawl/tree/main/img \"img\")| [img](https://github.com/mendableai/firecrawl/tree/main/img \"img\")| [Nick: clarification on open source vs cloud](https://github.com/mendableai/firecrawl/commit/b2f61da7c673eee6b806a3c236607614fd5b63a9 \"Nick: clarification on open source vs cloud\")| Sep 19, 2024  \n[.gitattributes](https://github.com/mendableai/firecrawl/blob/main/.gitattributes \".gitattributes\")| [.gitattributes](https://github.com/mendableai/firecrawl/blob/main/.gitattributes \".gitattributes\")| [Initial commit](https://github.com/mendableai/firecrawl/commit/a6c2a878119321a196f720cce4195e086f1c6b46 \"Initial commit\")| Apr 15, 2024  \n[.gitignore](https://github.com/mendableai/firecrawl/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/mendableai/firecrawl/blob/main/.gitignore \".gitignore\")| [(feat/extract) New re-ranker + multi entity extraction (](https://github.com/mendableai/firecrawl/commit/5e5b5ee0e264c942fb26baf83de9aaf86e070a6d \"\\(feat/extract\\) New re-ranker + multi entity extraction \\(#1061\\)\n* agent that decides if splits schema or not\n* split and merge properties done\n* wip\n* wip\n* changes\n* ch\n* array merge working!\n* comment\n* wip\n* dereferentiate schema\n* dereference schemas\n* Nick: new re-ranker\n* Create llm-links.txt\n* Nick: format\n* Update extraction-service.ts\n* wip: cooking schema mix and spread functions\n* wip\n* wip getting there!!!\n* nick:\n* moved functions to helpers\n* nick:\n* cant reproduce the error anymore\n* error handling all scrapes failed\n* fix\n* Nick: added the sitemap index\n* Update sitemap-index.ts\n* Update map.ts\n* deduplicate and merge arrays\n* added error handler for object transformations\n* Update url-processor.ts\n* Nick:\n* Nick: fixes\n* Nick: big improvements to rerank of multi-entity\n* Nick: working\n* Update reranker.ts\n* fixed transformations for nested objs\n* fix merge nulls\n* Nick: fixed error piping\n* Update queue-worker.ts\n* Update extraction-service.ts\n* Nick: format\n* Update queue-worker.ts\n* Update pnpm-lock.yaml\n* Update queue-worker.ts\n---------\nCo-authored-by: rafaelmmiller <150964962+rafaelsideguide@users.noreply.github.com>\nCo-authored-by: Thomas Kosmas <thomas510111@gmail.com>\")[#1061](https://github.com/mendableai/firecrawl/pull/1061)[)](https://github.com/mendableai/firecrawl/commit/5e5b5ee0e264c942fb26baf83de9aaf86e070a6d \"\\(feat/extract\\) New re-ranker + multi entity extraction \\(#1061\\)\n* agent that decides if splits schema or not\n* split and merge properties done\n* wip\n* wip\n* changes\n* ch\n* array merge working!\n* comment\n* wip\n* dereferentiate schema\n* dereference schemas\n* Nick: new re-ranker\n* Create llm-links.txt\n* Nick: format\n* Update extraction-service.ts\n* wip: cooking schema mix and spread functions\n* wip\n* wip getting there!!!\n* nick:\n* moved functions to helpers\n* nick:\n* cant reproduce the error anymore\n* error handling all scrapes failed\n* fix\n* Nick: added the sitemap index\n* Update sitemap-index.ts\n* Update map.ts\n* deduplicate and merge arrays\n* added error handler for object transformations\n* Update url-processor.ts\n* Nick:\n* Nick: fixes\n* Nick: big improvements to rerank of multi-entity\n* Nick: working\n* Update reranker.ts\n* fixed transformations for nested objs\n* fix merge nulls\n* Nick: fixed error piping\n* Update queue-worker.ts\n* Update extraction-service.ts\n* Nick: format\n* Update queue-worker.ts\n* Update pnpm-lock.yaml\n* Update queue-worker.ts\n---------\nCo-authored-by: rafaelmmiller <150964962+rafaelsideguide@users.noreply.github.com>\nCo-authored-by: Thomas Kosmas <thomas510111@gmail.com>\")| Jan 13, 2025  \n[.gitmodules](https://github.com/mendableai/firecrawl/blob/main/.gitmodules \".gitmodules\")| [.gitmodules](https://github.com/mendableai/firecrawl/blob/main/.gitmodules \".gitmodules\")| [fix(go-sdk): submodules](https://github.com/mendableai/firecrawl/commit/1baba3ce0a8f380143a7ff47fba2fd43e645928b \"fix\\(go-sdk\\): submodules\")| Aug 26, 2024  \n[CONTRIBUTING.md](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [Update CONTRIBUTING.md](https://github.com/mendableai/firecrawl/commit/0a44d11b83c5d15547e4f439e52f501947f5b1d9 \"Update CONTRIBUTING.md\nA letter seemed to be forgotten\")| Jan 31, 2025  \n[LICENSE](https://github.com/mendableai/firecrawl/blob/main/LICENSE \"LICENSE\")| [LICENSE](https://github.com/mendableai/firecrawl/blob/main/LICENSE \"LICENSE\")| [Update SDKs to MIT license](https://github.com/mendableai/firecrawl/commit/afb49e21e7cff595ebad9ce0b7aba13b88f39cf8 \"Update SDKs to MIT license\")| Jul 8, 2024  \n[README.md](https://github.com/mendableai/firecrawl/blob/main/README.md \"README.md\")| [README.md](https://github.com/mendableai/firecrawl/blob/main/README.md \"README.md\")| [docs: remove undefined \"required\" field (](https://github.com/mendableai/firecrawl/commit/42e92216b6cc659667307037c6079120b7493737 \"docs: remove undefined \"required\" field \\(#1282\\)\")[#1282](https://github.com/mendableai/firecrawl/pull/1282)[)](https://github.com/mendableai/firecrawl/commit/42e92216b6cc659667307037c6079120b7493737 \"docs: remove undefined \"required\" field \\(#1282\\)\")| Mar 2, 2025  \n[SELF_HOST.md](https://github.com/mendableai/firecrawl/blob/main/SELF_HOST.md \"SELF_HOST.md\")| [SELF_HOST.md](https://github.com/mendableai/firecrawl/blob/main/SELF_HOST.md \"SELF_HOST.md\")| [feat(self-host/ai): use any OpenAI-compatible API (](https://github.com/mendableai/firecrawl/commit/15489be542b2fe2ace992209f79b94504a61bc78 \"feat\\(self-host/ai\\): use any OpenAI-compatible API \\(#1245\\)\")[#1245](https://github.com/mendableai/firecrawl/pull/1245)[)](https://github.com/mendableai/firecrawl/commit/15489be542b2fe2ace992209f79b94504a61bc78 \"feat\\(self-host/ai\\): use any OpenAI-compatible API \\(#1245\\)\")| Feb 23, 2025  \n[docker-compose.yaml](https://github.com/mendableai/firecrawl/blob/main/docker-compose.yaml \"docker-compose.yaml\")| [docker-compose.yaml](https://github.com/mendableai/firecrawl/blob/main/docker-compose.yaml \"docker-compose.yaml\")| [feat(self-host/ai): pass in the ollama envs into docker compose (](https://github.com/mendableai/firecrawl/commit/78334e4ef4236c83a430726c670491fd8ed487ac \"feat\\(self-host/ai\\): pass in the ollama envs into docker compose \\(#1269\\)\")[#1269](https://github.com/mendableai/firecrawl/pull/1269)[)](https://github.com/mendableai/firecrawl/commit/78334e4ef4236c83a430726c670491fd8ed487ac \"feat\\(self-host/ai\\): pass in the ollama envs into docker compose \\(#1269\\)\")| Feb 27, 2025  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/mendableai/firecrawl/)\n  * [AGPL-3.0 license](https://github.com/mendableai/firecrawl/)\n\n\n###  [![](https://raw.githubusercontent.com/mendableai/firecrawl/main/img/firecrawl_logo.png)](https://raw.githubusercontent.com/mendableai/firecrawl/main/img/firecrawl_logo.png)\n[](https://github.com/mendableai/firecrawl/#----)\n[ ![License](https://camo.githubusercontent.com/a6f4431b80529dbeaa43c3c5fbcf4649f6b4ebbeb82d5a58abeb39ca3eeca8be/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d656e6461626c6561692f66697265637261776c) ](https://github.com/mendableai/firecrawl/blob/main/LICENSE) [ ![Downloads](https://camo.githubusercontent.com/9d76afe428b4085c8b7103f2f4e31da110ee154ad7320bace4348d92ac0c2450/68747470733a2f2f7374617469632e706570792e746563682f62616467652f66697265637261776c2d7079) ](https://pepy.tech/project/firecrawl-py) [ ![GitHub Contributors](https://camo.githubusercontent.com/036be24bb56e21d894aeb2234bbe65781361dfbbf374f04917d75c676fff339b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6d656e6461626c6561692f66697265637261776c2e737667) ](https://GitHub.com/mendableai/firecrawl/graphs/contributors) [ ![Visit firecrawl.dev](https://camo.githubusercontent.com/3576b8cb0e77344c001cc8456d28c830691cb96480d4b65be90f8a4c99dead56/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56697369742d66697265637261776c2e6465762d6f72616e6765) ](https://firecrawl.dev)\n[ ![Follow on X](https://camo.githubusercontent.com/610127222e603752676f0275682f12398f8e434706861d577c1f6688d999191c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466f6c6c6f772532306f6e253230582d3030303030303f7374796c653d666f722d7468652d6261646765266c6f676f3d78266c6f676f436f6c6f723d7768697465) ](https://twitter.com/firecrawl_dev) [ ![Follow on LinkedIn](https://camo.githubusercontent.com/8741d51bb8e1c8ae576ac05e875f826bcf80e8711dcf9225935bb78d5bb03802/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466f6c6c6f772532306f6e2532304c696e6b6564496e2d3030373742353f7374796c653d666f722d7468652d6261646765266c6f676f3d6c696e6b6564696e266c6f676f436f6c6f723d7768697465) ](https://www.linkedin.com/company/104100957) [ ![Join our Discord](https://camo.githubusercontent.com/886138c89a84dc2ad74d06900f364d736ccf753b2732d59fbd4106f6310f3616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4a6f696e2532306f7572253230446973636f72642d3538363546323f7374796c653d666f722d7468652d6261646765266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465) ](https://discord.com/invite/gSmWdAkdwd)\n# \ud83d\udd25 Firecrawl\n[](https://github.com/mendableai/firecrawl/#-firecrawl)\nEmpower your AI apps with clean data from any website. Featuring advanced scraping, crawling, and data extraction capabilities.\n_This repository is in development, and we\u2019re still integrating custom modules into the mono repo. It's not fully ready for self-hosted deployment yet, but you can run it locally._\n## What is Firecrawl?\n[](https://github.com/mendableai/firecrawl/#what-is-firecrawl)\n[Firecrawl](https://firecrawl.dev?ref=github) is an API service that takes a URL, crawls it, and converts it into clean markdown or structured data. We crawl all accessible subpages and give you clean data for each. No sitemap required. Check out our [documentation](https://docs.firecrawl.dev).\n_Pst. hey, you, join our stargazers :)_\n[ ![GitHub stars](https://camo.githubusercontent.com/dfb129359e7035e4c1b63b62315095c82738494425bb13c6acf5fae320d68f33/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d656e6461626c6561692f66697265637261776c2e7376673f7374796c653d736f6369616c266c6162656c3d53746172266d61784167653d32353932303030) ](https://github.com/mendableai/firecrawl)\n## How to use it?\n[](https://github.com/mendableai/firecrawl/#how-to-use-it)\nWe provide an easy to use API with our hosted version. You can find the playground and documentation [here](https://firecrawl.dev/playground). You can also self host the backend if you'd like.\nCheck out the following resources to get started:\n  * **API** : [Documentation](https://docs.firecrawl.dev/api-reference/introduction)\n  * **SDKs** : [Python](https://docs.firecrawl.dev/sdks/python), [Node](https://docs.firecrawl.dev/sdks/node), [Go](https://docs.firecrawl.dev/sdks/go), [Rust](https://docs.firecrawl.dev/sdks/rust)\n  * **LLM Frameworks** : [Langchain (python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/), [Langchain (js)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl), [Llama Index](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader), [Crew.ai](https://docs.crewai.com/), [Composio](https://composio.dev/tools/firecrawl/all), [PraisonAI](https://docs.praison.ai/firecrawl/), [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl), [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)\n  * **Low-code Frameworks** : [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl), [Langflow](https://docs.langflow.org/), [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl), [Cargo](https://docs.getcargo.io/integration/firecrawl), [Pipedream](https://pipedream.com/apps/firecrawl/)\n  * **Others** : [Zapier](https://zapier.com/apps/firecrawl/integrations), [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)\n  * Want an SDK or Integration? Let us know by opening an issue.\n\n\nTo run locally, refer to guide [here](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md).\n### API Key\n[](https://github.com/mendableai/firecrawl/#api-key)\nTo use the API, you need to sign up on [Firecrawl](https://firecrawl.dev) and get an API key.\n### Features\n[](https://github.com/mendableai/firecrawl/#features)\n  * [**Scrape**](https://github.com/mendableai/firecrawl/#scraping): scrapes a URL and get its content in LLM-ready format (markdown, structured data via [LLM Extract](https://github.com/mendableai/firecrawl/#llm-extraction-beta), screenshot, html)\n  * [**Crawl**](https://github.com/mendableai/firecrawl/#crawling): scrapes all the URLs of a web page and return content in LLM-ready format\n  * [**Map**](https://github.com/mendableai/firecrawl/#map-alpha): input a website and get all the website urls - extremely fast\n  * [**Extract**](https://github.com/mendableai/firecrawl/#extract): get structured data from single page, multiple pages or entire websites with AI.\n\n\n### Powerful Capabilities\n[](https://github.com/mendableai/firecrawl/#powerful-capabilities)\n  * **LLM-ready formats** : markdown, structured data, screenshot, HTML, links, metadata\n  * **The hard stuff** : proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration\n  * **Customizability** : exclude tags, crawl behind auth walls with custom headers, max crawl depth, etc...\n  * **Media parsing** : pdfs, docx, images\n  * **Reliability first** : designed to get the data you need - no matter how hard it is\n  * **Actions** : click, scroll, input, wait and more before extracting data\n  * **Batching (New)** : scrape thousands of URLs at the same time with a new async endpoint.\n\n\nYou can find all of Firecrawl's capabilities and how to use them in our [documentation](https://docs.firecrawl.dev)\n### Crawling\n[](https://github.com/mendableai/firecrawl/#crawling)\nUsed to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.\n```\ncurl -X POST https://api.firecrawl.dev/v1/crawl \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer fc-YOUR_API_KEY' \\\n  -d '{\n   \"url\": \"https://docs.firecrawl.dev\",\n   \"limit\": 10,\n   \"scrapeOptions\": {\n    \"formats\": [\"markdown\", \"html\"]\n   }\n  }'\n```\n\nReturns a crawl job id and the url to check the status of the crawl.\n```\n{\n \"success\": true,\n \"id\": \"123-456-789\",\n \"url\": \"https://api.firecrawl.dev/v1/crawl/123-456-789\"\n}\n```\n\n### Check Crawl Job\n[](https://github.com/mendableai/firecrawl/#check-crawl-job)\nUsed to check the status of a crawl job and get its result.\n```\ncurl -X GET https://api.firecrawl.dev/v1/crawl/123-456-789 \\\n -H 'Content-Type: application/json' \\\n -H 'Authorization: Bearer YOUR_API_KEY'\n```\n\n```\n{\n \"status\": \"completed\",\n \"total\": 36,\n \"creditsUsed\": 36,\n \"expiresAt\": \"2024-00-00T00:00:00.000Z\",\n \"data\": [\n  {\n   \"markdown\": \"[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...\",\n   \"html\": \"<!DOCTYPE html><html lang=\\\"en\\\" class=\\\"js-focus-visible lg:[--scroll-mt:9.5rem]\\\" data-js-focus-visible=\\\"\\\">...\",\n   \"metadata\": {\n    \"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\n    \"language\": \"en\",\n    \"sourceURL\": \"https://docs.firecrawl.dev/learn/rag-llama3\",\n    \"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\",\n    \"ogLocaleAlternate\": [],\n    \"statusCode\": 200\n   }\n  }\n ]\n}\n```\n\n### Scraping\n[](https://github.com/mendableai/firecrawl/#scraping)\nUsed to scrape a URL and get its content in the specified formats.\n```\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY' \\\n  -d '{\n   \"url\": \"https://docs.firecrawl.dev\",\n   \"formats\" : [\"markdown\", \"html\"]\n  }'\n```\n\nResponse:\n```\n{\n \"success\": true,\n \"data\": {\n  \"markdown\": \"Launch Week I is here! [See our Day 2 Release \ud83d\ude80](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[\ud83d\udca5 Get 2 months free...\",\n  \"html\": \"<!DOCTYPE html><html lang=\\\"en\\\" class=\\\"light\\\" style=\\\"color-scheme: light;\\\"><body class=\\\"__variable_36bd41 __variable_d7dc5d font-inter ...\",\n  \"metadata\": {\n   \"title\": \"Home - Firecrawl\",\n   \"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\n   \"language\": \"en\",\n   \"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\n   \"robots\": \"follow, index\",\n   \"ogTitle\": \"Firecrawl\",\n   \"ogDescription\": \"Turn any website into LLM-ready data.\",\n   \"ogUrl\": \"https://www.firecrawl.dev/\",\n   \"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\n   \"ogLocaleAlternate\": [],\n   \"ogSiteName\": \"Firecrawl\",\n   \"sourceURL\": \"https://firecrawl.dev\",\n   \"statusCode\": 200\n  }\n }\n}\n```\n\n### Map (Alpha)\n[](https://github.com/mendableai/firecrawl/#map-alpha)\nUsed to map a URL and get urls of the website. This returns most links present on the website.\n```\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY' \\\n  -d '{\n   \"url\": \"https://firecrawl.dev\"\n  }'\n```\n\nResponse:\n```\n{\n \"status\": \"success\",\n \"links\": [\n  \"https://firecrawl.dev\",\n  \"https://www.firecrawl.dev/pricing\",\n  \"https://www.firecrawl.dev/blog\",\n  \"https://www.firecrawl.dev/playground\",\n  \"https://www.firecrawl.dev/smart-crawl\",\n ]\n}\n```\n\n#### Map with search\n[](https://github.com/mendableai/firecrawl/#map-with-search)\nMap with `search` param allows you to search for specific urls inside a website.\n```\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY' \\\n  -d '{\n   \"url\": \"https://firecrawl.dev\",\n   \"search\": \"docs\"\n  }'\n```\n\nResponse will be an ordered list from the most relevant to the least relevant.\n```\n{\n \"status\": \"success\",\n \"links\": [\n  \"https://docs.firecrawl.dev\",\n  \"https://docs.firecrawl.dev/sdks/python\",\n  \"https://docs.firecrawl.dev/learn/rag-llama3\",\n ]\n}\n```\n\n### Extract\n[](https://github.com/mendableai/firecrawl/#extract)\nGet structured data from entire websites with a prompt and/or a schema.\nYou can extract structured data from one or multiple URLs, including wildcards:\nSingle Page: Example: <https://firecrawl.dev/some-page>\nMultiple Pages / Full Domain Example: <https://firecrawl.dev/>*\nWhen you use /*, Firecrawl will automatically crawl and parse all URLs it can discover in that domain, then extract the requested data.\n```\ncurl -X POST https://api.firecrawl.dev/v1/extract \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY' \\\n  -d '{\n   \"urls\": [\n    \"https://firecrawl.dev/*\", \n    \"https://docs.firecrawl.dev/\", \n    \"https://www.ycombinator.com/companies\"\n   ],\n   \"prompt\": \"Extract the company mission, whether it is open source, and whether it is in Y Combinator from the page.\",\n   \"schema\": {\n    \"type\": \"object\",\n    \"properties\": {\n     \"company_mission\": {\n      \"type\": \"string\"\n     },\n     \"is_open_source\": {\n      \"type\": \"boolean\"\n     },\n     \"is_in_yc\": {\n      \"type\": \"boolean\"\n     }\n    },\n    \"required\": [\n     \"company_mission\",\n     \"is_open_source\",\n     \"is_in_yc\"\n    ]\n   }\n  }'\n```\n\n```\n{\n \"success\": true,\n \"id\": \"44aa536d-f1cb-4706-ab87-ed0386685740\",\n \"urlTrace\": []\n}\n```\n\nIf you are using the sdks, it will auto pull the response for you:\n```\n{\n \"success\": true,\n \"data\": {\n  \"company_mission\": \"Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.\",\n  \"supports_sso\": false,\n  \"is_open_source\": true,\n  \"is_in_yc\": true\n }\n}\n```\n\n### LLM Extraction (Beta)\n[](https://github.com/mendableai/firecrawl/#llm-extraction-beta)\nUsed to extract structured data from scraped pages.\n```\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY' \\\n  -d '{\n   \"url\": \"https://www.mendable.ai/\",\n   \"formats\": [\"json\"],\n   \"jsonOptions\": {\n    \"schema\": {\n     \"type\": \"object\",\n     \"properties\": {\n      \"company_mission\": {\n           \"type\": \"string\"\n      },\n      \"supports_sso\": {\n           \"type\": \"boolean\"\n      },\n      \"is_open_source\": {\n           \"type\": \"boolean\"\n      },\n      \"is_in_yc\": {\n           \"type\": \"boolean\"\n      }\n     },\n     \"required\": [\n      \"company_mission\",\n      \"supports_sso\",\n      \"is_open_source\",\n      \"is_in_yc\"\n     ]\n    }\n   }\n  }'\n```\n\n```\n{\n \"success\": true,\n \"data\": {\n  \"content\": \"Raw Content\",\n  \"metadata\": {\n   \"title\": \"Mendable\",\n   \"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n   \"robots\": \"follow, index\",\n   \"ogTitle\": \"Mendable\",\n   \"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n   \"ogUrl\": \"https://mendable.ai/\",\n   \"ogImage\": \"https://mendable.ai/mendable_new_og1.png\",\n   \"ogLocaleAlternate\": [],\n   \"ogSiteName\": \"Mendable\",\n   \"sourceURL\": \"https://mendable.ai/\"\n  },\n  \"json\": {\n   \"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\n   \"supports_sso\": true,\n   \"is_open_source\": false,\n   \"is_in_yc\": true\n  }\n }\n}\n```\n\n### Extracting without a schema (New)\n[](https://github.com/mendableai/firecrawl/#extracting-without-a-schema-new)\nYou can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\n```\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY' \\\n  -d '{\n   \"url\": \"https://docs.firecrawl.dev/\",\n   \"formats\": [\"json\"],\n   \"jsonOptions\": {\n    \"prompt\": \"Extract the company mission from the page.\"\n   }\n  }'\n```\n\n### Interacting with the page with Actions (Cloud-only)\n[](https://github.com/mendableai/firecrawl/#interacting-with-the-page-with-actions-cloud-only)\nFirecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\nHere is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.\n```\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY' \\\n  -d '{\n    \"url\": \"google.com\",\n    \"formats\": [\"markdown\"],\n    \"actions\": [\n      {\"type\": \"wait\", \"milliseconds\": 2000},\n      {\"type\": \"click\", \"selector\": \"textarea[title=\\\"Search\\\"]\"},\n      {\"type\": \"wait\", \"milliseconds\": 2000},\n      {\"type\": \"write\", \"text\": \"firecrawl\"},\n      {\"type\": \"wait\", \"milliseconds\": 2000},\n      {\"type\": \"press\", \"key\": \"ENTER\"},\n      {\"type\": \"wait\", \"milliseconds\": 3000},\n      {\"type\": \"click\", \"selector\": \"h3\"},\n      {\"type\": \"wait\", \"milliseconds\": 3000},\n      {\"type\": \"screenshot\"}\n    ]\n  }'\n```\n\n### Batch Scraping Multiple URLs (New)\n[](https://github.com/mendableai/firecrawl/#batch-scraping-multiple-urls-new)\nYou can now batch scrape multiple URLs at the same time. It is very similar to how the /crawl endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.\n```\ncurl -X POST https://api.firecrawl.dev/v1/batch/scrape \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY' \\\n  -d '{\n   \"urls\": [\"https://docs.firecrawl.dev\", \"https://docs.firecrawl.dev/sdks/overview\"],\n   \"formats\" : [\"markdown\", \"html\"]\n  }'\n```\n\n### Search\n[](https://github.com/mendableai/firecrawl/#search)\nThe search endpoint combines web search with Firecrawl\u2019s scraping capabilities to return full page content for any query.\nInclude `scrapeOptions` with `formats: [\"markdown\"]` to get complete markdown content for each search result otherwise it defaults to getting SERP results (url, title, description).\n```\ncurl -X POST https://api.firecrawl.dev/v1/search \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY' \\\n  -d '{\n   \"query\": \"What is Mendable?\"\n  }'\n```\n\n```\n{\n \"success\": true,\n \"data\": [\n  {\n   \"url\": \"https://mendable.ai\",\n   \"title\": \"Mendable | AI for CX and Sales\",\n   \"description\": \"AI for CX and Sales\"\n  }\n ]\n}\n```\n\n## Using Python SDK\n[](https://github.com/mendableai/firecrawl/#using-python-sdk)\n### Installing Python SDK\n[](https://github.com/mendableai/firecrawl/#installing-python-sdk)\n```\npip install firecrawl-py\n```\n\n### Crawl a website\n[](https://github.com/mendableai/firecrawl/#crawl-a-website)\n```\nfrom firecrawl.firecrawl import FirecrawlApp\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n# Scrape a website:\nscrape_status = app.scrape_url(\n 'https://firecrawl.dev', \n params={'formats': ['markdown', 'html']}\n)\nprint(scrape_status)\n# Crawl a website:\ncrawl_status = app.crawl_url(\n 'https://firecrawl.dev', \n params={\n  'limit': 100, \n  'scrapeOptions': {'formats': ['markdown', 'html']}\n },\n poll_interval=30\n)\nprint(crawl_status)\n```\n\n### Extracting structured data from a URL\n[](https://github.com/mendableai/firecrawl/#extracting-structured-data-from-a-url)\nWith LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:\n```\nfrom firecrawl.firecrawl import FirecrawlApp\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\nclass ArticleSchema(BaseModel):\n  title: str\n  points: int\n  by: str\n  commentsURL: str\nclass TopArticlesSchema(BaseModel):\n  top: List[ArticleSchema] = Field(..., max_items=5, description=\"Top 5 stories\")\ndata = app.scrape_url('https://news.ycombinator.com', {\n  'formats': ['json'],\n  'jsonOptions': {\n    'schema': TopArticlesSchema.model_json_schema()\n  }\n})\nprint(data[\"json\"])\n```\n\n## Using the Node SDK\n[](https://github.com/mendableai/firecrawl/#using-the-node-sdk)\n### Installation\n[](https://github.com/mendableai/firecrawl/#installation)\nTo install the Firecrawl Node SDK, you can use npm:\n```\nnpm install @mendable/firecrawl-js\n```\n\n### Usage\n[](https://github.com/mendableai/firecrawl/#usage)\n  1. Get an API key from [firecrawl.dev](https://firecrawl.dev)\n  2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.\n\n\n```\nimport FirecrawlApp, { CrawlParams, CrawlStatusResponse } from '@mendable/firecrawl-js';\nconst app = new FirecrawlApp({apiKey: \"fc-YOUR_API_KEY\"});\n// Scrape a website\nconst scrapeResponse = await app.scrapeUrl('https://firecrawl.dev', {\n formats: ['markdown', 'html'],\n});\nif (scrapeResponse) {\n console.log(scrapeResponse)\n}\n// Crawl a website\nconst crawlResponse = await app.crawlUrl('https://firecrawl.dev', {\n limit: 100,\n scrapeOptions: {\n  formats: ['markdown', 'html'],\n }\n} satisfies CrawlParams, true, 30) satisfies CrawlStatusResponse;\nif (crawlResponse) {\n console.log(crawlResponse)\n}\n```\n\n### Extracting structured data from a URL\n[](https://github.com/mendableai/firecrawl/#extracting-structured-data-from-a-url-1)\nWith LLM extraction, you can easily extract structured data from any URL. We support zod schema to make it easier for you too. Here is how to use it:\n```\nimport FirecrawlApp from \"@mendable/firecrawl-js\";\nimport { z } from \"zod\";\nconst app = new FirecrawlApp({\n apiKey: \"fc-YOUR_API_KEY\"\n});\n// Define schema to extract contents into\nconst schema = z.object({\n top: z\n  .array(\n   z.object({\n    title: z.string(),\n    points: z.number(),\n    by: z.string(),\n    commentsURL: z.string(),\n   })\n  )\n  .length(5)\n  .describe(\"Top 5 stories on Hacker News\"),\n});\nconst scrapeResult = await app.scrapeUrl(\"https://news.ycombinator.com\", {\n jsonOptions: { extractionSchema: schema },\n});\nconsole.log(scrapeResult.data[\"json\"]);\n```\n\n## Open Source vs Cloud Offering\n[](https://github.com/mendableai/firecrawl/#open-source-vs-cloud-offering)\nFirecrawl is open source available under the AGPL-3.0 license.\nTo deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.\nFirecrawl Cloud is available at [firecrawl.dev](https://firecrawl.dev) and offers a range of features that are not available in the open source version:\n[![Open Source vs Cloud Offering](https://raw.githubusercontent.com/mendableai/firecrawl/main/img/open-source-cloud.png)](https://raw.githubusercontent.com/mendableai/firecrawl/main/img/open-source-cloud.png)\n## Contributing\n[](https://github.com/mendableai/firecrawl/#contributing)\nWe love contributions! Please read our [contributing guide](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md) before submitting a pull request. If you'd like to self-host, refer to the [self-hosting guide](https://github.com/mendableai/firecrawl/blob/main/SELF_HOST.md).\n_It is the sole responsibility of the end users to respect websites' policies when scraping, searching and crawling with Firecrawl. Users are advised to adhere to the applicable privacy policies and terms of use of the websites prior to initiating any scraping activities. By default, Firecrawl respects the directives specified in the websites' robots.txt files when crawling. By utilizing Firecrawl, you expressly agree to comply with these conditions._\n## Contributors\n[](https://github.com/mendableai/firecrawl/#contributors)\n[ ![contributors](https://camo.githubusercontent.com/746e466b28d4d8e4426db032e818f0d7cec125cca0a03fc64ff79a90fface14f/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d6d656e6461626c6561692f66697265637261776c) ](https://github.com/mendableai/firecrawl/graphs/contributors)\n## License Disclaimer\n[](https://github.com/mendableai/firecrawl/#license-disclaimer)\nThis project is primarily licensed under the GNU Affero General Public License v3.0 (AGPL-3.0), as specified in the LICENSE file in the root directory of this repository. However, certain components of this project are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.\nPlease note:\n  * The AGPL-3.0 license applies to all parts of the project unless otherwise specified.\n  * The SDKs and some UI components are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.\n  * When using or contributing to this project, ensure you comply with the appropriate license terms for the specific component you are working with.\n\n\nFor more details on the licensing of specific components, please refer to the LICENSE files in the respective directories or contact the project maintainers.\n[ \u2191 Back to Top \u2191 ](https://github.com/mendableai/firecrawl/#readme-top)\n## About\n\ud83d\udd25 Turn entire websites into LLM-ready markdown or structured data. Scrape, crawl and extract with a single API. \n[firecrawl.dev](https://firecrawl.dev \"https://firecrawl.dev\")\n### Topics\n[ markdown ](https://github.com/topics/markdown \"Topic: markdown\") [ crawler ](https://github.com/topics/crawler \"Topic: crawler\") [ data ](https://github.com/topics/data \"Topic: data\") [ scraper ](https://github.com/topics/scraper \"Topic: scraper\") [ ai ](https://github.com/topics/ai \"Topic: ai\") [ html-to-markdown ](https://github.com/topics/html-to-markdown \"Topic: html-to-markdown\") [ web-crawler ](https://github.com/topics/web-crawler \"Topic: web-crawler\") [ scraping ](https://github.com/topics/scraping \"Topic: scraping\") [ webscraping ](https://github.com/topics/webscraping \"Topic: webscraping\") [ rag ](https://github.com/topics/rag \"Topic: rag\") [ llm ](https://github.com/topics/llm \"Topic: llm\") [ ai-scraping ](https://github.com/topics/ai-scraping \"Topic: ai-scraping\")\n### Resources\n[ Readme ](https://github.com/mendableai/firecrawl/#readme-ov-file)\n### License\n[ AGPL-3.0 license ](https://github.com/mendableai/firecrawl/#AGPL-3.0-1-ov-file)\n[ Activity](https://github.com/mendableai/firecrawl/activity)\n[ Custom properties](https://github.com/mendableai/firecrawl/custom-properties)\n### Stars\n[ **33.5k** stars](https://github.com/mendableai/firecrawl/stargazers)\n### Watchers\n[ **177** watching](https://github.com/mendableai/firecrawl/watchers)\n### Forks\n[ **2.9k** forks](https://github.com/mendableai/firecrawl/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fmendableai%2Ffirecrawl&report=mendableai+%28user%29)\n##  [Releases 13](https://github.com/mendableai/firecrawl/releases)\n[ LLMs.txt API + Deep Research API - v1.6.0 Latest  Mar 7, 2025 ](https://github.com/mendableai/firecrawl/releases/tag/v1.6.0)\n[+ 12 releases](https://github.com/mendableai/firecrawl/releases)\n##  [Packages 1](https://github.com/orgs/mendableai/packages?repo_name=firecrawl)\n  * [ firecrawl ](https://github.com/orgs/mendableai/packages/container/package/firecrawl)\n\n\n##  [Contributors 81](https://github.com/mendableai/firecrawl/graphs/contributors)\n  * [ ![@nickscamara](https://avatars.githubusercontent.com/u/20311743?s=64&v=4) ](https://github.com/nickscamara)\n  * [ ![@mogery](https://avatars.githubusercontent.com/u/66118807?s=64&v=4) ](https://github.com/mogery)\n  * [ ![@rafaelsideguide](https://avatars.githubusercontent.com/u/150964962?s=64&v=4) ](https://github.com/rafaelsideguide)\n  * [ ![@ericciarla](https://avatars.githubusercontent.com/u/43451761?s=64&v=4) ](https://github.com/ericciarla)\n  * [ ![@calebpeffer](https://avatars.githubusercontent.com/u/44934913?s=64&v=4) ](https://github.com/calebpeffer)\n  * [ ![@mattjoyce](https://avatars.githubusercontent.com/u/278869?s=64&v=4) ](https://github.com/mattjoyce)\n  * [ ![@tomkosm](https://avatars.githubusercontent.com/u/10325851?s=64&v=4) ](https://github.com/tomkosm)\n  * [ ![@Sanix-Darker](https://avatars.githubusercontent.com/u/22576758?s=64&v=4) ](https://github.com/Sanix-Darker)\n  * [ ![@JakobStadlhuber](https://avatars.githubusercontent.com/u/20328243?s=64&v=4) ](https://github.com/JakobStadlhuber)\n  * [ ![@rishi-raj-jain](https://avatars.githubusercontent.com/u/46300090?s=64&v=4) ](https://github.com/rishi-raj-jain)\n  * [ ![@aparupganguly](https://avatars.githubusercontent.com/u/89296617?s=64&v=4) ](https://github.com/aparupganguly)\n  * [ ![@BexTuychiev](https://avatars.githubusercontent.com/u/56342698?s=64&v=4) ](https://github.com/BexTuychiev)\n  * [ ![@chand1012](https://avatars.githubusercontent.com/u/3521582?s=64&v=4) ](https://github.com/chand1012)\n  * [ ![@ftonato](https://avatars.githubusercontent.com/u/5417662?s=64&v=4) ](https://github.com/ftonato)\n\n\n[+ 67 contributors](https://github.com/mendableai/firecrawl/graphs/contributors)\n## Languages\n  * [ TypeScript 91.2% ](https://github.com/mendableai/firecrawl/search?l=typescript)\n  * [ Python 4.9% ](https://github.com/mendableai/firecrawl/search?l=python)\n  * [ Rust 2.6% ](https://github.com/mendableai/firecrawl/search?l=rust)\n  * [ Shell 0.6% ](https://github.com/mendableai/firecrawl/search?l=shell)\n  * [ JavaScript 0.5% ](https://github.com/mendableai/firecrawl/search?l=javascript)\n  * [ Dockerfile 0.1% ](https://github.com/mendableai/firecrawl/search?l=dockerfile)\n  * Other 0.1%\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n## TL;DR Summary\n\n**Firecrawl** is an API service that converts websites into LLM-ready markdown or structured data, enabling advanced scraping and data extraction. It supports various formats, including markdown and HTML, and offers features like crawling, mapping, and batch scraping. The service is open-source under AGPL-3.0 and provides a cloud version for enhanced capabilities. Key integrations include SDKs for Python, Node, and support for LLM frameworks like Langchain and Llama Index.\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\n[ Back to Articles](https://huggingface.co/blog)\n#  [ ](https://huggingface.co/blog/mlabonne/agentic-datagen/#the-rise-of-agentic-data-generation) The Rise of Agentic Data Generation \n[ Community Article](https://huggingface.co/blog/community) Published July 15, 2024\n[ Upvote 82 ](https://huggingface.co/login?next=%2Fblog%2Fmlabonne%2Fagentic-datagen)\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg) ](https://huggingface.co/julien-c \"julien-c\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg) ](https://huggingface.co/thomwolf \"thomwolf\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1589104979708-5df82bcada6d0311fd3d5402.jpeg) ](https://huggingface.co/Chuanming \"Chuanming\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png) ](https://huggingface.co/mrm8488 \"mrm8488\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1594214747713-5e9ecfc04957053f60648a3e.png) ](https://huggingface.co/lhoestq \"lhoestq\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/5f17f0a0925b9863e28ad517/fXIY5i9RLsIa1v3CCuVtt.jpeg) ](https://huggingface.co/victor \"victor\")\n  * +76\n\n\n[![Maxime Labonne's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/61b8e2ba285851687028d395/Rq3xWG7mJ3aCRoBsq340h.jpeg)](https://huggingface.co/mlabonne)\n[mlabonne Maxime Labonne](https://huggingface.co/mlabonne)\n  * [\ud83e\udd16 AgentInstruct: A Multi-Agent Approach](https://huggingface.co/blog/mlabonne/agentic-datagen/#\ud83e\udd16-agentinstruct-a-multi-agent-approach \"\ud83e\udd16 AgentInstruct: A Multi-Agent Approach\")\n  * [\u2694\ufe0f Arena Learning: A Competitive Refinement Approach](https://huggingface.co/blog/mlabonne/agentic-datagen/#\u2694\ufe0f-arena-learning-a-competitive-refinement-approach \"\u2694\ufe0f Arena Learning: A Competitive Refinement Approach\")\n  * [\ud83e\ude84 ArenaInstruct: Combining AgentInstruct and Arena Learning](https://huggingface.co/blog/mlabonne/agentic-datagen/#\ud83e\ude84-arenainstruct-combining-agentinstruct-and-arena-learning \"\ud83e\ude84 ArenaInstruct: Combining AgentInstruct and Arena Learning\")\n  * [Conclusion](https://huggingface.co/blog/mlabonne/agentic-datagen/#conclusion \"Conclusion\")\n\n\n[![image/webp](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/gBaXZ8JfdszshzxH3ejpj.webp)](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/gBaXZ8JfdszshzxH3ejpj.webp)\nWith the consolidation of LLM architectures, the quality of training data has become the most important factor in creating state-of-the-art models. This is true for both pre-training and post-training, where instruction datasets have a major impact on the final model. Two innovative approaches have recently emerged to address the challenge of generating high-quality instruction datasets for post-training LLMs: AgentInstruct and Arena Learning. Both frameworks come from Microsoft Research and leverage multiple LLMs to create and refine samples.\nIn this article, I want to explore both methods, analyze their similarities and differences, and see how we could combine them in a single end-to-end framework.\n##  [ ](https://huggingface.co/blog/mlabonne/agentic-datagen/#\ud83e\udd16-agentinstruct-a-multi-agent-approach) \ud83e\udd16 AgentInstruct: A Multi-Agent Approach \n[AgentInstruct](https://arxiv.org/abs/2407.03502) is an agentic framework by Mitra et al. (2024), designed to generate large-scale, diverse, and high-quality synthetic data. The framework uses a sophisticated pipeline that transforms raw text into refined instructions through multiple stages of processing. In the paper, the agents seem to be based on GPT-4, which is also used to evaluate data quality and hallucinations in some contexts.\n[![image/png](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/3eTPmHhseIuRFxuZZ_Tp4.png)](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/3eTPmHhseIuRFxuZZ_Tp4.png)\n_Figure from the AgentInstruct paper._\nThe AgentInstruct pipeline consists of four main steps:\n  * **Seed Collection** : Assemble a diverse collection of raw seeds, such as textbook chapters, web articles, and code snippets. These seeds serve as the foundation for generating new instructions.\n  * **Content Transformation** : One or more specialized agents modify each seed into an intermediate representation that simplifies instruction creation. These agents are designed to perform tasks like generating argument passages, debates, conversations, meeting transcripts, poems, satirical content, etc.\n  * **Seed Instruction Generation** : Multiple agents take the transformed seed and generate diverse instructions based on a pre-defined taxonomy of instruction types. For example, in the domain of reading comprehension, the taxonomy includes 43 question types, ranging from literal comprehension to critical analysis and inference.\n  * **Instruction Refinement** : The final stage involves iteratively enhancing the complexity and quality of the generated instructions. This is achieved through suggester-editor agent pairs. Suggester agents propose ways to increase instruction complexity, while editor agents modify the instructions accordingly.\n\n\nTo get a better idea of what each stage produces, I recommend reading the examples provided in the paper.\nEach flow in the AgentInstruct pipeline consists of multiple agents powered by LLMs. These agents can be equipped with tools like search APIs or code interpreters to enhance their capabilities. The roles of these agents are carefully defined in their system messages to ensure they perform their specific tasks effectively.\nThe authors of AgentInstruct implemented flows for 17 different skills, each with multiple subcategories. These skills cover a wide range of areas, including reading comprehension, question answering, coding, retrieval augmented generation, creative writing, tool use, and web control.\nUsing this comprehensive pipeline, the researchers generated approximately 22 million instructions. They combined this synthetic data with 3.8 million instructions from other sources to create a dataset of 25.8 million paired instructions. This dataset was then used to fine-tune the Mistral-7b model, resulting in the creation of the Orca-3 model.\n##  [ ](https://huggingface.co/blog/mlabonne/agentic-datagen/#\u2694\ufe0f-arena-learning-a-competitive-refinement-approach) \u2694\ufe0f Arena Learning: A Competitive Refinement Approach \n[Arena Learning](https://www.microsoft.com/en-us/research/uploads/prodnew/2024/07/WizardLM_ArenaLearning.pdf) by Luo, Suo, et al. (2024) takes a different approach to generating high-quality instruction data. Instead of creating instructions from scratch, it focuses on refining existing instruction datasets through a simulated competitive environment. It is not an agentic framework because tools are not provided to the models, but could easily be transformed into one.\n[![image/png](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/t6ppB65U2FMR8Oq0jPHj4.png)](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/t6ppB65U2FMR8Oq0jPHj4.png)\n_Figure from the Arena Learning paper._\nThe key components of the Arena Learning pipeline are:\n  * **Offline Pairwise LLM Arena** : Arena Learning creates a simulated arena where multiple LLMs compete against each other on a large set of instruction data. A judge LLM ([meta-llama/Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)) evaluates the responses from competing models for each instruction, providing rankings, scores, and explanations. This process effectively simulates human evaluation but at a much larger scale and lower cost.\n  * **Data Collection and Preprocessing** : The framework starts with a large corpus of conversational data collected from various open sources. This data goes through filtering, cleaning, and deduplication. Instructions that are too short, illegal/toxic, or too similar to benchmark test sets are removed. The refined dataset is then split into multiple parts for iterative training.\n  * **Iterative Battle and Model Evolution** : The process involves multiple rounds of battles and training:\n    1. An initial model (WizardLM-\u03b2-SFT-I0) is trained on a subset of data.\n    2. This model competes against other state-of-the-art LLMs on another data subset.\n    3. Instances where WizardLM-\u03b2 loses are collected, with the winning model's response used as the target for fine-tuning.\n    4. The process repeats for multiple iterations, with each iteration potentially using different training strategies (SFT, DPO, PPO).\n  * **Training Strategies** : Arena Learning employs multiple training strategies to improve the model:\n    * _Supervised Fine-Tuning (SFT)_ : Uses battle results to fine-tune the model on instances where it performed poorly.\n    * _Direct Preference Optimization (DPO)_ : Treats win/loss responses as choice/reject pairs for training.\n    * _Proximal Policy Optimization (PPO)_ : Uses battle results to train both a reward model and the language model.\n  * **WizardArena Evaluation** : The authors create an offline test set (WizardArena) with diverse and hard subsets. This is used to evaluate models through pairwise battles, with results used to compute Elo rankings. The evaluation closely aligns with human-based arenas but is much faster and cheaper.\n  * **Data Selection** : The pipeline uses various strategies to select high-quality training data, such as threshold-based filtering to control data size and quality, focusing on instances where the model underperforms, and gradually shifting towards more complex data in later iterations.\n\n\n[![image/png](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/FPsxlbNSwPOAuoFMfxxB3.png)](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/FPsxlbNSwPOAuoFMfxxB3.png)\n_Figure from the Arena Learning paper._\nThis framework allows for multiple iterations of battles and training, as illustrated with WizardLM-\u03b2. The model's capabilities are progressively strengthened, particularly in complex tasks. The process results in significant gains in Elo rankings, MT-bench scores, and other evaluation metrics.\nArena Learning focuses on improving areas where the model under training is currently lacking. A nice feature is that it doesn't require particularly powerful models like Claude 3.5 Sonnet or GPT-4o. Models with a similar level can be better in some tasks and domains, as well as more suited to answer certain prompt syntaxes. It means that the entire pipeline can be deployed using open-weight models, which is a big advantage if you already have a high-quality infrastructure.\n##  [ ](https://huggingface.co/blog/mlabonne/agentic-datagen/#\ud83e\ude84-arenainstruct-combining-agentinstruct-and-arena-learning) \ud83e\ude84 ArenaInstruct: Combining AgentInstruct and Arena Learning \nWhile both AgentInstruct and Arena Learning aim to generate high-quality data for post-training language models, they take fundamentally different approaches to achieve this goal. Understanding how they differ, as well as their strengths and weaknesses is a good first step to see how we could combine them. I selected four points I want to focus on:\n  * **Data Generation** : AgentInstruct starts from raw text, generating instructions from scratch through a multi-stage pipeline. This allows for the creation of entirely new content, potentially leading to greater diversity and novelty in the generated instructions. On the other hand, Arena Learning refines existing instruction datasets through simulated battles between models. This method leverages the quality of existing datasets while improving upon them through competitive evaluation.\n  * **Data Quality** : AgentInstruct relies on suggester-editor agent pairs for iterative refinement of instructions. This approach allows for fine-grained control over the complexity and quality of generated instructions. Arena Learning, in contrast, uses an LLM-as-a-judge to evaluate responses in simulated battles. It means that the entire data quality process is handled by a single model.\n  * **Diversity and Complexity** : AgentInstruct explicitly (i.e., manually) designs for diversity through a taxonomy of instruction types and multiple transformation agents. This structured approach ensures coverage across a wide range of skills and instruction types. Arena Learning's diversity comes from the variety of competing models and initial instruction datasets. While this may lead to less structured diversity, it could potentially capture more natural variations in instruction styles.\n  * **Flexibility** : AgentInstruct's pipeline allows for easy addition of new seed types and instruction categories, making it highly adaptable to new domains and tasks. Arena Learning's iterative battle process enables continuous improvement of the target model, potentially allowing it to adapt more quickly to new challenges and competing models.\n\n\nBased on this comparison, it's not too difficult to see how we can leverage the advantages of each framework. For instance, a taxonomy-based data generation is more steerable and could be improved upon by arena learning. But we could also use feedback signals to improve this first step over multiple iterations.\n[![image/png](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/r4ofxOegY2mqwGCPDB0qk.png)](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/r4ofxOegY2mqwGCPDB0qk.png)\nHere's how such a hybrid approach might work:\n  1. **AgentInstruct Instruction Generation** : Use AgentInstruct to create a broad and diverse base of instructions (no answers!) from raw text. This would ensure wide coverage of tasks and domains that are relevant for our use cases.\n  2. **Arena Learning Answer Generation** : Apply Arena Learning's competitive battle approach to refine and select the highest quality answers from a pool of models. This would combine AgentInstruct's ability to generate novel content with Arena Learning's robust quality control mechanism.\n  3. **Data Quality Evaluation** : Instead of relying on a single LLM-as-a-judge, we can use reward models or an [LLM-as-a-jury](https://arxiv.org/abs/2404.18796) to improve the data selection process.\n  4. **Diversity Feedback** : Use insights from Arena Learning battles to dynamically update AgentInstruct's instruction taxonomy. This would focus the generation process on producing more of the instruction types that prove most challenging or useful in real-world scenarios.\n  5. **Complexity Feedback** : Leverage Arena Learning's performance metrics to identify areas where instructions are too easy or too difficult. Use this information to guide AgentInstruct's complexity refinement process, ensuring a well-balanced dataset that challenges the model appropriately over several iterations.\n\n\nBy combining these approaches, we can create a powerful feedback loop between instruction generation and evaluation. This hybrid framework would benefit from AgentInstruct's ability to generate novel, diverse content and Arena Learning's competitive quality control and model improvement process. The result would be a more robust, effective, and continuously improving post-training dataset for LLMs.\n##  [ ](https://huggingface.co/blog/mlabonne/agentic-datagen/#conclusion) Conclusion \nIn conclusion, this article explored two recent approaches in synthetic data generation: AgentInstruct and Arena Learning. We proposed a hybrid solution that combines AgentInstruct's structured, taxonomy-based methodology with Arena Learning's iterative refinement using multiple LLMs. This combination leverages the strengths of both frameworks, allowing for a systematic generation of diverse data while enabling continuous improvement of the underlying taxonomy through feedback from the LLM pool. I feel like we might lose some quality by removing the suggester-editor agent pairs. Let me know if you have better ideas.\nStill, data quality evaluation is a significant challenge to perfect this approach. The current reliance on models like GPT-4 or Llama 3 70B Instruct as judges is imperfect and has known limitations (see my quick review [here](https://x.com/maximelabonne/status/1808473304518287595)). Improving the quality assessment stage could lead to more efficient datasets, achieving better performance with fewer samples. To know more about how to create high-quality datasets, check out my GitHub repo [\ud83d\udcbe LLM Datasets](https://github.com/mlabonne/llm-datasets).\n### Community\nEditPreview\nUpload images, audio, and videos by dragging in the text input, pasting, or clicking here. \nTap or paste here to upload images\nYour need to confirm your account before you can post a new comment. \nComment \n\u00b7 [Sign up](https://huggingface.co/join?next=%2Fblog%2Fmlabonne%2Fagentic-datagen) or [log in](https://huggingface.co/login?next=%2Fblog%2Fmlabonne%2Fagentic-datagen) to comment \n[ Upvote 82 ](https://huggingface.co/login?next=%2Fblog%2Fmlabonne%2Fagentic-datagen)\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg) ](https://huggingface.co/julien-c \"julien-c\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg) ](https://huggingface.co/thomwolf \"thomwolf\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1589104979708-5df82bcada6d0311fd3d5402.jpeg) ](https://huggingface.co/Chuanming \"Chuanming\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png) ](https://huggingface.co/mrm8488 \"mrm8488\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1594214747713-5e9ecfc04957053f60648a3e.png) ](https://huggingface.co/lhoestq \"lhoestq\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/5f17f0a0925b9863e28ad517/fXIY5i9RLsIa1v3CCuVtt.jpeg) ](https://huggingface.co/victor \"victor\")\n  * [![](https://huggingface.co/avatars/154bc7f4830f612d6fa16d0677fcd426.svg) ](https://huggingface.co/vikas \"vikas\")\n  * [![](https://huggingface.co/avatars/bf0ce9d02563d8c6c92adb43adbb7bb4.svg) ](https://huggingface.co/suhnylla \"suhnylla\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg) ](https://huggingface.co/vicgalle \"vicgalle\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/60078446e55258e41786a959/UGPCE4YqG9BVMSf0YauxL.png) ](https://huggingface.co/tokestermw \"tokestermw\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/6032802e1f993496bc14d9e3/w6hr-DEQot4VVkoyRIBiy.png) ](https://huggingface.co/osanseviero \"osanseviero\")\n  * [![](https://cdn-avatars.huggingface.co/v1/production/uploads/6033c55f60e3dd96631c908d/jy7cHHCBhnlzHKGbXIbj0.jpeg) ](https://huggingface.co/theainerd \"theainerd\")\n  * +70\n\n\nSystem theme \nCompany\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/) [](https://huggingface.co/)\nWebsite\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe article discusses two innovative frameworks for generating high-quality instruction datasets for LLMs: **AgentInstruct** and **Arena Learning**. AgentInstruct creates diverse instructions from raw text, while Arena Learning refines existing datasets through competitive evaluation. A proposed hybrid approach combines their strengths for improved data quality and diversity.\n```"
  },
  {
    "instruction": "[![logo](https://qdrant.tech/img/qdrant-logo.svg)](https://qdrant.tech/)\n  * [Qdrant](https://qdrant.tech/documentation/)\n  * [Cloud](https://qdrant.tech/documentation/cloud-intro/)\n  * [Build](https://qdrant.tech/documentation/build/)\n  * [Learn](https://qdrant.tech/articles/)\n  * [API Reference](https://api.qdrant.tech/api-reference)\n\n\nSearch\n[Log in](https://cloud.qdrant.io/login) [Start Free](https://cloud.qdrant.io/signup)\n![logo](https://qdrant.tech/img/qdrant-logo.svg)\nSearch\n  * [ Qdrant](https://qdrant.tech/documentation/)\n  * [ Cloud](https://qdrant.tech/documentation/cloud-intro/)\n  * [ Build](https://qdrant.tech/documentation/build/)\n  * [ Learn](https://qdrant.tech/articles/)\n  * [ API Reference](https://api.qdrant.tech/api-reference)\n\n\n### Learn\n[Vector Search Manuals](https://qdrant.tech/articles/vector-search-manuals/)\n[Qdrant Internals](https://qdrant.tech/articles/qdrant-internals/)\n[Data Exploration](https://qdrant.tech/articles/data-exploration/)\n[Machine Learning](https://qdrant.tech/articles/machine-learning/)\n[RAG & GenAI](https://qdrant.tech/articles/rag-and-genai/)\n[Practical Examples](https://qdrant.tech/articles/practicle-examples/)\n[Ecosystem](https://qdrant.tech/articles/ecosystem/)\n### Learn\n[Vector Search Manuals](https://qdrant.tech/articles/vector-search-manuals/)\n[Qdrant Internals](https://qdrant.tech/articles/qdrant-internals/)\n[Data Exploration](https://qdrant.tech/articles/data-exploration/)\n[Machine Learning](https://qdrant.tech/articles/machine-learning/)\n[RAG & GenAI](https://qdrant.tech/articles/rag-and-genai/)\n[Practical Examples](https://qdrant.tech/articles/practicle-examples/)\n[Ecosystem](https://qdrant.tech/articles/ecosystem/)\n#### Vector Search Manuals\nTake full control of your vector data with Qdrant. Learn how to easily store, organize, and optimize vectors for high-performance similarity search.\n[Learn More](https://qdrant.tech/articles/vector-search-manuals/)\n[![Preview](https://qdrant.tech/articles_data/indexing-optimization/preview/preview.jpg)Optimizing Memory for Bulk UploadsEfficient memory management is key when handling large-scale vector data. Learn how to optimize memory consumption during bulk uploads in Qdrant and keep your deployments performant under heavy load.Sabrina AquinoFebruary 13, 2025](https://qdrant.tech/articles/indexing-optimization/)[![Preview](https://qdrant.tech/articles_data/vector-search-resource-optimization/preview/preview.jpg)Vector Search Resource Optimization GuideLearn how to get the most from Qdrant's optimization features. Discover key tricks and best practices to boost vector search performance and reduce Qdrant's resource usage.David MyrielFebruary 09, 2025](https://qdrant.tech/articles/vector-search-resource-optimization/)[![Preview](https://qdrant.tech/articles_data/what-is-a-vector-database/preview/preview.jpg)An Introduction to Vector DatabasesDiscover what a vector database is, its core functionalities, and real-world applications.Sabrina AquinoOctober 09, 2024](https://qdrant.tech/articles/what-is-a-vector-database/)\n#### Qdrant Internals\nTake a look under the hood of Qdrant\u2019s high-performance vector search engine. Explore the architecture, components, and design principles the Qdrant Vector Search Engine is built on.\n[Learn More](https://qdrant.tech/articles/qdrant-internals/)\n[![Preview](https://qdrant.tech/articles_data/dedicated-vector-search/preview/preview.jpg)Built for Vector SearchWhy add-on vector search looks good \u2014 until you actually use it.Evgeniya Sukhodolskaya & Andrey VasnetsovFebruary 17, 2025](https://qdrant.tech/articles/dedicated-vector-search/)[![Preview](https://qdrant.tech/articles_data/gridstore-key-value-storage/preview/preview.jpg)Introducing Gridstore: Qdrant's Custom Key-Value StoreWhy and how we built our own key-value store. A short technical report on our procedure and results.Luis Cossio, Arnaud Gourlay & David MyrielFebruary 05, 2025](https://qdrant.tech/articles/gridstore-key-value-storage/)[![Preview](https://qdrant.tech/articles_data/immutable-data-structures/preview/preview.jpg)Qdrant Internals: Immutable Data StructuresLearn how immutable data structures improve vector search performance in Qdrant.Andrey VasnetsovAugust 20, 2024](https://qdrant.tech/articles/immutable-data-structures/)\n#### Data Exploration\nLearn how you can leverage vector similarity beyond just search. Reveal hidden patterns and insights in your data, provide recommendations, and navigate data space.\n[Learn More](https://qdrant.tech/articles/data-exploration/)\n[![Preview](https://qdrant.tech/articles_data/distance-based-exploration/preview/preview.jpg)Distance-based data explorationExplore your data under a new angle with Qdrant's tools for dimensionality reduction, clusterization, and visualization.Andrey VasnetsovMarch 11, 2025](https://qdrant.tech/articles/distance-based-exploration/)[![Preview](https://qdrant.tech/articles_data/discovery-search/preview/preview.jpg)Discovery needs contextDiscovery Search, an innovative way to constrain the vector space in which a search is performed, relying only on vectors.Luis Coss\u00edoJanuary 31, 2024](https://qdrant.tech/articles/discovery-search/)[![Preview](https://qdrant.tech/articles_data/vector-similarity-beyond-search/preview/preview.jpg)Vector Similarity: Going Beyond Full-Text Search | QdrantDiscover how vector similarity expands data exploration beyond full-text search. Explore diversity sampling and more for enhanced data discovery!Luis Coss\u00edoAugust 08, 2023](https://qdrant.tech/articles/vector-similarity-beyond-search/)\n#### Machine Learning\nExplore Machine Learning principles and practices which make modern semantic similarity search possible. Apply Qdrant and vector search capabilities to your ML projects.\n[Learn More](https://qdrant.tech/articles/machine-learning/)\n[![Preview](https://qdrant.tech/articles_data/search-feedback-loop/preview/preview.jpg)Relevance Feedback in Informational RetrievalRelerance feedback: from ancient history to LLMs. Why relevance feedback techniques are good on paper but not popular in neural search, and what we can do about it.Evgeniya SukhodolskayaMarch 27, 2025](https://qdrant.tech/articles/search-feedback-loop/)[![Preview](https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/preview/preview.jpg)Modern Sparse Neural Retrieval: From Theory to PracticeA comprehensive guide to modern sparse neural retrievers: COIL, TILDEv2, SPLADE, and more. Find out how they work and learn how to use them effectively.Evgeniya SukhodolskayaOctober 23, 2024](https://qdrant.tech/articles/modern-sparse-neural-retrieval/)[![Preview](https://qdrant.tech/articles_data/cross-encoder-integration-gsoc/preview/preview.jpg)Qdrant Summer of Code 2024 - ONNX Cross Encoders in PythonA summary of my work and experience at Qdrant Summer of Code 2024.Huong (Celine) HoangOctober 14, 2024](https://qdrant.tech/articles/cross-encoder-integration-gsoc/)\n#### RAG & GenAI\nLeverage Qdrant for Retrieval-Augmented Generation (RAG) and build AI Agents\n[Learn More](https://qdrant.tech/articles/rag-and-genai/)\n[![Preview](https://qdrant.tech/articles_data/agentic-rag/preview/preview.jpg)What is Agentic RAG? Building Agents with QdrantAgents are a new paradigm in AI, and they are changing how we build RAG systems. Learn how to build agents with Qdrant and which framework to choose.Kacper \u0141ukawskiNovember 22, 2024](https://qdrant.tech/articles/agentic-rag/)[![Preview](https://qdrant.tech/articles_data/rapid-rag-optimization-with-qdrant-and-quotient/preview/preview.jpg)Optimizing RAG Through an Evaluation-Based MethodologyLearn how Qdrant-powered RAG applications can be tested and iteratively improved using LLM evaluation tools like Quotient.Atita AroraJune 12, 2024](https://qdrant.tech/articles/rapid-rag-optimization-with-qdrant-and-quotient/)[![Preview](https://qdrant.tech/articles_data/semantic-cache-ai-data-retrieval/preview/preview.jpg)Semantic Cache: Accelerating AI with Lightning-Fast Data RetrievalSemantic cache is reshaping AI applications by enabling rapid data retrieval. Discover how its implementation benefits your RAG setup.Daniel Romero, David MyrielMay 07, 2024](https://qdrant.tech/articles/semantic-cache-ai-data-retrieval/)\n#### Practical Examples\nBuilding blocks and reference implementations to help you get started with Qdrant. Learn how to use Qdrant to solve real-world problems and build the next generation of AI applications.\n[Learn More](https://qdrant.tech/articles/practicle-examples/)\n[![Preview](https://qdrant.tech/articles_data/binary-quantization-openai/preview/preview.jpg)Optimizing OpenAI Embeddings: Enhance Efficiency with Qdrant's Binary QuantizationExplore how Qdrant's Binary Quantization can significantly improve the efficiency and performance of OpenAI's Ada-003 embeddings. Learn best practices for real-time search applications.Nirant KasliwalFebruary 21, 2024](https://qdrant.tech/articles/binary-quantization-openai/)[![Preview](https://qdrant.tech/articles_data/food-discovery-demo/preview/preview.jpg)Food Discovery DemoFeeling hungry? Find the perfect meal with Qdrant's multimodal semantic search.Kacper \u0141ukawskiSeptember 05, 2023](https://qdrant.tech/articles/food-discovery-demo/)[![Preview](https://qdrant.tech/articles_data/search-as-you-type/preview/preview.jpg)Semantic Search As You TypeTo show off Qdrant's performance, we show how to do a quick search-as-you-type that will come back within a few milliseconds.Andre BogusAugust 14, 2023](https://qdrant.tech/articles/search-as-you-type/)\n#### Ecosystem\nTools, libraries and integrations around Qdrant vector search engine.\n[Learn More](https://qdrant.tech/articles/ecosystem/)\n[![Preview](https://qdrant.tech/articles_data/dimension-reduction-qsoc/preview/preview.jpg)Qdrant Summer of Code 2024 - WASM based Dimension ReductionMy journey as a Qdrant Summer of Code 2024 participant working on enhancing vector visualization using WebAssembly (WASM) based dimension reduction.Jishan BhattacharyaAugust 31, 2024](https://qdrant.tech/articles/dimension-reduction-qsoc/)[![Preview](https://qdrant.tech/articles_data/fastembed/preview/preview.jpg)FastEmbed: Qdrant's Efficient Python Library for Embedding GenerationLearn how to accurately and efficiently create text embeddings with FastEmbed.Nirant KasliwalOctober 18, 2023](https://qdrant.tech/articles/fastembed/)[![Preview](https://qdrant.tech/articles_data/web-ui-gsoc/preview/preview.jpg)Google Summer of Code 2023 - Web UI for Visualization and ExplorationMy journey as a Google Summer of Code 2023 student working on the \"Web UI for Visualization and Exploration\" project for Qdrant.Kartik GuptaAugust 28, 2023](https://qdrant.tech/articles/web-ui-gsoc/)\n#### Ready to get started with Qdrant?\n[Start Free](https://qdrant.to/cloud/)\n\u00a9 2025 Qdrant. All Rights Reserved\n[Terms](https://qdrant.tech/legal/terms_and_conditions/) [Privacy Policy](https://qdrant.tech/legal/privacy-policy/) [Impressum](https://qdrant.tech/legal/impressum/)\n\u00d7\n[ Powered by ](https://qdrant.tech/)\nWe use cookies to learn more about you. At any time you can delete or block cookies through your browser settings.\n[Learn more](https://qdrant.tech/legal/privacy-policy/)[I accept](https://qdrant.tech/articles/)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nQdrant offers a high-performance vector search engine, enabling efficient storage and optimization of vector data. Key resources include manuals on vector search, machine learning applications, and retrieval-augmented generation (RAG). Explore practical examples and ecosystem tools to enhance AI applications.\n\n**Tags:** Generative AI, LLMs, Vector Search, Machine Learning, RAG\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/langflow-ai/langflow?tab=readme-ov-file/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Flangflow-ai%2Flangflow%3Ftab%3Dreadme-ov-file%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Flangflow-ai%2Flangflow%3Ftab%3Dreadme-ov-file%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=langflow-ai%2Flangflow) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/langflow-ai/langflow?tab=readme-ov-file/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/langflow-ai/langflow?tab=readme-ov-file/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/langflow-ai/langflow?tab=readme-ov-file/) to refresh your session. Dismiss alert\n{{ message }}\n[ langflow-ai ](https://github.com/langflow-ai) / **[langflow](https://github.com/langflow-ai/langflow) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Flangflow-ai%2Flangflow) You must be signed in to change notification settings\n  * [ Fork 5.8k ](https://github.com/login?return_to=%2Flangflow-ai%2Flangflow)\n  * [ Star  53.4k ](https://github.com/login?return_to=%2Flangflow-ai%2Flangflow)\n\n\nLangflow is a powerful tool for building and deploying AI-powered agents and workflows. \n[www.langflow.org](http://www.langflow.org \"http://www.langflow.org\")\n### License\n[ MIT license ](https://github.com/langflow-ai/langflow/blob/main/LICENSE)\n[ 53.4k stars ](https://github.com/langflow-ai/langflow/stargazers) [ 5.8k forks ](https://github.com/langflow-ai/langflow/forks) [ Branches ](https://github.com/langflow-ai/langflow/branches) [ Tags ](https://github.com/langflow-ai/langflow/tags) [ Activity ](https://github.com/langflow-ai/langflow/activity)\n[ Star  ](https://github.com/login?return_to=%2Flangflow-ai%2Flangflow)\n[ Notifications ](https://github.com/login?return_to=%2Flangflow-ai%2Flangflow) You must be signed in to change notification settings\n  * [ Code ](https://github.com/langflow-ai/langflow)\n  * [ Issues 381 ](https://github.com/langflow-ai/langflow/issues)\n  * [ Pull requests 493 ](https://github.com/langflow-ai/langflow/pulls)\n  * [ Discussions ](https://github.com/langflow-ai/langflow/discussions)\n  * [ Actions ](https://github.com/langflow-ai/langflow/actions)\n  * [ Projects 0 ](https://github.com/langflow-ai/langflow/projects)\n  * [ Security ](https://github.com/langflow-ai/langflow/security)\n  * [ Insights ](https://github.com/langflow-ai/langflow/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/langflow-ai/langflow)\n  * [ Issues  ](https://github.com/langflow-ai/langflow/issues)\n  * [ Pull requests  ](https://github.com/langflow-ai/langflow/pulls)\n  * [ Discussions  ](https://github.com/langflow-ai/langflow/discussions)\n  * [ Actions  ](https://github.com/langflow-ai/langflow/actions)\n  * [ Projects  ](https://github.com/langflow-ai/langflow/projects)\n  * [ Security  ](https://github.com/langflow-ai/langflow/security)\n  * [ Insights  ](https://github.com/langflow-ai/langflow/pulse)\n\n\n# langflow-ai/langflow\nmain\n[Branches](https://github.com/langflow-ai/langflow/branches)[Tags](https://github.com/langflow-ai/langflow/tags)\n[](https://github.com/langflow-ai/langflow/branches)[](https://github.com/langflow-ai/langflow/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[15,546 Commits](https://github.com/langflow-ai/langflow/commits/main/)[](https://github.com/langflow-ai/langflow/commits/main/)  \n[.devcontainer](https://github.com/langflow-ai/langflow/tree/main/.devcontainer \".devcontainer\")| [.devcontainer](https://github.com/langflow-ai/langflow/tree/main/.devcontainer \".devcontainer\")  \n[.github](https://github.com/langflow-ai/langflow/tree/main/.github \".github\")| [.github](https://github.com/langflow-ai/langflow/tree/main/.github \".github\")  \n[.vscode](https://github.com/langflow-ai/langflow/tree/main/.vscode \".vscode\")| [.vscode](https://github.com/langflow-ai/langflow/tree/main/.vscode \".vscode\")  \n[deploy](https://github.com/langflow-ai/langflow/tree/main/deploy \"deploy\")| [deploy](https://github.com/langflow-ai/langflow/tree/main/deploy \"deploy\")  \n[docker](https://github.com/langflow-ai/langflow/tree/main/docker \"docker\")| [docker](https://github.com/langflow-ai/langflow/tree/main/docker \"docker\")  \n[docker_example](https://github.com/langflow-ai/langflow/tree/main/docker_example \"docker_example\")| [docker_example](https://github.com/langflow-ai/langflow/tree/main/docker_example \"docker_example\")  \n[docs](https://github.com/langflow-ai/langflow/tree/main/docs \"docs\")| [docs](https://github.com/langflow-ai/langflow/tree/main/docs \"docs\")  \n[scripts](https://github.com/langflow-ai/langflow/tree/main/scripts \"scripts\")| [scripts](https://github.com/langflow-ai/langflow/tree/main/scripts \"scripts\")  \n[src](https://github.com/langflow-ai/langflow/tree/main/src \"src\")| [src](https://github.com/langflow-ai/langflow/tree/main/src \"src\")  \n[test-results](https://github.com/langflow-ai/langflow/tree/main/test-results \"test-results\")| [test-results](https://github.com/langflow-ai/langflow/tree/main/test-results \"test-results\")  \n[.composio.lock](https://github.com/langflow-ai/langflow/blob/main/.composio.lock \".composio.lock\")| [.composio.lock](https://github.com/langflow-ai/langflow/blob/main/.composio.lock \".composio.lock\")  \n[.env.example](https://github.com/langflow-ai/langflow/blob/main/.env.example \".env.example\")| [.env.example](https://github.com/langflow-ai/langflow/blob/main/.env.example \".env.example\")  \n[.eslintrc.json](https://github.com/langflow-ai/langflow/blob/main/.eslintrc.json \".eslintrc.json\")| [.eslintrc.json](https://github.com/langflow-ai/langflow/blob/main/.eslintrc.json \".eslintrc.json\")  \n[.gitattributes](https://github.com/langflow-ai/langflow/blob/main/.gitattributes \".gitattributes\")| [.gitattributes](https://github.com/langflow-ai/langflow/blob/main/.gitattributes \".gitattributes\")  \n[.gitignore](https://github.com/langflow-ai/langflow/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/langflow-ai/langflow/blob/main/.gitignore \".gitignore\")  \n[.pre-commit-config.yaml](https://github.com/langflow-ai/langflow/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/langflow-ai/langflow/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")  \n[CODE_OF_CONDUCT.md](https://github.com/langflow-ai/langflow/blob/main/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| [CODE_OF_CONDUCT.md](https://github.com/langflow-ai/langflow/blob/main/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")  \n[CONTRIBUTING.md](https://github.com/langflow-ai/langflow/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/langflow-ai/langflow/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")  \n[DEVELOPMENT.md](https://github.com/langflow-ai/langflow/blob/main/DEVELOPMENT.md \"DEVELOPMENT.md\")| [DEVELOPMENT.md](https://github.com/langflow-ai/langflow/blob/main/DEVELOPMENT.md \"DEVELOPMENT.md\")  \n[LICENSE](https://github.com/langflow-ai/langflow/blob/main/LICENSE \"LICENSE\")| [LICENSE](https://github.com/langflow-ai/langflow/blob/main/LICENSE \"LICENSE\")  \n[Makefile](https://github.com/langflow-ai/langflow/blob/main/Makefile \"Makefile\")| [Makefile](https://github.com/langflow-ai/langflow/blob/main/Makefile \"Makefile\")  \n[README.md](https://github.com/langflow-ai/langflow/blob/main/README.md \"README.md\")| [README.md](https://github.com/langflow-ai/langflow/blob/main/README.md \"README.md\")  \n[eslint.config.js](https://github.com/langflow-ai/langflow/blob/main/eslint.config.js \"eslint.config.js\")| [eslint.config.js](https://github.com/langflow-ai/langflow/blob/main/eslint.config.js \"eslint.config.js\")  \n[pyproject.toml](https://github.com/langflow-ai/langflow/blob/main/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/langflow-ai/langflow/blob/main/pyproject.toml \"pyproject.toml\")  \n[render.yaml](https://github.com/langflow-ai/langflow/blob/main/render.yaml \"render.yaml\")| [render.yaml](https://github.com/langflow-ai/langflow/blob/main/render.yaml \"render.yaml\")  \n[uv.lock](https://github.com/langflow-ai/langflow/blob/main/uv.lock \"uv.lock\")| [uv.lock](https://github.com/langflow-ai/langflow/blob/main/uv.lock \"uv.lock\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/langflow-ai/langflow?tab=readme-ov-file/)\n  * [Code of conduct](https://github.com/langflow-ai/langflow?tab=readme-ov-file/)\n  * [MIT license](https://github.com/langflow-ai/langflow?tab=readme-ov-file/)\n\n\n[![Langflow logo](https://github.com/langflow-ai/langflow/raw/main/docs/static/img/langflow-logo-color-black-solid.svg)](https://github.com/langflow-ai/langflow/blob/main/docs/static/img/langflow-logo-color-black-solid.svg)\n[![Release Notes](https://camo.githubusercontent.com/b42a9773c3bd6d301da1b42f222cda218625eb553987c48d49a934bad68dc884/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6c616e67666c6f772d61692f6c616e67666c6f773f7374796c653d666c61742d737175617265)](https://github.com/langflow-ai/langflow/releases) [![PyPI - License](https://camo.githubusercontent.com/9d525da30058c10a3a4c3339225203b08f2e5ea08582d046ff1fd43b4dfda173/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d6f72616e6765)](https://opensource.org/licenses/MIT) [![PyPI - Downloads](https://camo.githubusercontent.com/a7be9010dd5bb9e700c522cf64856e4629f329a715cfe2c433d677c9637100be/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6c616e67666c6f773f7374796c653d666c61742d737175617265)](https://pypistats.org/packages/langflow) [![GitHub star chart](https://camo.githubusercontent.com/f873abb4ad55fea567991c095a1366e7701a52d4351b6b8137ef0f805022d8bc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c616e67666c6f772d61692f6c616e67666c6f773f7374796c653d666c61742d737175617265)](https://star-history.com/#langflow-ai/langflow) [![Open Issues](https://camo.githubusercontent.com/acd20a78af067586622dc52a38b8951befb6216dbf14f56155cb02a03a95933d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f6c616e67666c6f772d61692f6c616e67666c6f773f7374796c653d666c61742d737175617265)](https://github.com/langflow-ai/langflow/issues) [![Open in HuggingFace](https://camo.githubusercontent.com/a4ff28c1dbabfaa46915ab215390308c2415c77b4b180e78909c08d74c174ad8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/Langflow/Langflow?duplicate=true) [![Twitter](https://camo.githubusercontent.com/1c8537992a7b4a4ee56a11a8b57bec194377a88ff4d7b0013a22f50d78fe2ad2/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f747769747465722e636f6d2f6c616e67666c6f772d61692e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f772532302534304c616e67666c6f77)](https://twitter.com/langflow) [![YouTube Channel Views](https://camo.githubusercontent.com/f699f4df2ef669a522242df5b8faa4ab9621a86047aea35e8b617a41bfd53452/68747470733a2f2f696d672e736869656c64732e696f2f796f75747562652f6368616e6e656c2f76696577732f55436e3262496e51726a6444594b45456d627077626c4c51)](https://www.youtube.com/@Langflow)\n[Langflow](https://langflow.org) is a powerful tool for building and deploying AI-powered agents and workflows. It provides developers with both a visual authoring experience and a built-in API server that turns every agent into an API endpoint that can be integrated into applications built on any framework or stack. Langflow comes with batteries included and supports all major LLMs, vector databases and a growing library of AI tools.\n## \u2728 Highlight features\n[](https://github.com/langflow-ai/langflow?tab=readme-ov-file/#-highlight-features)\n  1. **Visual Builder** to get started quickly and iterate.\n  2. **Access to Code** so developers can tweak any component using Python.\n  3. **Playground** to immediately test and iterate on their flows with step-by-step control.\n  4. **Multi-agent** orchestration and conversation management and retrieval.\n  5. **Deploy as an API** or export as JSON for Python apps.\n  6. **Observability** with LangSmith, LangFuse and other integrations.\n  7. **Enterprise-ready** security and scalability.\n\n\n## \u26a1\ufe0f Quickstart\n[](https://github.com/langflow-ai/langflow?tab=readme-ov-file/#\ufe0f-quickstart)\nLangflow works with Python 3.10 to 3.13.\nInstall with uv **(recommended)**\n```\nuv pip install langflow\n```\n\nInstall with pip\n```\npip install langflow\n```\n\n## \ud83d\udce6 Deployment\n[](https://github.com/langflow-ai/langflow?tab=readme-ov-file/#-deployment)\n### Self-managed\n[](https://github.com/langflow-ai/langflow?tab=readme-ov-file/#self-managed)\nLangflow is completely open source and you can deploy it to all major deployment clouds. Follow this [guide](https://docs.langflow.org/deployment-docker) to learn how to use Docker to deploy Langflow.\n### Fully-managed by DataStax\n[](https://github.com/langflow-ai/langflow?tab=readme-ov-file/#fully-managed-by-datastax)\nDataStax Langflow is a full-managed environment with zero setup. Developers can [sign up for a free account](https://astra.datastax.com/signup?type=langflow) to get started.\n## \u2b50 Stay up-to-date\n[](https://github.com/langflow-ai/langflow?tab=readme-ov-file/#-stay-up-to-date)\nStar Langflow on GitHub to be instantly notified of new releases.\n[![Star Langflow](https://private-user-images.githubusercontent.com/62335616/355231272-03168b17-a11d-4b2a-b0f7-c1cce69e5a2c.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDMzNTcxMzksIm5iZiI6MTc0MzM1NjgzOSwicGF0aCI6Ii82MjMzNTYxNi8zNTUyMzEyNzItMDMxNjhiMTctYTExZC00YjJhLWIwZjctYzFjY2U2OWU1YTJjLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAzMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMzMwVDE3NDcxOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTAwNmNmNmMxNTA2MzRmNDc3ZGUyYTQxM2RlNWI2NTA5MzQwOTEzYWVlMTAyYjA3NTljZDFkNWIzYzcyZDdhOWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.ZpNCQDSmlFPnVt7a30UxnLU4GorXqkiF-cvm6SI6930)](https://private-user-images.githubusercontent.com/62335616/355231272-03168b17-a11d-4b2a-b0f7-c1cce69e5a2c.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDMzNTcxMzksIm5iZiI6MTc0MzM1NjgzOSwicGF0aCI6Ii82MjMzNTYxNi8zNTUyMzEyNzItMDMxNjhiMTctYTExZC00YjJhLWIwZjctYzFjY2U2OWU1YTJjLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAzMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMzMwVDE3NDcxOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTAwNmNmNmMxNTA2MzRmNDc3ZGUyYTQxM2RlNWI2NTA5MzQwOTEzYWVlMTAyYjA3NTljZDFkNWIzYzcyZDdhOWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.ZpNCQDSmlFPnVt7a30UxnLU4GorXqkiF-cvm6SI6930) [ ![Star Langflow](https://private-user-images.githubusercontent.com/62335616/355231272-03168b17-a11d-4b2a-b0f7-c1cce69e5a2c.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDMzNTcxMzksIm5iZiI6MTc0MzM1NjgzOSwicGF0aCI6Ii82MjMzNTYxNi8zNTUyMzEyNzItMDMxNjhiMTctYTExZC00YjJhLWIwZjctYzFjY2U2OWU1YTJjLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAzMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMzMwVDE3NDcxOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTAwNmNmNmMxNTA2MzRmNDc3ZGUyYTQxM2RlNWI2NTA5MzQwOTEzYWVlMTAyYjA3NTljZDFkNWIzYzcyZDdhOWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.ZpNCQDSmlFPnVt7a30UxnLU4GorXqkiF-cvm6SI6930) ](https://private-user-images.githubusercontent.com/62335616/355231272-03168b17-a11d-4b2a-b0f7-c1cce69e5a2c.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDMzNTcxMzksIm5iZiI6MTc0MzM1NjgzOSwicGF0aCI6Ii82MjMzNTYxNi8zNTUyMzEyNzItMDMxNjhiMTctYTExZC00YjJhLWIwZjctYzFjY2U2OWU1YTJjLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAzMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMzMwVDE3NDcxOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTAwNmNmNmMxNTA2MzRmNDc3ZGUyYTQxM2RlNWI2NTA5MzQwOTEzYWVlMTAyYjA3NTljZDFkNWIzYzcyZDdhOWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.ZpNCQDSmlFPnVt7a30UxnLU4GorXqkiF-cvm6SI6930) [ ](https://private-user-images.githubusercontent.com/62335616/355231272-03168b17-a11d-4b2a-b0f7-c1cce69e5a2c.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDMzNTcxMzksIm5iZiI6MTc0MzM1NjgzOSwicGF0aCI6Ii82MjMzNTYxNi8zNTUyMzEyNzItMDMxNjhiMTctYTExZC00YjJhLWIwZjctYzFjY2U2OWU1YTJjLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAzMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMzMwVDE3NDcxOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTAwNmNmNmMxNTA2MzRmNDc3ZGUyYTQxM2RlNWI2NTA5MzQwOTEzYWVlMTAyYjA3NTljZDFkNWIzYzcyZDdhOWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.ZpNCQDSmlFPnVt7a30UxnLU4GorXqkiF-cvm6SI6930)\n## \ud83d\udc4b Contribute\n[](https://github.com/langflow-ai/langflow?tab=readme-ov-file/#-contribute)\nWe welcome contributions from developers of all levels. If you'd like to contribute, please check our [contributing guidelines](https://github.com/langflow-ai/langflow/blob/main/CONTRIBUTING.md) and help make Langflow more accessible.\n[![Star History Chart](https://camo.githubusercontent.com/58d1efb8328bec76b01ca956be18ea4deee6e00efa24117fb6a97c9a2ddc773f/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6c616e67666c6f772d61692f6c616e67666c6f7726747970653d54696d656c696e65)](https://star-history.com/#langflow-ai/langflow&Date)\n## \u2764\ufe0f Contributors\n[](https://github.com/langflow-ai/langflow?tab=readme-ov-file/#\ufe0f-contributors)\n[![langflow contributors](https://camo.githubusercontent.com/bd682f12cfee021ab3c48cd5507126e21fd879459127f41c23a3de55d7063b14/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d6c616e67666c6f772d61692f6c616e67666c6f77)](https://github.com/langflow-ai/langflow/graphs/contributors)\n## About\nLangflow is a powerful tool for building and deploying AI-powered agents and workflows. \n[www.langflow.org](http://www.langflow.org \"http://www.langflow.org\")\n### Topics\n[ multiagent ](https://github.com/topics/multiagent \"Topic: multiagent\") [ agents ](https://github.com/topics/agents \"Topic: agents\") [ react-flow ](https://github.com/topics/react-flow \"Topic: react-flow\") [ large-language-models ](https://github.com/topics/large-language-models \"Topic: large-language-models\") [ generative-ai ](https://github.com/topics/generative-ai \"Topic: generative-ai\") [ chatgpt ](https://github.com/topics/chatgpt \"Topic: chatgpt\")\n### Resources\n[ Readme ](https://github.com/langflow-ai/langflow?tab=readme-ov-file/#readme-ov-file)\n### License\n[ MIT license ](https://github.com/langflow-ai/langflow?tab=readme-ov-file/#MIT-1-ov-file)\n### Code of conduct\n[ Code of conduct ](https://github.com/langflow-ai/langflow?tab=readme-ov-file/#coc-ov-file)\n[ Activity](https://github.com/langflow-ai/langflow/activity)\n[ Custom properties](https://github.com/langflow-ai/langflow/custom-properties)\n### Stars\n[ **53.4k** stars](https://github.com/langflow-ai/langflow/stargazers)\n### Watchers\n[ **358** watching](https://github.com/langflow-ai/langflow/watchers)\n### Forks\n[ **5.8k** forks](https://github.com/langflow-ai/langflow/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Flangflow-ai%2Flangflow&report=langflow-ai+%28user%29)\n##  [Releases 250](https://github.com/langflow-ai/langflow/releases)\n[ 1.2.0 Latest  Feb 25, 2025 ](https://github.com/langflow-ai/langflow/releases/tag/1.2.0)\n[+ 249 releases](https://github.com/langflow-ai/langflow/releases)\n##  [Used by 1.3k](https://github.com/langflow-ai/langflow/network/dependents)\n[\n  * ![@ahmedmohamed7334](https://avatars.githubusercontent.com/u/85694013?s=64&v=4)\n  * ![@SmallBlindDealer](https://avatars.githubusercontent.com/u/174235597?s=64&v=4)\n  * ![@Nishchaypat](https://avatars.githubusercontent.com/u/100232114?s=64&v=4)\n  * ![@Kjm478](https://avatars.githubusercontent.com/u/188508722?s=64&v=4)\n  * ![@nimra-pixel](https://avatars.githubusercontent.com/u/63564293?s=64&v=4)\n  * ![@roguefunction](https://avatars.githubusercontent.com/u/112044743?s=64&v=4)\n  * ![@Justmalhar](https://avatars.githubusercontent.com/u/17442997?s=64&v=4)\n  * ![@bringesh2001](https://avatars.githubusercontent.com/u/72957994?s=64&v=4)\n\n+ 1,301  ](https://github.com/langflow-ai/langflow/network/dependents)\n##  [Contributors 264](https://github.com/langflow-ai/langflow/graphs/contributors)\n  * [ ![@ogabrielluiz](https://avatars.githubusercontent.com/u/24829397?s=64&v=4) ](https://github.com/ogabrielluiz)\n  * [ ![@anovazzi1](https://avatars.githubusercontent.com/u/70824914?s=64&v=4) ](https://github.com/anovazzi1)\n  * [ ![@lucaseduoli](https://avatars.githubusercontent.com/u/62335616?s=64&v=4) ](https://github.com/lucaseduoli)\n  * [ ![@Cristhianzl](https://avatars.githubusercontent.com/u/72977554?s=64&v=4) ](https://github.com/Cristhianzl)\n  * [ ![@autofix-ci\\[bot\\]](https://avatars.githubusercontent.com/in/243519?s=64&v=4) ](https://github.com/apps/autofix-ci)\n  * [ ![@igorrCarvalho](https://avatars.githubusercontent.com/u/64559670?s=64&v=4) ](https://github.com/igorrCarvalho)\n  * [ ![@mendonk](https://avatars.githubusercontent.com/u/59585235?s=64&v=4) ](https://github.com/mendonk)\n  * [ ![@rodrigosnader](https://avatars.githubusercontent.com/u/12815734?s=64&v=4) ](https://github.com/rodrigosnader)\n  * [ ![@italojohnny](https://avatars.githubusercontent.com/u/7606021?s=64&v=4) ](https://github.com/italojohnny)\n  * [ ![@ibiscp](https://avatars.githubusercontent.com/u/19699606?s=64&v=4) ](https://github.com/ibiscp)\n  * [ ![@gustavoschaedler](https://avatars.githubusercontent.com/u/38400824?s=64&v=4) ](https://github.com/gustavoschaedler)\n  * [ ![@edwinjosechittilappilly](https://avatars.githubusercontent.com/u/15969583?s=64&v=4) ](https://github.com/edwinjosechittilappilly)\n  * [ ![@genome21](https://avatars.githubusercontent.com/u/1147229?s=64&v=4) ](https://github.com/genome21)\n  * [ ![@cbornet](https://avatars.githubusercontent.com/u/11633333?s=64&v=4) ](https://github.com/cbornet)\n\n\n[+ 250 contributors](https://github.com/langflow-ai/langflow/graphs/contributors)\n## Languages\n  * [ Python 41.8% ](https://github.com/langflow-ai/langflow/search?l=python)\n  * [ JavaScript 29.7% ](https://github.com/langflow-ai/langflow/search?l=javascript)\n  * [ TypeScript 27.2% ](https://github.com/langflow-ai/langflow/search?l=typescript)\n  * [ CSS 0.8% ](https://github.com/langflow-ai/langflow/search?l=css)\n  * [ Dockerfile 0.2% ](https://github.com/langflow-ai/langflow/search?l=dockerfile)\n  * [ Makefile 0.2% ](https://github.com/langflow-ai/langflow/search?l=makefile)\n  * Other 0.1%\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary\n\n**Langflow** is an open-source tool for building AI-powered agents and workflows, supporting major LLMs and vector databases. Key features include a visual builder, API deployment, multi-agent orchestration, and enterprise-ready security. Installation is via `pip` or `uv`. \n\n**Tags:** #GenerativeAI #LLMs #Multiagent #AI #Langflow\n```"
  },
  {
    "instruction": "By clicking **\u201cAccept All Cookies\u201d** , you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts. View our [Privacy Policy](https://www.featurestore.org/privacy-policy) for more information.\n[Accept](https://www.featurestore.org/)\n#### Feature Store Summit 2024\n#### [View Sessions](https://www.featurestore.org/feature-store-summit-2024)\n![](https://logicalclocks.matomo.cloud/matomo.php?idsite=7&rec=1)\n[![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61d5bd3ac0dfb6cabe8d5ebc_FS%20Logo%20.svg)](https://www.featurestore.org/)\n[**SERVERLESS ML**](https://serverless-ml.org?utm_source=backlink&utm_medium=fs-org)[**BENCHMARKS**](https://www.featurestore.org/benchmarks)[**BLOG**](https://medium.com/data-for-ai)\n**LEARN****VIDEOS**\n[**WHAT IS A FEATURE STORE**](https://www.featurestore.org/what-is-a-feature-store)[**MLOPS DICTIONARY**](https://www.hopsworks.ai/mlops-dictionary?utm_source=fs.org&utm_medium=web)[**MEETUPS**](https://www.meetup.com/Feature-Stores-for-ML-Global-Meetup-Group/)[**FEATURE STORES FOR ML**](https://docs.featurestore.org)\n**WATCH****VIDEOS**\n[**MEETUP RECORDINGS**](https://www.featurestore.org/feature-store-meeetup-videos)[**FEATURE STORE SUMMIT 2024**](https://www.featurestore.org/feature-store-summit-2024)[**FEATURE STORE SUMMIT 2023**](https://www.featurestore.org/feature-store-summit-2023)[**FEATURE STORE SUMMIT 2022**](https://www.featurestore.org/feature-store-summit-2022)[**FEATURE STORE SUMMIT 2021**](https://www.featurestore.org/feature-store-summit-videos)\n[ CONTRIBUTE](https://www.featurestore.org/content-submission)[JOIN OUR SLACK](https://featurestoreorg.slack.com/join/shared_invite/zt-ssh8dec1-IsvwLdTsRMssjT4~Ru2RKg#/shared-invite/email)\n[![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61d5bd3ac0dfb6cabe8d5ebc_FS%20Logo%20.svg)](https://www.featurestore.org/)[**SERVERLESS ML - COURSE**](https://www.serverless-ml.org/)[**BLOG**](https://medium.com/data-for-ai)[**WHAT IS A FEATURE STORE**](https://www.featurestore.org/what-is-a-feature-store)[**MLOPS DICTIONARY**](https://www.hopsworks.ai/mlops-dictionary?utm_source=fs.org&utm_medium=web)[**MEETUPS**](https://www.meetup.com/Feature-Stores-for-ML-Global-Meetup-Group/)[**FEATURE STORE FOR ML**](https://docs.featurestore.org/)[**MEETUP VIDEOS**](https://www.featurestore.org/feature-store-meeetup-videos)[**FEATURE STORE SUMMIT 2021**](https://www.featurestore.org/feature-store-summit-videos)[**FEATURE STORE SUMMIT 2022**](https://www.featurestore.org/feature-store-summit-2022)[**JOIN OUR SLACK**](https://featurestoreorg.slack.com/join/shared_invite/zt-ssh8dec1-IsvwLdTsRMssjT4~Ru2RKg#/shared-invite/email)\n# Feature Stores for ML\n[Offline API Benchmark](https://github.com/featurestoreorg/featurestore-benchmarks/tree/main/fs-offline-benchmark#read-pandas-dataframe-benchmarks)\n[Online API Benchmark](https://github.com/featurestoreorg/featurestore-benchmarks/tree/main/fs-online-benchmark#benchmark-results)\n[Feature Freshness Benchmark](https://github.com/featurestoreorg/featurestore-benchmarks/tree/main/fs-feature-freshness-benchmark#results)\n[![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61d2ce2b1859c4c1a864c85c_icons8-community-64%20\\(1\\).png)Join the community![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61d804fac45d4fbb359bf315_icons8-chevron-right-48%20\\(1\\).png)We are a global community of data science professionals, researchers and engineers that enjoy **sharing ideas and learning** from each other.](https://featurestoreorg.slack.com/join/shared_invite/zt-ssh8dec1-IsvwLdTsRMssjT4~Ru2RKg#/shared-invite/email)[![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61d2ccca60606072e4bfdb31_meetup_logo_pink.png)Meetups![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61d804fac45d4fbb359bf315_icons8-chevron-right-48%20\\(1\\).png)We organize monthly meetups with **industry experts** and **leading companies** in the Data and AI field. Visit our Meetup page to see any upcoming events. ](https://www.meetup.com/Feature-Stores-for-ML-Global-Meetup-Group/)\n# FEATURED BLOG POSTS\n[![Streamlining Machine Learning Development with a Feature Store](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/63a0154984d8d44e659e9b32_1_CVAUcmqXkgin1ZpBvS2zWQ.webp)Machine Learning Streamlining Machine Learning Development with a Feature StoreByJohn Thomas](https://medium.com/@endeavordata/streamlining-machine-learning-development-with-a-feature-store-680ee6d45c64)\n[![Feature pipelines and feature stores \u2014 deep dive into system engineering and analytical tradeoffs](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/6391a22eea7cecfc4b7c6f70_1_hvi8lbFVoRwvnSa3mwCteQ.webp)Feature StoreFeature pipelines and feature stores \u2014 deep dive into system engineering and analytical tradeoffsByAssaf Pinhasi](https://medium.com/data-for-ai/feature-pipelines-and-feature-stores-deep-dive-into-system-engineering-and-analytical-tradeoffs-3c208af5e05f)\n[![Make a prediction every day with Serverless Machine Learning](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/63847804a4c798f0b28dceac_Orchest.io%20blog.png)Feature StoreMake a prediction every day with Serverless Machine LearningByLex Avstreikh](https://medium.com/@magiclex/make-a-prediction-every-day-with-serverless-machine-learning-5c8764bb7acd)\nHave an interesting blog idea? \n[Contribute](https://www.featurestore.org/content-submission)\n## [Read ![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/658d7b2b18dfa9bfbf983394_icons8-mouse-click-128.png)THE Feature StoreAdvanced Guide 2025](https://www.hopsworks.ai/dictionary/feature-store)\n[In-house](https://www.featurestore.org/fs-categories/in-house)\n[Open source](https://www.featurestore.org/fs-categories/open-source)\n[On-Prem](https://www.featurestore.org/fs-categories/on-prem)\n[Vendor](https://www.featurestore.org/fs-categories/vendor)\n[![Hopsworks](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61a77bd7a2e4345dc9c999ba_Hopsworks%20Icon%20Green.png)Hopsworks](https://www.hopsworks.ai/)\nUpdated on:\nOctober 18, 2024\nThe first open-source Feature Store and the first with a DataFrame API. Most data sources (batch/streaming) supported. Ingest features using SQL, Spark, Python, Flink. The only feature store supporting stream processing for writes. Available as managed platform and on-premises.\n**Company** : \nHopsworks\n[Hopsworks Documentation](https://docs.hopsworks.ai/3.1/?utm_source=backlink&utm_medium=fs-org)\nVendor\nOpen source\nOn-Prem\n![](https://img.youtube.com/vi/V-m_o38Ryeg/sddefault.jpg)\n[![Vertex AI](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61a8de596a90df9a9d12aa38_Vertex%20AI%20logo.png)Vertex AI](https://cloud.google.com/vertex-ai)\nUpdated on:\nAugust 11, 2023\nA centralized repository for organizing, storing, and serving ML features on the GCP Vertex platform. Vertex AI Feature Store supports BigQuery, GCS as data sources. Separate ingestion jobs after feature engineering in BigQuery. Offline is BigQuery, Online BigTable.\n**Company** : \nGoogle\n[Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs/featurestore)\nVendor\n![](https://img.youtube.com/vi/gT4qqHMiEpA/sddefault.jpg)\n[![SageMaker](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2e60570b4d7e18d15d1db_Amazon%20Sagemaker%20logo.png)SageMaker](https://aws.amazon.com/sagemaker/feature-store/)\nUpdated on:\nAugust 11, 2023\nSagemaker Feature Store integrates with other AWS services like Redshift, S3 as data sources and Sagemaker serving. It has a feature registry UI in Sagemaker, and Python/SQL APIs. Online FS is Dynamo, offline parquet/S3. \n**Company** : \nAmazon\n[AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html)\nVendor\n![](https://img.youtube.com/vi/pgwLhy1SNq8/sddefault.jpg)\n[![Databricks](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2ddce51e6a994ed071090_databricks-logo.png)Databricks](https://databricks.com/)\nUpdated on:\nAugust 11, 2023\nA Feature Store built around Spark Dataframes. Supports Spark/SQL for feature engineering with a UI in Databricks. Online FS is AWS RDS/MYSQL/Aurora. Offline is Delta Lake.\n**Company** : \nDatabricks\n[Databricks Documentation ](https://databricks.com/documentation)\nVendor\n![](https://img.youtube.com/vi/UySbya9ceXg/sddefault.jpg)\n[![Chronon](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2e73207e162dd9e2b921e_airbnb-logo-266x300%402x.png)Chronon](https://www.featurestore.org/)\nUpdated on:\nOctober 18, 2024\nOne of the first feature stores from 2018, orginially called Zipline and part of the Bighead ML Platform. Feature engineering is using a DSL that includes point-in-time correct training set backfills, scheduled updates, feature visualizations and automatic data quality monitoring.\n**Company** : \nAirBnB\n[Zipline: Machine Learning Data Management Platform](https://databricks.com/session/zipline-airbnbs-machine-learning-data-management-platform)\nIn-house\n![](https://img.youtube.com/vi/flpeNeo4ERQ/sddefault.jpg)\n[![Michelangelo](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2e51e591a4aca80ba3d35_Uber%20Logo.png)Michelangelo](https://eng.uber.com/)\nUpdated on:\nOctober 18, 2024\nThe mother of feature stores. Michelangelo is an end-to-end ML platfom and Palette is the features store. Features are defined in a DSL that translates into Spark and Flink jobs. Online FS is Redis/Cassandra. Offline is Hive.\n**Company** : \nUber\n[Uber's Machine Learning Platform](https://eng.uber.com/michelangelo-machine-learning-platform/)\nIn-house\n![](https://img.youtube.com/vi/nsZRRhM-AT8/sddefault.jpg)\n[![FBLearner](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2df80d7c1010bd67efb95_Facebook%20Logo.png)FBLearner](https://engineering.fb.com/)\nUpdated on:\nAugust 11, 2023\nInternal end-to-end ML Facebook platform that includes a feature store. It provides innovative functionality, like automatic generation of UI experiences from pipeline definitions and automatic parallelization of Python code.\n**Company** : \nFacebook\n[FBLearner Flow: Facebook\u2019s AI backbone](https://engineering.fb.com/2016/05/09/core-data/introducing-fblearner-flow-facebook-s-ai-backbone/)\nIn-house\n![](https://img.youtube.com/vi/4COOe5L2Dk4/sddefault.jpg)\n[![Overton](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2e583d64180458eaf0f92_Apple_logo_black.svg)Overton](https://machinelearning.apple.com/)\nUpdated on:\nAugust 11, 2023\nInternal end-to-end ML platform at Apple. It automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. It has been used in production to support multiple applications in both near-real-time applications and back-of-house processing.\n**Company** : \nApple\n[Overton: A Data System for Monitoring and Improving Machine-Learned Products](https://machinelearning.apple.com/research/overton)\nIn-house\n![](https://img.youtube.com/vi/Z9yKLlgaEZk/sddefault.jpg)\n[![Featureform](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2e263d641804e05af009d_Feature%20form%20logo.jpg)Featureform](https://www.featureform.com/)\nUpdated on:\nAugust 11, 2023\nFeatureForm is a virtual feature store platfrom - you plug in your offline and online data stores. It supports Flink, Snowflake, Airflow Kafka, and other frameworks. \n**Company** : \nFeatureform\n[Featureform Documentation](https://docs.featureform.com/)\nVendor\nOn-Prem\nOpen source\n![](https://img.youtube.com/vi/2HHNMVIXyn8/sddefault.jpg)\n[![Twitter](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61a8e4eac150b5d82b8a3100_2021%20Twitter%20logo%20-%20blue.png)Twitter](https://cortex.twitter.com/)\nUpdated on:\nAugust 11, 2023\nTwitter's first feature store was a set of shared feature libraries and metadata. Since then, they moved to building their own feature store, which they did by customizin feast for GCP.\n**Company** : \nTwitter\n[Twitter Engineering: Cortex](https://blog.twitter.com/engineering/en_us/tags.eng--cortex)\nIn-house\n![](https://img.youtube.com/vi/BqgJ1XL6Gzs/sddefault.jpg)\n[![Feast](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2dfe005e11543325ba22b_Feast%20Logo.png)Feast](https://feast.dev/)\nUpdated on:\nAugust 11, 2023\nOriginally developed as an open-source feature store by Go-JEK, Feast has been taken on by Tecton to be a minimal, configurable feature store. You can connect in different online/offline data stores and it can run on any platform. Feature engineering is done outside of Feast.\n**Company** : \nLinux Foundation\n[Feast Documentation](https://docs.feast.dev/)\nOpen source\n![](https://img.youtube.com/vi/DaNv-Wf1MBA/sddefault.jpg)\n[![Tecton](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2e6cc068193a3a4488506_Tecton%20Logo%20Icon.png)Tecton](https://www.tecton.ai/)\nUpdated on:\nAugust 11, 2023\nTecton.ai is a managed feature store that uses PySpark or SQL (Databricks or EMR) or Snowflake to compute features and DynamoDB to serve online features. It provides a Python-based DSL for orchestration and feature transformations that are computed as a PySpark job. Available on AWS.\n**Company** : \nTecton\n[Tecton Documentation](https://docs.tecton.ai/v2/)\nVendor\n![](https://img.youtube.com/vi/ZXkra9jiCU0/sddefault.jpg)\n[![Jukebox](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2e402068193c9194877d5_Spotify%20Logo.png)Jukebox](https://engineering.atspotify.com/)\nUpdated on:\nAugust 11, 2023\nSpotify built their own ML platform that leverages TensorFlow Extended (TFX) and Kubeflow. They focus on designing and analyzing their ML experiments instead of building and maintaining their own infrastructure, resulting in faster time from prototyping to production. \n**Company** : \nSpotify\n[Spotify Engineering: Machine Learning Infrastructure Through TensorFlow Extended and Kubeflow](https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/)\nIn-house\n![](https://img.youtube.com/vi/qv2DtDvpWx8/sddefault.jpg)\n[![Intuit](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2e398f50b086c3dc25119_Intuit%20Logo.jpeg)Intuit](https://medium.com/intuit-engineering/tagged/machine-learning)\nUpdated on:\nAugust 11, 2023\nIntuit have built a feature store as part of their data science platform. It was developed for AWS and uses S3 and Dynamo as its offline/online feature serving layers.\n**Company** : \nIntuit\n[Intuit Engineering: A Data Journey](https://medium.com/intuit-engineering/the-intuit-data-journey-d50e644ed279)\nIn-house\n![](https://img.youtube.com/vi/pynwAwhpU9M/sddefault.jpg)\n[![Iguazio ](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2e302819891fea09c34e9_Iguazio%20Logo.png)Iguazio ](https://www.iguazio.com/)\nUpdated on:\nAugust 11, 2023\nA centralized and versioned feature storre built around their MLRun open-source MLOps orchestration framework for ML model management. Uses V3IO as it offline and online feature stores.\n**Company** : \nIguazio \n[Iguazio Documentation](https://www.iguazio.com/docs/latest-release/)\nVendor\nOn-Prem\nOpen source\n![](https://img.youtube.com/vi/jZWa8wKXk2c/sddefault.jpg)\n[![Abacus.ai ](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2dc8681989147c79c1209_Abacus%20AI%20logo.png)Abacus.ai ](https://abacus.ai/)\nUpdated on:\nAugust 11, 2023\nThe platform allows to build real-time machine and deep learning features, upload ipython notebooks, monitor model drift, and set up CI/CD for machine learning systems.\n**Company** : \nAbacus.ai \n[Abacus Publications](https://abacus.ai/publications)\nVendor\n![](https://img.youtube.com/vi/_4K4QDOEZmk/sddefault.jpg)\n[![Doordash](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2df178d4e2bd1b997a70b_doordash-logo.png)Doordash](https://doordash.engineering/)\nUpdated on:\nAugust 11, 2023\nA ML Platform with an effective online prediction ecosystem. It serves traffic on a large number of ML Models, including ensemble models, through their Sibyl Prediction Service.They extended Redis with sharding and compression to work as their online feature store.\n**Company** : \nDoordash\n[Doordash: Building a Gigascale ML Feature Store with Redis](https://doordash.engineering/2020/11/19/building-a-gigascale-ml-feature-store-with-redis/)\nIn-house\n![](https://img.youtube.com/vi/D7SH2NjGlJQ/sddefault.jpg)\n[![Salesforce](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2e760ef42084c48e7daa2_Salesforce%20Logo%20Icon.png)Salesforce](https://www.salesforce.com/eu/products/einstein/overview/)\nUpdated on:\nAugust 11, 2023\nML Lake is a shared service that provides the right data, optimizes the right access patterns, and alleviates the machine learning application developer from having to manage data pipelines, storage, security and compliance. Built on an early version of Feast based around Spark.\n**Company** : \nSalesforce\n[Salesforce: AI Technology and Resources](https://www.salesforce.com/eu/resources/artificial-intelligence/#!page=1)\nIn-house\n![](https://img.youtube.com/vi/eHdiWzPUt3k/sddefault.jpg)\n[![H2O Feature Store](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/61d2e296297bb33f4315a304_H2O_logo_from_H2O.ai.png)H2O Feature Store](https://www.h2o.ai/feature-store/)\nUpdated on:\nOctober 3, 2023\nH2O.ai and AT&T co-created the H2O AI Feature Store to store, update, and share the features data scientists, developers, and engineers need to build AI models. \n**Company** : \nH2O.ai and AT&T \n[H2O.ai Documentation](https://docs.h2o.ai/featurestore/)\nVendor\nOn-Prem\n![](https://img.youtube.com/vi/y6CP0oZpm4k/sddefault.jpg)\n[![Feathr](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/625e612fac74bb28d7712f61_LinkedIn_logo_initials.png)Feathr](https://github.com/linkedin/feathr)\nUpdated on:\nAugust 11, 2023\nFeathr automatically computes your feature values and joins them to your training data, using point-in-time-correct semantics to avoid data leakage, and supports materializing and deploying your features for use online in production.\n**Company** : \nMicrosoft / Linkedin\n[Feathr's Documentation](https://engineering.linkedin.com/blog/2022/open-sourcing-feathr---linkedin-s-feature-store-for-productive-m)\nOpen source\n![](https://img.youtube.com/vi/u8nLY9Savxk/sddefault.jpg)\n[![Nexus](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/63514b9bfbcf94d6066b945c_Disney%20Logo%20Icon.png)Nexus](https://www.featurestore.org/)\nUpdated on:\nAugust 11, 2023\nNexus supports batch, near real-time, and real-time feature computation and has global scale for serving online and offline features from Redis and Delta Lake-s3, respectively.\n**Company** : \nDisney Streaming\n[Nexus Feature Store Details](https://content.hopsworks.ai/hubfs/Feature%20Store%20Summit%202022/FS%20Summit%2022%20-%20Disney%20Streaming.pdf)\nIn-house\n![](https://img.youtube.com/vi/OXQW1s2qTwM/sddefault.jpg)\n[![Qwak](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/64d5f55dc1bc4d6e5bd9616f_qwak%20logo.png)Qwak](https://www.qwak.com/)\nUpdated on:\nAugust 11, 2023\nQwak's feature store is a component of the Qwak ML platform, providing transformations, a feature registry and both an online and offline store. \n**Company** : \nQwak\n[Qwak Product Details](https://www.qwak.com/product/feature-store)\nVendor\n![](https://img.youtube.com/vi/a2Cg2EYiDMs/sddefault.jpg)\n[![Beast](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/64d5f6c6c697c456bba9eb4d_Robinhood-logo.png)Beast](https://robinhood.com/us/en/)\nUpdated on:\nAugust 11, 2023\nRobinhood built their own event-based real-time feature store based on Kafka and Flink.\n**Company** : \nRobinhood\n[Robinhood Engineering Blog](https://learn.robinhood.com/)\nIn-house\n![](https://img.youtube.com/vi/qDtdYo-dVXk/sddefault.jpg)\n[![Feature Byte](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/64d5fa5862dd96d36f5ecdef_FeatureByte-Logo.png)Feature Byte](https://featurebyte.com/)\nUpdated on:\nAugust 11, 2023\nFeatureByte is a solution that aims to simplify the process of preparing and managing data for AI models, making it easier for organizations to scale their AI efforts. \n**Company** : \nFeature Byte\n[Feature Byte Documentation](https://docs.featurebyte.com/0.2/about/workflow/)\nVendor\n![](https://img.youtube.com/vi/jGkRsmt8uvg/sddefault.jpg)\n[![Fennel](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/64d5fcbdc552c55c6ddf2752_Purple%20\\(1\\).png)Fennel](https://fennel.ai/)\nUpdated on:\nOctober 18, 2024\nFennel is a fully managed realtime feature platform from an-Facebook team. Powered by Rust, it is built ground up to be easy to use. It works natively with Python/Pandas, has beautiful APIs, installs in your cloud in minutes, and has best-in-class support for data/feature quality. \n**Company** : \nFennel \n[Fennel Documentation](https://fennel.ai/docs)\nVendor\n![](https://img.youtube.com/vi/RoVSokGcY8E/sddefault.jpg)\n[![OpenMLDB](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/650c3e60c5355266f4e45bb9_open%20ml%20db%20.png)OpenMLDB](https://openmldb.com/en/)\nUpdated on:\nSeptember 21, 2023\nAn open-source feature computing platform that offers unified SQL APIs and a shared execution plan generator for both offline and online engines, eliminating the need for cumbersome transformation and consistency verification.\n**Company** : \n4Paradigm\n[Documentation](https://openmldb.ai/docs/en/main/)\nOpen source\n![](https://img.youtube.com/vi/YWdjtVLxr-Y/sddefault.jpg)\n[![Chalk](https://cdn.prod.website-files.com/618ceae2a430c960c6f6b19a/67123931b7602710283ea79a_chalk%20full%20logo.png)Chalk](https://chalk.ai/)\nUpdated on:\nOctober 18, 2024\nChalk is a platform for building real-time ML applications that includes a real-time feature store.\n**Company** : \nChalk AI\n[Chalk Documentation](https://docs.chalk.ai/docs/what-is-chalk)\nVendor\n![](https://img.youtube.com/vi/XxjPjlUZ2k4/sddefault.jpg)\n# FEATURE STORE COMPARISON\n[Get Listed](https://hdx6m0gtea7.typeform.com/to/xnljrxsh)\n## Platform\n## Open Source\n## Offline \n## Online\n## **Real Time Ingestion**\n## **Feature Ingestion API**\n## **Write Amplification**\n## **Supported Platforms**\n## **Training API**\n## **Training Data**\n## Hopsworks\nAGPL-V3\nHudi/Hive and pluggable\nRonDB\n[Flink, Spark Streaming](https://www.featurestore.org/)\n[(Py)Spark, Python, SQL, Flink](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Hopsworks%20-%20Feature%20Store%20Summit.pdf)\nNo\nAWS, GCP, On-Prem\nSpark\nDataFrame (Spark or Pandas), files (.csv, .tfrecord, etc)\n## Michelangelo Palette\nNo\nHive\nCassandra\n[Flink, Spark Streaming](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Uber%20-%20Feature%20Store%20Summit.pdf)\n[Spark, DSL](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Uber%20-%20Feature%20Store%20Summit.pdf)\nNone\nProprietary\nSpark\nDataFrame (Pandas)\n## Chronon\nNo\nHive\nUnknown KV Store\n[Flink](https://www.featurestore.org/)\n[DSL](https://www.slideshare.net/databricks/ziplineairbnbs-declarative-feature-engineering-framework)\nNone\nProprietary\nSpark\nStreamed to models?\n## Twitter\nNo\nGCS\nManhatten, Cockroach\n[Unknown](https://www.featurestore.org/)\n[Python, BigQuery](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Twitter%20-%20Feature%20Store%20Summit.pdf)\nYes. Ingestion Jobs\nProprietary\nBigQuery \nDataFrame (Pandas)\n## Iguazio\nNo\nParquet\nV3IO, proprietary DB\n[Nuclio](https://www.featurestore.org/)\n[Spark, Python, Nuclio](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Twitter%20-%20Feature%20Store%20Summit.pdf)\nUnknown\nAWS, Azure, GCP, on-prem\nNo details\nDataFrame (Pandas)\n## Databricks\nNo\nDelta Lake\nMysql or Aurora\n[None](https://www.featurestore.org/)\n[Spark, SparkSQL](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Twitter%20-%20Feature%20Store%20Summit.pdf)\nUnknown\nUnknown\nSpark\nSpark Dataframes\n## SageMaker\nNo\nS3, Parquet\nDynamoDB\n[None](https://www.featurestore.org/)\n[Python](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/AWS%20-%20Feature%20Store%20Summit.pdf)\nYes. Ingestion Jobs\nAWS\nAurora\nDataFrame (Pandas)\n## Featureform\nMozilla\nPluggable\nPluggable\n[None](https://www.featurestore.org/)\n[Spark](https://www.featurestore.org/)\nNo\nAWS, Azure, GCP\nSpark\nDataFrame (Spark, Pandas)\n## Jukebox\nNo\nBigQuery\nBigTable\n[Scio](https://www.featurestore.org/)\n[Scio, BigQuery](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Spotify%20-%20Feature%20Store%20Summit.pdf)\nYes. Ingestion Jobs\nProprietary\nBigQuery \nDataFrame (Pandas)\n## Doordash\nNo \nSnowflake\nRedis\n[Flink](https://www.featurestore.org/)\n[Flink](https://doordash.engineering/2021/03/04/building-a-declarative-real-time-feature-engineering-framework/)\nUnknown\nProprietary\nSnowflake\nDataFrame (Pandas)\n## Salesforce\nNo\nS3, Iceberg\nDynamoDB\n[Unknown](https://www.featurestore.org/)\n[Unknown](https://www.featurestore.org/)\nYes. Ingestion Jobs\nProprietary\nIceberg\nDataFrame (Pandas)\n## Intuit\nNo\nS3\nGraphQL API, unknown backend\n[Beam](https://www.featurestore.org/)\n[Spark, Beam](https://www.usenix.org/sites/default/files/conference/protected-files/opml20_talks_45_slides_canchi.pdf)\nUnknown\nProprietary\nUnknown\nDataFrame (Pandas)\n## OLX\nNo\nKafka\nKafka\n[KSQLdb](https://www.featurestore.org/)\n[KSQLdb](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/OLX%20-%20Feature%20Store%20Summit.pdf)\nNo \nProprietary\nKSQLdb\nFrom feature logging\n## Continual\nNo\nSnowflake\nComing soon\n[Coming soon](https://www.featurestore.org/)\n[DBT](https://www.getdbt.com/)\nNo\nSnowflake, more coming\nSnowflake\nProprietary\n## Metarank\nYes\nN/A\nRedis\n[Flink](https://www.featurestore.org/)\n[YAML-based](https://content.hopsworks.ai/hubfs/Meetups/FS%20Meetup%2012%20-%20Findify.pdf)\nNo\nOpen-Source\nXGBoost, LightGBM\nCSV files?\n## Scribble Enrich \nNo\nPluggable\nPluggable\n[None](https://www.featurestore.org/)\n[Python](https://www.featurestore.org/)\nNo\nAWS, Azure, GCP, on-prem\nNo details\nDataFrame (Pandas)\n## Feathr\nYes\nPluggable\nPluggable\n[None](https://www.featurestore.org/)\n[Spark](https://www.featurestore.org/)\nNo\nAzure, AWS\nSpark\nDataFrames, files (.csv, .parquet, etc)\n## Nexus\nNo\nDelta Lake\nRedis\n[Spark Streaming](https://www.featurestore.org/)\n[Spark](https://www.featurestore.org/)\nUnknown\nUnknown\nSpark\nProprietary\n**PLATFORM - CATEGORIES**\n**DETAILS**\n# Hopsworks\n## Open - Source\nAGPL-V3\n## Feature Ingestion API\n[(Py)Spark, Python, SQL, Flink](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Hopsworks%20-%20Feature%20Store%20Summit.pdf)\n## Supported Platforms\nAWS, GCP, On-Prem\n# Michelangelo Palette\n## Open - Source\nNo\n## Feature Ingestion API\n[Spark, DSL](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Uber%20-%20Feature%20Store%20Summit.pdf)\n## Supported Platforms\nProprietary\n# Chronon\n## Open - Source\nNo\n## Feature Ingestion API\n[DSL](https://www.slideshare.net/databricks/ziplineairbnbs-declarative-feature-engineering-framework)\n## Supported Platforms\nProprietary\n# Twitter\n## Open - Source\nNo\n## Feature Ingestion API\n[Python, BigQuery](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Twitter%20-%20Feature%20Store%20Summit.pdf)\n## Supported Platforms\nProprietary\n# Iguazio\n## Open - Source\nNo\n## Feature Ingestion API\n[Spark, Python, Nuclio](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Twitter%20-%20Feature%20Store%20Summit.pdf)\n## Supported Platforms\nAWS, Azure, GCP, on-prem\n# Databricks\n## Open - Source\nNo\n## Feature Ingestion API\n[Spark, SparkSQL](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Twitter%20-%20Feature%20Store%20Summit.pdf)\n## Supported Platforms\nUnknown\n# SageMaker\n## Open - Source\nNo\n## Feature Ingestion API\n[Python](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/AWS%20-%20Feature%20Store%20Summit.pdf)\n## Supported Platforms\nAWS\n# Featureform\n## Open - Source\nMozilla\n## Feature Ingestion API\n[Spark](https://www.featurestore.org/)\n## Supported Platforms\nAWS, Azure, GCP\n# Jukebox\n## Open - Source\nNo\n## Feature Ingestion API\n[Scio, BigQuery](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/Spotify%20-%20Feature%20Store%20Summit.pdf)\n## Supported Platforms\nProprietary\n# Doordash\n## Open - Source\nNo \n## Feature Ingestion API\n[Flink](https://doordash.engineering/2021/03/04/building-a-declarative-real-time-feature-engineering-framework/)\n## Supported Platforms\nProprietary\n# Salesforce\n## Open - Source\nNo\n## Feature Ingestion API\n[Unknown](https://www.featurestore.org/)\n## Supported Platforms\nProprietary\n# Intuit\n## Open - Source\nNo\n## Feature Ingestion API\n[Spark, Beam](https://www.usenix.org/sites/default/files/conference/protected-files/opml20_talks_45_slides_canchi.pdf)\n## Supported Platforms\nProprietary\n# OLX\n## Open - Source\nNo\n## Feature Ingestion API\n[KSQLdb](https://content.logicalclocks.com/hubfs/Feature%20Store%20Summit%20Presentations%20PDFs/OLX%20-%20Feature%20Store%20Summit.pdf)\n## Supported Platforms\nProprietary\n# Continual\n## Open - Source\nNo\n## Feature Ingestion API\n[DBT](https://www.getdbt.com/)\n## Supported Platforms\nSnowflake, more coming\n# Metarank\n## Open - Source\nYes\n## Feature Ingestion API\n[YAML-based](https://content.hopsworks.ai/hubfs/Meetups/FS%20Meetup%2012%20-%20Findify.pdf)\n## Supported Platforms\nOpen-Source\n# Scribble Enrich \n## Open - Source\nNo\n## Feature Ingestion API\n[Python](https://www.featurestore.org/)\n## Supported Platforms\nAWS, Azure, GCP, on-prem\n# Feathr\n## Open - Source\nYes\n## Feature Ingestion API\n[Spark](https://www.featurestore.org/)\n## Supported Platforms\nAzure, AWS\n# Nexus\n## Open - Source\nNo\n## Feature Ingestion API\n[Spark](https://www.featurestore.org/)\n## Supported Platforms\nUnknown\n![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61d31926c8bd4184d185548d_tool.svg)\n**Feature Ingestion API:** What APIs and languages are supported for writing features to the feature store?\n![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61d31926c8bd4184d185548d_tool.svg)\n**Write Amplification:** Do you write your features more than once - .e.g, write to stable storage first, then run a separate job to ingest features?\n![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61d31926c8bd4184d185548d_tool.svg)\n**Training API (PIT Join Engine):** When you create training data from reusable features, you need to join the feature values together. What compute engine is used to perform this point-in-time JOIN?\n![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61d31926c8bd4184d185548d_tool.svg)\n**Training Data:** How is the training data made available to machine learning frameworks? As dataframes or files?\n# Build or Buy\nDeveloping a software can be extremely **costly** and **time-consuming** so reusability of different systems proves to be a reasonable solution, however the number of companies **building their own feature store** is on the rise.Learn more about the industry's conundrum by watching the relevant panel discussion from the first **Feature Store Summit.**\n# Build or Buy\nDeveloping a software can be extremely **costly** and **time-consuming** so reusability of different systems proves to be a reasonable solution, however the number of companies **building their own feature store** is on the rise.Learn more about the industry's conundrum by watching the relevant panel discussion from the latest **Feature Store Summit.**\n## DISCOVER\n  * [What is a Feature Store](https://www.featurestore.org/what-is-a-feature-store)\n  * [Contribute](https://www.featurestore.org/content-submission)[Get Listed](https://hdx6m0gtea7.typeform.com/to/xnljrxsh)\n\n\n## RESOURCES\n  * [Knowledge](https://docs.featurestore.org/)\n  * [Blog](https://medium.com/data-for-ai)\n  * [Meetups](https://www.meetup.com/Feature-Stores-for-ML-Global-Meetup-Group/)\n  * [Feature Store Summit](https://www.featurestoresummit.com/)\n\n\n## GENERAL\n  * [Privacy Policy](https://www.featurestore.org/privacy-policy)\n  * [Code of Conduct](https://www.featurestore.org/code-of-conduct)\n\n\n## SUBSCRIBE TO NEWSLETTER\nThank you! Your submission has been received!\nOops! Something went wrong while submitting the form.\n[![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61ae37b708781a1657d82c1f_Slack.png)](https://featurestoreorg.slack.com/ssb/redirect#/shared-invite/email)[![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61ae2f7c6227c564ffa7036a_Linkedin%20logo%20black%20icon.png)](https://www.linkedin.com/company/75052617/admin/)[![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61ae35d0e75563a8a412f290_Twitter..png)](https://twitter.com/FeaturestoreOrg)[![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61ae357353bcca6d7da0d2b8_YouTube.png)](https://www.youtube.com/@featurestoreorg)[![](https://cdn.prod.website-files.com/5f32a0dcc815d2a49c58481a/61af6210e6e13cd78a241977_meetup.png)](https://www.meetup.com/Feature-Stores-for-ML-Global-Meetup-Group/)\nPowered by [Hopsworks](https://www.hopsworks.ai/?utm_source=fs.org&utm_medium=website&utm_campaign=clicks&utm_id=traffic) \u00a9 2025 Featurestore.org. All rights reserved.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe **Feature Store Summit 2024** highlights advancements in feature stores for ML, showcasing various platforms like **Hopsworks**, **Google's Vertex AI**, and **Amazon's SageMaker**. Key insights include the rise of in-house feature stores, the importance of real-time ingestion, and the need for effective feature management to enhance ML workflows.\n```"
  },
  {
    "instruction": "[Chip Huyen](https://huyenchip.com/)\n[Blog](https://huyenchip.com/blog/) [Books](https://huyenchip.com/books/) [Events](https://huyenchip.com/events/)\nAI Guide\n  * [AI Roadmap](https://huyenchip.com/mlops/)\n  * [Llama Police](https://huyenchip.com/llama-police)\n  * [ML Interviews](https://huyenchip.com/ml-interviews-book/)\n\n\n[List 100](https://huyenchip.com/list-100/) [VN](https://huyenchip.com/vn/)\n[ ![Chip Huyen](https://huyenchip.com/assets/profile.jpeg) ](https://www.instagram.com/huyenchip19/)\nI\u2019m Chip Huyen, a writer and computer scientist. I grew up chasing grasshoppers in a small rice-farming village in Vietnam.\nMy focus is on ML/AI systems in production. I\u2019m currently exploring creative use cases of AI in education and entertainment.\nPreviously, I worked on ML tooling at NVIDIA (core dev of [**NeMo**](https://github.com/NVIDIA/NeMo)), Snorkel AI, and Netflix. I also founded and sold an AI infra startup.\nRunning a startup taught me that being a founder is incredibly hard, so I try to support founders in any way I can. Some startups I\u2019ve worked with included [Convai](https://convai.com/), [OctoAI](https://octo.ai/), and [Photoroom](https://www.photoroom.com/).\nI graduated from Stanford, where I taught [ML Systems](https://cs329s.stanford.edu). The lectures became the foundation for the book [**Designing Machine Learning Systems**](https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969?&_encoding=UTF8&tag=chiphuyen-20&linkCode=ur2&linkId=0a1dbab0e76f5996e29e1a97d45f14a5&camp=1789&creative=9325), which is an Amazon #1 bestseller in AI and has been translated into 10+ languages (very proud)!\nMy new book **AI Engineering** (2025) is currently the most read book on the [O\u2019Reilly](https://oreillymedia.pxf.io/c/5719111/2146021/15173) platform. It\u2019s also available on [Amazon](https://amzn.to/49j1cGS) and [Kindle](https://amzn.to/3Vq2ryu).\nI\u2019m active on [GitHub](https://github.com/chiphuyen) and aspire to become a [Goodreads](https://www.goodreads.com/chiphuyen) influencer. You can also find me on [Google Scholar](https://scholar.google.com/citations?user=pwg2DGwAAAAJ&hl=en). I frequently speak at and occasionally host [events](https://huyenchip.com/events).\nI enjoy learning about fun technical challenges and collaborating with great teams. [Reach out](https://huyenchip.com/communication) if you want to find a way to work together!\n  * hi@[thiswebsite]\n  * [chiphuyen](https://github.com/chiphuyen)\n  * [chipro](https://twitter.com/chipro)\n  * [chipiscrazy](https://facebook.com/chipiscrazy)\n  * [huyenchip19](https://instagram.com/huyenchip19)\n  * [chiphuyen](https://www.linkedin.com/in/chiphuyen)\n\n\nI help companies deploy machine learning into production. I write about AI applications, tooling, and best practices. \n![](https://huyenchip.com/)\n",
    "answer": "```markdown\n## TL;DR Summary\n\nChip Huyen is a writer and computer scientist focused on ML/AI systems in production, with experience at NVIDIA, Snorkel AI, and Netflix. He authored the bestselling book **Designing Machine Learning Systems** and is working on **AI Engineering** (2025). Huyen supports startups and shares insights on AI applications and tooling.\n```"
  },
  {
    "instruction": "[Skip to content](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#wp--skip-link--target)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nThe world\u2019s leading publication for data science, AI, and ML professionals.\nSign in\nSign out\n[Contributor Portal](https://contributor.insightmediagroup.io/)\n  * [Latest](https://towardsdatascience.com/latest/)\n  * [Editor\u2019s Picks](https://towardsdatascience.com/tag/editors-pick/)\n  * [Deep Dives](https://towardsdatascience.com/tag/deep-dives/)\n  * [Contribute](https://towardsdatascience.com/questions-96667b06af5/)\n  * [Newsletter](https://newsletter.towardsdatascience.com/subscription-to-the-newsletter)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\n\n\nToggle Mobile Navigation\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [X](https://x.com/TDataScience)\n\n\nToggle Search\nSearch\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\n# A Guide on 12 Tuning Strategies for Production-Ready RAG Applications\nHow to improve the performance of your Retrieval-Augmented Generation (RAG) pipeline with these \"hyperparameters\" and tuning strategies \n[Leonie Monigatti](https://towardsdatascience.com/author/iamleonie/)\nDec 6, 2023\n12 min read\nShare \n![Tuning Strategies for Retrieval-Augmented Generation Applications](https://towardsdatascience.com/wp-content/uploads/2023/12/114bRGUWZojH8Y1x12kA2IQ.png)Tuning Strategies for Retrieval-Augmented Generation Applications\nData Science is an experimental science. It starts with the \"No Free Lunch Theorem,\" which states that there is no one-size-fits-all algorithm that works best for every problem. And it results in data scientists using [experiment tracking systems](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133) to help them [tune the hyperparameters of their Machine Learning (ML) projects to achieve the best performance](https://medium.com/towards-data-science/intermediate-deep-learning-with-transfer-learning-f1aba5a814f).\nThis article looks at a [Retrieval-Augmented Generation (RAG) pipeline](https://medium.com/towards-data-science/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2) through the eyes of a data scientist. It discusses potential \"hyperparameters\" you can experiment with to improve your RAG pipeline\u2019s performance. Similar to experimentation in Deep Learning, where, e.g., data augmentation techniques are not a hyperparameter but a knob you can tune and experiment with, this article will also cover different strategies you can apply, which are not per se hyperparameters.\n> [**Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation**](https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2)\nThis article covers the following \"hyperparameters\" sorted by their relevant stage. In the [ingestion stage](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#4142) of a RAG pipeline, you can achieve performance improvements by:\n  * [Data cleaning](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#196c)\n  * [Chunking](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#e45f)\n  * [Embedding models](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#156e)\n  * [Metadata](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#2b47)\n  * [Multi-indexing](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#ce6c)\n  * [Indexing algorithms](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#4daa)\n\n\nAnd in the [inferencing stage (retrieval and generation)](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#ac53), you can tune:\n  * [Query transformations](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#a5e2)\n  * [Retrieval parameters](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#fa73)\n  * [Advanced retrieval strategies](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#a3bb)\n  * [Re-ranking models](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#341d)\n  * [LLMs](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#e9f9)\n  * [Prompt engineering](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#9c1c)\n\n\nNote that this article covers text-use cases of RAG. For multimodal RAG applications, different considerations may apply.\n## Ingestion Stage\nThe ingestion stage is a preparation step for building a RAG pipeline, similar to the data cleaning and preprocessing steps in an ML pipeline. Usually, the ingestion stage consists of the following steps:\n  1. Collect data\n  2. Chunk data\n  3. Generate vector embeddings of chunks\n  4. Store vector embeddings and chunks in a vector database\n\n![Ingestion stage of a RAG pipeline](https://towardsdatascience.com/wp-content/uploads/2023/12/14WQ2HYCrZO8VwOXeugeGAg.png)Ingestion stage of a RAG pipeline\nThis section discusses impactful techniques and hyperparameters that you can apply and tune to improve the relevance of the retrieved contexts in the inferencing stage.\n### Data cleaning\nLike any Data Science pipeline, the quality of your data heavily impacts the outcome in your RAG pipeline [8, 9]. Before moving on to any of the following steps, ensure that your data meets the following criteria:\n  * **Clean** : Apply at least some basic data cleaning techniques commonly used in Natural Language Processing, such as making sure all special characters are encoded correctly.\n  * **Correct** : Make sure your information is consistent and factually accurate to avoid conflicting information confusing your LLM.\n\n\n### Chunking\nChunking your documents is an essential preparation step for your external knowledge source in a RAG pipeline that can impact the performance [1, 8, 9]. It is a technique to generate logically coherent snippets of information, usually by breaking up long documents into smaller sections (but it can also combine smaller snippets into coherent paragraphs).\nOne consideration you need to make is the **choice of the chunking technique**. For example, in [LangChain, different text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) split up documents by different logics, such as by characters, tokens, etc. This depends on the type of data you have. For example, you will need to use different chunking techniques if your input data is code vs. if it is a Markdown file.\nThe ideal **length of your chunk (`chunk_size`)** depends on your use case: If your use case is question answering, you may need shorter specific chunks, but if your use case is summarization, you may need longer chunks. Additionally, if a chunk is too short, it might not contain enough context. On the other hand, if a chunk is too long, it might contain too much irrelevant information.\nAdditionally, you will need to think about a **\"rolling window\" between chunks (`overlap`)** to introduce some additional context.\n### Embedding models\nEmbedding models are at the core of your retrieval. The **quality of your embeddings** heavily impacts your retrieval results [1, 4]. Usually, the higher the dimensionality of the generated embeddings, the higher the precision of your embeddings.\nFor an idea of what alternative embedding models are available, you can look at the [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard), which covers 164 text embedding models (at the time of this writing).\n> [**MTEB Leaderboard \u2013 a Hugging Face Space by mteb**](https://huggingface.co/spaces/mteb/leaderboard)\nWhile you can use general-purpose embedding models out-of-the-box, it may make sense to **fine-tune your embedding model** to your specific use case in some cases to avoid out-of-domain issues later on [9]. According to experiments conducted by LlamaIndex, fine-tuning your embedding model can lead to a [5\u201310% performance increase in retrieval evaluation metrics](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb) [2].\nNote that you cannot fine-tune all embedding models (e.g., OpenAI\u2019s `text-ebmedding-ada-002` can\u2019t be fine-tuned at the moment).\n### Metadata\nWhen you store vector embeddings in a [vector database](https://medium.com/towards-data-science/explaining-vector-databases-in-3-levels-of-difficulty-fc392e48ab78), some vector databases let you store them together with metadata (or data that is not vectorized). **Annotating vector embeddings with metadata** can be helpful for additional post-processing of the search results, such as **metadata filtering** [1, 3, 8, 9]. For example, you could add metadata, such as the date, chapter, or subchapter reference.\n### Multi-indexing\nIf the metadata is not sufficient enough to provide additional information to separate different types of context logically, you may want to **experiment with multiple indexes** [1, 9]. For example, you can use different indexes for different types of documents. Note that you will have to incorporate some index routing at retrieval time [1, 9]. If you are interested in a deeper dive into metadata and separate collections, you might want to learn more about the concept of [native multi-tenancy](https://www.youtube.com/watch?v=KT2RFMTJKGs).\n### Indexing algorithms\nTo enable lightning-fast similarity search at scale, vector databases and vector indexing libraries use an Approximate Nearest Neighbor (ANN) search instead of a k-nearest neighbor (kNN) search. As the name suggests, ANN algorithms approximate the nearest neighbors and thus can be less precise than a kNN algorithm.\nThere are **different ANN algorithms** you could experiment with, such as [Facebook Faiss](https://github.com/facebookresearch/faiss) (clustering), [Spotify Annoy](https://github.com/spotify/annoy) (trees), [Google ScaNN](https://github.com/google-research/google-research/tree/master/scann) (vector compression), and [HNSWLIB](https://github.com/nmslib/hnswlib) (proximity graphs). Also, many of these ANN algorithms have some parameters you could tune, such as `ef`, `efConstruction`, and `maxConnections` for HNSW [1].\nAdditionally, you can enable vector compression for these indexing algorithms. Analogous to ANN algorithms, you will lose some precision with vector compression. However, depending on the choice of the vector compression algorithm and its tuning, you can optimize this as well.\nHowever, in practice, these parameters are already tuned by research teams of vector databases and vector indexing libraries during benchmarking experiments and not by developers of RAG systems. However, if you want to experiment with these parameters to squeeze out the last bits of performance, I recommend this article as a starting point:\n> [**An Overview on RAG Evaluation | Weaviate \u2013 vector database**](https://weaviate.io/blog/rag-evaluation#indexing-knobs)\n## Inferencing Stage (Retrieval & Generation)\nThe main components of the RAG pipeline are the retrieval and the generative components. This section mainly discusses strategies to improve the retrieval ([Query transformations](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#a5e2), [retrieval parameters](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#fa73), [advanced retrieval strategies](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#a3bb), and [re-ranking models](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#341d)) as this is the more impactful component of the two. But it also briefly touches on some strategies to improve the generation ([LLM](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#e9f9) and [prompt engineering](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#9c1c)).\n![Inference stage of a RAG pipeline](https://towardsdatascience.com/wp-content/uploads/2023/12/1tT14GpYfEMSqCjnt2UQOGQ.png)Inference stage of a RAG pipeline\n### Query transformations\nSince the search query to retrieve additional context in a RAG pipeline is also embedded into the vector space, its phrasing can also impact the search results. Thus, if your search query doesn\u2019t result in satisfactory search results, you can experiment with various [query transformation techniques](https://gpt-index.readthedocs.io/en/v0.6.9/how_to/query/query_transformations.html) [5, 8, 9], such as:\n  * **Rephrasing:** Use an LLM to rephrase the query and try again.\n  * **Hypothetical Document Embeddings (HyDE):** Use an LLM to generate a hypothetical response to the search query and use both for retrieval.\n  * **Sub-queries:** Break down longer queries into multiple shorter queries.\n\n\n### Retrieval parameters\nThe retrieval is an essential component of the RAG pipeline. The first consideration is whether semantic search will be sufficient for your use case or if you want to experiment with hybrid search.\nIn the latter case, you need to experiment with weighting the aggregation of sparse and dense retrieval methods in hybrid search [1, 4, 9]. Thus, tuning the parameter **`alpha`, which controls the weighting between semantic (`alpha = 1`) and keyword-based search (`alpha = 0`)**, will become necessary.\n> [**Improving Retrieval Performance in RAG Pipelines with Hybrid Search**](https://towardsdatascience.com/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)\nAlso, the **number of search results to retrieve** will play an essential role. The number of retrieved contexts will impact the length of the used context window (see [Prompt Engineering](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#9c1c)). Also, if you are using a re-ranking model, you need to consider how many contexts to input to the model (see [Re-ranking models](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#341d)).\nNote, while the used similarity measure for semantic search is a parameter you can change, you should not experiment with it but instead set it according to the used embedding model (e.g., `[text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)` supports cosine similarity or `[multi-qa-MiniLM-l6-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1#technical-details)` supports cosine similarity, dot product, and Euclidean distance).\n### Advanced retrieval strategies\nThis section could technically be its own article. For this overview, we will keep this as concise as possible. For an in-depth explanation of the following techniques, I recommend this DeepLearning.AI course:\n> [**Building and Evaluating Advanced RAG Applications**](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)\nThe underlying idea of this section is that the chunks for retrieval shouldn\u2019t necessarily be the same chunks used for the generation. Ideally, you would embed smaller chunks for retrieval (see [Chunking](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#e45f)) but retrieve bigger contexts. [7]\n  * **Sentence-window retrieval:** Do not just retrieve the relevant sentence, but the window of appropriate sentences before and after the retrieved one.\n  * **Auto-merging retrieval:** The documents are organized in a tree-like structure. At query time, separate but related, smaller chunks can be consolidated into a larger context.\n\n\n### Re-ranking models\nWhile semantic search retrieves context based on its semantic similarity to the search query, \"most similar\" doesn\u2019t necessarily mean \"most relevant\". **Re-ranking models** , such as [Cohere\u2019s Rerank](https://cohere.com/rerank?ref=txt.cohere.com&__hstc=14363112.8fc20f6b1a1ad8c0f80dcfed3741d271.1697800567394.1701091033915.1701173515537.7&__hssc=14363112.1.1701173515537&__hsfp=3638092843) model, can help eliminate irrelevant search results by computing a score for the relevance of the query for each retrieved context [1, 9].\n> \"most similar\" doesn\u2019t necessarily mean \"most relevant\"\nIf you are using a re-ranker model, you may need to re-tune the **number of search results** for the input of the re-ranker and how many of the reranked results you want to feed into the LLM.\nAs with the [embedding models](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#156e), you may want to experiment with **fine-tuning the re-ranker** to your specific use case.\n### LLMs\nThe **LLM is the core component** for generating the response. Similarly to the embedding models, there is a wide range of LLMs you can choose from depending on your requirements, such as open vs. proprietary models, inferencing costs, context length, etc. [1]\nAs with the [embedding models](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#156e) or [re-ranking models](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#341d), you may want to experiment with **fine-tuning the LLM** to your specific use case to incorporate specific wording or tone of voice.\n### Prompt engineering\nHow you phrase or **[engineer your prompt](https://towardsdatascience.com/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550)** will significantly impact the LLM\u2019s completion [1, 8, 9].\n```\nPlease base your answer only on the search results and nothing else!\n```\n```\nVery important! Your answer MUST be grounded in the search results provided. \nPlease explain why your answer is grounded in the search results!\n```\n\nAdditionally, using **few-shot examples** in your prompt can improve the quality of the completions.\nAs mentioned in [Retrieval parameters](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#fa73), the **number of contexts** fed into the prompt is a parameter you should experiment with [1]. While the performance of your RAG pipeline can improve with increasing relevant context, you can also run into a \"Lost in the Middle\" [6] effect where relevant context is not recognized as such by the LLM if it is placed in the middle of many contexts.\n## Summary\nAs more and more developers gain experience with prototyping RAG pipelines, it becomes more important to discuss strategies to bring RAG pipelines to production-ready performances. This article discussed different \"hyperparameters\" and other knobs you can tune in a RAG pipeline according to the relevant stages:\nThis article covered the following strategies in the [ingestion stage](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#4142):\n  * [Data cleaning](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#196c): Ensure data is clean and correct.\n  * [Chunking](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#e45f): Choice of chunking technique, chunk size (`chunk_size`) and chunk overlap (`overlap`).\n  * [Embedding models](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#156e): Choice of the embedding model, incl. dimensionality, and whether to fine-tune it.\n  * [Metadata](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#2b47): Whether to use metadata and choice of metadata.\n  * [Multi-indexing](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#ce6c): Decide whether to use multiple indexes for different data collections.\n  * [Indexing algorithms](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#4daa): Choice and tuning of ANN and vector compression algorithms can be tuned but are usually not tuned by practitioners.\n\n\nAnd the following strategies in the [inferencing stage (retrieval and generation)](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#ac53):\n  * [Query transformations](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#a5e2): Experiment with rephrasing, HyDE, or sub-queries.\n  * [Retrieval parameters](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#fa73): Choice of search technique (`alpha` if you have hybrid search enabled) and the number of retrieved search results.\n  * [Advanced retrieval strategies](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#a3bb): Whether to use advanced retrieval strategies, such as sentence-window or auto-merging retrieval.\n  * [Re-ranking models](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#341d): Whether to use a re-ranking model, choice of re-ranking model, number of search results to input into the re-ranking model, and whether to fine-tune the re-ranking model.\n  * [LLMs](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#e9f9): Choice of LLM and whether to fine-tune it.\n  * [Prompt engineering](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/#9c1c): Experiment with different phrasing and few-shot examples.\n\n\n## Enjoyed This Story?\n_[Subscribe for free](https://medium.com/subscribe/@iamleonie) to get notified when I publish a new story._\n> [**Get an email whenever Leonie Monigatti publishes.**](https://medium.com/@iamleonie/subscribe)\n_Find me on[LinkedIn](https://www.linkedin.com/in/804250ab/)_, _[Twitter](https://twitter.com/helloiamleonie), and [Kaggle](https://www.kaggle.com/iamleonie)!_\n## References\n### Literature\n[1] [Connor Shorten](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/None) and [Erika Cardenas](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/None) (2023). Weaviate Blog. [An Overview on RAG Evaluation](https://weaviate.io/blog/rag-evaluation) (accessed Nov. 27, 2023)\n[2] [Jerry Liu](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/None) (2023). LlamaIndex Blog. [Fine-Tuning Embeddings for RAG with Synthetic Data](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971) (accessed Nov. 28, 2023)\n[3] LlamaIndex Documentation (2023). [Building Performant RAG Applications for Production](https://gpt-index.readthedocs.io/en/stable/optimizing/production_rag.html) (accessed Nov. 28, 2023)\n[4] Voyage AI (2023). [Embeddings Drive the Quality of RAG: A Case Study of Chat.LangChain](https://blog.voyageai.com/2023/10/29/a-case-study-of-chat-langchain/) (accessed Dec. 5, 2023)\n[5] LlamaIndex Documentation (2023). [Query Transformations](https://gpt-index.readthedocs.io/en/v0.6.9/how_to/query/query_transformations.html) (accessed Nov. 28, 2023)\n[6] Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., & Liang, P. (2023). Lost in the middle: How language models use long contexts. _arXiv preprint arXiv:2307.03172_.\n[7] DeepLearning.AI (2023). [Building and Evaluating Advanced RAG Applications](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/) (accessed Dec 4, 2023)\n[8] [Ahmed Besbes](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/None) (2023). Towards Data Science. [Why Your RAG Is Not Reliable in a Production Environment](https://towardsdatascience.com/why-your-rag-is-not-reliable-in-a-production-environment-9e6a73b3eddb) (accessed Nov. 27, 2023)\n[9] [Matt Ambrogi](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439/None) (2023). Towards Data Science. [10 Ways to Improve the Performance of Retrieval Augmented Generation Systems](https://towardsdatascience.com/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c) (accessed Nov. 27, 2023)\n### Images\nIf not otherwise stated, all images are created by the author.\nWritten By\nLeonie Monigatti\n[See all from Leonie Monigatti](https://towardsdatascience.com/author/iamleonie/)\nTopics:\n[Artificial Intelligence](https://towardsdatascience.com/tag/artificial-intelligence/), [Data Scientist](https://towardsdatascience.com/tag/data-scientist/), [Machine Learning](https://towardsdatascience.com/tag/machine-learning/), [Programming](https://towardsdatascience.com/tag/programming/), [Rags](https://towardsdatascience.com/tag/rags/)\nShare this article:\n  * [ Share on Facebook  ](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439%2F&title=A%20Guide%20on%2012%20Tuning%20Strategies%20for%20Production-Ready%20RAG%20Applications)\n  * [ Share on LinkedIn  ](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439%2F&title=A%20Guide%20on%2012%20Tuning%20Strategies%20for%20Production-Ready%20RAG%20Applications)\n  * [ Share on X  ](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439%2F&text=A%20Guide%20on%2012%20Tuning%20Strategies%20for%20Production-Ready%20RAG%20Applications)\n\n\n## Related Articles\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/0c09RmbCCpfjAbSMq.png)\n## [Implementing Convolutional Neural Networks in TensorFlow](https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nStep-by-step code guide to building a Convolutional Neural Network \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nAugust 20, 2024\n6 min read\n  * ## [What Do Large Language Models \u201cUnderstand\u201d?](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA deep dive on the meaning of understanding and how it applies to LLMs \n[Tarik Dzekman](https://towardsdatascience.com/author/tarikdzekman/)\nAugust 21, 2024\n31 min read\n  * ![Photo by Krista Mangulsone on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/0GyVVTbgotH-DhGPH-scaled.jpg)\n## [How to Forecast Hierarchical Time Series](https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA beginner\u2019s guide to forecast reconciliation \n[Dr. Robert K\u00fcbler](https://towardsdatascience.com/author/dr-robert-kuebler/)\nAugust 20, 2024\n13 min read\n  * ![Photo by davisuko on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/1bAABgtZtAIG5YW1oEjW3pA-scaled.jpeg)\n## [Hands-on Time Series Anomaly Detection using Autoencoders, with Python](https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nHere\u2019s how to use Autoencoders to detect signals with anomalies in a few lines of\u2026 \n[Piero Paialunga](https://towardsdatascience.com/author/piero-paialunga/)\nAugust 21, 2024\n12 min read\n  * ![Image from Canva.](https://towardsdatascience.com/wp-content/uploads/2024/08/1UAA9jQVdqMXnwzYiz8Q53Q.png)\n## [3 AI Use Cases (That Are Not a Chatbot)](https://towardsdatascience.com/3-ai-use-cases-that-are-not-a-chatbot-f4f328a2707a/)\n[ Machine Learning ](https://towardsdatascience.com/category/artificial-intelligence/machine-learning/)\nFeature engineering, structuring unstructured data, and lead scoring \n[Shaw Talebi](https://towardsdatascience.com/author/shawhin/)\nAugust 21, 2024\n7 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2023/02/1VEUgT5T4absnTqBMOEuNig.png)\n## [Back To Basics, Part Uno: Linear Regression and Cost Function](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nAn illustrated guide on essential machine learning concepts \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nFebruary 3, 2023\n6 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/1kM8tfYcdaoccB1HX71YDig.png)\n## [Must-Know in Statistics: The Bivariate Normal Projection Explained](https://towardsdatascience.com/must-know-in-statistics-the-bivariate-normal-projection-explained-ace7b2f70b5b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nDerivation and practical examples of this powerful concept \n[Luigi Battistoni](https://towardsdatascience.com/author/lu-battistoni/)\nAugust 14, 2024\n7 min read\n  * ![Photo by Alex Geerts on Unsplash](https://towardsdatascience.com/wp-content/uploads/2020/11/0BF38u2sw4WQdaMLS-scaled.jpg)\n## [Our Columns](https://towardsdatascience.com/our-columns-53501f74c86d/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nColumns on TDS are carefully curated collections of posts on a particular idea or category\u2026 \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nNovember 14, 2020\n4 min read\n  * ![Image created by authors with GPT-4o](https://towardsdatascience.com/wp-content/uploads/2024/08/1vilI3Q4nlwqsAQLq3TOzSA.jpg)\n## [Optimizing Marketing Campaigns with Budgeted Multi-Armed Bandits](https://towardsdatascience.com/optimizing-marketing-campaigns-with-budgeted-multi-armed-bandits-a65fccd61878/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nWith demos, our new solution, and a video \n[Vadim Arzamasov](https://towardsdatascience.com/author/vadim-arzamasov/)\nAugust 16, 2024\n10 min read\n\n\n  * [YouTube](https://www.youtube.com/c/TowardsDataScience)\n  * [X](https://x.com/TDataScience)\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [Threads](https://www.threads.net/@towardsdatascience)\n  * [Bluesky](https://bsky.app/profile/towardsdatascience.com)\n\n\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nYour home for data science and Al. The world\u2019s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\n\u00a9  Insight Media Group, LLC 2025 \n  * [About](https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/)\n  * [Privacy Policy](https://towardsdatascience.com/privacy-policy/)\n  * [Terms of Use](https://towardsdatascience.com/website-terms-of-use/)\n\n\n[Towards Data Science is now independent!](https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/)\nCookies Settings\n## Sign up to our newsletter\nEmail address*\nFirst name*\nLast name*\nJob title*\nJob level*\nPlease SelectC-LevelVP/DirectorManager/SupervisorMid Level or Senior Non-Managerial StaffEntry Level/Junior StaffFreelancer/ContractorStudent/InternOther\nCompany name*\n  * I consent to receive newsletters and other communications from Towards Data Science publications.*\n\n\n![Company Logo](https://cdn.cookielaw.org/logos/static/ot_company_logo.png)\n## Privacy Preference Center\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer. [More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nAllow All\n###  Manage Consent Preferences\n#### Functional Cookies\nFunctional Cookies Active\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\n#### Strictly Necessary Cookies\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n#### Performance Cookies\nPerformance Cookies Active\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n#### Targeting Cookies\nTargeting Cookies Active\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nBack Button\n### Cookie List\nSearch Icon\nFilter Icon\nClear\ncheckbox label label\nApply Cancel\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nReject All Confirm My Choices\n[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-consent/)\nSome areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis guide outlines 12 tuning strategies for optimizing Retrieval-Augmented Generation (RAG) applications. Key strategies include data cleaning, chunking, embedding models, and query transformations. Focus on hyperparameters in both ingestion and inferencing stages to enhance performance.\n\n**Tags:** #GenerativeAI #RAG #LLMs #MachineLearning\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\nTransformers documentation \nTemplates\n# Transformers\n\ud83c\udfe1 View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersDistilabelEvaluateGradioHubHub Python LibraryHugging Face Generative AI Services (HUGS)Huggingface.jsInference API (serverless)Inference Endpoints (dedicated)LeaderboardsLightevalOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jssmolagentstimm\nSearch documentation\nmainv4.50.0v4.49.0v4.48.2v4.47.1v4.46.3v4.45.2v4.44.2v4.43.4v4.42.4v4.41.2v4.40.2v4.39.3v4.38.2v4.37.2v4.36.1v4.35.2v4.34.1v4.33.3v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-builder-html ARDEENESFRHIITJAKOPTTETRZH [ ](https://github.com/huggingface/transformers)\nGet started\n[Transformers ](https://huggingface.co/docs/transformers/en/index)[Installation ](https://huggingface.co/docs/transformers/en/installation)[Quickstart ](https://huggingface.co/docs/transformers/en/quicktour)\nBase classes\nInference\nPipeline API\nLLMs\nChat with models\n[Chat basics ](https://huggingface.co/docs/transformers/en/conversations)[Templates ](https://huggingface.co/docs/transformers/en/chat_templating)[Multimodal templates ](https://huggingface.co/docs/transformers/en/chat_templating_multimodal)[Template writing ](https://huggingface.co/docs/transformers/en/chat_templating_writing)[Tools and RAG ](https://huggingface.co/docs/transformers/en/chat_extras)\nOptimization\n[Agents ](https://huggingface.co/docs/transformers/en/agents)[Tools ](https://huggingface.co/docs/transformers/en/tools)\nTraining\nQuantization\nExport to production\nResources\nContribute\nAPI\n![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\nJoin the Hugging Face community\nand get access to the augmented documentation experience \nCollaborate on models, datasets and Spaces \nFaster examples with accelerated inference \nSwitch between documentation themes \n[Sign Up](https://huggingface.co/join)\nto get started\n# [](https://huggingface.co/docs/transformers/en/chat_templating#templates) Templates\nThe [chat pipeline](https://huggingface.co/docs/transformers/en/conversations) guide introduced [TextGenerationPipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.TextGenerationPipeline) and the concept of a chat prompt or chat template for conversing with a model. Underlying this high-level pipeline is the `apply_chat_template` method. A chat template is a part of the tokenizer and it specifies how to convert conversations into a single tokenizable string in the expected model format.\nIn the example below, Mistral-7B-Instruct and Zephyr-7B are finetuned from the same base model but they\u2019re trained with different chat formats. Without chat templates, you have to manually write formatting code for each model and even minor errors can hurt performance. Chat templates offer a universal way to format chat inputs to any model.\nMistral \nZephyr \nCopied\n```\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\nchat = [\n {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]\ntokenizer.apply_chat_template(chat, tokenize=False)\n```\n\nCopied\n```\n<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\n```\n\nThis guide explores `apply_chat_template` and chat templates in more detail.\n## [](https://huggingface.co/docs/transformers/en/chat_templating#applychattemplate) apply_chat_template\nChats should be structured as a list of dictionaries with `role` and `content` keys. The `role` key specifies the speaker (usually between you and the system), and the `content` key contains your message. For the system, the `content` is a high-level description of how the model should behave and respond when you\u2019re chatting with it.\nPass your messages to `apply_chat_template` to tokenize and format them. You can set [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` to indicate the start of a message.\nCopied\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n  {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n ]\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\nprint(tokenizer.decode(tokenized_chat[0]))\n```\n\nCopied\n```\n<|system|>\nYou are a friendly chatbot who always responds in the style of a pirate</s>\n<|user|>\nHow many helicopters can a human eat in one sitting?</s>\n<|assistant|>\n```\n\nNow pass the tokenized chat to [generate()](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/text_generation#transformers.GenerationMixin.generate) to generate a response.\nCopied\n```\noutputs = model.generate(tokenized_chat, max_new_tokens=128) \nprint(tokenizer.decode(outputs[0]))\n```\n\nCopied\n```\n<|system|>\nYou are a friendly chatbot who always responds in the style of a pirate</s>\n<|user|>\nHow many helicopters can a human eat in one sitting?</s>\n<|assistant|>\nMatey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n```\n\n### [](https://huggingface.co/docs/transformers/en/chat_templating#addgenerationprompt) add_generation_prompt\nThe [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) parameter adds tokens that indicate the start of a response. This ensures the chat model generates a system response instead of continuing a users message.\nNot all models require generation prompts, and some models, like [Llama](https://huggingface.co/docs/transformers/en/model_doc/llama), don\u2019t have any special tokens before the system response. In this case, [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) has no effect.\nCopied\n```\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\ntokenized_chat\n```\n\nCopied\n```\n<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n```\n\n### [](https://huggingface.co/docs/transformers/en/chat_templating#continuefinalmessage) continue_final_message\nThe [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) parameter controls whether the final message in the chat should be continued or not instead of starting a new one. It removes end of sequence tokens so that the model continues generation from the final message.\nThis is useful for \u201cprefilling\u201d a model response. In the example below, the model generates text that continues the JSON string rather than starting a new message. It can be very useful for improving the accuracy for instruction following when you know how to start its replies.\nCopied\n```\nchat = [\n  {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n  {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n]\nformatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)\nmodel.generate(**formatted_chat)\n```\n\nYou shouldn\u2019t use [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) and [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) together. The former adds tokens that start a new message, while the latter removes end of sequence tokens. Using them together returns an error.\n[TextGenerationPipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.TextGenerationPipeline) sets [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` by default to start a new message. However, if the final message in the chat has the \u201cassistant\u201d role, it assumes the message is a prefill and switches to `continue_final_message=True`. This is because most models don\u2019t support multiple consecutive assistant messages. To override this behavior, explicitly pass the [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) to the pipeline.\n## [](https://huggingface.co/docs/transformers/en/chat_templating#multiple-templates) Multiple templates\nA model may have several different templates for different use cases. For example, a model may have a template for regular chat, tool use, and RAG.\nWhen there are multiple templates, the chat template is a dictionary. Each key corresponds to the name of a template. `apply_chat_template` handles multiple templates based on their name. It looks for a template named `default` in most cases and if it can\u2019t find one, it raises an error.\nFor a tool calling template, if a user passes a `tools` parameter and a `tool_use` template exists, the tool calling template is used instead of `default`.\nTo access templates with other names, pass the template name to the `chat_template` parameter in `apply_chat_template`. For example, if you\u2019re using a RAG template then set `chat_template=\"rag\"`.\nIt can be confusing to manage multiple templates though, so we recommend using a single template for all use cases. Use Jinja statements like `if tools is defined` and `{% macro %}` definitions to wrap multiple code paths in a single template.\n## [](https://huggingface.co/docs/transformers/en/chat_templating#template-selection) Template selection\nIt is important to set a chat template format that matches the template format a model was pretrained on, otherwise performance may suffer. Even if you\u2019re training the model further, performance is best if the chat tokens are kept constant.\nBut if you\u2019re training a model from scratch or finetuning a model for chat, you have more options to select a template. For example, [ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md) is a popular format that is flexbile enough to handle many use cases. It even includes support for [generation prompts](https://huggingface.co/docs/transformers/en/chat_templating#add_generation_prompt), but it doesn\u2019t add beginning-of-string (`BOS`) or end-of-string (`EOS`) tokens. If your model expects `BOS` and `EOS` tokens, set `add_special_tokens=True` and make sure to add them to your template.\nCopied\n```\n{%- for message in messages %}\n  {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n{%- endfor %}\n```\n\nSet the template with the following logic to support [generation prompts](https://huggingface.co/docs/transformers/en/chat_templating#add_generation_prompt). The template wraps each message with `<|im_start|>` and `<|im_end|>` tokens and writes the role as a string. This allows you to easily customize the roles you want to train with.\nCopied\n```\ntokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n```\n\nThe `user`, `system` and `assistant` roles are standard roles in chat templates. We recommend using these roles when it makes sense, especially if you\u2019re using your model with the [TextGenerationPipeline](https://huggingface.co/docs/transformers/v4.50.0/en/main_classes/pipelines#transformers.TextGenerationPipeline).\nCopied\n```\n<|im_start|>system\nYou are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n<|im_start|>user\nHow are you?<|im_end|>\n<|im_start|>assistant\nI'm doing great!<|im_end|>\n```\n\n## [](https://huggingface.co/docs/transformers/en/chat_templating#model-training) Model training\nTraining a model with a chat template is a good way to ensure a chat template matches the tokens a model is trained on. Apply the chat template as a preprocessing step to your dataset. Set `add_generation_prompt=False` because the additional tokens to prompt an assistant response aren\u2019t helpful during training.\nAn example of preprocessing a dataset with a chat template is shown below.\nCopied\n```\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\nchat1 = [\n  {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n  {\"role\": \"assistant\", \"content\": \"The sun.\"}\n]\nchat2 = [\n  {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n  {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n]\ndataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\ndataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\nprint(dataset['formatted_chat'][0])\n```\n\nCopied\n```\n<|user|>\nWhich is bigger, the moon or the sun?</s>\n<|assistant|>\nThe sun.</s>\n```\n\nAfter this step, you can continue following the [training recipe](https://huggingface.co/docs/transformers/en/tasks/language_modeling) for causal language models using the `formatted_chat` column.\nSome tokenizers add special `<bos>` and `<eos>` tokens. Chat templates should already include all the necessary special tokens, and adding additional special tokens is often incorrect or duplicated, hurting model performance. When you format text with `apply_chat_template(tokenize=False)`, make sure you set `add_special_tokens=False` as well to avoid duplicating them.\nCopied\n```\napply_chat_template(messages, tokenize=False, add_special_tokens=False)\n```\n\nThis isn\u2019t an issue if `apply_chat_template(tokenize=True)`.\n[< > Update on GitHub](https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md)\n[\u2190Chat basics](https://huggingface.co/docs/transformers/en/conversations) [Multimodal templates\u2192](https://huggingface.co/docs/transformers/en/chat_templating_multimodal)\n[Templates](https://huggingface.co/docs/transformers/en/chat_templating#templates) [apply_chat_template](https://huggingface.co/docs/transformers/en/chat_templating#applychattemplate) [add_generation_prompt](https://huggingface.co/docs/transformers/en/chat_templating#addgenerationprompt) [continue_final_message](https://huggingface.co/docs/transformers/en/chat_templating#continuefinalmessage) [Multiple templates](https://huggingface.co/docs/transformers/en/chat_templating#multiple-templates) [Template selection](https://huggingface.co/docs/transformers/en/chat_templating#template-selection) [Model training](https://huggingface.co/docs/transformers/en/chat_templating#model-training)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe Hugging Face Transformers documentation details the use of chat templates for structuring conversations with models. Key features include the `apply_chat_template` method for formatting chats, options for training models with specific templates, and parameters like `add_generation_prompt` for controlling message generation. Proper template selection is crucial for optimal model performance.\n\n**Tags:** Generative AI, LLMs, Hugging Face, Transformers\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\n# [ Datasets:](https://huggingface.co/datasets)\n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/647479a17d131daf633c108d/IS5jNkSSB5wE2kbtW7QqA.png)](https://huggingface.co/Trelis)\n[Trelis](https://huggingface.co/Trelis)\n/\n[hh-rlhf-dpo](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo)\nlike 3\nFollow\n![](https://cdn-avatars.huggingface.co/v1/production/uploads/647479a17d131daf633c108d/IS5jNkSSB5wE2kbtW7QqA.png) Trelis 114\nArXiv: \narxiv: 2204.05862\nTags:  [ human-feedback ](https://huggingface.co/datasets?other=human-feedback)[ DPO ](https://huggingface.co/datasets?other=DPO)[ rlhf ](https://huggingface.co/datasets?other=rlhf)[ Llama 2 ](https://huggingface.co/datasets?other=Llama+2)\nLicense: \nother\n[ Dataset card ](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo)[ Files Files and versions ](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/tree/main)[ Community 1 ](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/discussions)\n##  You need to agree to share your contact information to access this dataset\nThis repository is publicly accessible, but you have to accept the conditions to access its files and content. \nAccess to this repo requires the purchase of a license (see link on model card below)\n[Log in ](https://huggingface.co/login?next=/datasets/Trelis/hh-rlhf-dpo) or [Sign Up ](https://huggingface.co/join?next=/datasets/Trelis/hh-rlhf-dpo) to review the conditions and access this dataset content. \n  * [Dataset Summary](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/#dataset-summary \"Dataset Summary\")\n  * [Human preference data about helpfulness and harmlessness (PM Data)](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/#human-preference-data-about-helpfulness-and-harmlessness-pm-data \"Human preference data about helpfulness and harmlessness \\(PM Data\\)\")\n  * [Red teaming data (not PM Data)](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/#red-teaming-data-not-pm-data \"Red teaming data \\(not PM Data\\)\")\n  * [Usage](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/#usage \"Usage\")\n  * [Contact](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/#contact \"Contact\")\n\n\n#  [ ](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/#dpo-formatted-helpful-and-harmless-rlhf-dataset) DPO formatted Helpful and Harmless RLHF Dataset \nThis dataset is built from [Anthropic's hh-rlhf dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf).\nIt is modified as follows:\n  * The prompt formatting is switched to the Llama 2 format with [INST] and [/INST]\n  * The data is split into prompt, chosen and rejected rows, as required by [HuggingFace's DPO trainer](https://huggingface.co/docs/trl/dpo_trainer).\n\n\nPurchase access to this dataset [here](https://buy.stripe.com/dR614QcWh95VdZ68xb). Purchase entitles the user to make use of the dataset for training large language models.\n  * The original dataset card follows below:\n\n\n#  [ ](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/#dataset-card-for-hh-rlhf) Dataset Card for HH-RLHF \n##  [ ](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/#dataset-summary) Dataset Summary \nThis repository provides access to two different kinds of data:\n  1. Human preference data about helpfulness and harmlessness from [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862). These data are meant to train preference (or reward) models for subsequent RLHF training. These data are _not_ meant for supervised training of dialogue agents. Training dialogue agents on these data is likely to lead to harmful models and this shold be avoided.\n  2. Human-generated and annotated red teaming dialogues from [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://www.anthropic.com/red_teaming.pdf). These data are meant to understand how crowdworkers red team models and what types of red team attacks are succesful or not. The data are _not_ meant for fine-tuning or preference modeling (use the data above for preference modeling). These data are entire transcripts of conversations that are derived from the harmlessness preference modeling data described above, where only the chosen response is incorporated into the overall transcript. Furthermore, the transcripts are annotated with human and automated measurements of how harmful the overall dialogues are.\n\n\n**Disclaimer** : The data (especially the harmlessness preference data and the red team data) contain content that may be offensive or upsetting. Topics include, but are not limited to, discriminatory language and discussions of abuse, violence, self-harm, exploitation, and other potentially upsetting subject matter. Please only engage with the data in accordance with your own personal risk tolerance. The data are intended for research purposes, especially research that can make models _less_ harmful. The views expressed in the data do not reflect the views of Anthropic or any of its employees. As mentioned above, these data are _not_ intended for training dialogue agents as this will likely lead to harmful model behavior. \nEach of these datasets are described further below. \n##  [ ](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/#human-preference-data-about-helpfulness-and-harmlessness-pm-data) Human preference data about helpfulness and harmlessness (PM Data) \nThe data are described in the paper: [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862). If you find the data useful, please cite the paper. The data format is very simple -- each line of the jsonl files contains a pair of texts, one \"chosen\" and one \"rejected\".\nFor **helpfulness** , the data are grouped into train/test splits in three tranches: from our base models (context-distilled 52B language models), via rejection sampling (mostly with best-of-16 sampling) against an early preference model, and a dataset sampled during our iterated \"online\" process. \nFor **harmlessness** , the data are only collected for our base models, but otherwise formatted in the same way.\nDetails about the data collection process and crowdworker population can be found in the paper, specifically in section 2 and appendix D. \n##  [ ](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/#red-teaming-data-not-pm-data) Red teaming data (not PM Data) \nThe data are described in the paper: [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://www.anthropic.com/red_teaming.pdf). If you find the data useful, please cite the paper. At a high level these data can be examined to understand what succesful red team attempts look like. \nDetails about the data and data collection procedures can be found in the Datasheet in the appendix of the paper. \nEach line of the jsonl file contains a dictionary with the following fields:\n  * `transcript` a text transcript of a conversation between a human adversary (red team member) and an AI assistant\n  * `min_harmlessness_score_transcript` a real value score of the harmlessness of the AI assistant (lower is more harmful) as obtained from a preference model\n  * `num_params` number of parameters in the language model powering the AI assistant\n  * `model_type` type of model powering the AI assistant\n  * `rating` the red team member's rating of how successful they were at breaking the AI assistant (Likert scale, higher is more successful) \n  * `task_description` a short text description written by the red team member about how they tried to red team the AI assistant\n  * `task_description_harmlessness_score` a real value score of the harmlessness of the task description (lower is more harmful) as obtained from a preference model\n  * `red_team_member_id` an arbitrary identifier of the red team member. one red team member can generate multiple red team attacks\n  * `is_upworker` a binary indicator that is true if the red team member was from the crowd platform Upwork or false if they were from MTurk\n  * `tags` a list of up to 6 tags per transcript. tags are short descriptions of the red team attempts generated by crowdworkers who reviewed red team data post-hoc. tags were only provided for a random sample of 1000 red team attempts for two of four model types.\n\n\n##  [ ](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/#usage) Usage \nEach of the above datasets is located in a separate sub-directory. To load an individual subset, use the `data_dir` argument of the `load_dataset()` function as follows:\n```\nfrom datasets import load_dataset\n# Load all helpfulness/harmless subsets (share the same schema)\ndataset = load_dataset(\"Anthropic/hh-rlhf\")\n# Load one of the harmless subsets\ndataset = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"harmless-base\")\n# Load the red teaming subset\ndataset = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"red-team-attempts\")\n\n```\n\n##  [ ](https://huggingface.co/datasets/Trelis/hh-rlhf-dpo/#contact) Contact \nThe original authors host this dataset on GitHub here: <https://github.com/anthropics/hh-rlhf> You can submit inquiries to: redteam@anthropic.com \n\nDownloads last month\n    14\nSystem theme \nCompany\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/) [](https://huggingface.co/)\nWebsite\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe dataset **hh-rlhf-dpo** by Trelis provides human feedback data for training language models, focusing on helpfulness and harmlessness. It includes preference data and red teaming dialogues, emphasizing research to reduce model harms. Access requires a license.\n\n**Tags:** Generative AI, LLMs, human-feedback, DPO, rlhf, Llama 2\n```"
  },
  {
    "instruction": "[Lil'Log](https://lilianweng.github.io/ \"Lil'Log \\(Alt + H\\)\")\n  * |\n\n\n  * [ Posts ](https://lilianweng.github.io/ \"Posts\")\n  * [ Archive ](https://lilianweng.github.io/archives \"Archive\")\n  * [ Search ](https://lilianweng.github.io/search/ \"Search \\(Alt + /\\)\")\n  * [ Tags ](https://lilianweng.github.io/tags/ \"Tags\")\n  * [ FAQ ](https://lilianweng.github.io/faq \"FAQ\")\n\n\n#  LLM Powered Autonomous Agents \nDate: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng \nTable of Contents\n  * [Agent System Overview](https://lilianweng.github.io/posts/2023-06-23-agent/#agent-system-overview)\n  * [Component One: Planning](https://lilianweng.github.io/posts/2023-06-23-agent/#component-one-planning)\n    * [Task Decomposition](https://lilianweng.github.io/posts/2023-06-23-agent/#task-decomposition)\n    * [Self-Reflection](https://lilianweng.github.io/posts/2023-06-23-agent/#self-reflection)\n  * [Component Two: Memory](https://lilianweng.github.io/posts/2023-06-23-agent/#component-two-memory)\n    * [Types of Memory](https://lilianweng.github.io/posts/2023-06-23-agent/#types-of-memory)\n    * [Maximum Inner Product Search (MIPS)](https://lilianweng.github.io/posts/2023-06-23-agent/#maximum-inner-product-search-mips)\n  * [Component Three: Tool Use](https://lilianweng.github.io/posts/2023-06-23-agent/#component-three-tool-use)\n  * [Case Studies](https://lilianweng.github.io/posts/2023-06-23-agent/#case-studies)\n    * [Scientific Discovery Agent](https://lilianweng.github.io/posts/2023-06-23-agent/#scientific-discovery-agent)\n    * [Generative Agents Simulation](https://lilianweng.github.io/posts/2023-06-23-agent/#generative-agents-simulation)\n    * [Proof-of-Concept Examples](https://lilianweng.github.io/posts/2023-06-23-agent/#proof-of-concept-examples)\n  * [Challenges](https://lilianweng.github.io/posts/2023-06-23-agent/#challenges)\n  * [Citation](https://lilianweng.github.io/posts/2023-06-23-agent/#citation)\n  * [References](https://lilianweng.github.io/posts/2023-06-23-agent/#references)\n\n\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT), [GPT-Engineer](https://github.com/AntonOsika/gpt-engineer) and [BabyAGI](https://github.com/yoheinakajima/babyagi), serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n# Agent System Overview[#](https://lilianweng.github.io/posts/2023-06-23-agent/#agent-system-overview)\nIn a LLM-powered autonomous agent system, LLM functions as the agent\u2019s brain, complemented by several key components:\n  * **Planning**\n    * Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n    * Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n  * **Memory**\n    * Short-term memory: I would consider all the in-context learning (See [Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)) as utilizing short-term memory of the model to learn.\n    * Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n  * **Tool use**\n    * The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n\n![](https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png) Fig. 1. Overview of a LLM-powered autonomous agent system.\n# Component One: Planning[#](https://lilianweng.github.io/posts/2023-06-23-agent/#component-one-planning)\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n## Task Decomposition[#](https://lilianweng.github.io/posts/2023-06-23-agent/#task-decomposition)\n[**Chain of thought**](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#chain-of-thought-cot) (CoT; [Wei et al. 2022](https://arxiv.org/abs/2201.11903)) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to \u201cthink step by step\u201d to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model\u2019s thinking process.\n**Tree of Thoughts** ([Yao et al. 2023](https://arxiv.org/abs/2305.10601)) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like `\"Steps for XYZ.\\n1.\"`, `\"What are the subgoals for achieving XYZ?\"`, (2) by using task-specific instructions; e.g. `\"Write a story outline.\"` for writing a novel, or (3) with human inputs.\nAnother quite distinct approach, **LLM+P** ([Liu et al. 2023](https://arxiv.org/abs/2304.11477)), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into \u201cProblem PDDL\u201d, then (2) requests a classical planner to generate a PDDL plan based on an existing \u201cDomain PDDL\u201d, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n## Self-Reflection[#](https://lilianweng.github.io/posts/2023-06-23-agent/#self-reflection)\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n**ReAct** ([Yao et al. 2023](https://arxiv.org/abs/2210.03629)) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\n```\nThought: ...\nAction: ...\nObservation: ...\n... (Repeated many times)\ncopy\n```\n![](https://lilianweng.github.io/posts/2023-06-23-agent/react.png) Fig. 2. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: [Yao et al. 2023](https://arxiv.org/abs/2210.03629)).\nIn both experiments on knowledge-intensive tasks and decision-making tasks, `ReAct` works better than the `Act`-only baseline where `Thought: \u2026` step is removed.\n**Reflexion** ([Shinn & Labash 2023](https://arxiv.org/abs/2303.11366)) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may _decide to reset_ the environment to start a new trial depending on the self-reflection results.\n![](https://lilianweng.github.io/posts/2023-06-23-agent/reflexion.png) Fig. 3. Illustration of the Reflexion framework. (Image source: [Shinn & Labash, 2023](https://arxiv.org/abs/2303.11366))\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent\u2019s working memory, up to three, to be used as context for querying LLM.\n![](https://lilianweng.github.io/posts/2023-06-23-agent/reflexion-exp.png) Fig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: [Shinn & Labash, 2023](https://arxiv.org/abs/2303.11366))\n**Chain of Hindsight** (CoH; [Liu et al. 2023](https://arxiv.org/abs/2302.02676)) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\nThe training dataset in their experiments is a combination of [WebGPT comparisons](https://huggingface.co/datasets/openai/webgpt_comparisons), [summarization from human feedback](https://github.com/openai/summarize-from-feedback) and [human preference dataset](https://github.com/anthropics/hh-rlhf).\n![](https://lilianweng.github.io/posts/2023-06-23-agent/CoH.png) Fig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: [Liu et al. 2023](https://arxiv.org/abs/2302.02676))\nThe idea of CoH is to present a history of sequentially improved outputs in context and train the model to take on the trend to produce better outputs. **Algorithm Distillation** (AD; [Laskin et al. 2023](https://arxiv.org/abs/2210.14215)) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an _algorithm_ is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\n![](https://lilianweng.github.io/posts/2023-06-23-agent/algorithm-distillation.png) Fig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: [Laskin et al. 2023](https://arxiv.org/abs/2210.14215)).\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by [UCB](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#upper-confidence-bounds)), RL^2 ([Duan et al. 2017](https://arxiv.org/abs/1611.02779); used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\n![](https://lilianweng.github.io/posts/2023-06-23-agent/algorithm-distillation-results.png) Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with [A3C](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#a3c) for \"dark\" environments and [DQN](http://lilianweng.github.io/posts/2018-02-19-rl-overview/#deep-q-network) for watermaze.(Image source: [Laskin et al. 2023](https://arxiv.org/abs/2210.14215))\n# Component Two: Memory[#](https://lilianweng.github.io/posts/2023-06-23-agent/#component-two-memory)\n(Big thank you to ChatGPT for helping me draft this section. I\u2019ve learned a lot about the human brain and data structure for fast MIPS in my [conversations](https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389) with ChatGPT.)\n## Types of Memory[#](https://lilianweng.github.io/posts/2023-06-23-agent/#types-of-memory)\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\n  1. **Sensory Memory** : This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\n  2. **Short-Term Memory** (STM) or **Working Memory** : It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items ([Miller 1956](https://lilianweng.github.io/posts/2023-06-23-agent/psychclassics.yorku.ca/Miller/)) and lasts for 20-30 seconds.\n  3. **Long-Term Memory** (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n     * Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\n     * Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n\n![](https://lilianweng.github.io/posts/2023-06-23-agent/memory.png) Fig. 8. Categorization of human memory.\nWe can roughly consider the following mappings:\n  * Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\n  * Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\n  * Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n\n\n## Maximum Inner Product Search (MIPS)[#](https://lilianweng.github.io/posts/2023-06-23-agent/#maximum-inner-product-search-mips)\nThe external memory can alleviate the restriction of finite attention span. A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search ([MIPS](https://en.wikipedia.org/wiki/Maximum_inner-product_search)). To optimize the retrieval speed, the common choice is the _approximate nearest neighbors (ANN)\u200b_ algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\nA couple common choices of ANN algorithms for fast MIPS:\n  * [**LSH**](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) (Locality-Sensitive Hashing): It introduces a _hashing_ function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\n  * [**ANNOY**](https://github.com/spotify/annoy) (Approximate Nearest Neighbors Oh Yeah): The core data structure are _random projection trees_ , a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\n  * [**HNSW**](https://arxiv.org/abs/1603.09320) (Hierarchical Navigable Small World): It is inspired by the idea of [small world networks](https://en.wikipedia.org/wiki/Small-world_network) where most nodes can be reached by any other nodes within a small number of steps; e.g. \u201csix degrees of separation\u201d feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can\u2019t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\n  * [**FAISS**](https://github.com/facebookresearch/faiss) (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist _clustering_ of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\n  * [**ScaNN**](https://github.com/google-research/google-research/tree/master/scann) (Scalable Nearest Neighbors): The main innovation in ScaNN is _anisotropic vector quantization_. It quantizes a data point $x_i$ to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\n\n![](https://lilianweng.github.io/posts/2023-06-23-agent/mips.png) Fig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: [Google Blog, 2020](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html))\nCheck more MIPS algorithms and performance comparison in [ann-benchmarks.com](https://ann-benchmarks.com/).\n# Component Three: Tool Use[#](https://lilianweng.github.io/posts/2023-06-23-agent/#component-three-tool-use)\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\n![](https://lilianweng.github.io/posts/2023-06-23-agent/sea-otter.png) Fig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: [Animals using tools](https://www.popularmechanics.com/science/animals/g39714258/animals-using-tools/))\n**MRKL** ([Karpas et al. 2022](https://arxiv.org/abs/2205.00445)), short for \u201cModular Reasoning, Knowledge and Language\u201d, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of \u201cexpert\u201d modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, _knowing when to and how to use the tools are crucial_ , determined by the LLM capability.\nBoth **TALM** (Tool Augmented Language Models; [Parisi et al. 2022](https://arxiv.org/abs/2205.12255)) and **Toolformer** ([Schick et al. 2023](https://arxiv.org/abs/2302.04761)) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the [\u201cExternal APIs\u201d section](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#external-apis) of Prompt Engineering.\nChatGPT [Plugins](https://openai.com/blog/chatgpt-plugins) and OpenAI API [function calling](https://platform.openai.com/docs/guides/gpt/function-calling) are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\n**HuggingGPT** ([Shen et al. 2023](https://arxiv.org/abs/2303.17580)) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\n![](https://lilianweng.github.io/posts/2023-06-23-agent/hugging-gpt.png) Fig. 11. Illustration of how HuggingGPT works. (Image source: [Shen et al. 2023](https://arxiv.org/abs/2303.17580))\nThe system comprises of 4 stages:\n**(1) Task planning** : LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\nInstruction:\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning. \n**(2) Model selection** : LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\nInstruction:\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list. \n**(3) Task execution** : Expert models execute on the specific tasks and log results.\nInstruction:\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path. \n**(4) Response generation** : LLM receives the execution results and provides summarized results to users.\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\n**API-Bank** ([Li et al. 2023](https://arxiv.org/abs/2304.08244)) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\n![](https://lilianweng.github.io/posts/2023-06-23-agent/api-bank-process.png) Fig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: [Li et al. 2023](https://arxiv.org/abs/2304.08244))\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\n  1. Whether an API call is needed.\n  2. Identify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\n  3. Response based on the API results: the model can choose to refine and call again if results are not satisfied.\n\n\nThis benchmark evaluates the agent\u2019s tool use capabilities at three levels:\n  * Level-1 evaluates the ability to _call the API_. Given an API\u2019s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\n  * Level-2 examines the ability to _retrieve the API_. The model needs to search for possible APIs that may solve the user\u2019s requirement and learn how to use them by reading documentation.\n  * Level-3 assesses the ability to _plan API beyond retrieve and call_. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\n\n\n# Case Studies[#](https://lilianweng.github.io/posts/2023-06-23-agent/#case-studies)\n## Scientific Discovery Agent[#](https://lilianweng.github.io/posts/2023-06-23-agent/#scientific-discovery-agent)\n**ChemCrow** ([Bran et al. 2023](https://arxiv.org/abs/2304.05376)) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in [LangChain](https://github.com/hwchase17/langchain), reflects what was previously described in the [ReAct](https://lilianweng.github.io/posts/2023-06-23-agent/#react) and [MRKLs](https://lilianweng.github.io/posts/2023-06-23-agent/#mrkl) and combines CoT reasoning with tools relevant to the tasks:\n  * The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\n  * It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - `Thought, Action, Action Input, Observation`.\n\n\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\n[Boiko et al. (2023)](https://arxiv.org/abs/2304.05332) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\nFor example, when requested to `\"develop a novel anticancer drug\"`, the model came up with the following reasoning steps:\n  1. inquired about current trends in anticancer drug discovery;\n  2. selected a target;\n  3. requested a scaffold targeting these compounds;\n  4. Once the compound was identified, the model attempted its synthesis.\n\n\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n## Generative Agents Simulation[#](https://lilianweng.github.io/posts/2023-06-23-agent/#generative-agents-simulation)\n**Generative Agents** ([Park, et al. 2023](https://arxiv.org/abs/2304.03442)) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n  * **Memory** stream: is a long-term memory module (external database) that records a comprehensive list of agents\u2019 experience in natural language. \n    * Each element is an _observation_ , an event directly provided by the agent. - Inter-agent communication can trigger new natural language statements.\n  * **Retrieval** model: surfaces the context to inform the agent\u2019s behavior, according to relevance, recency and importance. \n    * Recency: recent events have higher scores\n    * Importance: distinguish mundane from core memories. Ask LM directly.\n    * Relevance: based on how related it is to the current situation / query.\n  * **Reflection** mechanism: synthesizes memories into higher level inferences over time and guides the agent\u2019s future behavior. They are _higher-level summaries of past events_ (<- note that this is a bit different from [self-reflection](https://lilianweng.github.io/posts/2023-06-23-agent/#self-reflection) above) \n    * Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n  * **Planning & Reacting**: translate the reflections and the environment information into actions \n    * Planning is essentially in order to optimize believability at the moment vs in time.\n    * Prompt template: `{Intro of an agent X}. Here is X's plan today in broad strokes: 1)`\n    * Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n    * Environment information is present in a tree structure.\n\n![](https://lilianweng.github.io/posts/2023-06-23-agent/generative-agents.png) Fig. 13. The generative agent architecture. (Image source: [Park et al. 2023](https://arxiv.org/abs/2304.03442))\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\n## Proof-of-Concept Examples[#](https://lilianweng.github.io/posts/2023-06-23-agent/#proof-of-concept-examples)\n[AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT) has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\nHere is the system message used by AutoGPT, where `{{...}}` are user inputs:\n```\nYou are {{ai-name}}, {{user-provided AI bot description}}.\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\nGOALS:\n1. {{user-provided goal 1}}\n2. {{user-provided goal 2}}\n3. ...\n4. ...\n5. ...\nConstraints:\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n3. No user assistance\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n5. Use subprocesses for commands that will not terminate within a few minutes\nCommands:\n1. Google Search: \"google\", args: \"input\": \"<search>\"\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\n5. List GPT Agents: \"list_agents\", args:\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n19. Do Nothing: \"do_nothing\", args:\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\nResources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\nYou should only respond in JSON format as described below\nResponse Format:\n{\n  \"thoughts\": {\n    \"text\": \"thought\",\n    \"reasoning\": \"reasoning\",\n    \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n    \"criticism\": \"constructive self-criticism\",\n    \"speak\": \"thoughts summary to say to user\"\n  },\n  \"command\": {\n    \"name\": \"command name\",\n    \"args\": {\n      \"arg name\": \"value\"\n    }\n  }\n}\nEnsure the response can be parsed by Python json.loads\ncopy\n```\n\n[GPT-Engineer](https://github.com/AntonOsika/gpt-engineer) is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in `{{user input text}}`.\n```\n[\n {\n  \"role\": \"system\",\n  \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n },\n {\n  \"role\": \"user\",\n  \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\n\"\n },\n {\n  \"role\": \"assistant\",\n  \"content\": \"Summary of areas that need clarification:\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2. Details about the MVC components (e.g. which components are in each file)\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying question:\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\n },\n {\n  \"role\": \"user\",\n  \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\"\n },\n {\n  \"role\": \"assistant\",\n  \"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\"\n },\n {\n  \"role\": \"user\",\n  \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\n }\n]\ncopy\n```\n\nThen after these clarification, the agent moved into the code writing mode with a different system message. System message:\nYou will get instructions for code to write. You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code. Make sure that every detail of the architecture is, in the end, implemented as code. \nThink step by step and reason yourself to the right decisions to make sure we get it right. You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\nThen you will output the content of each file including ALL code. Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that FILENAME is the lowercase file name including the file extension, LANG is the markup code block language for the code\u2019s language, and CODE is the code:\nFILENAME\n```\nCODE\ncopy\n```\n\nYou will start with the \u201centrypoint\u201d file, then go to the ones that are imported by that file, and so on. Please note that the code should be fully functional. No placeholders.\nFollow a language and framework appropriate best practice file naming convention. Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other. Ensure to implement all code, if you are unsure, write a plausible implementation. Include module dependency or package manager dependency definition file. Before you finish, double check that all parts of the architecture is present in the files.\nUseful to know: You almost always put different classes in different files. For Python, you always create an appropriate requirements.txt file. For NodeJS, you always create an appropriate package.json file. You always add a comment briefly describing the purpose of the function definition. You try to add comments explaining very complex bits of logic. You always follow the best practices for the requested languages in terms of describing the code written as a defined package/project.\nPython toolbelt preferences:\n  * pytest\n  * dataclasses\n\n\nConversatin samples:\n```\n[\n {\n  \"role\": \"system\",\n  \"content\": \"You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\n\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\"\n },\n # \u2026 same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\n {\n  \"role\": \"assistant\",\n  \"content\": \"Assumptions:\\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\n },\n {\n  \"role\": \"user\",\n  \"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional. No placeholders.\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\"\n }\n]\ncopy\n```\n\n# Challenges[#](https://lilianweng.github.io/posts/2023-06-23-agent/#challenges)\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\n  * **Finite context length** : The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n  * **Challenges in long-term planning and task decomposition** : Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n  * **Reliability of natural language interface** : Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\n\n\n# Citation[#](https://lilianweng.github.io/posts/2023-06-23-agent/#citation)\nCited as:\n> Weng, Lilian. (Jun 2023). \u201cLLM-powered Autonomous Agents\u201d. Lil\u2019Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\nOr\n```\n@article{weng2023agent,\n title  = \"LLM-powered Autonomous Agents\",\n author = \"Weng, Lilian\",\n journal = \"lilianweng.github.io\",\n year  = \"2023\",\n month  = \"Jun\",\n url   = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n}\ncopy\n```\n\n# References[#](https://lilianweng.github.io/posts/2023-06-23-agent/#references)\n[1] Wei et al. [\u201cChain of thought prompting elicits reasoning in large language models.\u201d](https://arxiv.org/abs/2201.11903) NeurIPS 2022\n[2] Yao et al. [\u201cTree of Thoughts: Dliberate Problem Solving with Large Language Models.\u201d](https://arxiv.org/abs/2305.10601) arXiv preprint arXiv:2305.10601 (2023).\n[3] Liu et al. [\u201cChain of Hindsight Aligns Language Models with Feedback \u201c](https://arxiv.org/abs/2302.02676) arXiv preprint arXiv:2302.02676 (2023).\n[4] Liu et al. [\u201cLLM+P: Empowering Large Language Models with Optimal Planning Proficiency\u201d](https://arxiv.org/abs/2304.11477) arXiv preprint arXiv:2304.11477 (2023).\n[5] Yao et al. [\u201cReAct: Synergizing reasoning and acting in language models.\u201d](https://arxiv.org/abs/2210.03629) ICLR 2023.\n[6] Google Blog. [\u201cAnnouncing ScaNN: Efficient Vector Similarity Search\u201d](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) July 28, 2020.\n[7] <https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389>\n[8] Shinn & Labash. [\u201cReflexion: an autonomous agent with dynamic memory and self-reflection\u201d](https://arxiv.org/abs/2303.11366) arXiv preprint arXiv:2303.11366 (2023).\n[9] Laskin et al. [\u201cIn-context Reinforcement Learning with Algorithm Distillation\u201d](https://arxiv.org/abs/2210.14215) ICLR 2023.\n[10] Karpas et al. [\u201cMRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.\u201d](https://arxiv.org/abs/2205.00445) arXiv preprint arXiv:2205.00445 (2022).\n[11] Nakano et al. [\u201cWebgpt: Browser-assisted question-answering with human feedback.\u201d](https://arxiv.org/abs/2112.09332) arXiv preprint arXiv:2112.09332 (2021).\n[12] Parisi et al. [\u201cTALM: Tool Augmented Language Models\u201d](https://arxiv.org/abs/2205.12255)\n[13] Schick et al. [\u201cToolformer: Language Models Can Teach Themselves to Use Tools.\u201d](https://arxiv.org/abs/2302.04761) arXiv preprint arXiv:2302.04761 (2023).\n[14] Weaviate Blog. [Why is Vector Search so fast?](https://weaviate.io/blog/why-is-vector-search-so-fast) Sep 13, 2022.\n[15] Li et al. [\u201cAPI-Bank: A Benchmark for Tool-Augmented LLMs\u201d](https://arxiv.org/abs/2304.08244) arXiv preprint arXiv:2304.08244 (2023).\n[16] Shen et al. [\u201cHuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace\u201d](https://arxiv.org/abs/2303.17580) arXiv preprint arXiv:2303.17580 (2023).\n[17] Bran et al. [\u201cChemCrow: Augmenting large-language models with chemistry tools.\u201d](https://arxiv.org/abs/2304.05376) arXiv preprint arXiv:2304.05376 (2023).\n[18] Boiko et al. [\u201cEmergent autonomous scientific research capabilities of large language models.\u201d](https://arxiv.org/abs/2304.05332) arXiv preprint arXiv:2304.05332 (2023).\n[19] Joon Sung Park, et al. [\u201cGenerative Agents: Interactive Simulacra of Human Behavior.\u201d](https://arxiv.org/abs/2304.03442) arXiv preprint arXiv:2304.03442 (2023).\n[20] AutoGPT. <https://github.com/Significant-Gravitas/Auto-GPT>\n[21] GPT-Engineer. <https://github.com/AntonOsika/gpt-engineer>\n  * [Nlp](https://lilianweng.github.io/tags/nlp/)\n  * [Language-Model](https://lilianweng.github.io/tags/language-model/)\n  * [Agent](https://lilianweng.github.io/tags/agent/)\n  * [Steerability](https://lilianweng.github.io/tags/steerability/)\n  * [Prompting](https://lilianweng.github.io/tags/prompting/)\n\n[ \u00ab  Adversarial Attacks on LLMs ](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/) [ \u00bb Prompt Engineering ](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n[ ](https://twitter.com/intent/tweet/?text=LLM%20Powered%20Autonomous%20Agents&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2023-06-23-agent%2f&hashtags=nlp%2clanguage-model%2cagent%2csteerability%2cprompting) [ ](https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2023-06-23-agent%2f&title=LLM%20Powered%20Autonomous%20Agents&summary=LLM%20Powered%20Autonomous%20Agents&source=https%3a%2f%2flilianweng.github.io%2fposts%2f2023-06-23-agent%2f) [ ](https://reddit.com/submit?url=https%3a%2f%2flilianweng.github.io%2fposts%2f2023-06-23-agent%2f&title=LLM%20Powered%20Autonomous%20Agents) [ ](https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flilianweng.github.io%2fposts%2f2023-06-23-agent%2f) [ ](https://api.whatsapp.com/send?text=LLM%20Powered%20Autonomous%20Agents%20-%20https%3a%2f%2flilianweng.github.io%2fposts%2f2023-06-23-agent%2f) [ ](https://telegram.me/share/url?text=LLM%20Powered%20Autonomous%20Agents&url=https%3a%2f%2flilianweng.github.io%2fposts%2f2023-06-23-agent%2f)\n\u00a9 2025 [Lil'Log](https://lilianweng.github.io/) Powered by [Hugo](https://gohugo.io/) & [PaperMod](https://git.io/hugopapermod) [ ](https://lilianweng.github.io/posts/2023-06-23-agent/#top \"Go to Top \\(Alt + G\\)\")\n",
    "answer": "```markdown\n# TL;DR Summary\n\nLilian Weng's article discusses LLM-powered autonomous agents, emphasizing their components: planning, memory, and tool use. Key insights include task decomposition techniques, self-reflection mechanisms, and the challenges of finite context length and reliability in natural language interfaces. Case studies highlight applications in scientific discovery and generative simulations.\n```"
  },
  {
    "instruction": "[Lil'Log](https://lilianweng.github.io/ \"Lil'Log \\(Alt + H\\)\")\n  * |\n\n\n  * [ Posts ](https://lilianweng.github.io/ \"Posts\")\n  * [ Archive ](https://lilianweng.github.io/archives \"Archive\")\n  * [ Search ](https://lilianweng.github.io/search/ \"Search \\(Alt + /\\)\")\n  * [ Tags ](https://lilianweng.github.io/tags/ \"Tags\")\n  * [ FAQ ](https://lilianweng.github.io/faq \"FAQ\")\n\n\n# \ud83d\udc4b Welcome to Lil\u2019Log\nHi, this is Lilian. I\u2019m documenting my learning notes in this blog since 2017. Based on the number of grammar mistakes in my posts, you can tell how much ChatGPT is involved \ud83d\ude09.\n[ ](https://twitter.com/lilianweng/ \"Twitter\") [ ](https://scholar.google.com/citations?user=dCa-pW8AAAAJ&hl=en&oi=ao \"Other\") [ ](https://lilianweng.github.io/index.xml \"Rss\") [ ](https://github.com/lilianweng \"Github\")\n## Reward Hacking in Reinforcement Learning \nReward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function. With the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user\u2019s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models. ...\nDate: November 28, 2024 | Estimated Reading Time: 37 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2024-11-28-reward-hacking/)\n## Extrinsic Hallucinations in LLMs \nHallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge. There are two types of hallucination: In-context hallucination: The model output should be consistent with the source content in context. Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so. This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable. ...\nDate: July 7, 2024 | Estimated Reading Time: 30 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2024-07-07-hallucination/)\n## Diffusion Models for Video Generation \nDiffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task\u2014using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because: It has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model. In comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs. \ud83e\udd51 Required Pre-read: Please make sure you have read the previous blog on \u201cWhat are Diffusion Models?\u201d for image generation before continue here. ...\nDate: April 12, 2024 | Estimated Reading Time: 20 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2024-04-12-diffusion-video/)\n## Thinking about High-Quality Human Data \n[Special thank you to Ian Kivlichan for many useful pointers (E.g. the 100+ year old Nature paper \u201cVox populi\u201d) and nice feedback. \ud83d\ude4f ] High-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or RLHF labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution. The community knows the value of high quality data, but somehow we have this subtle impression that \u201cEveryone wants to do the model work, not the data work\u201d (Sambasivan et al. 2021). ...\nDate: February 5, 2024 | Estimated Reading Time: 21 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2024-02-05-human-data-quality/)\n## Adversarial Attacks on LLMs \nThe use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired. A large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content. ...\nDate: October 25, 2023 | Estimated Reading Time: 33 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)\n## LLM Powered Autonomous Agents \nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent\u2019s brain, complemented by several key components: Planning Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results. Memory Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn. Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval. Tool use The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more. Fig. 1. Overview of a LLM-powered autonomous agent system. Component One: Planning A complicated task usually involves many steps. An agent needs to know what they are and plan ahead. ...\nDate: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2023-06-23-agent/)\n## Prompt Engineering \nPrompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics. This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation. ...\nDate: March 15, 2023 | Estimated Reading Time: 21 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n## The Transformer Family Version 2.0 \nMany new Transformer architecture improvements have been proposed since my last post on \u201cThe Transformer Family\u201d about three years ago. Here I did a big refactoring and enrichment of that 2020 post \u2014 restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length. Notations Symbol Meaning $d$ The model size / hidden state dimension / positional encoding size. $h$ The number of heads in multi-head attention layer. $L$ The segment length of input sequence. $N$ The total number of attention layers in the model; not considering MoE. $\\mathbf{X} \\in \\mathbb{R}^{L \\times d}$ The input sequence where each element has been mapped into an embedding vector of shape $d$, same as the model size. $\\mathbf{W}^k \\in \\mathbb{R}^{d \\times d_k}$ The key weight matrix. $\\mathbf{W}^q \\in \\mathbb{R}^{d \\times d_k}$ The query weight matrix. $\\mathbf{W}^v \\in \\mathbb{R}^{d \\times d_v}$ The value weight matrix. Often we have $d_k = d_v = d$. $\\mathbf{W}^k_i, \\mathbf{W}^q_i \\in \\mathbb{R}^{d \\times d_k/h}; \\mathbf{W}^v_i \\in \\mathbb{R}^{d \\times d_v/h}$ The weight matrices per head. $\\mathbf{W}^o \\in \\mathbb{R}^{d_v \\times d}$ The output weight matrix. $\\mathbf{Q} = \\mathbf{X}\\mathbf{W}^q \\in \\mathbb{R}^{L \\times d_k}$ The query embedding inputs. $\\mathbf{K} = \\mathbf{X}\\mathbf{W}^k \\in \\mathbb{R}^{L \\times d_k}$ The key embedding inputs. $\\mathbf{V} = \\mathbf{X}\\mathbf{W}^v \\in \\mathbb{R}^{L \\times d_v}$ The value embedding inputs. $\\mathbf{q}_i, \\mathbf{k}_i \\in \\mathbb{R}^{d_k}, \\mathbf{v}_i \\in \\mathbb{R}^{d_v}$ Row vectors in query, key, value matrices, $\\mathbf{Q}$, $\\mathbf{K}$ and $\\mathbf{V}$. $S_i$ A collection of key positions for the $i$-th query $\\mathbf{q}_i$ to attend to. $\\mathbf{A} \\in \\mathbb{R}^{L \\times L}$ The self-attention matrix between a input sequence of lenght $L$ and itself. $\\mathbf{A} = \\text{softmax}(\\mathbf{Q}\\mathbf{K}^\\top / \\sqrt{d_k})$. $a_{ij} \\in \\mathbf{A}$ The scalar attention score between query $\\mathbf{q}_i$ and key $\\mathbf{k}_j$. $\\mathbf{P} \\in \\mathbb{R}^{L \\times d}$ position encoding matrix, where the $i$-th row $\\mathbf{p}_i$ is the positional encoding for input $\\mathbf{x}_i$. Transformer Basics The Transformer (which will be referred to as \u201cvanilla Transformer\u201d to distinguish it from other enhanced versions; Vaswani, et al., 2017) model has an encoder-decoder architecture, as commonly used in many NMT models. Later simplified Transformer was shown to achieve great performance in language modeling tasks, like in encoder-only BERT or decoder-only GPT. ...\nDate: January 27, 2023 | Estimated Reading Time: 46 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n## Large Transformer Model Inference Optimization \n[Updated on 2023-01-24: add a small section on Distillation.] Large transformer models are mainstream nowadays, creating SoTA results for a variety of tasks. They are powerful but very expensive to train and use. The extremely high inference cost, in both time and memory, is a big bottleneck for adopting a powerful transformer for solving real-world tasks at scale. Why is it hard to run inference for large transformer models? Besides the increasing size of SoTA models, there are two main factors contributing to the inference challenge (Pope et al. 2022): ...\nDate: January 10, 2023 | Estimated Reading Time: 9 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)\n## Some Math behind Neural Tangent Kernel \nNeural networks are well known to be over-parameterized and can often easily fit data with near-zero training loss with decent generalization performance on test dataset. Although all these parameters are initialized at random, the optimization process can consistently lead to similarly good outcomes. And this is true even when the number of model parameters exceeds the number of training data points. Neural tangent kernel (NTK) (Jacot et al. 2018) is a kernel to explain the evolution of neural networks during training via gradient descent. It leads to great insights into why neural networks with enough width can consistently converge to a global minimum when trained to minimize an empirical loss. In the post, we will do a deep dive into the motivation and definition of NTK, as well as the proof of a deterministic convergence at different initializations of neural networks with infinite width by characterizing NTK in such a setting. ...\nDate: September 8, 2022 | Estimated Reading Time: 17 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2022-09-08-ntk/)\n## Generalized Visual Language Models \nProcessing images to generate text, such as image captioning and visual question-answering, has been studied for years. Traditionally such systems rely on an object detection network as a vision encoder to capture visual features and then produce text via a text decoder. Given a large amount of existing literature, in this post, I would like to only focus on one approach for solving vision language tasks, which is to extend pre-trained generalized language models to be capable of consuming visual signals. ...\nDate: June 9, 2022 | Estimated Reading Time: 25 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2022-06-09-vlm/)\n## Learning with not Enough Data Part 3: Data Generation \nHere comes the Part 3 on learning with not enough data (Previous: Part 1 and Part 2). Let\u2019s consider two approaches for generating synthetic data for training. Augmented data. Given a set of existing training samples, we can apply a variety of augmentation, distortion and transformation to derive new data points without losing the key attributes. We have covered a bunch of augmentation methods on text and images in a previous post on contrastive learning. For the sake of post completeness, I duplicate the section on data augmentation here with some edits. New data. Given few or even no data points, we can rely on powerful pretrained models to generate a number of new data points. This is especially true in recent years given the fast progress in large pretrained language models (LM). Few shot prompting is shown to be effective for LM to learn within context without extra training. Data Augmentation The goal of data augmentation is to modify the input format (e.g. text wording, visual appearance) while the semantic meaning stays unchanged. ...\nDate: April 15, 2022 | Estimated Reading Time: 28 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2022-04-15-data-gen/)\n## Learning with not Enough Data Part 2: Active Learning \nThis is part 2 of what to do when facing a limited amount of labeled data for supervised learning tasks. This time we will get some amount of human labeling work involved, but within a budget limit, and therefore we need to be smart when selecting which samples to label. ...\nDate: February 20, 2022 | Estimated Reading Time: 22 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2022-02-20-active-learning/)\n## Learning with not Enough Data Part 1: Semi-Supervised Learning \nWhen facing a limited amount of labeled data for supervised learning tasks, four approaches are commonly discussed. ...\nDate: December 5, 2021 | Estimated Reading Time: 26 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2021-12-05-semi-supervised/)\n## How to Train Really Large Models on Many GPUs? \n[Updated on 2022-03-13: add expert choice routing.] [Updated on 2022-06-10]: Greg and I wrote a shorted and upgraded version of this post, published on OpenAI Blog: \u201cTechniques for Training Large Neural Networks\u201d ...\nDate: September 24, 2021 | Estimated Reading Time: 21 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2021-09-25-train-large/)\n## What are Diffusion Models? \n[Updated on 2021-09-19: Highly recommend this blog post on score-based generative modeling by Yang Song (author of several key papers in the references)]. [Updated on 2022-08-27: Added classifier-free guidance, GLIDE, unCLIP and Imagen. [Updated on 2022-08-31: Added latent diffusion model. [Updated on 2024-04-13: Added progressive distillation, consistency models, and the Model Architecture section. ...\nDate: July 11, 2021 | Estimated Reading Time: 32 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)\n## Contrastive Representation Learning \nThe goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in self-supervised learning. ...\nDate: May 31, 2021 | Estimated Reading Time: 39 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2021-05-31-contrastive/)\n## Reducing Toxicity in Language Models \nLarge pretrained language models are trained over a sizable collection of online data. They unavoidably acquire certain toxic behavior and biases from the Internet. Pretrained language models are very powerful and have shown great success in many NLP tasks. However, to safely deploy them for practical real-world applications demands a strong safety control over the model generation process. ...\nDate: March 21, 2021 | Estimated Reading Time: 23 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/)\n## Controllable Neural Text Generation \n[Updated on 2021-02-01: Updated to version 2.0 with several work added and many typos fixed.] [Updated on 2021-05-26: Add P-tuning and Prompt Tuning in the \u201cprompt design\u201d section.] [Updated on 2021-09-19: Add \u201cunlikelihood training\u201d.] ...\nDate: January 2, 2021 | Estimated Reading Time: 42 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/)\n## How to Build an Open-Domain Question Answering System? \n[Updated on 2020-11-12: add an example on closed-book factual QA using OpenAI API (beta). A model that can answer any question with regard to factual knowledge can lead to many useful and practical applications, such as working as a chatbot or an AI assistant\ud83e\udd16. In this post, we will review several common approaches for building such an open-domain question answering system. ...\nDate: October 29, 2020 | Estimated Reading Time: 33 min | Author: Lilian Weng [](https://lilianweng.github.io/posts/2020-10-29-odqa/) [ \u00bb](https://lilianweng.github.io/page/2/) \u00a9 2025 [Lil'Log](https://lilianweng.github.io/) Powered by [Hugo](https://gohugo.io/) & [PaperMod](https://git.io/hugopapermod) [ ](https://lilianweng.github.io/#top \"Go to Top \\(Alt + G\\)\")\n",
    "answer": "```markdown\n# TL;DR Summary\n\nLilian Weng's blog covers key topics in AI, including:\n- **Reward Hacking**: Challenges in RL for language models.\n- **Extrinsic Hallucinations**: Need for factual outputs in LLMs.\n- **Diffusion Models**: Advancements in video generation.\n- **Adversarial Attacks**: Risks in LLM deployment.\n- **Prompt Engineering**: Techniques for model alignment.\n- **Data Quality**: Importance of high-quality human data for ML.\n\nExplore insights on LLMs, data generation, and model training.\n```"
  },
  {
    "instruction": "[![Ahead of AI](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)](https://magazine.sebastianraschka.com/)\n# [![Ahead of AI](https://substackcdn.com/image/fetch/e_trim:10:white/e_trim:10:transparent/h_72,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe553b9ca-3e8c-442b-969a-22556b86d1e5_448x212.png)](https://magazine.sebastianraschka.com/)\nSubscribeSign in\n#### Share this post\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1655aff8-c9bd-4a93-9e72-f1911359a667_1646x1090.jpeg)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Encoder And Decoder LLMs](https://substack.com/home/post/p-128842439?utm_campaign=post&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n![](https://substackcdn.com/image/fetch/w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg)\nDiscover more from Ahead of AI\nAhead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\nOver 95,000 subscribers\nSubscribe\nBy subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).\nAlready have an account? Sign in\n# Understanding Encoder And Decoder LLMs\n[![](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg)](https://substack.com/@rasbt)\n[Sebastian Raschka, PhD](https://substack.com/@rasbt)\nJun 17, 2023\n163\n#### Share this post\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1655aff8-c9bd-4a93-9e72-f1911359a667_1646x1090.jpeg)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Encoder And Decoder LLMs](https://substack.com/home/post/p-128842439?utm_campaign=post&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n[5](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comments)10\n[Share](javascript:void\\(0\\))\nSeveral people asked me to dive a bit deeper into large language model (LLM) jargon and explain some of the more technical terms we nowadays take for granted. This includes references to \"encoder-style\" and \"decoder-style\" LLMs. What do these terms mean? \nLet's get to it:  _**What are the differences between encoder- and decoder-based language transformers?**_\n## **Encoder- And Decoder-Style Transformers**\nFundamentally, both encoder- and decoder-style architectures use the same self-attention layers to encode word tokens. However, the main difference is that encoders are designed to learn embeddings that can be used for various predictive modeling tasks such as classification. In contrast, decoders are designed to generate new texts, for example, answering user queries.\n**The original transformer**\nThe original transformer architecture (_[Attention Is All You Need, 2017](https://arxiv.org/abs/1706.03762)_), which was developed for English-to-French and English-to-German language translation, utilized both an encoder and a decoder, as illustrated in the figure below.\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png)Illustration of the original transformer architecture proposed in [Attention Is All You Need, 2017](https://arxiv.org/abs/1706.03762)\nIn the figure above, the input text (that is, the sentences of the text that is to be translated) is first tokenized into individual word tokens, which are then encoded via an embedding layer before it enters the encoder part. Then, after adding a positional encoding vector to each embedded word, the embeddings go through a multi-head self-attention layer. The multi-head attention layer is followed by an \"Add & normalize\" step, which performs a layer normalization and adds the original embeddings via a skip connection (also known as a residual or shortcut connection). Finally, after entering a \"fully connected layer,\" which is a small multilayer perceptron consisting of two fully connected layers with a nonlinear activation function in between, the outputs are again added and normalized before they are passed to a multi-head self-attention layer of the decoder part.\nThe decoder part in the figure above has a similar overall structure as the encoder part. The key difference is that the inputs and outputs are different. The encoder receives the input text that is to be translated, and the decoder generates the translated text.\n**Encoders**\nThe encoder part in the original transformer, illustrated in the preceding figure, is responsible for understanding and extracting the relevant information from the input text. It then outputs a continuous representation (embedding) of the input text that is passed to the decoder. Finally, the decoder generates the translated text (target language) based on the continuous representation received from the encoder.\nOver the years, various encoder-only architectures have been developed based on the encoder module of the original transformer model outlined above. Notable examples include BERT (_[Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018](https://arxiv.org/abs/1810.04805)_) and RoBERTa (_[A Robustly Optimized BERT Pretraining Approach, 2018](https://arxiv.org/abs/1907.11692)_).\nBERT (**B** idirectional **E** ncoder **R** epresentations from **T** ransformers) is an encoder-only architecture based on the Transformer's encoder module. The BERT model is pretrained on a large text corpus using masked language modeling (illustrated in the figure below) and next-sentence prediction tasks.\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8fc5971-d59b-4d5a-bb2a-d8ac6d2aa23a_2022x1224.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8fc5971-d59b-4d5a-bb2a-d8ac6d2aa23a_2022x1224.png)Illustration of the masked language modeling pretraining objective used in BERT-style transformers.\nThe main idea behind masked language modeling is to mask (or replace) random word tokens in the input sequence and then train the model to predict the original masked tokens based on the surrounding context.\nNext to the masked language modeling pretraining task illustrated in the figure above, the next-sentence prediction task asks the model to predict whether the original document's sentence order of two randomly shuffled sentences is correct. For example, two sentences, in random order, are separated by the [SEP] token:\n  * [CLS] Toast is a simple yet delicious food [SEP] It\u2019s often served with butter, jam, or honey.\n  * [CLS] It\u2019s often served with butter, jam, or honey. [SEP] Toast is a simple yet delicious food.\n\n\nThe [CLS] token is a placeholder token for the model, prompting the model to return a _True_ or _False_ label indicating whether the sentences are in the correct order or not.\nThe masked language and next-sentence pretraining objectives (which are a form of self-supervised learning, as discussed in Chapter 2) allow BERT to learn rich contextual representations of the input texts, which can then be finetuned for various downstream tasks like sentiment analysis, question-answering, and named entity recognition.\nRoBERTa (**R** obustly **o** ptimized **BERT****a** pproach) is an optimized version of BERT. It maintains the same overall architecture as BERT but employs several training and optimization improvements, such as larger batch sizes, more training data, and eliminating the next-sentence prediction task. These changes resulted in RoBERTa achieving better performance on various natural language understanding tasks than BERT.\n**Decoders**\nComing back to the original transformer architecture outlined at the beginning of this section, the multi-head self-attention mechanism in the decoder is similar to the one in the encoder, but it is masked to prevent the model from attending to future positions, ensuring that the predictions for position _i_ can depend only on the known outputs at positions less than _i_. As illustrated in the figure below, the decoder generates the output word by word.\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c55ad5-cc86-4421-b0bc-bbd967409ca6_1162x1022.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3c55ad5-cc86-4421-b0bc-bbd967409ca6_1162x1022.png)Illustration of the next-sentence prediction task used in the original transformer.\nThis masking (shown explicitly in the figure above, although it happens internally in the decoder's multi-head self-attention mechanism) is essential to maintain the autoregressive property of the transformer model during training and inference. The autoregressive property ensures that the model generates output tokens one at a time and uses previously generated tokens as context for generating the next word token.\nOver the years, researchers have built upon the original encoder-decoder transformer architecture and developed several decoder-only models that have proven to be highly effective in various natural language processing tasks. The most notable models include the GPT family.\nThe GPT (**G** enerative **P** re-trained **T** ransformer) series are decoder-only models pretrained on large-scale unsupervised text data and finetuned for specific tasks such as text classification, sentiment analysis, question-answering, and summarization. The GPT models, including GPT-2, (_[GPT-3 Language Models are Few-Shot Learners, 2020](https://arxiv.org/abs/2005.14165)_), and the more recent GPT-4, have shown remarkable performance in various benchmarks and are currently the most popular architecture for natural language processing.\nOne of the most notable aspects of GPT models is their emergent properties. Emergent properties refer to the abilities and skills that a model develops due to its next-word prediction pretraining. Even though these models were only taught to predict the next word, the pretrained models are capable of text summarization, translation, question answering, classification, and more. Furthermore, these models can perform new tasks without updating the model parameters via in-context learning, which is discussed in more detail in Chapter 18.\n**Encoder-decoder hybrids**\nNext to the traditional encoder and decoder architectures, there have been advancements in the development of new encoder-decoder models that leverage the strengths of both components. These models often incorporate novel techniques, pre-training objectives, or architectural modifications to enhance their performance in various natural language processing tasks. Some notable examples of these new encoder-decoder models include \n  * BART (_[Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension, 2019](https://arxiv.org/abs/1910.13461)_) \n  * and T5 (_[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, 2019](https://arxiv.org/abs/1910.10683)_).\n\n\nEncoder-decoder models are typically used for natural language processing tasks that involve understanding input sequences and generating output sequences, often with different lengths and structures. They are particularly good at tasks where there is a complex mapping between the input and output sequences and where it is crucial to capture the relationships between the elements in both sequences. Some common use cases for encoder-decoder models include text translation and summarization.\n**Terminology and jargon**\nAll of these methods, encoder-only, decoder-only, and encoder-decoder models, are sequence-to-sequence models (often abbreviated as _seq2seq_). Note that while we refer to BERT-style methods as encoder-only, the description _encoder-only_ may be misleading since these methods also _decode_ the embeddings into output tokens or text during pretraining.\nIn other words, both encoder-only and decoder-only architectures are \"decoding.\" However, the encoder-only architectures, in contrast to decoder-only and encoder-decoder architectures, are not decoding in an autoregressive fashion. Autoregressive decoding refers to generating output sequences one token at a time, conditioning each token on the previously generated tokens. Encoder-only models do not generate coherent output sequences in this manner. Instead, they focus on understanding the input text and producing task-specific outputs, such as labels or token predictions.\n**Conclusion**\nIn brief, encoder-style models are popular for learning embeddings used in classification tasks, encoder-decoder-style models are used in generative tasks where the output heavily relies on the input (for example, translation and summarization), and decoder-only models are used for other types of generative tasks including Q&A. Since the first transformer architecture emerged, hundreds of encoder-only, decoder-only, and encoder-decoder hybrids have been developed, as summarized in the figure below.\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcce3c437-4b9c-4d15-947d-7c177c9518e5_4258x5745.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcce3c437-4b9c-4d15-947d-7c177c9518e5_4258x5745.png)An overview of _**some**_ of the most popular large language transformers organized by architecture type and developers.\nWhile encoder-only models gradually lost in popularity, decoder-only models like GPT exploded in popularity thanks to breakthrough in text generation via GPT-3, ChatGPT, and GPT-4. However, encoder-only models are still very useful for training predictive models based on text embeddings versus generating texts.\n_This magazine is a personal passion project that does not offer direct compensation. However, for those who wish to support me, please consider purchasing a copy of[one of my books](https://sebastianraschka.com/books). If you find them insightful and beneficial, please feel free to recommend them to your friends and colleagues._\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27b6d27d-1052-4a4f-a419-a69cdd36704a_1118x454.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27b6d27d-1052-4a4f-a419-a69cdd36704a_1118x454.png)[Machine Learning with PyTorch and Scikit-Learn](https://www.amazon.com/Machine-Learning-PyTorch-Scikit-Learn-scikit-learn-ebook-dp-B09NW48MR1/dp/B09NW48MR1/), [Machine Learning Q and AI](https://nostarch.com/machine-learning-and-ai-beyond-basics), and [Build a Large Language Model (from Scratch)](http://mng.bz/M96o)\n**Your support means a great deal! Thank you!**\n#### Subscribe to Ahead of AI\nBy Sebastian Raschka \u00b7 Hundreds of paid subscribers\nAhead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\nSubscribe\nBy subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64769218-5ab1-4442-b194-623564737424_1080x1080.jpeg)](https://substack.com/profile/96251662-kak)\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F55c2183f-7755-4692-8063-59df2751f834_800x800.png)](https://substack.com/profile/85853406-sairam-sundaresan)\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc59205ed-f7b0-4c31-8810-1097c56a778f_1200x1600.jpeg)](https://substack.com/profile/1134131-sid-kapur)\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc95711cf-0604-4af6-a3e0-47be3b58a2dd_1364x1364.jpeg)](https://substack.com/profile/147444258-lukasz-ostrowski)\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa635f57b-3633-43d2-bc80-950afcaa42cb_144x144.png)](https://substack.com/profile/108287441-gabriel)\n163 Likes\u2219\n[10 Restacks](https://substack.com/note/p-128842439/restacks?utm_source=substack&utm_content=facepile-restacks)\n163\n#### Share this post\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1655aff8-c9bd-4a93-9e72-f1911359a667_1646x1090.jpeg)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Encoder And Decoder LLMs](https://substack.com/home/post/p-128842439?utm_campaign=post&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n[5](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comments)10\n[Share](javascript:void\\(0\\))\n#### Discussion about this post\nCommentsRestacks\n![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png)\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4daf1168-7d63-41f1-a70a-972a9c563d33_144x144.png)](https://substack.com/profile/11904497-richard-hackathorn?utm_source=comment)\n[Richard Hackathorn](https://substack.com/profile/11904497-richard-hackathorn?utm_source=substack-feed-item)\n[Dec 31, 2023](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comment/46346150 \"Dec 31, 2023, 12:06 PM\")Edited\nLiked by Sebastian Raschka, PhD\nThis LLM encoder/decoder stuff messes with my mind! There is something fundamental here that I'm not getting. HELP... \ud83e\udd14 I have been fascinated with autoencoders, which take an example from feature space and ENCODE it into point in latent space and then DECODE it back into a reconstructed example in feature space, thus allowing a reconstruction loss to be calculated. [ref: Python ML 3Ed, Chap 17] \n1) Should LLM decoders be called 'generators' like in GANs? \n2) That single line that connects LLM encoder to its decoder... Is that the same data that one receives as an embedding from the LLM API? \n3) For a decoder-only LLM, is its input always an embedding vector? Or, where do the model weights come from?\n4) Is it possible to take an LLM embedding, reconstruct its initial input, and calculate the reconstruction loss? If true, this would enable us to map the fine (manifold) structures in these mysterious LLM latent spaces. Loved your old examples of putting/removing smiles on celebrity faces. Like to find a few hallucinations lurking in LLM latent spaces! \ud83d\ude2e\nExpand full comment\n[Like (2)](javascript:void\\(0\\))ReplyShare\n[1 reply by Sebastian Raschka, PhD](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comment/46346150)\n[![](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce196789-7b5a-436a-bd65-1078805f1ee6_420x420.png)](https://substack.com/profile/185804816-viswa-kumar?utm_source=comment)\n[Viswa Kumar](https://substack.com/profile/185804816-viswa-kumar?utm_source=substack-feed-item)\n[Nov 7](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comment/76035897 \"Nov 7, 2024, 10:48 AM\")\nLiked by Sebastian Raschka, PhD\nI agree the term encoder / decoder is overloaded since almost all architectures would essentially perform the encoding / decoding as a function. Engineers are not good at naming not only vars after all \ud83e\udd23\nExpand full comment\n[Like (1)](javascript:void\\(0\\))ReplyShare\n[3 more comments...](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder/comments)\nTopLatestDiscussions\n[Understanding Reasoning LLMs](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)\n[Methods and Strategies for Building and Refining Reasoning Models](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)\nFeb 5 \u2022\n[Sebastian Raschka, PhD](https://substack.com/@rasbt)\n833\n#### Share this post\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Reasoning LLMs](https://substack.com/home/post/p-156484949?utm_campaign=post&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n[31](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms/comments)[](javascript:void\\(0\\))\n![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png)\n[Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models)\n[A Cross-Section of the Most Relevant Literature To Get Up to Speed](https://magazine.sebastianraschka.com/p/understanding-large-language-models)\nApr 16, 2023 \u2022\n[Sebastian Raschka, PhD](https://substack.com/@rasbt)\n886\n#### Share this post\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding Large Language Models](https://substack.com/home/post/p-115060492?utm_campaign=post&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n[54](https://magazine.sebastianraschka.com/p/understanding-large-language-models/comments)[](javascript:void\\(0\\))\n![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png)\n[Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)\n[This article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama.](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)\nJan 14, 2024\n341\n#### Share this post\n[![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69bfee26-ea3b-42a6-8a1a-6b8187852082_738x564.png)![Ahead of AI](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AIUnderstanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs](https://substack.com/home/post/p-140464659?utm_campaign=post&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n[41](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention/comments)[](javascript:void\\(0\\))\n![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69bfee26-ea3b-42a6-8a1a-6b8187852082_738x564.png)\nSee all\nReady for more?\nSubscribe\n\u00a9 2025 Sebastian Raschka\n[Privacy](https://substack.com/privacy) \u2219 [Terms](https://substack.com/tos) \u2219 [Collection notice](https://substack.com/ccpa#personal-data-collected)\n[ Start Writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)\n[Substack](https://substack.com) is the home for great culture\n#### Share\n[](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder?utm_campaign=unknown&utm_medium=web)\nCopy linkFacebookEmailNotesMore\n## Create your profile\nName (Required)HandleBioEmail (Required)\nSubscribe to the newsletter\nundefined subscriptions will be displayed on your profile (edit)\nSkip for now\nI agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).\nSave & Post Comment\n## Only paid subscribers can comment on this post\n[Subscribe](https://magazine.sebastianraschka.com/subscribe?simple=true&next=https%3A%2F%2Fmagazine.sebastianraschka.com%2Fp%2Funderstanding-encoder-and-decoder&utm_source=paywall&utm_medium=web&utm_content=128842439)\n[Already a paid subscriber? **Sign in**](https://substack.com/sign-in?redirect=%2Fp%2Funderstanding-encoder-and-decoder&for_pub=sebastianraschka&change_user=false)\n#### Check your email\nFor your security, we need to re-authenticate you.\nClick the link we sent to , or [click here to sign in](https://substack.com/sign-in?redirect=%2Fp%2Funderstanding-encoder-and-decoder&for_pub=sebastianraschka&with_password=true).\n",
    "answer": "```markdown\n# TL;DR: Understanding Encoder and Decoder LLMs\n\nEncoder and decoder LLMs serve different purposes: encoders learn embeddings for tasks like classification, while decoders generate text. Notable models include BERT (encoder) and GPT (decoder). Hybrid models like BART and T5 leverage both architectures for tasks like translation and summarization.\n```"
  },
  {
    "instruction": "[Skip to content](https://towardsdatascience.com/how-to-build-a-graph-rag-app-b323fc33ba06/#wp--skip-link--target)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nThe world\u2019s leading publication for data science, AI, and ML professionals.\nSign in\nSign out\n[Contributor Portal](https://contributor.insightmediagroup.io/)\n  * [Latest](https://towardsdatascience.com/latest/)\n  * [Editor\u2019s Picks](https://towardsdatascience.com/tag/editors-pick/)\n  * [Deep Dives](https://towardsdatascience.com/tag/deep-dives/)\n  * [Contribute](https://towardsdatascience.com/questions-96667b06af5/)\n  * [Newsletter](https://newsletter.towardsdatascience.com/subscription-to-the-newsletter)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\n\n\nToggle Mobile Navigation\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [X](https://x.com/TDataScience)\n\n\nToggle Search\nSearch\n# How to Build a Graph RAG App\nUsing knowledge graphs and AI to retrieve, filter, and summarize medical journal articles \n[Steve Hedden](https://towardsdatascience.com/author/steve-hedden/)\nDec 30, 2024\n30 min read\nShare \n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_fsYI8Riyq2lAsH8wTCuKhw.webp)Image by Author\n_The accompanying code for the app and notebook are_[ _here._](https://github.com/SteveHedden/kg_llm/tree/main/graphRAGapp)\nKnowledge graphs (KGs) and Large Language Models (LLMs) are a match made in heaven. My [previous](https://medium.com/towards-data-science/how-to-implement-knowledge-graphs-and-large-language-models-llms-together-at-the-enterprise-level-cf2835475c47) [posts](https://medium.com/towards-data-science/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759) discuss the complementarities of these two technologies in more detail but the short version is, \u201csome of the main weaknesses of LLMs, that they are black-box models and struggle with factual knowledge, are some of KGs\u2019 greatest strengths. KGs are, essentially, collections of facts, and they are fully interpretable.\u201d\nThis article is all about building a simple Graph RAG app. What is RAG? RAG, or Retrieval-Augmented Generation, is about **retrieving** relevant information to **augment** a prompt that is sent to an LLM, which **generates** a response. Graph RAG is RAG that uses a knowledge graph as part of the retrieval portion. If you\u2019ve never heard of Graph RAG, or want a refresher, I\u2019d watch [this video](https://www.youtube.com/watch?v=knDDGYHnnSI).\nThe basic idea is that, rather than sending your prompt directly to an LLM, which was not trained on your data, you can supplement your prompt with the relevant information needed for the LLM to answer your prompt accurately. The example I use often is copying a job description and my resume into ChatGPT to write a cover letter. The LLM is able to provide a much more relevant response to my prompt, \u2018write me a cover letter,\u2019 if I give it my resume and the description of the job I am applying for. Since knowledge graphs are built to store knowledge, they are a perfect way to store internal data and supplement LLM prompts with additional context, improving the accuracy and contextual understanding of the responses.\nThis technology has many, many, applications such [customer service bots](https://arxiv.org/pdf/2404.17723), [drug](https://academic.oup.com/bioinformatics/article/40/6/btae353/7687047) [discovery](https://blog.biostrand.ai/integrating-knowledge-graphs-and-large-language-models-for-next-generation-drug-discovery), [automated regulatory report generation in life sciences](https://www.weave.bio/), [talent acquisition and management for HR](https://beamery.com/resources/news/beamery-announces-talentgpt-the-world-s-first-generative-ai-for-hr), [legal research and writing](https://legal.thomsonreuters.com/blog/retrieval-augmented-generation-in-legal-tech/), and [wealth advisor assistants](https://www.cnbc.com/amp/2023/03/14/morgan-stanley-testing-openai-powered-chatbot-for-its-financial-advisors.html). Because of the wide applicability and the potential to improve the performance of LLM tools, Graph RAG (that\u2019s the term I\u2019ll use here) has been blowing up in popularity. Here is a graph showing interest over time based on Google searches.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_vuRkEjW9AcQPld5PsqWjkg.png)Source: <https://trends.google.com/>\nGraph RAG has experienced a surge in search interest, even surpassing terms like knowledge graphs and retrieval-augmented generation. Note that Google Trends measures  _relative_ search interest, not absolute number of searches. The spike in July 2024 for searches of Graph RAG coincides with the week Microsoft [announced](https://www.microsoft.com/en-us/research/blog/graphrag-new-tool-for-complex-data-discovery-now-on-github/) that their GraphRAG application would be available on [GitHub](https://github.com/microsoft/graphrag).\nThe excitement around Graph RAG is broader than just Microsoft, however. Samsung acquired RDFox, a knowledge graph company, in July of 2024. The [article announcing that acquisition](https://news.samsung.com/global/samsung-electronics-announces-acquisition-of-oxford-semantic-technologies-uk-based-knowledge-graph-startup) did not mention Graph RAG explicitly, but in [this article in Forbes](https://www.forbes.com/sites/zakdoffman/2024/11/09/samsung-confirms-new-upgrade-choice-millions-of-galaxy-owners-must-now-decide/) published in November 2024, a Samsung spokesperson stated, \u201cWe plan to develop knowledge graph technology, one of the main technologies of personalized AI, and organically connect with generated AI to support user-specific services.\u201d\nIn October 2024, Ontotext, a leading graph database company, and Semantic Web company, the maker of PoolParty, a knowledge graph curation platform, merged to form [Graphwise](https://graphwise.ai/). According to [the press release](https://www.prnewswire.com/news-releases/semantic-web-company-and-ontotext-merge-to-create-knowledge-graph-and-ai-powerhouse-graphwise-302283427.html?utm_source=chatgpt.com), the merger aims to \u201cdemocratize the evolution of Graph RAG as a category.\u201d\nWhile some of the buzz around Graph RAG may come from the broader excitement surrounding chatbots and generative AI, it reflects a genuine evolution in how knowledge graphs are being applied to solve complex, real-world problems. One example is that LinkedIn [applied Graph RAG](https://arxiv.org/pdf/2404.17723) to improve their customer service technical support. Because the tool was able to retrieve the relevant data (like previously solved similar tickets or questions) to feed the LLM, the responses were more accurate and the mean resolution time dropped from 40 hours to 15 hours.\nThis post will go through the construction of a pretty simple, but I think illustrative, example of how Graph RAG can work in practice. The end result is an app that a non-technical user can interact with. Like my last post, I will use a dataset consisting of medical journal articles from PubMed. The idea is that this is an app that someone in the medical field could use to do literature review. The same principles can be applied to many use cases however, which is why Graph RAG is so exciting.\nThe structure of the app, along with this post is as follows:\nStep zero is preparing the data. I will explain the details below but the overall goal is to vectorize the raw data and, separately, turn it into an RDF graph. As long as we keep URIs tied to the articles before we vectorize, we can navigate across a graph of articles and a vector space of articles. Then, we can:\n  1. **Search Articles:** use the power of the vector database to do an initial search of relevant articles given a search term. I will use vector similarity to retrieve articles with the most similar vectors to that of the search term.\n  2. **Refine Terms:** explore the [Medical Subject Headings (MeSH) biomedical vocabulary](https://id.nlm.nih.gov/mesh/) to select terms to use to filter the articles from step 1. This controlled vocabulary contains medical terms, alternative names, narrower concepts, and many other properties and relationships.\n  3. **Filter & Summarize: **use the MeSH terms to filter the articles to avoid \u2018context poisoning\u2019. Then send the remaining articles to an LLM along with an additional prompt like, \u201csummarize in bullets.\u201d\n\n\nSome notes on this app and tutorial before we get started:\n  * This set-up uses knowledge graphs exclusively for metadata. This is only possible because each article in my dataset has already been tagged with terms that are part of a rich controlled vocabulary. I am using the graph for structure and semantics and the vector database for similarity-based retrieval, ensuring each technology is used for what it does best. Vector similarity can tell us \u201cesophageal cancer\u201d is semantically similar to \u201cmouth cancer\u201d, but knowledge graphs can tell us the details of the relationship between \u201cesophageal cancer\u201d and \u201cmouth cancer.\u201d\n  * The data I used for this app is a collection of medical journal articles from PubMed (more on the data below). I chose this dataset because it is structured (tabular) but also contains text in the form of abstracts for each article, and because it is already tagged with topical terms that are aligned with a well-established controlled vocabulary (MeSH). Because these are medical articles, I have called this app \u2018Graph RAG for Medicine.\u2019 But this same structure can be applied to any domain and is not specific to the medical field.\n  * What I hope this tutorial and app demonstrate is that you can improve the results of your RAG application in terms of accuracy and explainability by incorporating a knowledge graph into the retrieval step. I will show how KGs can improve the accuracy of RAG applications in two ways: by giving the user a way of filtering the context to ensure the LLM is only being fed the most relevant information; and by using domain specific controlled vocabularies with dense relationships that are maintained and curated by domain experts to do the filtering.\n  * What this tutorial and app don\u2019t directly showcase are two other significant ways KGs can enhance RAG applications: governance, access control, and regulatory compliance; and efficiency and scalability. For governance, KGs can do more than filter content for relevancy to improve accuracy \u2014 they can enforce data governance policies. For instance, if a user lacks permission to access certain content, that content can be excluded from their RAG pipeline. On the efficiency and scalability side, KGs can help ensure RAG applications don\u2019t die on the shelf. While it\u2019s easy to create an impressive one-off RAG app (that\u2019s literally the purpose of this tutorial), many companies struggle with a proliferation of disconnected POCs that lack a cohesive framework, structure, or platform. That means many of those apps are not going to survive long. A metadata layer powered by KGs can break down data silos, providing the foundation needed to build, scale, and maintain RAG applications effectively. Using a rich controlled vocabulary like MeSH for the metadata tags on these articles is a way of ensuring this Graph RAG app can be integrated with other systems and reducing the risk that it becomes a silo.\n\n\n# Step 0: Prepare the data\n_The code to prepare the data is in_[ _this_](https://github.com/SteveHedden/kg_llm/blob/main/graphRAGapp/VectorVsKG_updated.ipynb) _notebook._\nAs mentioned, I\u2019ve again decided to use [this](https://www.kaggle.com/datasets/owaiskhan9654/pubmed-multilabel-text-classification) dataset of 50,000 research articles from the PubMed repository (License [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)). This dataset contains the title of the articles, their abstracts, as well as a field for metadata tags. These tags are from the Medical Subject Headings (MeSH) controlled vocabulary thesaurus. The PubMed articles are really just metadata on the articles \u2014 there are abstracts for each article but we don\u2019t have the full text. The data is already in tabular format and tagged with MeSH terms.\nWe can vectorize this tabular dataset directly. We could turn it into a graph (RDF) before we vectorize, but I didn\u2019t do that for this app and I don\u2019t know that it would help the final results for this kind of data. The most important thing about vectorizing the raw data is that we add [Unique Resource Identifiers](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier) (URIs) to each article first. A URI is a unique ID for navigating RDF data and it is necessary for us to go back and forth between vectors and entities in our graph. Additionally, we will create a separate collection in our vector database for the MeSH terms. This will allow the user to search for relevant terms without having prior knowledge of this controlled vocabulary. Below is a diagram of what we are doing to prepare our data.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_RiYNNyxGIEGClobHedTnuQ.png)Image by Author\nWe have two collections in our vector database to query: articles and terms. We also have the data represented as a graph in RDF format. Since MeSH has an API, I am just going to query the API directly to get alternative names and narrower concepts for terms.\n## Vectorize data in Weaviate\nFirst import the required packages and set up the Weaviate client:\n```\nimport weaviate<br>from weaviate.util import generate_uuid5<br>from weaviate.classes.init import Auth<br>import os<br>import json<br>import pandas as pd<br><br>client = weaviate.connect_to_weaviate_cloud(<br>  cluster_url=\"XXX\", # Replace with your Weaviate Cloud URL<br>  auth_credentials=Auth.api_key(\"XXX\"), # Replace with your Weaviate Cloud key<br>  headers={'X-OpenAI-Api-key': \"XXX\"} # Replace with your OpenAI API key<br>)\n```\n\nRead in the PubMed journal articles. I am using [Databricks](https://www.databricks.com/) to run this notebook so you may need to change this, depending on where you run it. The goal here is just to get the data into a pandas DataFrame.\n```\ndf = spark.sql(\"SELECT * FROM workspace.default.pub_med_multi_label_text_classification_dataset_processed\").toPandas()\n```\n\nIf you\u2019re running this locally, just do:\n```\ndf = pd.read_csv(\"PubMed Multi Label Text Classification Dataset Processed.csv\")\n```\n\nThen clean the data up a bit:\n```\nimport numpy as np<br># Replace infinity values with NaN and then fill NaN values<br>df.replace([np.inf, -np.inf], np.nan, inplace=True)<br>df.fillna('', inplace=True)<br><br># Convert columns to string type<br>df['Title'] = df['Title'].astype(str)<br>df['abstractText'] = df['abstractText'].astype(str)<br>df['meshMajor'] = df['meshMajor'].astype(str)\n```\n\nNow we need to create a URI for each article and add that in as a new column. This is important because the URI is the way we can connect the vector representation of an article with the knowledge graph representation of the article.\n```\nimport urllib.parse<br>from rdflib import Graph, RDF, RDFS, Namespace, URIRef, Literal<br><br><br># Function to create a valid URI<br>def create_valid_uri(base_uri, text):<br>  if pd.isna(text):<br>    return None<br>  # Encode text to be used in URI<br>  sanitized_text = urllib.parse.quote(text.strip().replace(' ', '_').replace('\"', '').replace('<', '').replace('>', '').replace(\"'\", \"_\"))<br>  return URIRef(f\"{base_uri}/{sanitized_text}\")<br><br><br># Function to create a valid URI for Articles<br>def create_article_uri(title, base_namespace=\"http://example.org/article/\"):<br>  \"\"\"<br>  Creates a URI for an article by replacing non-word characters with underscores and URL-encoding.<br><br>  Args:<br>    title (str): The title of the article.<br>    base_namespace (str): The base namespace for the article URI.<br><br>  Returns:<br>    URIRef: The formatted article URI.<br>  \"\"\"<br>  if pd.isna(title):<br>    return None<br>  # Replace non-word characters with underscores<br>  sanitized_title = re.sub(r'\\W+', '_', title.strip())<br>  # Condense multiple underscores into a single underscore<br>  sanitized_title = re.sub(r'_+', '_', sanitized_title)<br>  # URL-encode the term<br>  encoded_title = quote(sanitized_title)<br>  # Concatenate with base_namespace without adding underscores<br>  uri = f\"{base_namespace}{encoded_title}\"<br>  return URIRef(uri)<br><br># Add a new column to the DataFrame for the article URIs<br>df['Article_URI'] = df['Title'].apply(lambda title: create_valid_uri(\"http://example.org/article\", title))\n```\n\nWe also want to create a DataFrame of all of the MeSH terms that are used to tag the articles. This will be helpful later when we want to search for similar MeSH terms.\n```\n# Function to clean and parse MeSH terms<br>def parse_mesh_terms(mesh_list):<br>  if pd.isna(mesh_list):<br>    return []<br>  return [<br>    term.strip().replace(' ', '_')<br>    for term in mesh_list.strip(\"[]'\").split(',')<br>  ]<br><br># Function to create a valid URI for MeSH terms<br>def create_valid_uri(base_uri, text):<br>  if pd.isna(text):<br>    return None<br>  sanitized_text = urllib.parse.quote(<br>    text.strip()<br>    .replace(' ', '_')<br>    .replace('\"', '')<br>    .replace('<', '')<br>    .replace('>', '')<br>    .replace(\"'\", \"_\")<br>  )<br>  return f\"{base_uri}/{sanitized_text}\"<br><br># Extract and process all MeSH terms<br>all_mesh_terms = []<br>for mesh_list in df[\"meshMajor\"]:<br>  all_mesh_terms.extend(parse_mesh_terms(mesh_list))<br><br># Deduplicate terms<br>unique_mesh_terms = list(set(all_mesh_terms))<br><br># Create a DataFrame of MeSH terms and their URIs<br>mesh_df = pd.DataFrame({<br>  \"meshTerm\": unique_mesh_terms,<br>  \"URI\": [create_valid_uri(\"http://example.org/mesh\", term) for term in unique_mesh_terms]<br>})<br><br># Display the DataFrame<br>print(mesh_df)\n```\n\nVectorize the articles DataFrame:\n```\nfrom weaviate.classes.config import Configure<br><br><br>#define the collection<br>articles = client.collections.create(<br>  name = \"Article\",<br>  vectorizer_config=Configure.Vectorizer.text2vec_openai(), # If set to \"none\" you must always provide vectors yourself. Could be any other \"text2vec-*\" also.<br>  generative_config=Configure.Generative.openai(), # Ensure the `generative-openai` module is used for generative queries<br>)<br><br>#add ojects<br>articles = client.collections.get(\"Article\")<br><br>with articles.batch.dynamic() as batch:<br>  for index, row in df.iterrows():<br>    batch.add_object({<br>      \"title\": row[\"Title\"],<br>      \"abstractText\": row[\"abstractText\"],<br>      \"Article_URI\": row[\"Article_URI\"],<br>      \"meshMajor\": row[\"meshMajor\"],<br>    })\n```\n\nNow vectorize the MeSH terms:\n```\n#define the collection<br>terms = client.collections.create(<br>  name = \"term\",<br>  vectorizer_config=Configure.Vectorizer.text2vec_openai(), # If set to \"none\" you must always provide vectors yourself. Could be any other \"text2vec-*\" also.<br>  generative_config=Configure.Generative.openai(), # Ensure the `generative-openai` module is used for generative queries<br>)<br><br>#add ojects<br>terms = client.collections.get(\"term\")<br><br>with terms.batch.dynamic() as batch:<br>  for index, row in mesh_df.iterrows():<br>    batch.add_object({<br>      \"meshTerm\": row[\"meshTerm\"],<br>      \"URI\": row[\"URI\"],<br>    })\n```\n\nYou can, at this point, run semantic search, similarity search, and RAG directly against the vectorized dataset. I won\u2019t go through all of that here but you can look at the code in my [accompanying notebook](https://github.com/SteveHedden/kg_llm/tree/main/graphRAGapp) to do that.\n## Turn data into a knowledge graph\nI am just using the same code we used in the [last post](https://medium.com/towards-data-science/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759) to do this. We are basically turning every row in the data into an \u201cArticle\u201d entity in our KG. Then we are giving each of these articles properties for title, abstract, and MeSH terms. We are also turning every MeSH term into an entity as well. This code also adds random dates to each article for a property called date published and a random number between 1 and 10 to a property called access. We won\u2019t use those properties in this demo. Below is a visual representation of the graph we are creating from the data.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_R1NIm9fLYdKFTsXzZmH33w.png)\nHere is how to iterate through the DataFrame and turn it into RDF data:\n```\nfrom rdflib import Graph, RDF, RDFS, Namespace, URIRef, Literal<br>from rdflib.namespace import SKOS, XSD<br>import pandas as pd<br>import urllib.parse<br>import random<br>from datetime import datetime, timedelta<br>import re<br>from urllib.parse import quote<br><br># --- Initialization ---<br>g = Graph()<br><br># Define namespaces<br>schema = Namespace('http://schema.org/')<br>ex = Namespace('http://example.org/')<br>prefixes = {<br>  'schema': schema,<br>  'ex': ex,<br>  'skos': SKOS,<br>  'xsd': XSD<br>}<br>for p, ns in prefixes.items():<br>  g.bind(p, ns)<br><br># Define classes and properties<br>Article = URIRef(ex.Article)<br>MeSHTerm = URIRef(ex.MeSHTerm)<br>g.add((Article, RDF.type, RDFS.Class))<br>g.add((MeSHTerm, RDF.type, RDFS.Class))<br><br>title = URIRef(schema.name)<br>abstract = URIRef(schema.description)<br>date_published = URIRef(schema.datePublished)<br>access = URIRef(ex.access)<br><br>g.add((title, RDF.type, RDF.Property))<br>g.add((abstract, RDF.type, RDF.Property))<br>g.add((date_published, RDF.type, RDF.Property))<br>g.add((access, RDF.type, RDF.Property))<br><br># Function to clean and parse MeSH terms<br>def parse_mesh_terms(mesh_list):<br>  if pd.isna(mesh_list):<br>    return []<br>  return [term.strip() for term in mesh_list.strip(\"[]'\").split(',')]<br><br># Enhanced convert_to_uri function<br>def convert_to_uri(term, base_namespace=\"http://example.org/mesh/\"):<br>  \"\"\"<br>  Converts a MeSH term into a standardized URI by replacing spaces and special characters with underscores,<br>  ensuring it starts and ends with a single underscore, and URL-encoding the term.<br><br>  Args:<br>    term (str): The MeSH term to convert.<br>    base_namespace (str): The base namespace for the URI.<br><br>  Returns:<br>    URIRef: The formatted URI.<br>  \"\"\"<br>  if pd.isna(term):<br>    return None # Handle NaN or None terms gracefully<br>  <br>  # Step 1: Strip existing leading and trailing non-word characters (including underscores)<br>  stripped_term = re.sub(r'^\\W+|\\W+$', '', term)<br>  <br>  # Step 2: Replace non-word characters with underscores (one or more)<br>  formatted_term = re.sub(r'\\W+', '_', stripped_term)<br>  <br>  # Step 3: Replace multiple consecutive underscores with a single underscore<br>  formatted_term = re.sub(r'_+', '_', formatted_term)<br>  <br>  # Step 4: URL-encode the term to handle any remaining special characters<br>  encoded_term = quote(formatted_term)<br>  <br>  # Step 5: Add single leading and trailing underscores<br>  term_with_underscores = f\"_{encoded_term}_\"<br>  <br>  # Step 6: Concatenate with base_namespace without adding an extra underscore<br>  uri = f\"{base_namespace}{term_with_underscores}\"<br><br>  return URIRef(uri)<br><br># Function to generate a random date within the last 5 years<br>def generate_random_date():<br>  start_date = datetime.now() - timedelta(days=5*365)<br>  random_days = random.randint(0, 5*365)<br>  return start_date + timedelta(days=random_days)<br><br># Function to generate a random access value between 1 and 10<br>def generate_random_access():<br>  return random.randint(1, 10)<br><br># Function to create a valid URI for Articles<br>def create_article_uri(title, base_namespace=\"http://example.org/article\"):<br>  \"\"\"<br>  Creates a URI for an article by replacing non-word characters with underscores and URL-encoding.<br><br>  Args:<br>    title (str): The title of the article.<br>    base_namespace (str): The base namespace for the article URI.<br><br>  Returns:<br>    URIRef: The formatted article URI.<br>  \"\"\"<br>  if pd.isna(title):<br>    return None<br>  # Encode text to be used in URI<br>  sanitized_text = urllib.parse.quote(title.strip().replace(' ', '_').replace('\"', '').replace('<', '').replace('>', '').replace(\"'\", \"_\"))<br>  return URIRef(f\"{base_namespace}/{sanitized_text}\")<br><br># Loop through each row in the DataFrame and create RDF triples<br>for index, row in df.iterrows():<br>  article_uri = create_article_uri(row['Title'])<br>  if article_uri is None:<br>    continue<br>  <br>  # Add Article instance<br>  g.add((article_uri, RDF.type, Article))<br>  g.add((article_uri, title, Literal(row['Title'], datatype=XSD.string)))<br>  g.add((article_uri, abstract, Literal(row['abstractText'], datatype=XSD.string)))<br>  <br>  # Add random datePublished and access<br>  random_date = generate_random_date()<br>  random_access = generate_random_access()<br>  g.add((article_uri, date_published, Literal(random_date.date(), datatype=XSD.date)))<br>  g.add((article_uri, access, Literal(random_access, datatype=XSD.integer)))<br>  <br>  # Add MeSH Terms<br>  mesh_terms = parse_mesh_terms(row['meshMajor'])<br>  for term in mesh_terms:<br>    term_uri = convert_to_uri(term, base_namespace=\"http://example.org/mesh/\")<br>    if term_uri is None:<br>      continue<br>    <br>    # Add MeSH Term instance<br>    g.add((term_uri, RDF.type, MeSHTerm))<br>    g.add((term_uri, RDFS.label, Literal(term.replace('_', ' '), datatype=XSD.string)))<br>    <br>    # Link Article to MeSH Term<br>    g.add((article_uri, schema.about, term_uri))<br><br># Path to save the file<br>file_path = \"/Workspace/PubMedGraph.ttl\"<br><br># Save the file<br>g.serialize(destination=file_path, format='turtle')<br><br>print(f\"File saved at {file_path}\")\n```\n\nOK, so now we have a vectorized version of the data, and a graph (RDF) version of the data. Each vector has a URI associated with it, which corresponds to an entity in the KG, so we can go back and forth between the data formats.\n# Build an app\nI decided to use [Streamlit](https://streamlit.io/) to build the interface for this graph RAG app. Similar to the last blog post, I have kept the user flow the same.\n  1. **Search Articles:** First, the user searches for articles using a search term. This relies exclusively on the vector database. The user\u2019s search term(s) is sent to the vector database and the ten articles nearest the term in vector space are returned.\n  2. **Refine Terms:** Second, the user decides the MeSH terms to use to filter the returned results. Since we also vectorized the MeSH terms, we can have the user enter a natural language prompt to get the most relevant MeSH terms. Then, we allow the user to expand these terms to see their alternative names and narrower concepts. The user can select as many terms as they want for their filter criteria.\n  3. **Filter & Summarize: **Third, the user applies the selected terms as filters to the original ten journal articles. We can do this since the PubMed articles are tagged with MeSH terms. Finally, we let the user enter an additional prompt to send to the LLM along with the filtered journal articles. This is the generative step of the RAG app.\n\n\nLet\u2019s go through these steps one at a time. You can see the full app and code on my GitHub, but here is the structure:\n```\n-- app.py (a python file that drives the app and calls other functions as needed)<br>-- query_functions (a folder containing python files with queries)<br> -- rdf_queries.py (python file with RDF queries)<br> -- weaviate_queries.py (python file containing weaviate queries)<br>-- PubMedGraph.ttl (the pubmed data in RDF format, stored as a ttl file)\n```\n\n## Search Articles\nFirst, want to do is implement Weaviate\u2019s [vector similarity search](https://weaviate.io/developers/weaviate/search/similarity). Since our articles are vectorized, we can send a search term to the vector database and get similar articles back.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_SkbzqnwLMiDoxavXhSoxyg.png)Image by Author\nThe main function that searches for relevant journal articles in the vector database is in app.py:\n```\n# --- TAB 1: Search Articles ---<br>with tab_search:<br>  st.header(\"Search Articles (Vector Query)\")<br>  query_text = st.text_input(\"Enter your vector search term (e.g., Mouth Neoplasms):\", key=\"vector_search\")<br><br>  if st.button(\"Search Articles\", key=\"search_articles_btn\"):<br>    try:<br>      client = initialize_weaviate_client()<br>      article_results = query_weaviate_articles(client, query_text)<br><br>      # Extract URIs here<br>      article_uris = [<br>        result[\"properties\"].get(\"article_URI\")<br>        for result in article_results<br>        if result[\"properties\"].get(\"article_URI\")<br>      ]<br><br>      # Store article_uris in the session state<br>      st.session_state.article_uris = article_uris<br><br>      st.session_state.article_results = [<br>        {<br>          \"Title\": result[\"properties\"].get(\"title\", \"N/A\"),<br>          \"Abstract\": (result[\"properties\"].get(\"abstractText\", \"N/A\")[:100] + \"...\"),<br>          \"Distance\": result[\"distance\"],<br>          \"MeSH Terms\": \", \".join(<br>            ast.literal_eval(result[\"properties\"].get(\"meshMajor\", \"[]\"))<br>            if result[\"properties\"].get(\"meshMajor\") else []<br>          ),<br><br>        }<br>        for result in article_results<br>      ]<br>      client.close()<br>    except Exception as e:<br>      st.error(f\"Error during article search: {e}\")<br><br>  if st.session_state.article_results:<br>    st.write(\"**Search Results for Articles:**\")<br>    st.table(st.session_state.article_results)<br>  else:<br>    st.write(\"No articles found yet.\")\n```\n\nThis function uses the queries stored in weaviate_queries to establish the Weaviate client (initialize_weaviate_client) and search for articles (query_weaviate_articles). Then we display the returned articles in a table, along with their abstracts, distance (how close they are to the search term), and the MeSH terms that they are tagged with.\nThe function to query Weaviate in weaviate_queries.py looks like this:\n```\n# Function to query Weaviate for Articles<br>def query_weaviate_articles(client, query_text, limit=10):<br>  # Perform vector search on Article collection<br>  response = client.collections.get(\"Article\").query.near_text(<br>    query=query_text,<br>    limit=limit,<br>    return_metadata=MetadataQuery(distance=True)<br>  )<br><br>  # Parse response<br>  results = []<br>  for obj in response.objects:<br>    results.append({<br>      \"uuid\": obj.uuid,<br>      \"properties\": obj.properties,<br>      \"distance\": obj.metadata.distance,<br>    })<br>  return results\n```\n\nAs you can see, I put a limit of ten results here just to make it simpler, but you can change that. This is just using vector similarity search in Weaviate to return relevant results.\nThe end result in the app looks like this:\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_SghQV-iph3ZQftZiNC6fNg.png)Image by Author\nAs a demo, I will search the term \u201ctreatments for mouth cancer\u201d. As you can see, 10 articles are returned, mostly relevant. This demonstrates both the strengths and weaknesses of vector based retrieval.\nThe strength is that we can build a semantic search functionality on our data with minimal effort. As you can see above, all we did was set up the client and send the data to a vector database. Once our data has been vectorized, we can do semantic searches, similarity searches, and even RAG. I have put some of that in the notebook accompanying this post, but there\u2019s a lot more in Weaviate\u2019s [official docs](https://weaviate.io/developers/weaviate).\nThe weakness of vector based retrieval, as I mentioned above are that they are black-box and struggle with factual knowledge. In our example, it looks like most of the articles are about some kind of treatment or therapy for some kind of cancer. Some of the articles are about mouth cancer specifically, some are about a sub-type of mouth cancer like gingival cancer (cancer of the gums), and palatal cancer (cancer of the palate). But there are also articles about nasopharyngeal cancer (cancer of the upper throat), mandibular cancer (cancer of the jaw), and esophageal cancer (cancer of the esophagus). None of these (upper throat, jaw, or esophagus) are considered mouth cancer. It is understandable why an article about a specific cancer radiation therapy for nasopharyngeal neoplasms would be considered similar to the prompt \u201ctreatments for mouth cancer\u201d but it may not be relevant if you are only looking for treatments for mouth cancer. If we were to plug these ten articles directly into our prompt to the LLM and ask it to \u201csummarize the different treatment options,\u201d we would be getting incorrect information.\nThe purpose of RAG is to give an LLM a very specific set of additional information to better answer your question \u2014 if that information is incorrect or irrelevant, it can lead to misleading responses from the LLM. This is often referred to as \u201ccontext poisoning\u201d. What is especially dangerous about context poisoning is that the response isn\u2019t necessarily factually inaccurate (the LLM may accurately summarize the treatment options we feed it), and it isn\u2019t necessarily based on an inaccurate piece of data (presumably the journal articles themselves are accurate), it\u2019s just using the wrong data to answer your question. In this example, the user could be reading about how to treat the wrong kind of cancer, which seems very bad.\n## Refine Terms\nKGs can help improve the accuracy of responses and reduce the likelihood of context poisoning by refining the results from the vector database. The next step is for selecting what MeSH terms we want to use to filter the articles. First, we do another vector similarity search against the vector database but on the Terms collection. This is because the user may not be familiar with the MeSH controlled vocabulary. In our example above, I searched for, \u201ctherapies for mouth cancer\u201d, but \u201cmouth cancer\u201d is not a term in MeSH \u2014 they use \u201cMouth Neoplasms\u201d. We want the user to be able to start exploring the MeSH terms without having a prior understanding of them \u2014 this is good practice regardless of the metadata used to tag the content.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_38L-k83P82FOmNwCnKhPdQ.png)Image by Author\nThe function to get relevant MeSH terms is nearly identical to the previous Weaviate query. Just replace Article with term:\n```\n# Function to query Weaviate for MeSH Terms<br>def query_weaviate_terms(client, query_text, limit=10):<br>  # Perform vector search on MeshTerm collection<br>  response = client.collections.get(\"term\").query.near_text(<br>    query=query_text,<br>    limit=limit,<br>    return_metadata=MetadataQuery(distance=True)<br>  )<br><br>  # Parse response<br>  results = []<br>  for obj in response.objects:<br>    results.append({<br>      \"uuid\": obj.uuid,<br>      \"properties\": obj.properties,<br>      \"distance\": obj.metadata.distance,<br>    })<br>  return results\n```\n\nHere is what it looks like in the app:\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_OI1hVbBIMyHTEZy7Oy4FGQ.png)Image by Author\nAs you can see, I searched for \u201cmouth cancer\u201d and the most similar terms were returned. Mouth cancer was not returned, as that is not a term in MeSH, but Mouth Neoplasms is on the list.\nThe next step is to allow the user to expand the returned terms to see alternative names and narrower concepts. This requires querying the [MeSH API](https://id.nlm.nih.gov/mesh/). This was the trickiest part of this app for a number of reasons. The biggest problem is that Streamlit requires that everything has a unique ID but MeSH terms can repeat \u2014 if one of the returned concepts is a child of another, then when you expand the parent you will have a duplicate of the child. I think I took care of most of the big issues and the app should work, but there are probably bugs to find at this stage.\nThe functions we rely on are found in rdf_queries.py. We need one to get the alternative names for a term:\n```\n# Fetch alternative names and triples for a MeSH term<br>def get_concept_triples_for_term(term):<br>  term = sanitize_term(term) # Sanitize input term<br>  sparql = SPARQLWrapper(\"https://id.nlm.nih.gov/mesh/sparql\")<br>  query = f\"\"\"<br>  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#><br>  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#><br>  PREFIX meshv: <http://id.nlm.nih.gov/mesh/vocab#><br>  PREFIX mesh: <http://id.nlm.nih.gov/mesh/><br><br>  SELECT ?subject ?p ?pLabel ?o ?oLabel<br>  FROM <http://id.nlm.nih.gov/mesh><br>  WHERE {{<br>    ?subject rdfs:label \"{term}\"@en .<br>    ?subject ?p ?o .<br>    FILTER(CONTAINS(STR(?p), \"concept\"))<br>    OPTIONAL {{ ?p rdfs:label ?pLabel . }}<br>    OPTIONAL {{ ?o rdfs:label ?oLabel . }}<br>  }}<br>  \"\"\"<br>  try:<br>    sparql.setQuery(query)<br>    sparql.setReturnFormat(JSON)<br>    results = sparql.query().convert()<br><br>    triples = set()<br>    for result in results[\"results\"][\"bindings\"]:<br>      obj_label = result.get(\"oLabel\", {}).get(\"value\", \"No label\")<br>      triples.add(sanitize_term(obj_label)) # Sanitize term before adding<br><br>    # Add the sanitized term itself to ensure it's included<br>    triples.add(sanitize_term(term))<br>    return list(triples)<br><br>  except Exception as e:<br>    print(f\"Error fetching concept triples for term '{term}': {e}\")<br>    return []\n```\n\nWe also need functions to get the narrower (child) concepts for a given term. I have two functions that achieve this \u2014 one that gets the immediate children of a term and one recursive function that returns all children of a given depth.\n```\n# Fetch narrower concepts for a MeSH term<br>def get_narrower_concepts_for_term(term):<br>  term = sanitize_term(term) # Sanitize input term<br>  sparql = SPARQLWrapper(\"https://id.nlm.nih.gov/mesh/sparql\")<br>  query = f\"\"\"<br>  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#><br>  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#><br>  PREFIX meshv: <http://id.nlm.nih.gov/mesh/vocab#><br>  PREFIX mesh: <http://id.nlm.nih.gov/mesh/><br><br>  SELECT ?narrowerConcept ?narrowerConceptLabel<br>  WHERE {{<br>    ?broaderConcept rdfs:label \"{term}\"@en .<br>    ?narrowerConcept meshv:broaderDescriptor ?broaderConcept .<br>    ?narrowerConcept rdfs:label ?narrowerConceptLabel .<br>  }}<br>  \"\"\"<br>  try:<br>    sparql.setQuery(query)<br>    sparql.setReturnFormat(JSON)<br>    results = sparql.query().convert()<br><br>    concepts = set()<br>    for result in results[\"results\"][\"bindings\"]:<br>      subject_label = result.get(\"narrowerConceptLabel\", {}).get(\"value\", \"No label\")<br>      concepts.add(sanitize_term(subject_label)) # Sanitize term before adding<br><br>    return list(concepts)<br><br>  except Exception as e:<br>    print(f\"Error fetching narrower concepts for term '{term}': {e}\")<br>    return []<br><br># Recursive function to fetch narrower concepts to a given depth<br>def get_all_narrower_concepts(term, depth=2, current_depth=1):<br>  term = sanitize_term(term) # Sanitize input term<br>  all_concepts = {}<br>  try:<br>    narrower_concepts = get_narrower_concepts_for_term(term)<br>    all_concepts[sanitize_term(term)] = narrower_concepts<br><br>    if current_depth < depth:<br>      for concept in narrower_concepts:<br>        child_concepts = get_all_narrower_concepts(concept, depth, current_depth + 1)<br>        all_concepts.update(child_concepts)<br><br>  except Exception as e:<br>    print(f\"Error fetching all narrower concepts for term '{term}': {e}\")<br><br>  return all_concepts\n```\n\nThe other important part of step 2 is to allow the user to select terms to add to a list of \u201cSelected Terms\u201d. These will appear in the sidebar on the left of the screen. There are a lot of things that can improve this step like:\n  * There is no way to clear all but you can clear the cache or refresh the browser if needed.\n  * There is no way to \u2018select all narrower concepts\u2019 which would be helpful.\n  * There is no option to add rules for filtering. Right now, we are just assuming that the article must contain term A OR term B OR term C etc. The rankings at the end are based on the number of terms the articles are tagged with.\n\n\nHere is what it looks like in the app:\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_PHOQxQnPdDCiISTrbIeCHA.png)Image by Author\nI can expand Mouth Neoplasms to see the alternative names, in this case, \u201cCancer of Mouth\u201d, along with all of the narrower concepts. As you can see, most of the narrower concepts have their own children, which you can expand as well. For the purposes of this demo, I am going to select all children of Mouth Neoplasms.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_1Nodj6EQ51R8GPuliWqONQ.png)Image by Author\nThis step is important not just because it allows the user to filter the search results, but also because it is a way for the user to explore the MeSH graph itself and learn from it. For example, this would be the place for the user to learn that nasopharyngeal neoplasms are not a subset of mouth neoplasms.\n## Filter & Summarize\nNow that you\u2019ve got your articles and your filter terms, you can apply the filter and summarize the results. This is where we bring the original 10 articles returned in step one together with the refined list of MeSH terms. We allow the user to add additional context to the prompt before sending it to the LLM.\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_fIT2zmJGryYgpUT6iZr01A.png)Image by Author\nThe way we do this filtering is that we need to get the URIs for the 10 articles from the original search. Then we can query our knowledge graph for which of those articles have been tagged with the associated MeSH terms. Additionally, we save the abstracts of these articles for use in the next step. This would be the place where we could filter based on access control or other user-controlled parameters like author, filetype, date published, etc. I didn\u2019t include any of that in this app but I did add in properties for access control and date published in case we want to add that in this UI later.\nHere is what the code looks like in app.py:\n```\n    if st.button(\"Filter Articles\"):<br>      try:<br>        # Check if we have URIs from tab 1<br>        if \"article_uris\" in st.session_state and st.session_state.article_uris:<br>          article_uris = st.session_state.article_uris<br><br>          # Convert list of URIs into a string for the VALUES clause or FILTER<br>          article_uris_string = \", \".join([f\"<{str(uri)}>\" for uri in article_uris])<br><br>          SPARQL_QUERY = \"\"\"<br>          PREFIX schema: <http://schema.org/><br>          PREFIX ex: <http://example.org/><br><br>          SELECT ?article ?title ?abstract ?datePublished ?access ?meshTerm<br>          WHERE {{<br>           ?article a ex:Article ;<br>                schema:name ?title ;<br>                schema:description ?abstract ;<br>                schema:datePublished ?datePublished ;<br>                ex:access ?access ;<br>                schema:about ?meshTerm .<br><br>           ?meshTerm a ex:MeSHTerm .<br><br>           FILTER (?article IN ({article_uris}))<br>          }}<br>          \"\"\"<br>          # Insert the article URIs into the query<br>          query = SPARQL_QUERY.format(article_uris=article_uris_string)<br>        else:<br>          st.write(\"No articles selected from Tab 1.\")<br>          st.stop()<br><br>        # Query the RDF and save results in session state<br>        top_articles = query_rdf(LOCAL_FILE_PATH, query, final_terms)<br>        st.session_state.filtered_articles = top_articles<br><br>        if top_articles:<br><br>          # Combine abstracts from top articles and save in session state<br>          def combine_abstracts(ranked_articles):<br>            combined_text = \" \".join(<br>              [f\"Title: {data['title']} Abstract: {data['abstract']}\" for article_uri, data in<br>               ranked_articles]<br>            )<br>            return combined_text<br><br><br>          st.session_state.combined_text = combine_abstracts(top_articles)<br><br>        else:<br>          st.write(\"No articles found for the selected terms.\")<br>      except Exception as e:<br>        st.error(f\"Error filtering articles: {e}\")\n```\n\nThis uses the function query_rdf in the rdf_queries.py file. That function looks like this:\n```\n# Function to query RDF using SPARQL<br>def query_rdf(local_file_path, query, mesh_terms, base_namespace=\"http://example.org/mesh/\"):<br>  if not mesh_terms:<br>    raise ValueError(\"The list of MeSH terms is empty or invalid.\")<br><br>  print(\"SPARQL Query:\", query)<br><br>  # Create and parse the RDF graph<br>  g = Graph()<br>  g.parse(local_file_path, format=\"ttl\")<br><br>  article_data = {}<br><br>  for term in mesh_terms:<br>    # Convert the term to a valid URI<br>    mesh_term_uri = convert_to_uri(term, base_namespace)<br>    #print(\"Term:\", term, \"URI:\", mesh_term_uri)<br><br>    # Perform SPARQL query with initBindings<br>    results = g.query(query, initBindings={'meshTerm': mesh_term_uri})<br><br>    for row in results:<br>      article_uri = row['article']<br>      if article_uri not in article_data:<br>        article_data[article_uri] = {<br>          'title': row['title'],<br>          'abstract': row['abstract'],<br>          'datePublished': row['datePublished'],<br>          'access': row['access'],<br>          'meshTerms': set()<br>        }<br>      article_data[article_uri]['meshTerms'].add(str(row['meshTerm']))<br>    #print(\"DEBUG article_data:\", article_data)<br><br>  # Rank articles by the number of matching MeSH terms<br>  ranked_articles = sorted(<br>    article_data.items(),<br>    key=lambda item: len(item[1]['meshTerms']),<br>    reverse=True<br>  )<br>  return ranked_articles[:10]\n```\n\nAs you can see, this function also converts the MeSH terms to URIs so we can filter using the graph. Be careful in the way you convert terms to URIs and ensure it aligns with the other functions.\nHere is what it looks like in the app:\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_lAAPdTjfFOzD1DM3QUofAA.png)Image by Author\nAs you can see, the two MeSH terms we selected from the previous step are here. If I click \u201cFilter Articles,\u201d it will filter the original 10 articles using our filter criteria in step 2. The articles will be returned with their full abstracts, along with their tagged MeSH terms (see image below).\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_wUsJ9sIQdMcro6ximwPxFg.png)Image by Author\nThere are 5 articles returned. Two are tagged with \u201cmouth neoplasms,\u201d one with \u201cgingival neoplasms,\u201d and two with \u201cpalatal neoplasms\u201d.\nNow that we have a refined list of articles we want to use to generate a response, we can move to the final step. We want to send these articles to an LLM to generate a response but we can also add in additional context to the prompt. I have a default prompt that says, \u201cSummarize the key information here in bullet points. Make it understandable to someone without a medical degree.\u201d For this demo, I am going to adjust the prompt to reflect our original search term:\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_zu6wCN1c8f4AqI3Gzfq5tA.png)\nThe results are as follows:\n![](https://towardsdatascience.com/wp-content/uploads/2025/03/1_3efzwyaNjWsWFizCwhCb0g.png)\nThe results look better to me, mostly because I know that the articles we are summarizing are, presumably, about treatments for mouth cancer. The dataset doesn\u2019t contain the actual journal articles, just the abstracts. So these results are just summaries of summaries. There may be some value to this, but if we were building a real app and not just a demo, this is the step where we could incorporate the full text of the articles. Alternatively, this is where the user/researcher would go read these articles themselves, rather than relying exclusively on the LLM for the summaries.\n# Conclusion\nThis tutorial demonstrates how combining vector databases and knowledge graphs can significantly enhance RAG applications. By leveraging vector similarity for initial searches and structured knowledge graph metadata for filtering and organization, we can build a system that delivers accurate, explainable, and domain-specific results. The integration of MeSH, a well-established controlled vocabulary, highlights the power of domain expertise in curating metadata, which ensures that the retrieval step aligns with the unique needs of the application while maintaining interoperability with other systems. This approach is not limited to medicine \u2014 its principles can be applied across domains wherever structured data and textual information coexist.\nThis tutorial underscores the importance of leveraging each technology for what it does best. Vector databases excel at similarity-based retrieval, while knowledge graphs shine in providing context, structure, and semantics. Additionally, scaling RAG applications demands a metadata layer to break down data silos and enforce governance policies. Thoughtful design, rooted in domain-specific metadata and robust governance, is the path to building RAG systems that are not only accurate but also scalable.\nWritten By\nSteve Hedden\n[See all from Steve Hedden](https://towardsdatascience.com/author/steve-hedden/)\nTopics:\nShare this article:\n  * [ Share on Facebook  ](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-graph-rag-app-b323fc33ba06%2F&title=How%20to%20Build%20a%20Graph%20RAG%20App)\n  * [ Share on LinkedIn  ](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-graph-rag-app-b323fc33ba06%2F&title=How%20to%20Build%20a%20Graph%20RAG%20App)\n  * [ Share on X  ](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-graph-rag-app-b323fc33ba06%2F&text=How%20to%20Build%20a%20Graph%20RAG%20App)\n\n\n## Related Articles\n  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/Hybrid-Architecture_01-snapshot.png)\n## [The Art of Hybrid Architectures](https://towardsdatascience.com/the-art-of-hybrid-architectures/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nCombining CNNs and Transformers to Elevate Fine-Grained Visual Classification \n[Eric Chung](https://towardsdatascience.com/author/eric-chung/)\nMarch 28, 2025\n32 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/ai-generated-city-banner-integration.png)\n## [A Little More Conversation, A Little Less Action \u2014 A Case Against Premature Data Integration](https://towardsdatascience.com/a-little-more-conversation-a-little-less-action-a-case-against-premature-data-integration/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nRunning a large data integration project before embarking on the ML part is easily a\u2026 \n[Daniel Bakkelund](https://towardsdatascience.com/author/danielbakkelund/)\nMarch 28, 2025\n14 min read\n  * ![3D Reconstruction cover image](https://towardsdatascience.com/wp-content/uploads/2025/03/cover-3d-reconstruction.jpg)\n## [Master the 3D Reconstruction Process: A Step-by-Step Guide](https://towardsdatascience.com/master-the-3d-reconstruction-process-step-by-step-guide/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nLearn the complete 3D reconstruction pipeline from feature extraction to dense matching. Master photogrammetry with\u2026 \n[Florent Poux, Ph.D.](https://towardsdatascience.com/author/florentpoux/)\nMarch 28, 2025\n17 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/nik-n1ccr-zVG68-unsplash-scaled-1.jpg)\n## [AI Agents from Zero to Hero \u2014 Part 3](https://towardsdatascience.com/ai-agents-from-zero-to-hero-part-3/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nBuild from scratch using only Ollama (no GPU, no APIKEY) \n[Mauro Di Pietro](https://towardsdatascience.com/author/maurodp/)\nMarch 28, 2025\n14 min read\n  * ## [From Physics to Probability: Hamiltonian Mechanics for Generative Modeling and MCMC](https://towardsdatascience.com/from-physics-to-probability-hamiltonian-mechanics-for-generative-modeling-and-mcmc/)\n[ Math ](https://towardsdatascience.com/category/math/)\nHamiltonian mechanics is a way to describe how physical systems, like planets or pendulums, move\u2026 \n[Soran Ghaderi](https://towardsdatascience.com/author/soran-ghaderi/)\nMarch 28, 2025\n17 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/Screenshot-2025-03-26-at-10.54.07\u202fPM.png)\n## [How to Format Your TDS Draft: A Quick(ish) Guide](https://towardsdatascience.com/how-to-format-your-tds-draft-a-quickish-guide/)\n[ Writing ](https://towardsdatascience.com/category/writing/)\nEverything you need to know about creating a draft on our Contributor Portal \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nMarch 28, 2025\n12 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/image1-1024x576-1.png)\n## [Data Science: From School to Work, Part III](https://towardsdatascience.com/data-science-from-school-to-work-part-iii/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nGood practices for Python error handling and logging \n[Vincent Margot](https://towardsdatascience.com/author/vmargot/)\nMarch 27, 2025\n12 min read\n  * ![A colorful, hand-drawn, cartoon-style illustration set against a vibrant blue background. In the center stands a cute, vintage-style robot with a white body and red accents \\(shoulders, hands, waist, and feet\\). Above the robot's head are three speech bubbles. The left bubble is orange and contains the text \"EN\" in bold black letters, representing English. The center bubble is white with the Chinese character \"\u4e2d\" \\(for Chinese\\), and the right bubble is yellow with the Japanese character \"\u65e5\" \\(for Japanese\\). The robot appears to be choosing or processing languages, symbolizing multilingual communication or translation.](https://towardsdatascience.com/wp-content/uploads/2025/03/article-cover-robot-ratio.png)\n## [Japanese-Chinese Translation with GenAI: What Works and What Doesn\u2019t](https://towardsdatascience.com/japanese-chinese-translation-with-genai-what-works-and-what-doesnt/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nEvaluating GenAI in Japanese-Chinese translation: current limits and opportunities \n[Qian (Alex) Wan](https://towardsdatascience.com/author/qianalex-wan/)\nMarch 27, 2025\n20 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2025/03/featured-image.png)\n## [Talk to Videos](https://towardsdatascience.com/talk-to-videos/)\n[ Large Language Models ](https://towardsdatascience.com/category/artificial-intelligence/large-language-models/)\nDeveloping an interactive AI application for video-based learning in education and business \n[Umair Ali Khan](https://towardsdatascience.com/author/umairali-khan/)\nMarch 27, 2025\n28 min read\n\n\n  * [YouTube](https://www.youtube.com/c/TowardsDataScience)\n  * [X](https://x.com/TDataScience)\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [Threads](https://www.threads.net/@towardsdatascience)\n  * [Bluesky](https://bsky.app/profile/towardsdatascience.com)\n\n\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nYour home for data science and Al. The world\u2019s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\n\u00a9  Insight Media Group, LLC 2025 \n  * [About](https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/)\n  * [Privacy Policy](https://towardsdatascience.com/privacy-policy/)\n  * [Terms of Use](https://towardsdatascience.com/website-terms-of-use/)\n\n\n[Towards Data Science is now independent!](https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/)\nCookies Settings\n## Sign up to our newsletter\nEmail address*\nFirst name*\nLast name*\nJob title*\nJob level*\nPlease SelectC-LevelVP/DirectorManager/SupervisorMid Level or Senior Non-Managerial StaffEntry Level/Junior StaffFreelancer/ContractorStudent/InternOther\nCompany name*\n  * I consent to receive newsletters and other communications from Towards Data Science publications.*\n\n\n![Company Logo](https://cdn.cookielaw.org/logos/static/ot_company_logo.png)\n## Privacy Preference Center\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer. [More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nAllow All\n###  Manage Consent Preferences\n#### Functional Cookies\nFunctional Cookies Active\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\n#### Strictly Necessary Cookies\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n#### Performance Cookies\nPerformance Cookies Active\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n#### Targeting Cookies\nTargeting Cookies Active\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nBack Button\n### Cookie List\nSearch Icon\nFilter Icon\nClear\ncheckbox label label\nApply Cancel\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nReject All Confirm My Choices\n[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-consent/)\nSome areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThis article outlines building a Graph RAG app using knowledge graphs (KGs) and Large Language Models (LLMs) to enhance information retrieval in medical literature. It emphasizes the synergy between KGs and LLMs, improving accuracy and contextual understanding while mitigating context poisoning. The app allows users to search, refine, and summarize articles effectively, showcasing the potential of Graph RAG across various domains.\n```"
  },
  {
    "instruction": "[![Common Crawl Logo](https://cdn.prod.website-files.com/6479b8d98bf5dcb4a69c4f31/649b5869af56f6df617cfb1f_CC_Logo_Blue_Auto.svg)](https://commoncrawl.org/)\n  * The Data\n[Overview](https://commoncrawl.org/overview)[Web Graphs](https://commoncrawl.org/web-graphs)[Latest Crawl](https://commoncrawl.org/latest-crawl)[Crawl Stats](https://commoncrawl.github.io/cc-crawl-statistics/)[Graph Stats](https://commoncrawl.github.io/cc-webgraph-statistics/)[Errata](https://commoncrawl.org/errata)\n  * Resources\n[Get Started](https://commoncrawl.org/get-started)[AI Agent](https://commoncrawl.org/ai-agent)[Blog](https://commoncrawl.org/blog)[Examples](https://commoncrawl.org/examples)[Use Cases](https://commoncrawl.org/use-cases)[CCBot](https://commoncrawl.org/ccbot)[Infra Status](https://status.commoncrawl.org)[FAQ](https://commoncrawl.org/faq)\n  * Community\n[Research Papers](https://commoncrawl.org/research-papers)[Mailing List Archive](https://groups.google.com/g/common-crawl)[Hugging Face](https://huggingface.co/commoncrawl)[Discord](https://discord.gg/njaVFh7avF)[Collaborators](https://commoncrawl.org/collaborators)\n  * About\n[Team](https://commoncrawl.org/team)[Jobs](https://commoncrawl.org/jobs)[Mission](https://commoncrawl.org/mission)[Impact](https://commoncrawl.org/impact)[Privacy Policy](https://commoncrawl.org/privacy-policy)[Terms of Use](https://commoncrawl.org/terms-of-use)\n  * Search\n  * [Contact Us](https://commoncrawl.org/contact-us)\n\n\n# Common Crawl maintains a free, open repository of web crawl data that can be used by anyone.\n# Common Crawl is a 501(c)(3) non\u2013profit founded in 2007.\u200dWe make wholesale extraction, transformation and analysis of open web data accessible to researchers.\n[Overview](https://commoncrawl.org/overview)\n## Over 250 billion pages spanning 18 years.\n## Free and open corpus since 2007.\n## Cited in over 10,000 research papers.\n## 3\u20135 billion new pages added each month.\n### Featured Papers:\n#### [Research on Free Expression OnlineJeffrey Knockel, Jakub Dalek, Noura Aljizawi, Mohamed Ahmed, Levi Meletti, and Justin LauBanned Books: Analysis of Censorship on Amazon.com](https://citizenlab.ca/2024/11/analysis-of-censorship-on-amazon-com/)\n#### [Analyzing the Australian Web with Web Graphs: Harmonic Centrality at the Domain LevelXian Gong, Paul X. McCarthy, Marian-Andrei Rizoiu, Paolo BoldiHarmony in the Australian Domain Space](https://arxiv.org/pdf/2404.10006)\n#### [The Dangers of Hijacked HyperlinksKevin Saric, Felix Savins, Gowri Sankar Ramachandran, Raja Jurdak, Surya NepalHyperlink Hijacking: Exploiting Erroneous URL Links to Phantom Domains](https://dl.acm.org/doi/10.1145/3589334.3645510)\n#### [Enhancing Computational AnalysisZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya GuoDeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)\n#### [Computation and LanguageAsier Guti\u00e9rrez-Fandi\u00f1o, David P\u00e9rez-Fern\u00e1ndez, Jordi Armengol-Estap\u00e9, David Griol, Zoraida CallejasesCorpius: A Massive Spanish Crawling Corpus](https://arxiv.org/abs/2206.15147)\n#### [The Web as a Graph (Master's Thesis)Marius L\u00f8vold J\u00f8rgensen, UiT Norges Arktiske Universitet BacklinkDB: A Purpose-Built Backlink Database Management System](https://munin.uit.no/handle/10037/28861)\n#### [Internet Security: Phishing WebsitesAsadullah Safi, Satwinder SinghA Systematic Literature Review on Phishing Website Detection Techniques](https://doi.org/10.1016/j.jksuci.2023.01.004)\n[More on Google Scholar](https://scholar.google.com/scholar?q=common+crawl)[Curated BibTeX Dataset](https://github.com/commoncrawl/cc-citations/)\n### Latest Blog Post:\n[![Submission to the UK\u2019s Copyright and AI Consultation](https://cdn.prod.website-files.com/647b1c7a9990bad2048d3711/64eb2e14ae5e8c4c70ec5392_news2.webp)](https://commoncrawl.org/blog/submission-to-the-uks-copyright-and-ai-consultation)\n[News](https://commoncrawl.org/)### [Submission to the UK\u2019s Copyright and AI Consultation](https://commoncrawl.org/blog/submission-to-the-uks-copyright-and-ai-consultation)\nRead our submission to the UK government's Copyright and AI consultation, supporting a legal exception for text and data mining (TDM) while respecting creators\u2019 rights.\n![Common\u00a0Crawl builds and maintains an open repository of web crawl data that can be accessed and analyzed by anyone.](https://cdn.prod.website-files.com/647b1c7a9990bad2048d3711/64e1481c88168f56d3a58898_648de3e6dfa993c0af776fcf_Common_Crawl_Rev3_LPX_Color%20Icon.svg)\nCommon Crawl Foundation\nCommon Crawl builds and maintains an open repository of web crawl data that can be accessed and analyzed by anyone.\n[![Common Crawl Logo](https://cdn.prod.website-files.com/6479b8d98bf5dcb4a69c4f31/649b5869af56f6df617cfb1f_CC_Logo_Blue_Auto.svg)](https://commoncrawl.org/)\n## The Data\n### [Overview](https://commoncrawl.org/overview)\n### [Web Graphs](https://commoncrawl.org/web-graphs)\n### [Latest Crawl](https://commoncrawl.org/latest-crawl)\n### [Crawl Stats](https://commoncrawl.github.io/cc-crawl-statistics/)\n### [Graph Stats](https://commoncrawl.github.io/cc-webgraph-statistics/)\n### [Errata](https://commoncrawl.org/errata)\n## Resources\n### [Get Started](https://commoncrawl.org/get-started)\n### [AI Agent](https://commoncrawl.org/ai-agent)\n### [Blog](https://commoncrawl.org/blog)\n### [Examples](https://commoncrawl.org/examples)\n### [Use Cases](https://commoncrawl.org/use-cases)\n### [CCBot](https://commoncrawl.org/ccbot)\n### [Infra Status](https://status.commoncrawl.org)\n### [FAQ](https://commoncrawl.org/faq)\n## Community\n### [Research Papers](https://commoncrawl.org/research-papers)\n### [Mailing List Archive](https://groups.google.com/g/common-crawl)\n### [Hugging Face](https://huggingface.co/commoncrawl)\n### [Discord](https://discord.gg/njaVFh7avF)\n### [Collaborators](https://commoncrawl.org/collaborators)\n## About\n### [Team](https://commoncrawl.org/team)\n### [Jobs](https://commoncrawl.org/jobs)\n### [Mission](https://commoncrawl.org/mission)\n### [Impact](https://commoncrawl.org/impact)\n### [Privacy Policy](https://commoncrawl.org/privacy-policy)\n### [Terms of Use](https://commoncrawl.org/terms-of-use)\n[![Twitter Logo](https://cdn.prod.website-files.com/6479b8d98bf5dcb4a69c4f31/6675a2f3394182102f502e32_twitter-x.svg)](https://x.com/commoncrawl)[![LinkedIn Logo](https://cdn.prod.website-files.com/6479b8d98bf5dcb4a69c4f31/6493999d57ad8123b314b6cf_linkedin%20\\(1\\).svg)](https://www.linkedin.com/company/common-crawl/)[![LinkedIn Logo](https://cdn.prod.website-files.com/6479b8d98bf5dcb4a69c4f31/6602100396780a14617cc8ab_discord.svg)](https://discord.gg/njaVFh7avF)\n\u00a9 2025 Common Crawl\n",
    "answer": "```markdown\n# TL;DR Summary\n\nCommon Crawl is a non-profit organization maintaining a free repository of web crawl data since 2007, with over 250 billion pages. It supports research, is cited in 10,000+ papers, and advocates for text and data mining rights in the UK. \n\n**Tags:** #CommonCrawl #WebData #Research #TDM\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/scrapy/scrapy/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=scrapy%2Fscrapy) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/scrapy/scrapy/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/scrapy/scrapy/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/scrapy/scrapy/) to refresh your session. Dismiss alert\n{{ message }}\n[ scrapy ](https://github.com/scrapy) / **[scrapy](https://github.com/scrapy/scrapy) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy) You must be signed in to change notification settings\n  * [ Fork 10.7k ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy)\n  * [ Star  54.7k ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy)\n\n\nScrapy, a fast high-level web crawling & scraping framework for Python. \n[scrapy.org](https://scrapy.org \"https://scrapy.org\")\n### License\n[ BSD-3-Clause license ](https://github.com/scrapy/scrapy/blob/master/LICENSE)\n[ 54.7k stars ](https://github.com/scrapy/scrapy/stargazers) [ 10.7k forks ](https://github.com/scrapy/scrapy/forks) [ Branches ](https://github.com/scrapy/scrapy/branches) [ Tags ](https://github.com/scrapy/scrapy/tags) [ Activity ](https://github.com/scrapy/scrapy/activity)\n[ Star  ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy)\n[ Notifications ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy) You must be signed in to change notification settings\n  * [ Code ](https://github.com/scrapy/scrapy)\n  * [ Issues 437 ](https://github.com/scrapy/scrapy/issues)\n  * [ Pull requests 185 ](https://github.com/scrapy/scrapy/pulls)\n  * [ Discussions ](https://github.com/scrapy/scrapy/discussions)\n  * [ Actions ](https://github.com/scrapy/scrapy/actions)\n  * [ Projects 0 ](https://github.com/scrapy/scrapy/projects)\n  * [ Wiki ](https://github.com/scrapy/scrapy/wiki)\n  * [ Security 10 ](https://github.com/scrapy/scrapy/security)\n  * [ Insights ](https://github.com/scrapy/scrapy/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/scrapy/scrapy)\n  * [ Issues  ](https://github.com/scrapy/scrapy/issues)\n  * [ Pull requests  ](https://github.com/scrapy/scrapy/pulls)\n  * [ Discussions  ](https://github.com/scrapy/scrapy/discussions)\n  * [ Actions  ](https://github.com/scrapy/scrapy/actions)\n  * [ Projects  ](https://github.com/scrapy/scrapy/projects)\n  * [ Wiki  ](https://github.com/scrapy/scrapy/wiki)\n  * [ Security  ](https://github.com/scrapy/scrapy/security)\n  * [ Insights  ](https://github.com/scrapy/scrapy/pulse)\n\n\n# scrapy/scrapy\nmaster\n[**46** Branches](https://github.com/scrapy/scrapy/branches)[**119** Tags](https://github.com/scrapy/scrapy/tags)\n[](https://github.com/scrapy/scrapy/branches)[](https://github.com/scrapy/scrapy/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n[![wRAR](https://avatars.githubusercontent.com/u/241039?v=4&size=40)](https://github.com/wRAR)[wRAR](https://github.com/scrapy/scrapy/commits?author=wRAR)[Convert test_downloadermiddleware_robotstxt.py from callbacks to awai\u2026](https://github.com/scrapy/scrapy/commit/9f99da8f865efff11f6c8736567b8fbd4413091c)Mar 24, 2025[9f99da8](https://github.com/scrapy/scrapy/commit/9f99da8f865efff11f6c8736567b8fbd4413091c) \u00b7 Mar 24, 2025\n## History\n[10,758 Commits](https://github.com/scrapy/scrapy/commits/master/)[](https://github.com/scrapy/scrapy/commits/master/)  \n[.github](https://github.com/scrapy/scrapy/tree/master/.github \".github\")| [.github](https://github.com/scrapy/scrapy/tree/master/.github \".github\")| [Codecov: Add test analytics (](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")[#6741](https://github.com/scrapy/scrapy/pull/6741)[)](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")| Mar 21, 2025  \n[artwork](https://github.com/scrapy/scrapy/tree/master/artwork \"artwork\")| [artwork](https://github.com/scrapy/scrapy/tree/master/artwork \"artwork\")| [artwork/README.rst: add missing articles (](https://github.com/scrapy/scrapy/commit/874a87976830ed519ee49742c5b427618fa362e8 \"artwork/README.rst: add missing articles \\(#5827\\)\")[#5827](https://github.com/scrapy/scrapy/pull/5827)[)](https://github.com/scrapy/scrapy/commit/874a87976830ed519ee49742c5b427618fa362e8 \"artwork/README.rst: add missing articles \\(#5827\\)\")| Feb 14, 2023  \n[docs](https://github.com/scrapy/scrapy/tree/master/docs \"docs\")| [docs](https://github.com/scrapy/scrapy/tree/master/docs \"docs\")| [Syntax Error Fixed (](https://github.com/scrapy/scrapy/commit/3ca882fba86750199c2f41ef24b5495a4afa7988 \"Syntax Error Fixed \\(#6738\\)\n* Syntax error fix issue #6731\n* test case added\n* extra logic removed\n* mock spider fixture\n* Update scrapy/utils/misc.py\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\n* settings.rst updated\n* settings.rst updated\n* settings.rst updated\n---------\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\")[#6738](https://github.com/scrapy/scrapy/pull/6738)[)](https://github.com/scrapy/scrapy/commit/3ca882fba86750199c2f41ef24b5495a4afa7988 \"Syntax Error Fixed \\(#6738\\)\n* Syntax error fix issue #6731\n* test case added\n* extra logic removed\n* mock spider fixture\n* Update scrapy/utils/misc.py\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\n* settings.rst updated\n* settings.rst updated\n* settings.rst updated\n---------\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\")| Mar 20, 2025  \n[extras](https://github.com/scrapy/scrapy/tree/master/extras \"extras\")| [extras](https://github.com/scrapy/scrapy/tree/master/extras \"extras\")| [Run and fix linkcheck. (](https://github.com/scrapy/scrapy/commit/ce5a132f12341a4118edb7c8ae3b7c2a27306057 \"Run and fix linkcheck. \\(#6524\\)\")[#6524](https://github.com/scrapy/scrapy/pull/6524)[)](https://github.com/scrapy/scrapy/commit/ce5a132f12341a4118edb7c8ae3b7c2a27306057 \"Run and fix linkcheck. \\(#6524\\)\")| Nov 4, 2024  \n[scrapy](https://github.com/scrapy/scrapy/tree/master/scrapy \"scrapy\")| [scrapy](https://github.com/scrapy/scrapy/tree/master/scrapy \"scrapy\")| [Syntax Error Fixed (](https://github.com/scrapy/scrapy/commit/3ca882fba86750199c2f41ef24b5495a4afa7988 \"Syntax Error Fixed \\(#6738\\)\n* Syntax error fix issue #6731\n* test case added\n* extra logic removed\n* mock spider fixture\n* Update scrapy/utils/misc.py\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\n* settings.rst updated\n* settings.rst updated\n* settings.rst updated\n---------\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\")[#6738](https://github.com/scrapy/scrapy/pull/6738)[)](https://github.com/scrapy/scrapy/commit/3ca882fba86750199c2f41ef24b5495a4afa7988 \"Syntax Error Fixed \\(#6738\\)\n* Syntax error fix issue #6731\n* test case added\n* extra logic removed\n* mock spider fixture\n* Update scrapy/utils/misc.py\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\n* settings.rst updated\n* settings.rst updated\n* settings.rst updated\n---------\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\")| Mar 20, 2025  \n[sep](https://github.com/scrapy/scrapy/tree/master/sep \"sep\")| [sep](https://github.com/scrapy/scrapy/tree/master/sep \"sep\")| [Remove trailing whitespace](https://github.com/scrapy/scrapy/commit/bee74fb753afa373ecdb1dbf272af9a108b14b22 \"Remove trailing whitespace\")| Mar 11, 2025  \n[tests](https://github.com/scrapy/scrapy/tree/master/tests \"tests\")| [tests](https://github.com/scrapy/scrapy/tree/master/tests \"tests\")| [Convert test_downloadermiddleware_robotstxt.py from callbacks to awai\u2026](https://github.com/scrapy/scrapy/commit/9f99da8f865efff11f6c8736567b8fbd4413091c \"Convert test_downloadermiddleware_robotstxt.py from callbacks to awaits. \\(#6743\\)\")| Mar 24, 2025  \n[tests_typing](https://github.com/scrapy/scrapy/tree/master/tests_typing \"tests_typing\")| [tests_typing](https://github.com/scrapy/scrapy/tree/master/tests_typing \"tests_typing\")| [Drop Python 3.8 Support (](https://github.com/scrapy/scrapy/commit/53916630723a86277836053e1c54fe50655f0bd5 \"Drop Python 3.8 Support \\(#6472\\)\")[#6472](https://github.com/scrapy/scrapy/pull/6472)[)](https://github.com/scrapy/scrapy/commit/53916630723a86277836053e1c54fe50655f0bd5 \"Drop Python 3.8 Support \\(#6472\\)\")| Oct 16, 2024  \n[.git-blame-ignore-revs](https://github.com/scrapy/scrapy/blob/master/.git-blame-ignore-revs \".git-blame-ignore-revs\")| [.git-blame-ignore-revs](https://github.com/scrapy/scrapy/blob/master/.git-blame-ignore-revs \".git-blame-ignore-revs\")| [chore: fix some typos in comments (](https://github.com/scrapy/scrapy/commit/5f67c01d1d2cc8a0104361b0323d755e22ed93dc \"chore: fix some typos in comments \\(#6317\\)\nSigned-off-by: TechVest <techdashen@qq.com>\")[#6317](https://github.com/scrapy/scrapy/pull/6317)[)](https://github.com/scrapy/scrapy/commit/5f67c01d1d2cc8a0104361b0323d755e22ed93dc \"chore: fix some typos in comments \\(#6317\\)\nSigned-off-by: TechVest <techdashen@qq.com>\")| Apr 17, 2024  \n[.gitattributes](https://github.com/scrapy/scrapy/blob/master/.gitattributes \".gitattributes\")| [.gitattributes](https://github.com/scrapy/scrapy/blob/master/.gitattributes \".gitattributes\")| [Maybe the problem is not in the code after all](https://github.com/scrapy/scrapy/commit/24ba5a71aca9b368eee21bd7d9043a7d26dba403 \"Maybe the problem is not in the code after all\")| Aug 12, 2020  \n[.gitignore](https://github.com/scrapy/scrapy/blob/master/.gitignore \".gitignore\")| [.gitignore](https://github.com/scrapy/scrapy/blob/master/.gitignore \".gitignore\")| [Codecov: Add test analytics (](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")[#6741](https://github.com/scrapy/scrapy/pull/6741)[)](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")| Mar 21, 2025  \n[.pre-commit-config.yaml](https://github.com/scrapy/scrapy/blob/master/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/scrapy/scrapy/blob/master/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [Remove trailing whitespace](https://github.com/scrapy/scrapy/commit/bee74fb753afa373ecdb1dbf272af9a108b14b22 \"Remove trailing whitespace\")| Mar 11, 2025  \n[.readthedocs.yml](https://github.com/scrapy/scrapy/blob/master/.readthedocs.yml \".readthedocs.yml\")| [.readthedocs.yml](https://github.com/scrapy/scrapy/blob/master/.readthedocs.yml \".readthedocs.yml\")| [chore(docs): refactor config (](https://github.com/scrapy/scrapy/commit/e0c828b7f665d8c82e17787996634bc072e416ae \"chore\\(docs\\): refactor config \\(#6623\\)\")[#6623](https://github.com/scrapy/scrapy/pull/6623)[)](https://github.com/scrapy/scrapy/commit/e0c828b7f665d8c82e17787996634bc072e416ae \"chore\\(docs\\): refactor config \\(#6623\\)\")| Jan 20, 2025  \n[AUTHORS](https://github.com/scrapy/scrapy/blob/master/AUTHORS \"AUTHORS\")| [AUTHORS](https://github.com/scrapy/scrapy/blob/master/AUTHORS \"AUTHORS\")| [Scrapinghub \u2192 Zyte](https://github.com/scrapy/scrapy/commit/f30f53b3cc958770406628411db3d93c925db59e \"Scrapinghub \u2192 Zyte\")| Feb 2, 2021  \n[CODE_OF_CONDUCT.md](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| [CODE_OF_CONDUCT.md](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| [Update Code of Conduct to Contributor Covenant v2.1](https://github.com/scrapy/scrapy/commit/9f45be439de8a3b9a6d201c33e98b408a73c02bb \"Update Code of Conduct to Contributor Covenant v2.1\")| Oct 27, 2022  \n[CONTRIBUTING.md](https://github.com/scrapy/scrapy/blob/master/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/scrapy/scrapy/blob/master/CONTRIBUTING.md \"CONTRIBUTING.md\")| [Be consistent with domain used for links to documentation website](https://github.com/scrapy/scrapy/commit/65d631329a1434ec013f24341e4b8520241aec70 \"Be consistent with domain used for links to documentation website\")| Jan 30, 2019  \n[INSTALL.md](https://github.com/scrapy/scrapy/blob/master/INSTALL.md \"INSTALL.md\")| [INSTALL.md](https://github.com/scrapy/scrapy/blob/master/INSTALL.md \"INSTALL.md\")| [Update and rename INSTALL to INSTALL.md](https://github.com/scrapy/scrapy/commit/e60e8224a23ddd4fd98c54318d128f6744f40a6a \"Update and rename INSTALL to INSTALL.md\")| Oct 6, 2022  \n[LICENSE](https://github.com/scrapy/scrapy/blob/master/LICENSE \"LICENSE\")| [LICENSE](https://github.com/scrapy/scrapy/blob/master/LICENSE \"LICENSE\")| [added oxford commas to LICENSE](https://github.com/scrapy/scrapy/commit/9bd5444a42e82a05d2791fd10a205a9dd99303a6 \"added oxford commas to LICENSE\")| Jun 1, 2018  \n[MANIFEST.in](https://github.com/scrapy/scrapy/blob/master/MANIFEST.in \"MANIFEST.in\")| [MANIFEST.in](https://github.com/scrapy/scrapy/blob/master/MANIFEST.in \"MANIFEST.in\")| [Integrating configs into pyproject.toml (](https://github.com/scrapy/scrapy/commit/8c23da943c5e892515f4fa2eb57229839802010a \"Integrating configs into pyproject.toml \\(#6547\\)\")[#6547](https://github.com/scrapy/scrapy/pull/6547)[)](https://github.com/scrapy/scrapy/commit/8c23da943c5e892515f4fa2eb57229839802010a \"Integrating configs into pyproject.toml \\(#6547\\)\")| Nov 19, 2024  \n[NEWS](https://github.com/scrapy/scrapy/blob/master/NEWS \"NEWS\")| [NEWS](https://github.com/scrapy/scrapy/blob/master/NEWS \"NEWS\")| [added NEWS file pointing to docs/news.rst](https://github.com/scrapy/scrapy/commit/f5b87dbef8f3c657e31eeb026d8a77b0ccc77c29 \"added NEWS file pointing to docs/news.rst\")| Apr 28, 2012  \n[README.rst](https://github.com/scrapy/scrapy/blob/master/README.rst \"README.rst\")| [README.rst](https://github.com/scrapy/scrapy/blob/master/README.rst \"README.rst\")| [Remove trailing whitespace](https://github.com/scrapy/scrapy/commit/bee74fb753afa373ecdb1dbf272af9a108b14b22 \"Remove trailing whitespace\")| Mar 11, 2025  \n[SECURITY.md](https://github.com/scrapy/scrapy/blob/master/SECURITY.md \"SECURITY.md\")| [SECURITY.md](https://github.com/scrapy/scrapy/blob/master/SECURITY.md \"SECURITY.md\")| [Bump version: 2.11.2 \u2192 2.12.0](https://github.com/scrapy/scrapy/commit/b1f9e56693cd2000ddcea922306f726f3e9339af \"Bump version: 2.11.2 \u2192 2.12.0\")| Nov 18, 2024  \n[codecov.yml](https://github.com/scrapy/scrapy/blob/master/codecov.yml \"codecov.yml\")| [codecov.yml](https://github.com/scrapy/scrapy/blob/master/codecov.yml \"codecov.yml\")| [codecov config: disable project check, tweak PR comments](https://github.com/scrapy/scrapy/commit/9ce03d096d7ed569071a1951d9e327d7294e1a83 \"codecov config: disable project check, tweak PR comments\")| May 18, 2017  \n[conftest.py](https://github.com/scrapy/scrapy/blob/master/conftest.py \"conftest.py\")| [conftest.py](https://github.com/scrapy/scrapy/blob/master/conftest.py \"conftest.py\")| [Fix running simple tests with --reactor=default. (](https://github.com/scrapy/scrapy/commit/872924721344502cca92a72ce68d5f78a9b9e5bc \"Fix running simple tests with --reactor=default. \\(#6735\\)\")[#6735](https://github.com/scrapy/scrapy/pull/6735)[)](https://github.com/scrapy/scrapy/commit/872924721344502cca92a72ce68d5f78a9b9e5bc \"Fix running simple tests with --reactor=default. \\(#6735\\)\")| Mar 15, 2025  \n[pyproject.toml](https://github.com/scrapy/scrapy/blob/master/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/scrapy/scrapy/blob/master/pyproject.toml \"pyproject.toml\")| [Convert remaining unittest assert* calls, use the tmp_path fixture. (](https://github.com/scrapy/scrapy/commit/eb654aa1a8d2ef6433957fcc1361420b6141094e \"Convert remaining unittest assert* calls, use the tmp_path fixture. \\(#6725\\)\")[#\u2026](https://github.com/scrapy/scrapy/pull/6725)| Mar 11, 2025  \n[tox.ini](https://github.com/scrapy/scrapy/blob/master/tox.ini \"tox.ini\")| [tox.ini](https://github.com/scrapy/scrapy/blob/master/tox.ini \"tox.ini\")| [Codecov: Add test analytics (](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")[#6741](https://github.com/scrapy/scrapy/pull/6741)[)](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")| Mar 21, 2025  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/scrapy/scrapy/)\n  * [Code of conduct](https://github.com/scrapy/scrapy/)\n  * [BSD-3-Clause license](https://github.com/scrapy/scrapy/)\n  * [Security](https://github.com/scrapy/scrapy/)\n\n\n[![https://scrapy.org/img/scrapylogo.png](https://camo.githubusercontent.com/664eb5eae435b24275fd1740cb050d5079478554958dc25f44a063b40ee65551/68747470733a2f2f7363726170792e6f72672f696d672f7363726170796c6f676f2e706e67)](https://scrapy.org/)\n## Scrapy\n[](https://github.com/scrapy/scrapy/#scrapy)\n[![PyPI Version](https://camo.githubusercontent.com/7f23a6a9b0298efa3e2944ff9502a91459d70dd0d5244cb444fc342e1533c8f6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f5363726170792e737667) ](https://pypi.org/pypi/Scrapy) [![Supported Python Versions](https://camo.githubusercontent.com/6bfde535940031459b168bf7591e43cac9d5e0b3d1209f282ca1624cd63e4d14/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f5363726170792e737667) ](https://pypi.org/pypi/Scrapy) [![Ubuntu](https://github.com/scrapy/scrapy/workflows/Ubuntu/badge.svg) ](https://github.com/scrapy/scrapy/actions?query=workflow%3AUbuntu) [![Windows](https://github.com/scrapy/scrapy/workflows/Windows/badge.svg) ](https://github.com/scrapy/scrapy/actions?query=workflow%3AWindows) [![Wheel Status](https://camo.githubusercontent.com/55f433059ee431d5b2ffd26fdea757a2b88923121950c6840987f5563900e40f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f776865656c2d7965732d627269676874677265656e2e737667) ](https://pypi.org/pypi/Scrapy) [![Coverage report](https://camo.githubusercontent.com/cbe200ed5f89e6f0076d98a4d7f2deac81dcf7db4bad2401a83337a7d136ed4e/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f7363726170792f7363726170792f6d61737465722e737667) ](https://codecov.io/github/scrapy/scrapy?branch=master) [![Conda Version](https://camo.githubusercontent.com/32de2f70318d9be0205e95ca2ff5c2946a26b7e3519890747e4d37c0aa709526/68747470733a2f2f616e61636f6e64612e6f72672f636f6e64612d666f7267652f7363726170792f6261646765732f76657273696f6e2e737667) ](https://anaconda.org/conda-forge/scrapy)\n### Overview\n[](https://github.com/scrapy/scrapy/#overview)\nScrapy is a BSD-licensed fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.\nScrapy is maintained by [Zyte](https://www.zyte.com/) (formerly Scrapinghub) and [many other contributors](https://github.com/scrapy/scrapy/graphs/contributors).\nCheck the Scrapy homepage at <https://scrapy.org> for more information, including a list of features.\n### Requirements\n[](https://github.com/scrapy/scrapy/#requirements)\n  * Python 3.9+\n  * Works on Linux, Windows, macOS, BSD\n\n\n### Install\n[](https://github.com/scrapy/scrapy/#install)\nThe quick way:\n```\npip install scrapy\n```\n\nSee the install section in the documentation at <https://docs.scrapy.org/en/latest/intro/install.html> for more details.\n### Documentation\n[](https://github.com/scrapy/scrapy/#documentation)\nDocumentation is available online at <https://docs.scrapy.org/> and in the `docs` directory.\n### Releases\n[](https://github.com/scrapy/scrapy/#releases)\nYou can check <https://docs.scrapy.org/en/latest/news.html> for the release notes.\n### Community (blog, twitter, mail list, IRC)\n[](https://github.com/scrapy/scrapy/#community-blog-twitter-mail-list-irc)\nSee <https://scrapy.org/community/> for details.\n### Contributing\n[](https://github.com/scrapy/scrapy/#contributing)\nSee <https://docs.scrapy.org/en/master/contributing.html> for details.\n#### Code of Conduct\n[](https://github.com/scrapy/scrapy/#code-of-conduct)\nPlease note that this project is released with a Contributor [Code of Conduct](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md).\nBy participating in this project you agree to abide by its terms. Please report unacceptable behavior to opensource@zyte.com.\n### Companies using Scrapy\n[](https://github.com/scrapy/scrapy/#companies-using-scrapy)\nSee <https://scrapy.org/companies/> for a list.\n### Commercial Support\n[](https://github.com/scrapy/scrapy/#commercial-support)\nSee <https://scrapy.org/support/> for details.\n## About\nScrapy, a fast high-level web crawling & scraping framework for Python. \n[scrapy.org](https://scrapy.org \"https://scrapy.org\")\n### Topics\n[ python ](https://github.com/topics/python \"Topic: python\") [ crawler ](https://github.com/topics/crawler \"Topic: crawler\") [ framework ](https://github.com/topics/framework \"Topic: framework\") [ scraping ](https://github.com/topics/scraping \"Topic: scraping\") [ crawling ](https://github.com/topics/crawling \"Topic: crawling\") [ web-scraping ](https://github.com/topics/web-scraping \"Topic: web-scraping\") [ hacktoberfest ](https://github.com/topics/hacktoberfest \"Topic: hacktoberfest\") [ web-scraping-python ](https://github.com/topics/web-scraping-python \"Topic: web-scraping-python\")\n### Resources\n[ Readme ](https://github.com/scrapy/scrapy/#readme-ov-file)\n### License\n[ BSD-3-Clause license ](https://github.com/scrapy/scrapy/#BSD-3-Clause-1-ov-file)\n### Code of conduct\n[ Code of conduct ](https://github.com/scrapy/scrapy/#coc-ov-file)\n### Security policy\n[ Security policy ](https://github.com/scrapy/scrapy/#security-ov-file)\n[ Activity](https://github.com/scrapy/scrapy/activity)\n[ Custom properties](https://github.com/scrapy/scrapy/custom-properties)\n### Stars\n[ **54.7k** stars](https://github.com/scrapy/scrapy/stargazers)\n### Watchers\n[ **1.8k** watching](https://github.com/scrapy/scrapy/watchers)\n### Forks\n[ **10.7k** forks](https://github.com/scrapy/scrapy/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy&report=scrapy+%28user%29)\n##  [Releases 39](https://github.com/scrapy/scrapy/releases)\n[ 2.12.0 Latest  Nov 18, 2024 ](https://github.com/scrapy/scrapy/releases/tag/2.12.0)\n[+ 38 releases](https://github.com/scrapy/scrapy/releases)\n##  [Packages 0](https://github.com/orgs/scrapy/packages?repo_name=scrapy)\nNo packages published \n##  [Used by 52.2k](https://github.com/scrapy/scrapy/network/dependents)\n[\n  * ![@PHFernandes9](https://avatars.githubusercontent.com/u/194980440?s=64&v=4)\n  * ![@Victor-Genius](https://avatars.githubusercontent.com/u/200241745?s=64&v=4)\n  * ![@Ayaaayaaa](https://avatars.githubusercontent.com/u/146444257?s=64&v=4)\n  * ![@maichi98](https://avatars.githubusercontent.com/u/58106854?s=64&v=4)\n  * ![@hiteshsurya17](https://avatars.githubusercontent.com/u/83537863?s=64&v=4)\n  * ![@ximengailun](https://avatars.githubusercontent.com/u/145632297?s=64&v=4)\n  * ![@moizez256](https://avatars.githubusercontent.com/u/28323489?s=64&v=4)\n  * ![@dkffkrsm](https://avatars.githubusercontent.com/u/47800954?s=64&v=4)\n\n+ 52,146  ](https://github.com/scrapy/scrapy/network/dependents)\n##  [Contributors 577](https://github.com/scrapy/scrapy/graphs/contributors)\n  * [ ![@pablohoffman](https://avatars.githubusercontent.com/u/185212?s=64&v=4) ](https://github.com/pablohoffman)\n  * [ ![@dangra](https://avatars.githubusercontent.com/u/37369?s=64&v=4) ](https://github.com/dangra)\n  * [ ![@wRAR](https://avatars.githubusercontent.com/u/241039?s=64&v=4) ](https://github.com/wRAR)\n  * [ ![@kmike](https://avatars.githubusercontent.com/u/107893?s=64&v=4) ](https://github.com/kmike)\n  * [ ![@Gallaecio](https://avatars.githubusercontent.com/u/705211?s=64&v=4) ](https://github.com/Gallaecio)\n  * [ ![@elacuesta](https://avatars.githubusercontent.com/u/1731933?s=64&v=4) ](https://github.com/elacuesta)\n  * [ ![@redapple](https://avatars.githubusercontent.com/u/886296?s=64&v=4) ](https://github.com/redapple)\n  * [ ![@void](https://avatars.githubusercontent.com/u/25639?s=64&v=4) ](https://github.com/void)\n  * [ ![@curita](https://avatars.githubusercontent.com/u/213781?s=64&v=4) ](https://github.com/curita)\n  * [ ![@eliasdorneles](https://avatars.githubusercontent.com/u/37565?s=64&v=4) ](https://github.com/eliasdorneles)\n  * [ ![@lopuhin](https://avatars.githubusercontent.com/u/424613?s=64&v=4) ](https://github.com/lopuhin)\n  * [ ![@Laerte](https://avatars.githubusercontent.com/u/5853172?s=64&v=4) ](https://github.com/Laerte)\n  * [ ![@whalebot-helmsman](https://avatars.githubusercontent.com/u/1234272?s=64&v=4) ](https://github.com/whalebot-helmsman)\n  * [ ![@adityaa30](https://avatars.githubusercontent.com/u/37837542?s=64&v=4) ](https://github.com/adityaa30)\n\n\n[+ 563 contributors](https://github.com/scrapy/scrapy/graphs/contributors)\n## Languages\n  * [ Python 99.8% ](https://github.com/scrapy/scrapy/search?l=python)\n  * Other 0.2%\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n## TL;DR Summary\n\nScrapy is a fast, high-level web crawling and scraping framework for Python, maintained by Zyte. It supports Python 3.9+ and is used for data mining, monitoring, and automated testing. Installation is simple via `pip install scrapy`. More info at [scrapy.org](https://scrapy.org).\n```"
  },
  {
    "instruction": "# Notes\n\n\n\n<child_page>\n# ML Pipeline Architecture Design Patterns (With 10 Real-World Examples)\n\n[https://neptune.ai/blog/ml-pipeline-architecture-design-patterns](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns)\n\n# Summary\n\nThis article explores ML pipeline architecture design patterns, highlighting the distinction between ML pipeline architecture and design. It emphasizes the importance of following these patterns for efficiency, scalability, reproducibility, and standardization. The article outlines key stages in ML pipelines, including data ingestion, preprocessing, feature engineering, model training, evaluation, deployment, and monitoring. It then delves into 10 ML pipeline architecture examples, such as single leader architecture, directed acyclic graphs (DAGs), foreach pattern, synchronous training, parameter server architecture, and Ring-AllReduce architecture. Each pattern is explained with its advantages, disadvantages, and real-world applications, providing a comprehensive overview of ML pipeline architectures in practice.\n---\n\n# Details\n\n Two terms are often used interchangeably, yet they hold distinct meanings.\n- ML pipeline architecture\u00a0is like the high-level musical score for the symphony. It\u00a0outlines the components, stages, and workflows within the ML pipeline.\n- In contrast,\u00a0ML pipeline design\u00a0is a deep dive into the composition of the ML pipeline,\u00a0dealing with the tools, paradigms, techniques, and programming languages used to implement the pipeline and its components.\n\nFour reasons for following ML pipeline architecture and designs:\n- Efficiency\n- Scalability\n- Templating and reproducibility\n- Standardization\n\nMLOps: monitoring, maintaining, and deployment stages\n\nMain stages within ML pipelines:\n- Data Ingestion (e.g., Apache Kafka, Amazon Kinesis)\n- Data Preprocessing (e.g., pandas, NumPy)\n- Feature Engineering and Selection (e.g., Scikit-learn, Feature Tools)\n- Model Training (e.g., TensorFlow, PyTorch)\n- Model Evaluation (e.g., Scikit-learn, MLflow)\n- Model Deployment (e.g., TensorFlow Serving, TFX)\n- Monitoring and Maintenance (e.g., Prometheus, Grafana)\n# 10 ML pipeline architecture examples\n\n# Single leader architecture\n\n- Master-slave architecture\n- Pros: Scale horizontally for read operations\n- Cons: Scale vertically for write operations\n\n# Directed acyclic graphs (DAG)\n\nDirected graphs are made up of nodes, edges, and directions. The nodes represent processes; edges in graphs depict relationships between processes, and the direction of the edges signifies the flow of process execution or data/signal transfer within the graph.\nFor instance, a condition in graphs where loops between vertices or nodes are disallowed. This type of graph is called an acyclic graph\nDAGs can easily be sorted topologically.\nUsing DAGs provides an efficient way to execute processes and tasks in various applications, including big data analytics, machine learning, and artificial intelligence, where task dependencies and the order of execution are crucial.\nIn modern machine learning pipelines that are expected to be adaptable and operate within dynamic environments or workflows, DAGs are unsuitable for modelling and managing these systems or pipelines, primarily because DAGs are ideal for static workflows with predefined dependencies.\u00a0\n# Foreach pattern\n\nThe foreach pattern is a code execution paradigm that iteratively executes a piece of code for the number of times an item appears within a collection or set of data.\nFor example, the foreach pattern can be used in the model training stage of ML pipelines, where a model is repeatedly exposed to different partitions of the dataset for training and others for testing over a specified amount of time.\n[Image](No URL)\n\n\nUtilizing the DAG architecture and foreach pattern in an ML pipeline enables a robust, scalable, and manageable ML pipeline solution.\u00a0\nBelow is an illustration of an ML pipeline leveraging DAG and foreach pattern. The flowchart represents a machine learning pipeline where each stage (Data Collection, Data Preprocessing, Feature Extraction, Model Training, Model Validation, and Prediction Generation) is represented as a Directed Acyclic Graph (DAG) node. Within each stage, the \u201cforeach\u201d pattern is used to apply a specific operation to each item in a collection:\n[Image](No URL)\nBut there are some disadvantages to it as well.\nWhen utilizing the foreach pattern in data or feature processing stages, all data must be loaded into memory before the operations can be executed. This can lead to poor computational performance, mainly when processing large volumes of data that may exceed available memory resources. For instance, in a use-case where the dataset is several terabytes large, the system may run out of memory, slow down, or even crash if it attempts to load all the data simultaneously.\nAnother limitation of the foreach pattern lies in the execution order of elements within a data collection. The foreach pattern does not guarantee a consistent order of execution or order in the same form the data was loaded. Inconsistent order of execution within foreach patterns can be problematic in scenarios where the sequence in which data or features are processed is significant (e.g., time-series)\n# Synchronous training\n\nIn this context, synchronous training involves a coordinated effort among all independent computational units, referred to as \u2018workers\u2019. Each worker holds a partition of the model and updates its parameters using its portion of the evenly distributed data. \n[Image](No URL)\n\nSynchronous Training is relevant to scenarios or use cases where there is a need for even distribution of training data across compute resources, uniform computational capacity across all resources, and low latency communication between these independent resources. \nCompared to asynchronous methods, synchronous training often achieves superior results as workers\u2019 synchronized and uniform operation reduces variance in parameter updates at each step.\nSynchronous training may pose time efficiency issues as it requires the completion of tasks by all workers before proceeding to the next step.\u00a0\n# Parameter server architecture\n\nThis architecture operates on the principle of server-client relationships, where the client nodes, referred to as \u2018workers\u2019, are assigned specific tasks such as handling data, managing model partitions, and executing defined operations.\nOn the other hand, the server node plays a central role in managing and aggregating the updated model parameters and is also responsible for communicating these updates to the client nodes.\n[Image](No URL)\n# Ring-AllReduce architecture\n\nThe workers independently compute their gradients during backward propagation on their own partition of the training data. A ring-like structure is applied to ensure each worker on a device has a model with parameters that include the gradient updates made on all other independent workers.\nhis is achieved by passing the sum of gradients from one worker to the next worker in the ring, which then adds its own computed gradient to the sum and passes it on to the following worker. This process is repeated until all the workers have the complete sum of the gradients aggregated from all workers in the ring.\n[Image](No URL)\nReal-world applications involving distributed machine learning training, particularly in scenarios requiring handling extensive datasets (e.g., Meta, Google)\n\n- It enables effective data parallelism by ensuring optimal utilization of computational resources. Each worker node holds a complete copy of the model and is responsible for training on its subset of the data.\u00a0\n- Another advantage of Ring-AllReduce is that it allows for the aggregation of model parameter updates across multiple devices.\n# Others\n\n- Embeddings\n- Data parallelism\n- Model parallelism\n- Federated learning\n</child_page>\n\n\n\n<child_page>\n# Natural workflow of an ML system: from model development to ops\n\n[https://nicolay-gerold.notion.site/Season-3-MLOps-How-AI-Is-Built-2be82e4d93604c0594d84475f0a77721](https://nicolay-gerold.notion.site/Season-3-MLOps-How-AI-Is-Built-2be82e4d93604c0594d84475f0a77721)\n---\n\n# 1. Model Development: Building the Porcelain\n\n# Data Automation\n\n- Implementing automated processes for data collection, cleaning, and preparation\n- Ensuring data quality and consistency through automated pipelines\n# Data Quality and Validation\n\n- Implementing Data Quality Gates and Automated Data Quality Monitoring\n- Designing Data Validation Systems That Scale\n- Utilizing tools like Great Expectations and Pydantic for data validation\n# Version Control for Data\n\n- Implementing Git-Like Operations for Data\n- Data Version Control with lakeFS: A Practical Guide\n- Managing Dataset Evolution in Production\n# Experiment Management and Reproducibility\n\n- Building a Reproducible ML Pipeline with tools like Weights & Biases (W&B) or Comet\n- From Prototype to Production: Effective Experiment Management\n- Scaling ML Experiments: Infrastructure and Best Practices\n# Model Validation and Testing\n\n- Beyond Accuracy: Comprehensive Model Validation Strategies\n- Building Model Validation Pipelines That Scale\n- Testing Model Behavior: From Unit Tests to Integration\n# 2. Deployment & Serving\n\n# Security and Abuse Prevention\n\n- Preventing abuse of customer-facing ML & AI Systems\n- Implementing access controls and monitoring for potential misuse\n# Batch Processing Optimization\n\n- Maximizing throughput for batch workloads\n- Best practices for efficient batch processing in ML systems\n# Gradual Rollout and Testing\n\n- Canary Testing for ML: Implementing gradual rollouts and monitoring performance\n- A/B Testing for ML: Designing and executing controlled experiments to compare model versions\n# Continuous Integration and Deployment (CI/CD)\n\n- Implementing CI/CD pipelines specifically for ML workflows\n- Automating model deployment and updates\n# Model Lifecycle Management\n\n- Strategies for managing the entire lifecycle of ML models\n- Version control, monitoring, and updating models in production\n# Model Serving Patterns\n\n- Comparing Model Serving Patterns: Batch, Real-Time, Micro-Batch, Edge\n- Infrastructure Choices: Self-Hosted, Cloud Services, Edge/Browser\n# Distributed Inference\n\n- Techniques for distributed inference across various model types (LLMs, Embedding Models, CV, Classification)\n- Leveraging tools like Triton for efficient distributed inference\n# Self-Hosting AI\n\n- Strategies and considerations for self-hosting AI models\n- Case studies of organizations managing their own GPU clusters for AI inference\n# AI on Edge Devices\n\n- Serving AI to unknown devices: Bringing AI to local devices\n- Lessons from Federated Learning for distributed model training and inference\n# 3. Quality & Reliability\n\n# Testing\n\n- Building a Test-Driven ML Development Workflow\n- Implementing Data Validation: From Schema to Semantics\n- Designing Model Test Suites: Beyond Accuracy Metrics\n- Performance Testing ML Systems: Load, Stress, and Scale\n- Integration Testing for ML: Combining Code, Data, and Models\n# Monitoring & Observability\n\n- Building Real-Time Data Drift Detection Systems\n- Implementing Production Model Quality Monitors\n- Designing ML-Specific Logging Systems\n- Distributed Tracing for ML Systems: End-to-End Visibility\n- Monitoring Complex ML Pipelines: From Data to Prediction\n# Incident Recovery\n\n- Building Alert Systems for ML Applications\n- Debugging ML Systems: From Model to Infrastructure\n- Designing Recovery Procedures for ML Services\n- Creating ML Incident Playbooks: Lessons from Production\n# 4. Automation & Scale: Building the Plumbing\n\n# CI/CD for ML\n\n- Building CI Pipelines for ML: Beyond Standard Software Testing\n- Implementing Continuous Training: From Data to Deployment\n- Automating Model Evaluation in CI/CD Pipelines\n- Creating Deployment Pipelines for ML Services\n- Building Self-Healing ML Systems: Automated Recovery and Rollback\n# Scale Management\n\n- Scaling Model Training: From Single GPU to Cluster\n- Building Distributed Inference Systems: Architecture and Implementation\n- Optimizing Resource Usage in Large-Scale ML Systems\n- Managing Multi-Model ML Services: Infrastructure and Orchestration\n- Optimizing ML Infrastructure Costs: A Practical Guide\n# Container Orchestration\n\n- Orchestrating containers for training and inference\n- Managing GPU Resources in a Multi-Team Environment\n- Building Elastic ML Training Infrastructure\n# Security & Governance\n\n- Implementing robust access control mechanisms\n- Establishing model governance frameworks\n- Abuse Prevention (e.g., user request rate limiting)\n- Ensuring compliance with data protection regulations\nThis structured overview provides a comprehensive look at the key aspects of ML model development, deployment, and serving, covering everything from initial data preparation to large-scale production systems. Each section addresses critical components of the ML lifecycle, emphasizing best practices, tools, and strategies for building robust, scalable, and reliable ML systems.\n</child_page>\n\n\n---\n\n# Community\n\n[https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/)\n[https://neptune.ai/blog/mlops-tools-platforms-landscape](https://neptune.ai/blog/mlops-tools-platforms-landscape)\n[https://www.youtube.com/watch?v=gxw1gMYP0WM&t=1171s](https://www.youtube.com/watch?v=gxw1gMYP0WM&t=1171s)\n[From MLOps to ML Systems with Feature/Training/Inference Pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)\n[Building an End-to-End MLOps Pipeline with Open-Source Tools](https://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-with-open-source-tools-d8bacbf4184f)\n[CI/CD for Machine Learning in 2024: Best Practices to Build, Train, and Deploy](https://medium.com/infer-qwak/ci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2)\nThe FTI Pipeline Architecture & Feature Store series:\n\t- [Modularity and Composability for AI Systems with AI Pipelines and Shared Storage](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)\n\t- [A Taxonomy for Data Transformations in AI Systems](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)\n\n[https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)\n# Papers\n\n\t- [Practitioners guide to MLOps: A framework for continuous delivery and automation of machine learning.](https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf)\n\t- [The Hopsworks Feature Store for Machine Learning](https://dl.acm.org/doi/10.1145/3626246.3653389)\n\n# Blogs\n\n\t- [https://www.hopsworks.ai/blog](https://www.hopsworks.ai/blog)\n\t- [https://www.qwak.com/blog](https://www.qwak.com/blog)\n\t- [https://www.featurestore.org/](https://www.featurestore.org/)\n\n# Courses\n\n\t- [https://github.com/DataTalksClub/mlops-zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)\n\t- [https://github.com/GokuMohandas/Made-With-ML](https://github.com/GokuMohandas/Made-With-ML)",
    "answer": "```markdown\n# TL;DR Summary\n\nThe documents discuss ML pipeline architecture and MLOps, emphasizing design patterns for efficiency, scalability, and reproducibility. Key stages include data ingestion, preprocessing, training, and deployment. Examples like DAGs and synchronous training illustrate practical applications. Best practices for model development, deployment, and monitoring are outlined, focusing on automation, security, and CI/CD integration.\n```"
  },
  {
    "instruction": "Visit booth 3171 at Google Cloud Next to see how to unlock real-time decisions at scale[More info](https://aerospike.com/events/google-cloud-next-2025)\n[](https://aerospike.com/)\nProducts\nSolutions\nResources\nCompany\n[Contact](https://aerospike.com/forms/contact-us/)\nLogin\n[Try now](https://aerospike.com/try-now)\n[](https://aerospike.com/search)\n![](https://aerospike.com/bg-image-lossless.jpeg)\n# Build for infinite scale, speed, and savings\nThe massively scalable, millisecond latency, real-time database.\n[Try Aerospike now](https://aerospike.com/try-now)\nAerospike is a distributed NoSQL database with blazing-fast reads/writes and unmatched uptime.\n<1ms\nRead and write times\n10x\nIngestion volume\n80%\nFewer servers\n## What Aerospike can do for you\n### Always-on, secure managed Aerospike\nFocus on delivering value to customers rather than managing, optimizing, and securing your database and infrastructure.\n### Be ready for an AI-driven future\nWith the ability to ingest massive amounts of streaming data from tens of thousands of sources, Aerospike helps deliver more accurate AI.\n### Eliminate friction\nSimultaneously achieve superior performance, increased scale and reduced operational costs.\n### Why leading companies choose Aerospike\n[See all customer stories](https://aerospike.com/customers)\n[![Airtel-logo](https://images.ctfassets.net/drk57q8lctrm/7cP2voPqGtx2FiT8ZxNLWD/6a8335bbf349063cacdf593686e56e8f/airtel-logo.svg)](https://aerospike.com/customers/airtel-aerospike-customer-story/)\n[![DBS Logo](https://images.ctfassets.net/drk57q8lctrm/6HCaDtzlif8UXKYbQ7ZMQM/323bfcabd27a1d75591e32bdfc72920d/dbs-bank-logo.svg)](https://aerospike.com/resources/webinars/dbs-fireside-chat-modernising-finserv-applications-for-real-time-decisioning/)\n[![PayPal logo](https://images.ctfassets.net/drk57q8lctrm/21FLkQ2lbOCWynXsDZvXO5/485a163f199ef7749b914e54d4dc3335/paypal-logo.webp)](https://aerospike.com/customers/paypal-aerospike-customer-story/)\n[![Adobe logo ](https://images.ctfassets.net/drk57q8lctrm/19gJVR9SNHCZgZSb83bAXc/c1ce53e62d38ca893a0d861cf676efb7/Adobe_Corporate_logo.svg)](https://aerospike.com/customers/adobe-aerospike-customer-story/)\n[![Rakuten logo](https://images.ctfassets.net/drk57q8lctrm/76dCdJCwCiIly5IgXRtlgC/9a35bc5ff6fe8bd572a5a224c10f37d8/rakuten-logo.svg)](https://aerospike.com/resources/webinars/rakuten-aerospike-targeted-ads/)\n[![Lexis Nexis Risk Solutions logo](https://images.ctfassets.net/drk57q8lctrm/6Fhy5z8QTKJnWMFSazkX7p/7ebea22be2ac161b290c8771396578cd/lexisnexis-risk-solutions-logo.svg)](https://aerospike.com/customers/lexisnexis-aerospike-customer-story/)\n[![Criteo logo](https://images.ctfassets.net/drk57q8lctrm/75m8dGKOGuwQfjO0rfUXdJ/370e6f00d94358b16214be1ec588dfa9/criteo-logo.svg)](https://aerospike.com/resources/customer-stories/criteo-real-time-ad-bidding/)\n[![Experian logo](https://images.ctfassets.net/drk57q8lctrm/1NIUjFYmJX050g1GvqzmMh/801006d72b93f918156136e4ad7f1025/experian-logo.svg)](https://aerospike.com/blog/welcome-experian-to-aerospike-cloud-managed-service/)\n[![Snap logo](https://images.ctfassets.net/drk57q8lctrm/1M9Ubs4AXsV6CLXYC0QDDd/66746d329222392bb8cf72e499dc45d9/snap-inc-logo.svg)](https://aerospike.com/resources/videos/snapchat-any-cloud-strategy/)\n[![Sony Interactive Entertainment logo](https://images.ctfassets.net/drk57q8lctrm/2k20vVwOIkWEXpm1ySoSJ7/ba007cb9ea4b962172c1c29c0d55d8f2/sony-interactive-entertainment-logo.webp)](https://aerospike.com/resources/customer-stories/sony)\n## Multi-model for developers\n[Aerospike Graph](https://aerospike.com/products/graph-database/), utilizing Apache TinkerPop and Gremlin, suits applications that rely on vital data relationships, ensuring easy integration and powerful traversal capabilities.\nSupport for flexible [JSON document models](https://aerospike.com/blog/query-json-documents-faster-and-more-with-new-cdt-indexing/) to query support to store, search, and manage complex, hierarchical data sets and workloads.\nDevelop applications that draw similarity-based insights against very large data sets to make complex decisions in real-time with [Aerospike Vector Search](https://aerospike.com/products/vector-database-search-llm/).\nAerospike's [Key-Value Store](https://aerospike.com/docs/server/guide/kvs) provides a high-performance, scalable solution for applications that require simple, efficient access to large volumes of data with low-latency read and write operations, making it ideal for caching, session management, and real-time analytics.\n[Docs](https://aerospike.com/docs)[Try now](https://aerospike.com/try-now)\n## Infinite scalability, lowest latency, lowest costs\nCount on the real-time database that\u2019s always on \u2013 globally.\n[![Grow](https://images.ctfassets.net/drk57q8lctrm/4lpdvTGoNmLafmN8zFSl8M/06b1fbc97eec57ad9522fd6669fba909/grow_icon_1_.svg?w=200&h=200&fit=pad)Grow from gigabytes to petabytes without tradeoffsPredictable and consistent performance, for any amount of data.How we make it happen](https://aerospike.com/products/database)\n[![Do more](https://images.ctfassets.net/drk57q8lctrm/4iNkpw5boUp5A5GzkZqbCB/066321b6aa1438e5a860357ac34de847/do_more_icon.svg?w=200&h=200&fit=pad)Do more with sub-millisecond accessSee how our patented Hybrid Memory Architecture\u2122 helps you work faster.All about HMA](https://aerospike.com/blog/hybrid-memory-architecture-optimization/)\n[![CO2](https://images.ctfassets.net/drk57q8lctrm/47x7Fv3TEU2UTkhb1ycsDr/8d292eaac52e14b93487049261b12ebb/co2_icon.svg?w=200&h=200&fit=pad)Meet sustainability and profitability goalsUse fewer servers than ever before to reduce your carbon footprint and infrastructure costs.Too good to be true?](https://aerospike.com/solutions/use-cases/database-sustainability/)\n## Latest from Aerospike\nLearn more from technical insights and success stories.\n[Resource library](https://aerospike.com/resources/)[Blog](https://aerospike.com/blog)\n  * [BlogIntroducing ACID Transactions in Aerospike 8.0Aerospike 8.0 brings distributed ACID transactions, building on years of strong consistency experience. Perfect for applications that demand data integrity. ](https://aerospike.com/blog/aerospike8-transactions/)\n  * [BlogAerospike 8.0: Distributed ACID Transactions for DevelopersDive into Aerospike 8.0's groundbreaking distributed ACID transactions, designed to enhance consistency and scalability for developers. Explore how this update revolutionizes database operations.](https://aerospike.com/blog/aerospike8-developers/)\n  * [White paperFive signs you\u2019ve outgrown CouchbaseFrom increasing costs to poor performance at scale, Couchbase users often face roadblocks. Here are five clear signs your business needs a more scalable, efficient database solution.](https://aerospike.com/resources/white-papers/five-signs-youve-outgrown-couchbase/)\n\n\nAbout\n[Careers](https://aerospike.com/careers)\n[Legal notices](https://aerospike.com/legal)\n[Privacy policy](https://aerospike.com/legal/privacy-policy)\n[Terms of service](https://aerospike.com/legal/terms-conditions)\n[Marketing preference center](https://pages.aerospike.com/Subscription_Management.html)\n[Update cookie preferences](https://aerospike.com/)\nSupport\n[Support portal](https://aerospike.my.site.com/login)\n[Knowledge base](https://support.aerospike.com/s/)\n[Community forum](https://discuss.aerospike.com/)\n(+1) 408-462-AERO (2376)\n2440 W. El Camino Real, Suite 100 Mountain View, CA 94040\n[Contact us](https://aerospike.com/forms/contact-us/)\n[](https://discord.gg/NfC93wJEJU)[](https://github.com/aerospike)[](https://stackoverflow.com/questions/tagged/aerospike)[](https://www.youtube.com/aerospike)[](https://twitter.com/aerospikedb)[](https://www.linkedin.com/company/2696852)\n[](https://aerospike.com/)\n\u00a9 2025 Aerospike, Inc. \u2013 All rights reservedAerospike\u00ae is a registered trademark of Aerospike, Inc.\n",
    "answer": "```markdown\n## TL;DR Summary\n\nVisit booth 3171 at Google Cloud Next to explore Aerospike's real-time, scalable NoSQL database. It offers sub-millisecond latency, supports AI-driven applications, and reduces operational costs. Key clients include Airtel, PayPal, and Adobe. Learn more about its multi-model capabilities and ACID transactions in Aerospike 8.0.\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fmongodb-developer%2FGenAI-Showcase%2Ftree%2Fmain%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fmongodb-developer%2FGenAI-Showcase%2Ftree%2Fmain%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=mongodb-developer%2FGenAI-Showcase) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/) to refresh your session. Dismiss alert\n{{ message }}\n[ mongodb-developer ](https://github.com/mongodb-developer) / **[GenAI-Showcase](https://github.com/mongodb-developer/GenAI-Showcase) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Fmongodb-developer%2FGenAI-Showcase) You must be signed in to change notification settings\n  * [ Fork 558 ](https://github.com/login?return_to=%2Fmongodb-developer%2FGenAI-Showcase)\n  * [ Star  3.1k ](https://github.com/login?return_to=%2Fmongodb-developer%2FGenAI-Showcase)\n\n\nGenAI Cookbook \n[www.mongodb.com/cloud/atlas/register](https://www.mongodb.com/cloud/atlas/register \"https://www.mongodb.com/cloud/atlas/register\")\n### License\n[ MIT license ](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/LICENSE)\n[ 3.1k stars ](https://github.com/mongodb-developer/GenAI-Showcase/stargazers) [ 558 forks ](https://github.com/mongodb-developer/GenAI-Showcase/forks) [ Branches ](https://github.com/mongodb-developer/GenAI-Showcase/branches) [ Tags ](https://github.com/mongodb-developer/GenAI-Showcase/tags) [ Activity ](https://github.com/mongodb-developer/GenAI-Showcase/activity)\n[ Star  ](https://github.com/login?return_to=%2Fmongodb-developer%2FGenAI-Showcase)\n[ Notifications ](https://github.com/login?return_to=%2Fmongodb-developer%2FGenAI-Showcase) You must be signed in to change notification settings\n  * [ Code ](https://github.com/mongodb-developer/GenAI-Showcase)\n  * [ Issues 4 ](https://github.com/mongodb-developer/GenAI-Showcase/issues)\n  * [ Pull requests 6 ](https://github.com/mongodb-developer/GenAI-Showcase/pulls)\n  * [ Actions ](https://github.com/mongodb-developer/GenAI-Showcase/actions)\n  * [ Projects 0 ](https://github.com/mongodb-developer/GenAI-Showcase/projects)\n  * [ Security ](https://github.com/mongodb-developer/GenAI-Showcase/security)\n  * [ Insights ](https://github.com/mongodb-developer/GenAI-Showcase/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/mongodb-developer/GenAI-Showcase)\n  * [ Issues  ](https://github.com/mongodb-developer/GenAI-Showcase/issues)\n  * [ Pull requests  ](https://github.com/mongodb-developer/GenAI-Showcase/pulls)\n  * [ Actions  ](https://github.com/mongodb-developer/GenAI-Showcase/actions)\n  * [ Projects  ](https://github.com/mongodb-developer/GenAI-Showcase/projects)\n  * [ Security  ](https://github.com/mongodb-developer/GenAI-Showcase/security)\n  * [ Insights  ](https://github.com/mongodb-developer/GenAI-Showcase/pulse)\n\n\n# mongodb-developer/GenAI-Showcase\nmain\n[**21** Branches](https://github.com/mongodb-developer/GenAI-Showcase/branches)[**0** Tags](https://github.com/mongodb-developer/GenAI-Showcase/tags)\n[](https://github.com/mongodb-developer/GenAI-Showcase/branches)[](https://github.com/mongodb-developer/GenAI-Showcase/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n[![RichmondAlake](https://avatars.githubusercontent.com/u/16384755?v=4&size=40)](https://github.com/RichmondAlake)[RichmondAlake](https://github.com/mongodb-developer/GenAI-Showcase/commits?author=RichmondAlake)[Merge pull request](https://github.com/mongodb-developer/GenAI-Showcase/commit/f20829ba5abc29b1399127e2eae17655f5fd7aa5) [#102](https://github.com/mongodb-developer/GenAI-Showcase/pull/102) [from mongodb-developer/postgres_performance_g\u2026](https://github.com/mongodb-developer/GenAI-Showcase/commit/f20829ba5abc29b1399127e2eae17655f5fd7aa5)Mar 27, 2025[f20829b](https://github.com/mongodb-developer/GenAI-Showcase/commit/f20829ba5abc29b1399127e2eae17655f5fd7aa5) \u00b7 Mar 27, 2025\n## History\n[502 Commits](https://github.com/mongodb-developer/GenAI-Showcase/commits/main/)[](https://github.com/mongodb-developer/GenAI-Showcase/commits/main/)  \n[.github](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/.github \".github\")| [.github](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/.github \".github\")| [Adding codeowners file](https://github.com/mongodb-developer/GenAI-Showcase/commit/e1c1f8321b84923ed492f93ed758605d6406d583 \"Adding codeowners file\")| Jan 15, 2025  \n[apps](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/apps \"apps\")| [apps](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/apps \"apps\")| [Pre-commit checks are green](https://github.com/mongodb-developer/GenAI-Showcase/commit/fbe5e9c8275e0c755391352dfb8be62f46793302 \"Pre-commit checks are green\")| Mar 19, 2025  \n[misc](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/misc \"misc\")| [misc](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/misc \"misc\")| [Updating README](https://github.com/mongodb-developer/GenAI-Showcase/commit/4d8e8b6aa48b7c01b41bfae99b3d4ed9f6f9a781 \"Updating README\")| Feb 5, 2025  \n[notebooks](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/notebooks \"notebooks\")| [notebooks](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/notebooks \"notebooks\")| [Merge pull request](https://github.com/mongodb-developer/GenAI-Showcase/commit/f20829ba5abc29b1399127e2eae17655f5fd7aa5 \"Merge pull request #102 from mongodb-developer/postgres_performance_guidance\nPostgres performance guidance\") [#102](https://github.com/mongodb-developer/GenAI-Showcase/pull/102) [from mongodb-developer/postgres_performance_g\u2026](https://github.com/mongodb-developer/GenAI-Showcase/commit/f20829ba5abc29b1399127e2eae17655f5fd7aa5 \"Merge pull request #102 from mongodb-developer/postgres_performance_guidance\nPostgres performance guidance\")| Mar 27, 2025  \n[partners](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/partners \"partners\")| [partners](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/partners \"partners\")| [Pre-commit checks are green](https://github.com/mongodb-developer/GenAI-Showcase/commit/fbe5e9c8275e0c755391352dfb8be62f46793302 \"Pre-commit checks are green\")| Mar 19, 2025  \n[tools](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/tools \"tools\")| [tools](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/tools \"tools\")| [Updating README](https://github.com/mongodb-developer/GenAI-Showcase/commit/4d8e8b6aa48b7c01b41bfae99b3d4ed9f6f9a781 \"Updating README\")| Feb 5, 2025  \n[.gitignore](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/.gitignore \".gitignore\")| [INTPYTHON-442 Add autoformatting to repo](https://github.com/mongodb-developer/GenAI-Showcase/commit/e5c87fe01fbb766c071ad2ce8e51699307785336 \"INTPYTHON-442 Add autoformatting to repo\")| Dec 10, 2024  \n[.pre-commit-config.yaml](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [Update Jupyter notebooks](https://github.com/mongodb-developer/GenAI-Showcase/commit/4ab0cc3c42da8902b8ab5a036d848b36726e2534 \"Update Jupyter notebooks\nSigned-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>\")| Mar 6, 2025  \n[CONTRIBUTING.md](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [Updating contribution guidelines](https://github.com/mongodb-developer/GenAI-Showcase/commit/6fb2054ef54ab2bcc4e3f28590480ce659ca3b53 \"Updating contribution guidelines\")| Jan 15, 2025  \n[LICENSE](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/LICENSE \"LICENSE\")| [LICENSE](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/LICENSE \"LICENSE\")| [Create LICENSE](https://github.com/mongodb-developer/GenAI-Showcase/commit/97e92db59bde3667f5fab3ac18874faa1fcd04f3 \"Create LICENSE\nMIT\")| Oct 1, 2024  \n[README.md](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/README.md \"README.md\")| [README.md](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/README.md \"README.md\")| [updated readme](https://github.com/mongodb-developer/GenAI-Showcase/commit/8079e20ab82932aaf624b3e058ffa1ed33c05236 \"updated readme\")| Mar 20, 2025  \n[ruff.toml](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/ruff.toml \"ruff.toml\")| [ruff.toml](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/ruff.toml \"ruff.toml\")| [Updating README](https://github.com/mongodb-developer/GenAI-Showcase/commit/4d8e8b6aa48b7c01b41bfae99b3d4ed9f6f9a781 \"Updating README\")| Feb 5, 2025  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/)\n  * [MIT license](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/)\n\n\n## MongoDB's GenAI Showcase\n[](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/#mongodbs-genai-showcase)\nWelcome to MongoDB's Generative AI Showcase Repository!\nWhether you are just starting out on your Generative AI journey, or looking to build advanced GenAI applications, we've got you covered. This repository has an exhaustive list of examples and sample applications that cover Retrieval-Augmented Generation (RAG), AI Agents, and industry-specific use cases.\nDiscover how MongoDB integrates into RAG pipelines and AI Agents, serving as a vector database, operational database, and memory provider.\n## Contents\n[](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/#contents)\nThis repo mainly contains:\nFolder | Description  \n---|---  \n[`notebooks`](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/notebooks/README.md) | Jupyter notebooks examples for RAG, agentic applications, evaluations etc.  \n[`apps`](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/apps/README.md) | Javascipt and Python apps and demos  \n[`partners`](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/partners/README.md) | Contributions from our AI partners  \n## Getting Started\n[](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/#getting-started)\nYou will need to connect to a MongoDB cluster to run any of the apps or examples in this repo. Follow these steps to get set up:\n  * Register for a [free MongoDB Atlas account](https://www.mongodb.com/cloud/atlas/register)\n  * [Create a new database cluster](https://www.mongodb.com/docs/guides/atlas/cluster/)\n  * [Obtain the connection string](https://www.mongodb.com/docs/guides/atlas/connection-string/) for your database cluster\n\n\n## Contributing\n[](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/#contributing)\nWe welcome contributions! Please read our [Contribution Guidelines](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/CONTRIBUTING.md) for more information on how to participate.\n## License\n[](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/#license)\nThis project is licensed under the [MIT License](https://github.com/mongodb-developer/GenAI-Showcase/blob/main/LICENSE).\n## Getting Support\n[](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/#getting-support)\nAs you work through these examples, if you encounter any problems, please [open a new issue](https://github.com/mongodb-developer/GenAI-Showcase/issues/new).\n## Additional Resources\n[](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/#additional-resources)\n  * [AI Learning Hub](https://www.mongodb.com/resources/use-cases/artificial-intelligence?utm_campaign=ai_learning_hub&utm_source=github&utm_medium=referral)\n  * [GenAI Community Forum](https://www.mongodb.com/community/forums/c/generative-ai/162)\n  * [Tutorials and code examples from our official docs](https://github.com/mongodb/docs-notebooks)\n\n\n## About\nGenAI Cookbook \n[www.mongodb.com/cloud/atlas/register](https://www.mongodb.com/cloud/atlas/register \"https://www.mongodb.com/cloud/atlas/register\")\n### Topics\n[ artificial-intelligence ](https://github.com/topics/artificial-intelligence \"Topic: artificial-intelligence\") [ agents ](https://github.com/topics/agents \"Topic: agents\") [ rag ](https://github.com/topics/rag \"Topic: rag\") [ llms ](https://github.com/topics/llms \"Topic: llms\") [ generative-ai ](https://github.com/topics/generative-ai \"Topic: generative-ai\")\n### Resources\n[ Readme ](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/#readme-ov-file)\n### License\n[ MIT license ](https://github.com/mongodb-developer/GenAI-Showcase/tree/main/#MIT-1-ov-file)\n[ Activity](https://github.com/mongodb-developer/GenAI-Showcase/activity)\n[ Custom properties](https://github.com/mongodb-developer/GenAI-Showcase/custom-properties)\n### Stars\n[ **3.1k** stars](https://github.com/mongodb-developer/GenAI-Showcase/stargazers)\n### Watchers\n[ **73** watching](https://github.com/mongodb-developer/GenAI-Showcase/watchers)\n### Forks\n[ **558** forks](https://github.com/mongodb-developer/GenAI-Showcase/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fmongodb-developer%2FGenAI-Showcase&report=mongodb-developer+%28user%29)\n##  [Releases](https://github.com/mongodb-developer/GenAI-Showcase/releases)\nNo releases published\n##  [Packages 0](https://github.com/orgs/mongodb-developer/packages?repo_name=GenAI-Showcase)\nNo packages published \n##  [Contributors 23](https://github.com/mongodb-developer/GenAI-Showcase/graphs/contributors)\n  * [ ![@RichmondAlake](https://avatars.githubusercontent.com/u/16384755?s=64&v=4) ](https://github.com/RichmondAlake)\n  * [ ![@Pash10g](https://avatars.githubusercontent.com/u/16643200?s=64&v=4) ](https://github.com/Pash10g)\n  * [ ![@ajosh0504](https://avatars.githubusercontent.com/u/30438249?s=64&v=4) ](https://github.com/ajosh0504)\n  * [ ![@anaiyaraisin](https://avatars.githubusercontent.com/u/86017775?s=64&v=4) ](https://github.com/anaiyaraisin)\n  * [ ![@snarvaez](https://avatars.githubusercontent.com/u/1014889?s=64&v=4) ](https://github.com/snarvaez)\n  * [ ![@blink1073](https://avatars.githubusercontent.com/u/2096628?s=64&v=4) ](https://github.com/blink1073)\n  * [ ![@LuceCarter](https://avatars.githubusercontent.com/u/6980734?s=64&v=4) ](https://github.com/LuceCarter)\n  * [ ![@codeSTACKr](https://avatars.githubusercontent.com/u/52665907?s=64&v=4) ](https://github.com/codeSTACKr)\n  * [ ![@cozypet](https://avatars.githubusercontent.com/u/7107896?s=64&v=4) ](https://github.com/cozypet)\n  * [ ![@ranfysvalle02](https://avatars.githubusercontent.com/u/6230773?s=64&v=4) ](https://github.com/ranfysvalle02)\n  * [ ![@ArturoNereu](https://avatars.githubusercontent.com/u/263776?s=64&v=4) ](https://github.com/ArturoNereu)\n  * [ ![@timotheekelly](https://avatars.githubusercontent.com/u/63365669?s=64&v=4) ](https://github.com/timotheekelly)\n  * [ ![@seldo](https://avatars.githubusercontent.com/u/185893?s=64&v=4) ](https://github.com/seldo)\n  * [ ![@caseyclements](https://avatars.githubusercontent.com/u/737181?s=64&v=4) ](https://github.com/caseyclements)\n\n\n[+ 9 contributors](https://github.com/mongodb-developer/GenAI-Showcase/graphs/contributors)\n## Languages\n  * [ Jupyter Notebook 94.9% ](https://github.com/mongodb-developer/GenAI-Showcase/search?l=jupyter-notebook)\n  * [ TypeScript 4.5% ](https://github.com/mongodb-developer/GenAI-Showcase/search?l=typescript)\n  * [ Python 0.4% ](https://github.com/mongodb-developer/GenAI-Showcase/search?l=python)\n  * [ JavaScript 0.1% ](https://github.com/mongodb-developer/GenAI-Showcase/search?l=javascript)\n  * [ CSS 0.1% ](https://github.com/mongodb-developer/GenAI-Showcase/search?l=css)\n  * [ HTML 0.0% ](https://github.com/mongodb-developer/GenAI-Showcase/search?l=html)\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary\n\nMongoDB's **GenAI Showcase** repository provides resources for building Generative AI applications, including examples of Retrieval-Augmented Generation (RAG) and AI Agents. It features Jupyter notebooks, JavaScript, and Python apps, and encourages contributions. Users must connect to a MongoDB cluster to utilize the resources.\n\n**Tags:** Generative AI, RAG, AI Agents, LLMs\n```"
  },
  {
    "instruction": "[![VectorHub by Superlinked](https://superlinked.com/vectorhub/_next/static/media/vectorhub-logo.d71e47bd.svg)](https://superlinked.com/vectorhub/)\n[Building Blocks](https://superlinked.com/vectorhub/building-blocks)[Articles](https://superlinked.com/vectorhub/all-articles)[Contributing](https://superlinked.com/vectorhub/contributing/contributing)[VDB Comparison](https://superlinked.com/vector-db-comparison/)\n[---](https://github.com/superlinked/superlinked)\nSearch\nSubscribe\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nMenu\nImprove your vector search[![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow-primary.2ff1db1a.svg)Try Superlinked](https://github.com/superlinked/superlinked)\nHome\nManifesto\nMost Recent\nCompare Vector DBs\nContributeSubscribe\nSearch\nTable of Contents\nRAPTOR's basic steps\nBuilding a RAPTOR tree\nCreating the Chunks\nHierarchical clustering and dimensionality reduction - setup\nHierarchical clustering\nRAPTOR tree\nOur query example\nRAPTOR query methods\nImplementing collapsed tree retrieval\nSetting up vector database and RAG\nHow did our RAPTOR RAG do?\nYour turn\nContributors\n  1. [Articles](https://superlinked.com/vectorhub/all-articles)\n/\n  2. [Improve rag with raptor](https://superlinked.com/vectorhub/articles/improve-rag-with-raptor/)\n\n\nPublication Date: October 29, 2024|\n#RAG\n|Update on Github\n# Improving RAG with RAPTOR\n## Takeaways\n[Notebook Code](https://colab.research.google.com/github/superlinked/VectorHub/blob/main/docs/assets/use_cases/improve-rag-with-raptor/raptor_with_rag.ipynb)\n  * RAPTOR solves standard RAG's chunking issues through hierarchical clustering\n\n\n  * Tree construction process: chunk docs -> embed chunks -> cluster -> summarize recursively\n\n\n  * Collapsed tree method works better for factual queries vs tree traversal\n\n\n  * Implementation uses LanceDB + embeddings via sentence-transformers\n\n\n  * RAPTOR outperforms vanilla RAG by preserving document relationships\n\n\n  * Enables both hierarchical and flat retrieval through single vector store\n\n\nTraditional [RAG](https://superlinked.com/vectorhub/articles/retrieval-augmented-generation) setups commonly split documents into fixed-size chunks. But this creates problems. If key concepts span multiple chunks, the embeddings can lose the semantic coherence of the original text. LLM queries that retrieve single chunks frequently _miss_ their relationship to crucial pieces of information buried inside other chunks. This leads to incomplete or misleading responses. **Because its chunk embeddings lack any weighting or hierarchical structure, traditional RAG's flat retrieval returns results based only on similarity or relevance scores. Key insights are often lost.**\nSo, **is there a way of getting our embeddings to preserve the relationships and hierarchical structure that exists within source documents, so that our retrieval can surface key insights, and do it efficiently**?\nYes! It's a form of semantic chunking called **hierarchical-cluster-embedding-based chunking**. Below, we'll look closely at a recent and innovative implementation of hierarchical clustering called [RAPTOR](https://arxiv.org/pdf/2401.18059) (Recursive Abstractive Processing for Tree Organized Retrieval). We'll walk you step-by-step through setting up RAPTOR RAG. Then we'll run an example query on a financial news document to evaluate **how RAPTOR performs against vanilla RAG**.\nLet's get started!\n## RAPTOR's basic steps\nRAPTOR organizes information in a tree-like structure, progressively narrowing as it moves upwards, representing original documents at higher and higher levels of abstraction, so it can retain the original meaning in full. Let's break this into steps.\n![High level overview of RAPTOR](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/improve-rag-with-raptor/raptor-7.png)\n_Above: \"Figure 1. Tree construction process\" from the[RAPTOR paper](https://arxiv.org/pdf/2401.18059)_\n**RAPTOR tree construction - basic steps:**\n  * **starting documents (leaves)** : RAPTOR first splits each raw document into chunks\n  * **embedding and clustering** : we embed these chunks ([RAPTOR authors](https://arxiv.org/pdf/2401.18059) use the UMAP dimensionality reduction algorithm to minimize chunk embedding size), and, based on their vector embeddings, cluster the chunks (using Gaussian Mixture Models (GMM) to generate distributions that consider the entire training space; each point can belong to more than one distribution) to ensure effective grouping of high-dimensional vector embeddings\n  * **summarizing clusters** : we summarize the clusters of similar chunks into a node of higher-level abstractions\n  * **recursive iteration** : we treat these nodes in turn as chunks, clustering them, summarizing these clusters, iteratively, building a tree-like structure that encapsulates the relationships and hierarchical structures inherent in the raw documents we started from\n\n\n![Raptor workflow](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/improve-rag-with-raptor/raptor-2.png)\nIn short, RAPTOR chunks raw documents, embeds the chunks, groups embeddings into clusters, summarizes each cluster, then (recursively) chunks these summaries, embeds, summarizes again, and so on, moving upwards to a comprehensive and efficient data representation, preserving nuances and key points.\n## Building a RAPTOR tree\nNow that we\u2019ve laid out RAPTOR's basic steps, let\u2019s walk through how to _implement_ a RAPTOR tree.\nWe start with our basic setup and imports.\n```\n\npip install lancedb scikit-learn openai torch sentence_transformers tiktoken umap-learn PyPDF2\n\n```\n```\n\nimport os\nimport uuid\nimport tiktoken\nimport re\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\nimport umap.umap_ as umap\nimport matplotlib.pyplot as plt\nfrom openai import OpenAI\nfrom typing import List, Tuple, Optional, Dict\nfrom sklearn.mixture import GaussianMixture\nfrom sentence_transformers import SentenceTransformer\n\nopenai_api_key =\"sk-XXXXXXXXXXXXXXX\"client = OpenAI(api_key=openai_api_key)\n\n```\n\nNote: Don't forget to input your open_ai_key above (we'll be using gpt-4 below).\n### Creating the Chunks\nFirst, create a directory_path to your source documents. To simplify things, our use case example will use just [one financial document](https://www.nttdata.com/global/en/-/media/nttdataglobal/1_files/investors/financial-results/2021/fy2021_fs_3q.pdf) as a source. You can of course load any source documents (and alter the query below) in your own experiments!\n(Note: make sure you use an unencrypted version of this file to avoid errors when PyPDF2 tries to read the file.)\n```\n\ndirectory_path ='/content/data'ifnot os.path.exists(directory_path):  os.makedirs(directory_path)print(f\"Directory '{directory_path}' created successfully.\")else:print(f\"Directory '{directory_path}' already exists.\")\n\n```\n\nNow, let's break our text documents down into smaller chunks.\n```\n\nimport PyPDF2\n\n# Function to extract text from a PDF filedefextract_pdf_text(file_path):withopen(file_path,'rb')asfile:    reader = PyPDF2.PdfReader(file)    text =\"\"for page_num inrange(len(reader.pages)):      page = reader.pages[page_num]      text += page.extract_text()return text\n\n# Function to split text into chunks with overlapdefsplit_text(text, chunk_size=1000, chunk_overlap=50):  chunks =[]  start =0while start <len(text):    end =min(start + chunk_size,len(text))    chunks.append(text[start:end])    start += chunk_size - chunk_overlap\nreturn chunks\n\n# Function to process all PDFs in a directorydefprocess_directory(directory_path, chunk_size=1000, chunk_overlap=50):  all_chunks =[]# Iterate over all PDF files in the directoryfor filename in os.listdir(directory_path):if filename.endswith(\".pdf\"):      file_path = os.path.join(directory_path, filename)print(f\"Processing file: {file_path}\")      \n# Step 1: Extract text from the PDF      pdf_text = extract_pdf_text(file_path)      \n# Step 2: Split the extracted text into chunks      chunks = split_text(pdf_text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)      \n# Append chunks from this file to the overall list      all_chunks.extend(chunks)  \nreturn all_chunks\n\ndirectory_path = os.path.join(os.getcwd(),\"data\")# Specify your directory pathchunk_size =1000chunk_overlap =50\n# Process all PDF files in the directory and get the chunkschunks = process_directory(directory_path, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n# Optional: Print the number of chunks and preview some of the chunksprint(f\"Total number of chunks: {len(chunks)}\")for i, chunk inenumerate(chunks[:5]):# Show first 5 chunks as a previewprint(f\"Chunk {i+1}:\\n{chunk}\\n\")\n\n```\n\nNow that we have our chunks, let's move on and set up our embedding process, first by initializing our embedding model. We'll use `all-MiniLM-L6-v2` (Sentence Transformers), but you can choose any embedding model that fits your use case.\n```\n\nembedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n```\n\n### Hierarchical clustering and dimensionality reduction - setup\nTo preserve the essential structures and meaning of our data, we need to cluster it so we can easily access both its broad categories and its intricate details. One efficient way of doing this is with a **hierarchical clustering structure**. This is where UMAP comes in. Traditional clustering methods like Gaussian Mixture Models (GMM) often struggle with clustering high-dimensional data chunks. We can make things easier for our GMM by first using UMAP (Uniform Manifold Approximation and Projection) to simplify our data but preserve its essential structures.\nUMAP's **`n_neighbors`**parameter dictates how much of the data's neighborhood UMAP considers during dimensionality reduction. Higher`n_neighbors` tells UMAP to look at many neighbors, maintaining the data's global structure and producing larger, more general clusters - **global clusters**. Lower `n_neighbors`, conversely, prompts UMAP to focus on close relationships, preserving local structure and forming smaller, more detailed, **local clusters**.\n```\n\ndefdimensionality_reduction(  embeddings: np.ndarray,  target_dim:int,  clustering_type:str,  metric:str=\"cosine\",)-> np.ndarray:if clustering_type ==\"local\":    n_neighbors =max(2,min(10,len(embeddings)-1))    min_dist =0.01elif clustering_type ==\"global\":    n_neighbors =max(2,min(int((len(embeddings)-1)**0.5),len(embeddings)//10,len(embeddings)-1))    min_dist =0.1else:raise ValueError(\"clustering_type must be either 'local' or 'global'\")\n  umap_model = umap.UMAP(    n_neighbors=n_neighbors,    min_dist=min_dist,    n_components=target_dim,    metric=metric,)return umap_model.fit_transform(embeddings)\n\n```\n\nTo determine the number of clusters that optimally captures our data's relationships and structure, we'll use the Elbow Method and the Bayesian Information Criterion (BIC). The Elbow Method plots explain variance against the number of clusters, helping us know when adding more clusters results in diminishing returns, while BIC evaluates models based on their fit to the data while penalizing complexity.\nOnce we've determined our `optimal_cluster_number`, we'll fit a GMM with it, predict the probability of each embedding belonging to each cluster, and assign labels to our embeddings.\n```\n\ndefcompute_inertia(embeddings: np.ndarray, labels: np.ndarray, centroids: np.ndarray)->float:return np.sum(np.min(np.sum((embeddings[:, np.newaxis]- centroids)**2, axis=2), axis=1))\ndefoptimal_cluster_number(  embeddings: np.ndarray,  max_clusters:int=50,  random_state:int= SEED\n)->int:  max_clusters =min(max_clusters,len(embeddings))  number_of_clusters = np.arange(1, max_clusters +1)  inertias =[]  bic_scores =[]  \nfor n in number_of_clusters:    gmm = GaussianMixture(n_components=n, random_state=random_state)    labels = gmm.fit_predict(embeddings)    centroids = gmm.means_\n    inertia = compute_inertia(embeddings, labels, centroids)    inertias.append(inertia)    bic_scores.append(gmm.bic(embeddings))  \n  inertia_changes = np.diff(inertias)  elbow_optimal = number_of_clusters[np.argmin(inertia_changes)+1]  bic_optimal = number_of_clusters[np.argmin(bic_scores)]  \nreturnmax(elbow_optimal, bic_optimal)\ndefgmm_clustering(  embeddings: np.ndarray,  threshold:float,  random_state:int= SEED\n)-> Tuple[List[np.ndarray],int]:  n_clusters = optimal_cluster_number(embeddings, random_state=random_state)  gm = GaussianMixture(n_components=n_clusters, random_state=random_state, n_init=2)  gm.fit(embeddings)  probs = gm.predict_proba(embeddings)  labels =[np.where(prob > threshold)[0]for prob in probs]return labels, n_clusters \n\n```\n\nSo far, we've chunked our documents and defined functions - for embedding our chunks, reducing their dimensions using UMAP, and clustering them. Now let's set up the recursive hierarchical clustering that is RAPTOR's defining feature.\n## Hierarchical clustering\nTo set up our hierarchical clustering, we define several functions:\n  * a clustering_algorithm function to manage both global and local clustering using GMMs - first globally, then locally within each global cluster (start with a broad overview, and then zoom in for more detailed clusters before summarizing)\n  * a generate_summary function to generate text summaries using a language model\n  * an embed clusters function to manage the embedding and initial clustering\n  * an embed cluster summaries function for summarizing each cluster\n  * a recursive_embedding_with_cluster_summarization function that leverages these methods to create a multi-level hierarchical clustering and summarization structure\n\n```\n\n#define our clustering algorithmdefclustering_algorithm(  embeddings: np.ndarray,  target_dim:int,  threshold:float,  random_state:int= SEED\n)-> Tuple[List[np.ndarray],int]:iflen(embeddings)<= target_dim +1:return[np.array([0])for _ inrange(len(embeddings))],1  \n# Global clustering  reduced_global_embeddings = dimensionality_reduction(embeddings, target_dim,\"global\")  global_clusters, n_global_clusters = gmm_clustering(reduced_global_embeddings, threshold, random_state=random_state)\n  all_local_clusters =[np.array([])for _ inrange(len(embeddings))]  total_clusters =0\n# Local clustering within each global clusterfor i inrange(n_global_clusters):    global_cluster_mask = np.array([i in gc for gc in global_clusters])    global_cluster_embeddings = embeddings[global_cluster_mask]\niflen(global_cluster_embeddings)<= target_dim +1:# Assign all points in this global cluster to a single local clusterfor idx in np.where(global_cluster_mask)[0]:        all_local_clusters[idx]= np.append(all_local_clusters[idx], total_clusters)      total_clusters +=1continue\ntry:      reduced_local_embeddings = dimensionality_reduction(global_cluster_embeddings, target_dim,\"local\")      local_clusters, n_local_clusters = gmm_clustering(reduced_local_embeddings, threshold, random_state=random_state)\n# Assign local cluster IDsfor j inrange(n_local_clusters):        local_cluster_mask = np.array([j in lc for lc in local_clusters])        global_indices = np.where(global_cluster_mask)[0]        local_indices = global_indices[local_cluster_mask]for idx in local_indices:          all_local_clusters[idx]= np.append(all_local_clusters[idx], j + total_clusters)\n      total_clusters += n_local_clusters\nexcept Exception as e:print(f\"Error in local clustering for global cluster {i}: {str(e)}\")# Assign all points in this global cluster to a single local clusterfor idx in np.where(global_cluster_mask)[0]:        all_local_clusters[idx]= np.append(all_local_clusters[idx], total_clusters)      total_clusters +=1\nreturn all_local_clusters, total_clusters\n  \ndefgenerate_summary(context:str)->str:  prompt =f\"\"\"\n  Provide the Summary for the given context. Here are some additional instructions for you:\n  Instructions:\n  1. Don't make things up, Just use the contexts and generate the relevant summary.\n  2. Don't mix the numbers, Just use the numbers in the context.\n  3. Don't try to use fancy words, stick to the basics of the language that is being used in the context.\n  Context: {context}  \"\"\"  response = client.chat.completions.create(    model=\"gpt-4\",    messages=[{\"role\":\"system\",\"content\":\"You are a helpful assistant that summarizes text.\"},{\"role\":\"user\",\"content\": prompt}],    max_tokens=200,    n=1,    stop=None,    temperature=0.7)  summary = response.choices[0].message.content.strip()return summary\n\ndefembed_clusters(  texts: List[str],  target_dim:int=10,  threshold:float=0.1)-> pd.DataFrame:  textual_embeddings = np.array(embedding_model.encode(texts))  clusters, number_of_clusters = clustering_algorithm(textual_embeddings, target_dim, threshold)print(f\"Number of clusters: {number_of_clusters}\")return pd.DataFrame({\"texts\": texts,\"embedding\":list(textual_embeddings),\"clusters\": clusters\n})\ndefembed_cluster_summaries(  texts: List[str],  level:int,  target_dim:int=10,  threshold:float=0.1)-> Tuple[pd.DataFrame, pd.DataFrame]:  df_clusters = embed_clusters(texts, target_dim, threshold)  main_list =[]  \nfor _, row in df_clusters.iterrows():for cluster in row[\"clusters\"]:      main_list.append({\"text\": row[\"texts\"],\"embedding\": row[\"embedding\"],\"clusters\": cluster\n})  \n  main_df = pd.DataFrame(main_list)  unique_clusters = main_df[\"clusters\"].unique()iflen(unique_clusters)==0:return df_clusters, pd.DataFrame(columns=[\"summaries\",\"level\",\"clusters\"])\nprint(f\"--Generated {len(unique_clusters)} clusters--\")\n  summaries =[]for cluster in unique_clusters:    text_in_df = main_df[main_df[\"clusters\"]== cluster]    unique_texts = text_in_df[\"text\"].tolist()    text =\"------\\n------\".join(unique_texts)    summary = generate_summary(text)    summaries.append(summary)\n  df_summaries = pd.DataFrame({\"summaries\": summaries,\"level\":[level]*len(summaries),\"clusters\": unique_clusters\n})\nreturn df_clusters, df_summaries\n\ndefrecursive_embedding_with_cluster_summarization(  texts: List[str],  number_of_levels:int=3,  level:int=1,  target_dim:int=10,  threshold:float=0.1)-> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:if level > number_of_levels:return{}  \n  results ={}  df_clusters, df_summaries = embed_cluster_summaries(texts, level, target_dim, threshold)  results[level]=(df_clusters, df_summaries)  \nif df_summaries.empty orlen(df_summaries['clusters'].unique())==1:print(f\"No more unique clusters found at level {level}. Stopping recursion.\")return results\n  \nif level < number_of_levels:    next_level_texts = df_summaries['summaries'].tolist()    next_level_results = recursive_embedding_with_cluster_summarization(      next_level_texts,      number_of_levels,      level +1,      target_dim,      threshold\n)    results.update(next_level_results)  \nreturn results\n\n```\n\nNow we define a process_text_hierarchy function that takes a hierarchical text dataset, processes it to generate embeddings and cluster summaries, and returns the processed results organized by hierarchical level.\n```\n\ndefprocess_text_hierarchy(  texts: List[str],  number_of_levels:int=3,  target_dim:int=10,  threshold:float=0.1)-> Dict[str, pd.DataFrame]:  hierarchy_results = recursive_embedding_with_cluster_summarization(    texts, number_of_levels, target_dim=target_dim, threshold=threshold\n)  \n  processed_results ={}for level,(df_clusters, df_summaries)in hierarchy_results.items():if df_clusters.empty or df_summaries.empty:print(f\"No data for level {level}. Skipping.\")continue    processed_results[f\"level_{level}_clusters\"]= df_clusters\n    processed_results[f\"level_{level}_summaries\"]= df_summaries\n  \nreturn processed_results\n\nresults = process_text_hierarchy(chunks, number_of_levels=3)\n\n```\n\nAfter several rounds of clustering and summarization, we have our RAPTOR tree.\n### RAPTOR tree\n  * **leaf nodes (base level)** - at the tree's base, we have our original, unprocessed text chunks - the raw data inputs that RAPTOR RAG uses for analysis and retrieval\n  * **intermediate summary nodes** - moving up the tree, we cluster our chunks, and summarize the cluster, creating summary nodes - each summary node captures the meaning of the cluster it represents, facilitating efficient retrieval and accurate contextual understanding\n  * **hierarchical embeddings (multi-level summarization)** - each level above the base (leaf) has summary nodes capturing the meaning of lower level summary nodes, continuing until we reach the top (or root node) of the tree\n\n\nWith our RAPTOR tree built, starting from leaf nodes at the bottom and summarized nodes in between, it\u2019s time to query the RAG.\n### Our query example\nFor our use case we'll use a straightforward example - a query about financial news, aimed at understanding an increase in NTT DATA's net income attributable to shareholders:\n```\n\nquery = \"NTT DATA's net income attributable to shareholders increased from \u00a569,227 million in Q3 FY2020 to \u00a5110,191 million in Q3 FY2021. How does this growth align with their acquisition strategy, particularly considering their stated reasons for acquiring Nexient, LLC and the provisional goodwill recorded in this transaction?\"\n\n```\n\nTo keep things simple in our implementation, our data source will be [this pdf](https://www.nttdata.com/global/en/-/media/nttdataglobal/1_files/investors/financial-results/2021/fy2021_fs_3q.pdf).\nNow that we have our query and raw source document, let's choose the right RAPTOR method for querying our RAPTOR tree.\n### RAPTOR query methods\nRAPTOR has two distinct strategies for querying the RAPTOR tree: tree traversal and collapsed tree.\n  1. **Tree traversal** method - traverses the tree layer by layer, first selecting the most relevant (top-k in query vector cosine similarity) root nodes, then (in the next layer) navigating each selected root node's children to again find the top-k most relevant nodes, continuing on until it reaches the leaf nodes (original text chunks). Then, it concatenates the text from all the selected nodes, returning it as the query result.\n  2. **Collapsed tree** method - flattens the tree into a single layer, directly comparing the query embedding to the vector embeddings of all the leaf nodes (the original text chunks) and summary nodes. For factual, keyword-based queries where you need specific details, collapsed tree returns more relevant and accurate results (with lower latency) than the tree traversal method.\n\n\n![How the retrieval happens](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/improve-rag-with-raptor/raptor-4.png) _Above: \"Figure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms\" from the[RAPTOR paper](https://arxiv.org/pdf/2401.1805)_\nIf our query demanded complex multi-level reasoning, and a contextually rich and precise result, it would make sense to use tree traversal. But for specific queries requiring specific factual information - like our financial news query, we want to be able to directly compare our query embedding with the vector embeddings of all nodes (both leaf and summary), efficiently bypassing RAPTOR's hierarchical structure and going straight to the most relevant data points.\nBut even though the collapsed tree method's retrieval bypasses the RAPTOR tree's hierarchy, it still capitalizes on the RAPTOR tree's hierarchical encapsulation of meaning to retrieve context. Because the collapsed tree method treats summarized nodes from higher levels simply as additional (same level) chunks, we can pull in higher-level summaries (the global perspective) alongside granular details in just one pass. We want our retrieval to get both an overall perspective and pinpoint very specific details of a particular company's financial quarter.\nFor our purposes, the collapsed tree method is a better fit than tree traversal.\nSo how do we implement it?\n## Implementing collapsed tree retrieval\nCollapsed tree retrieval retrieves top-k nodes (highest cosine similarity to the query vector). To do this, we gather our original root document text chunks and summarized nodes at each level for all the clusters into one big list. Next, we embed all these data points, storing them in a [vector database](https://superlinked.com/vector-db-comparison), which we'll query with our RAG.\nFirst, the embeddings.\n```\n\nraptor_texts =[]for level, row in results.items():if level.endswith(\"clusters\"):    raptor_texts.extend(row[\"texts\"])else:    raptor_texts.extend(row[\"summaries\"])    \nraptor_embeddings = embedding_model.encode(raptor_texts)len(raptor_embeddings)\n\n```\n\n### Setting up vector database and RAG\nNow, we set up a LanceDB vector database.\n```\n\nraptor_embeddings = embedding_model.encode(raptor_texts)\nraptor_dict ={\"texts\":[],\"embeddings\":[]}for texts, embeddings inzip(raptor_texts, raptor_embeddings):  raptor_dict[\"texts\"].append(texts)  raptor_dict[\"embeddings\"].append(embeddings.tolist())\n\n```\n```\n\nimport lancedb\nimport pyarrow as pa\nfrom lancedb.pydantic import Vector, LanceModel\n\nuri =\"lancedb_database\"db = lancedb.connect(uri)\nclassRAG(LanceModel):  texts :str  embeddings : Vector(384)\ntable_name =\"rag_with_raptor\"raptor_table = db.create_table(table_name, schema = RAG, mode=\"overwrite\")raptor_table.add(rag_raptor_df)raptor_table.create_fts_index(\"texts\", replace=True)\n\n```\n\nNext, we define our results generation function...\n```\n\ndefgenerate_results(  query :str,  context_text :str)->str:\n  prompt =f\"\"\"\n  Based on the context provided, use it to answer the query. \n  query : {query}  Instructions:\n  1. Don't make things up, Just use the contexts and generate the relevant answer.\n  2. Don't mix the numbers, Just use the numbers in the context.\n  3. Don't try to use fancy words, stick to the basics of the language that is being used in the context.\n{context_text}  \"\"\"  response = client.chat.completions.create(    model=\"gpt-4\",    messages=[{\"role\":\"system\",\"content\":\"You are a helpful assistant that answers query and give the answers.\"},{\"role\":\"user\",\"content\": prompt}],    max_tokens=200,    n=1,    stop=None,    temperature=0.7)  answer = response.choices[0].message.content.strip()return answer\n\n```\n\n...and, finally, input our query.\n```\n\nquery =\"NTT DATA's net income attributable to shareholders increased from \u00a569,227 million in Q3 FY2020 to \u00a5110,191 million in Q3 FY2021. How does this growth align with their acquisition strategy, particularly considering their stated reasons for acquiring Nexient, LLC and the provisional goodwill recorded in this transaction?\"\n\n```\n\nWe want our retrieval to address our query's key points. First, we expect our retrieval to take account of the provided factual data regarding NTT DATA's increased shareholder income. Second, we need our retrieval to answer how this increase fits in with NTT Data's acquisition strategy (in particular the Nexient acquisition rationale). Let's keep these in mind when we examine our RAPTOR RAG results. For comparison purposes, we'll also create a vanilla (i.e., normal) RAG:\n```\n\nnormal_embeddings = embedding_model.encode(chunks)# default chunks from our data\nnormal_dict ={\"texts\":[],\"embeddings\":[]}for texts, embeddings inzip(chunks, normal_embeddings):  normal_dict[\"texts\"].append(texts)  normal_dict[\"embeddings\"].append(embeddings.tolist())  \nrag_normal_df = pd.DataFrame(normal_dict)\ntable_name =\"rag_without_raptor\"normal_table = db.create_table(table_name, schema = RAG, mode=\"overwrite\")normal_table.add(rag_normal_df)normal_table.create_fts_index(\"texts\", replace=True)\n\n```\n\nBecause we now have cluster-level summary nodes in addition to our original text chunks, our RAPTOR RAG has more chunks than our vanilla / normal RAG.\n![Number of chunks in RAPTOR](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/improve-rag-with-raptor/raptor-5.png)\n### How did our RAPTOR RAG do?\nLet's generate and then compare and evaluate our results.\n```\n\nraptor_contexts = raptor_table.search(query).limit(5).select([\"texts\"]).to_list()raptor_context_text =\"------\\n\\n\".join([context[\"texts\"]for context in raptor_contexts])raptor_context_text =\"------\\n\\n\"+ raptor_context_text\n\nnormal_contexts = normal_table.search(query).limit(5).select([\"texts\"]).to_list()normal_context_text =\"------\\n\\n\".join([context[\"texts\"]for context in normal_contexts])normal_context_text =\"------\\n\\n\"+ normal_context_text\n\nraptor_answer = generate_results(query, raptor_context_text)normal_answer = generate_results(query, normal_context_text)\n\n```\n\n![Comparison for a Query](https://raw.githubusercontent.com/superlinked/VectorHub/main/docs/assets/use_cases/improve-rag-with-raptor/raptor-6.png)\nRAPTOR RAG performed **better than vanilla RAG** at handling retrieval on our hierarchically chunked and embedded source document. RAPTOR retrieved specific details about NTT's financial growth for the specified quarter, and connected this growth to the broader acquisition strategy, pulling relevant context from our example [source document](https://www.nttdata.com/global/en/-/media/nttdataglobal/1_files/investors/financial-results/2021/fy2021_fs_3q.pdf). (Note: collapsed tree retrieval will _work well_ in this kind of use case - _a factual, keyword-based query requiring specific details_ - whether you have multiple source chunks or even many source documents.) Whereas, our vanilla RAG, while correctly identifying the specific details, failed to elucidate either NTT's strategy or a relationship between their quarterly growth, broader strategy, and the particular (Nexient) acquisition in question.\n## Your turn\nNow it's your turn to try out RAPTOR RAG! Here's the Google [colab](https://colab.research.google.com/github/superlinked/VectorHub/blob/main/docs/assets/use_cases/improve-rag-with-raptor/raptor_with_rag.ipynb).\nTo learn more about the intricacies of RAPTOR, check out their official [GitHub repository](https://github.com/parthsarthi03/raptor/tree/master). For an even deeper dive, we highly recommend the official [paper](https://arxiv.org/pdf/2401.18059)!\n## Contributors\n  * [Vipul Maheshwari, author](https://www.linkedin.com/in/vipulmaheshwarii/)\n  * [Robert Turner, editor](https://www.linkedin.com/in/robertdhayanturner/)\n\n\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nStay updated with VectorHub\nSubscribe\n![arrow](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nContinue Reading\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/research_agent_3a22caf3b8.png)\n#Superlinked\nSimplifying Complex Research with AI\nLearn how to build an AI research assistant that finds, summarizes, and answers questions about scie...\nMarch 23, 2025\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/rag_feedback_878ba93f75.png)\n#RAG\nYour RAG application is a communication system\nEffective RAG can take the best features of language models and moderate their shortcomings. In this...\nJuly 28, 2024\n![](https://innovative-ants-bf39f838ee.media.strapiapp.com/RAG_Evaluation_with_Arize_Phoenix_2x_1_ea16819113.png)\n#RAG\n#Quality And Evaluation\nRAG Evaluation with Arize Phoenix\nThis article covers a step by step implementation of RAG evaluation using a framework tool called Ar...\nJune 13, 2024\n[![Superlinked](https://superlinked.com/vectorhub/_next/static/media/superlinked-logo.9ad3911a.svg)](https://superlinked.com/)\nSubscribe\n![](https://superlinked.com/vectorhub/_next/static/media/thick-arrow.99bec954.svg)\nBy subscribing, you agree to our [Terms and Conditions](https://www.superlinked.com/policies/terms-and-conditions).\nAbout\n[Company](https://superlinked.com/)[Careers](https://superlinked.notion.site/Work-at-Superlinked-541f2d74ce714c889a1c6dba6dd0dbf1)[Index](https://superlinked.com/vectorhub/siteindex)\nSupport\nContact Us[Terms of Use](https://superlinked.com/policies/terms-and-conditions)[Privacy Policy](https://superlinked.com/policies/privacy-policy)[Cookie Policy](https://superlinked.com/policies/cookie-policy)\nSocial\n[Github](https://github.com/superlinked/superlinked)[X (Twitter)](https://twitter.com/superlinked)[LinkedIn](https://www.linkedin.com/company/superlinked/)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nRAPTOR enhances RAG by addressing chunking issues through hierarchical clustering, preserving document relationships. It outperforms traditional RAG in factual queries by using a collapsed tree method for efficient retrieval, enabling nuanced insights from complex data structures.\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\n# \n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg)](https://huggingface.co/tiiuae)\n[tiiuae](https://huggingface.co/tiiuae)\n/\n[falcon-40b](https://huggingface.co/tiiuae/falcon-40b)\nlike 2.42k\nFollow\n![](https://cdn-avatars.huggingface.co/v1/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg) Technology Innovation Institute 1.27k\n[ Text Generation ](https://huggingface.co/models?pipeline_tag=text-generation)[ Transformers ](https://huggingface.co/models?library=transformers)[ PyTorch ](https://huggingface.co/models?library=pytorch)[ Safetensors ](https://huggingface.co/models?library=safetensors)\ntiiuae/falcon-refinedweb\n4 languages\n[ falcon ](https://huggingface.co/models?other=falcon)[ custom_code ](https://huggingface.co/models?other=custom_code)[ text-generation-inference ](https://huggingface.co/models?other=text-generation-inference)\narxiv: 6 papers\nLicense: apache-2.0\n[ Model card ](https://huggingface.co/tiiuae/falcon-40b)[ Files Files and versions ](https://huggingface.co/tiiuae/falcon-40b/tree/main)[ Community 115 ](https://huggingface.co/tiiuae/falcon-40b/discussions)\nTrain \nDeploy \nUse this model \n  * [\ud83d\ude80 Falcon-40B](https://huggingface.co/tiiuae/falcon-40b/#\ud83d\ude80-falcon-40b \"\ud83d\ude80 Falcon-40B\")\n    * [Why use Falcon-40B?](https://huggingface.co/tiiuae/falcon-40b/#why-use-falcon-40b \"Why use Falcon-40B?\")\n  * [Model Card for Falcon-40B](https://huggingface.co/tiiuae/falcon-40b/#model-card-for-falcon-40b \"Model Card for Falcon-40B\")\n    * [Model Details](https://huggingface.co/tiiuae/falcon-40b/#model-details \"Model Details\")\n      * [Model Description](https://huggingface.co/tiiuae/falcon-40b/#model-description \"Model Description\")\n      * [Model Source](https://huggingface.co/tiiuae/falcon-40b/#model-source \"Model Source\")\n    * [Uses](https://huggingface.co/tiiuae/falcon-40b/#uses \"Uses\")\n      * [Direct Use](https://huggingface.co/tiiuae/falcon-40b/#direct-use \"Direct Use\")\n      * [Out-of-Scope Use](https://huggingface.co/tiiuae/falcon-40b/#out-of-scope-use \"Out-of-Scope Use\")\n    * [Bias, Risks, and Limitations](https://huggingface.co/tiiuae/falcon-40b/#bias-risks-and-limitations \"Bias, Risks, and Limitations\")\n      * [Recommendations](https://huggingface.co/tiiuae/falcon-40b/#recommendations \"Recommendations\")\n    * [How to Get Started with the Model](https://huggingface.co/tiiuae/falcon-40b/#how-to-get-started-with-the-model \"How to Get Started with the Model\")\n    * [Training Details](https://huggingface.co/tiiuae/falcon-40b/#training-details \"Training Details\")\n      * [Training Data](https://huggingface.co/tiiuae/falcon-40b/#training-data \"Training Data\")\n      * [Training Procedure](https://huggingface.co/tiiuae/falcon-40b/#training-procedure \"Training Procedure\")\n    * [Evaluation](https://huggingface.co/tiiuae/falcon-40b/#evaluation \"Evaluation\")\n    * [Technical Specifications](https://huggingface.co/tiiuae/falcon-40b/#technical-specifications \"Technical Specifications\")\n      * [Model Architecture and Objective](https://huggingface.co/tiiuae/falcon-40b/#model-architecture-and-objective \"Model Architecture and Objective\")\n      * [Compute Infrastructure](https://huggingface.co/tiiuae/falcon-40b/#compute-infrastructure \"Compute Infrastructure\")\n    * [Citation](https://huggingface.co/tiiuae/falcon-40b/#citation \"Citation\")\n    * [License](https://huggingface.co/tiiuae/falcon-40b/#license \"License\")\n    * [Contact](https://huggingface.co/tiiuae/falcon-40b/#contact \"Contact\")\n\n\n#  [ ](https://huggingface.co/tiiuae/falcon-40b/#\ud83d\ude80-falcon-40b) \ud83d\ude80 Falcon-40B \n**Falcon-40B is a 40B parameters causal decoder-only model built by[TII](https://www.tii.ae) and trained on 1,000B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. It is made available under the Apache 2.0 license.**\n_Paper coming soon \ud83d\ude0a._\n\ud83e\udd17 To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading [this great blogpost fron HF](https://huggingface.co/blog/falcon)!\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#why-use-falcon-40b) Why use Falcon-40B? \n  * **It is the best open-source model currently available.** Falcon-40B outperforms [LLaMA](https://github.com/facebookresearch/llama), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1), [MPT](https://huggingface.co/mosaicml/mpt-7b), etc. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n  * **It features an architecture optimized for inference** , with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \n  * **It is made available under a permissive Apache 2.0 license allowing for commercial use** , without any royalties or restrictions.\n  * \u26a0\ufe0f **This is a raw, pretrained model, which should be further finetuned for most usecases.** If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct).\n\n\n\ud83d\udcb8 **Looking for a smaller, less expensive model?** [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) is Falcon-40B's little brother!\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nmodel = \"tiiuae/falcon-40b\"\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n  \"text-generation\",\n  model=model,\n  tokenizer=tokenizer,\n  torch_dtype=torch.bfloat16,\n  trust_remote_code=True,\n  device_map=\"auto\",\n)\nsequences = pipeline(\n  \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n  max_length=200,\n  do_sample=True,\n  top_k=10,\n  num_return_sequences=1,\n  eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n  print(f\"Result: {seq['generated_text']}\")\n\n```\n\n\ud83d\udca5 **Falcon LLMs require PyTorch 2.0 for use with`transformers`!**\nFor fast inference with Falcon, check-out [Text Generation Inference](https://github.com/huggingface/text-generation-inference)! Read more in this [blogpost](https://huggingface.co/tiiuae/falcon-40b/blob/main/\\(https://huggingface.co/blog/falcon). \nYou will need **at least 85-100GB of memory** to swiftly run inference with Falcon-40B.\n#  [ ](https://huggingface.co/tiiuae/falcon-40b/#model-card-for-falcon-40b) Model Card for Falcon-40B \n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#model-details) Model Details \n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#model-description) Model Description \n  * **Developed by:** <https://www.tii.ae>;\n  * **Model type:** Causal decoder-only;\n  * **Language(s) (NLP):** English, German, Spanish, French (and limited capabilities in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish);\n  * **License:** Apache 2.0 license.\n\n\n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#model-source) Model Source \n  * **Paper:** _coming soon_.\n\n\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#uses) Uses \n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#direct-use) Direct Use \nResearch on large language models; as a foundation for further specialization and finetuning for specific usecases (e.g., summarization, text generation, chatbot, etc.)\n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#out-of-scope-use) Out-of-Scope Use \nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. \n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#bias-risks-and-limitations) Bias, Risks, and Limitations \nFalcon-40B is trained mostly on English, German, Spanish, French, with limited capabilities also in in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish. It will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#recommendations) Recommendations \nWe recommend users of Falcon-40B to consider finetuning it for the specific set of tasks of interest, and for guardrails and appropriate precautions to be taken for any production use.\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#how-to-get-started-with-the-model) How to Get Started with the Model \n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nmodel = \"tiiuae/falcon-40b\"\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n  \"text-generation\",\n  model=model,\n  tokenizer=tokenizer,\n  torch_dtype=torch.bfloat16,\n  trust_remote_code=True,\n  device_map=\"auto\",\n)\nsequences = pipeline(\n  \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n  max_length=200,\n  do_sample=True,\n  top_k=10,\n  num_return_sequences=1,\n  eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n  print(f\"Result: {seq['generated_text']}\")\n\n```\n\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#training-details) Training Details \n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#training-data) Training Data \nFalcon-40B was trained on 1,000B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile ([Gao et al., 2020](https://arxiv.org/abs/2101.00027)). \n**Data source** | **Fraction** | **Tokens** | **Sources**  \n---|---|---|---  \n[RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | 75% | 750B | massive web crawl  \nRefinedWeb-Europe | 7% | 70B | European massive web crawl  \nBooks | 6% | 60B  \nConversations | 5% | 50B | Reddit, StackOverflow, HackerNews  \nCode | 5% | 50B  \nTechnical | 2% | 20B | arXiv, PubMed, USPTO, etc.  \nRefinedWeb-Europe is made of the following languages:\n**Language** | **Fraction of multilingual data** | **Tokens**  \n---|---|---  \nGerman | 26% | 18B  \nSpanish | 24% | 17B  \nFrench | 23% | 16B  \n_Italian_ | 7% | 5B  \n_Portuguese_ | 4% | 3B  \n_Polish_ | 4% | 3B  \n_Dutch_ | 4% | 3B  \n_Romanian_ | 3% | 2B  \n_Czech_ | 3% | 2B  \n_Swedish_ | 2% | 1B  \nThe data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.\n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#training-procedure) Training Procedure \nFalcon-40B was trained on 384 A100 40GB GPUs, using a 3D parallelism strategy (TP=8, PP=4, DP=12) combined with ZeRO.\n####  [ ](https://huggingface.co/tiiuae/falcon-40b/#training-hyperparameters) Training Hyperparameters \n**Hyperparameter** | **Value** | **Comment**  \n---|---|---  \nPrecision | `bfloat16`  \nOptimizer | AdamW  \nLearning rate | 1.85e-4 | 4B tokens warm-up, cosine decay to 1.85e-5  \nWeight decay | 1e-1  \nZ-loss | 1e-4  \nBatch size | 1152 | 100B tokens ramp-up  \n####  [ ](https://huggingface.co/tiiuae/falcon-40b/#speeds-sizes-times) Speeds, Sizes, Times \nTraining started in December 2022 and took two months. \n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#evaluation) Evaluation \n_Paper coming soon._\nSee the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#technical-specifications) Technical Specifications \n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#model-architecture-and-objective) Model Architecture and Objective \nFalcon-40B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).\nThe architecture is broadly adapted from the GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the following differences:\n  * **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));\n  * **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));\n  * **Decoder-block:** parallel attention/MLP with a two layer norms.\n\n\nFor multiquery, we are using an internal variant which uses independent key and values per tensor parallel degree.\n**Hyperparameter** | **Value** | **Comment**  \n---|---|---  \nLayers | 60  \n`d_model` | 8192  \n`head_dim` | 64 | Reduced to optimise for FlashAttention  \nVocabulary | 65024  \nSequence length | 2048  \n###  [ ](https://huggingface.co/tiiuae/falcon-40b/#compute-infrastructure) Compute Infrastructure \n####  [ ](https://huggingface.co/tiiuae/falcon-40b/#hardware) Hardware \nFalcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances. \n####  [ ](https://huggingface.co/tiiuae/falcon-40b/#software) Software \nFalcon-40B was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.)\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#citation) Citation \n_Paper coming soon_ \ud83d\ude0a. In the meanwhile, you can use the following information to cite: \n```\n@article{falcon40b,\n title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n year={2023}\n}\n\n```\n\nTo learn more about the pretraining dataset, see the \ud83d\udcd3 [RefinedWeb paper](https://arxiv.org/abs/2306.01116).\n```\n@article{refinedweb,\n title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n journal={arXiv preprint arXiv:2306.01116},\n eprint={2306.01116},\n eprinttype = {arXiv},\n url={https://arxiv.org/abs/2306.01116},\n year={2023}\n}\n\n```\n\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#license) License \nFalcon-40B is made available under the Apache 2.0 license.\n##  [ ](https://huggingface.co/tiiuae/falcon-40b/#contact) Contact \nfalconllm@tii.ae \n\nDownloads last month\n    144,698 \nSafetensors[](https://huggingface.co/docs/safetensors)\nModel size\n41.8B params\nTensor type\nBF16 \n\u00b7\nInference Providers [NEW](https://huggingface.co/blog/inference-providers)\n[ Text Generation](https://huggingface.co/tasks/text-generation \"Learn more about text-generation\")\nThis model isn't deployed by any Inference Provider. [\ud83d\ude4b Ask for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=tiiuae/falcon-40b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btiiuae%2Ffalcon-40b%5D\\(%2Ftiiuae%2Ffalcon-40b\\)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A\\(optional\\)%20Which%20providers%20are%20you%20interested%20in%3F%20\\(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6\\)%0A)\nHF Inference deployability: The model authors have turned it off explicitly.\n##  Model tree for tiiuae/falcon-40b [](https://huggingface.co/docs/hub/model-cards#specifying-a-base-model)\nAdapters\n[10 models](https://huggingface.co/models?other=base_model:adapter:tiiuae/falcon-40b)\nFinetunes\n[4 models](https://huggingface.co/models?other=base_model:finetune:tiiuae/falcon-40b)\nQuantizations\n[4 models](https://huggingface.co/models?other=base_model:quantized:tiiuae/falcon-40b)\n##  Dataset used to train tiiuae/falcon-40b\n#### [tiiuae/falcon-refinedweb Viewer \u2022  Updated Jun 20, 2023 \u2022  968M \u2022  41.3k \u2022  842  ](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n##  Spaces using tiiuae/falcon-40b 100\n[\ud83d\udcac HuggingFaceH4/falcon-chat](https://huggingface.co/spaces/HuggingFaceH4/falcon-chat)[\ud83c\udfc6 eduagarcia/open_pt_llm_leaderboard](https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard)[\ud83c\udfc6 BAAI/open_cn_llm_leaderboard](https://huggingface.co/spaces/BAAI/open_cn_llm_leaderboard)[\ud83d\ude3b Sharathhebbar24/One-stop-for-Open-source-models](https://huggingface.co/spaces/Sharathhebbar24/One-stop-for-Open-source-models)[\ud83d\udc83 EvanTHU/MotionLLM](https://huggingface.co/spaces/EvanTHU/MotionLLM)[\ud83d\ude3b GTBench/GTBench](https://huggingface.co/spaces/GTBench/GTBench)[\ud83c\udfc6 Vikhrmodels/small-shlepa-lb](https://huggingface.co/spaces/Vikhrmodels/small-shlepa-lb)[\ud83d\ude80 Justinrune/LLaMA-Factory](https://huggingface.co/spaces/Justinrune/LLaMA-Factory)[\ud83c\udfc6 kz-transformers/kaz-llm-lb](https://huggingface.co/spaces/kz-transformers/kaz-llm-lb)[\ud83d\ude80 awacke1/Deepseek-HPC-GPU-KEDA](https://huggingface.co/spaces/awacke1/Deepseek-HPC-GPU-KEDA)[\ud83d\udd25 HemaAM/GPT_train_on_LLaMa](https://huggingface.co/spaces/HemaAM/GPT_train_on_LLaMa)[\ud83c\udfa8 OPTML-Group/UnlearnCanvas-Benchmark](https://huggingface.co/spaces/OPTML-Group/UnlearnCanvas-Benchmark) + 95 Spaces + 88 Spaces\n##  Collection including tiiuae/falcon-40b\n#### [Falcon Collection  8 items \u2022  Updated Feb 13 \u2022 17](https://huggingface.co/collections/tiiuae/falcon-64fb432660017eeec9837b5a)\nSystem theme \nCompany\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/) [](https://huggingface.co/)\nWebsite\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)\n",
    "answer": "```markdown\n# TL;DR Summary of Falcon-40B Model\n\n**Falcon-40B** is a 40B parameter causal decoder-only model by **TII**, trained on 1,000B tokens from **RefinedWeb**. It excels in text generation and is available under the **Apache 2.0 license**. Recommended for research and fine-tuning, it requires **85-100GB memory** for inference.\n```"
  },
  {
    "instruction": "[Skip to content](https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada/#wp--skip-link--target)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nThe world\u2019s leading publication for data science, AI, and ML professionals.\nSign in\nSign out\n[Contributor Portal](https://contributor.insightmediagroup.io/)\n  * [Latest](https://towardsdatascience.com/latest/)\n  * [Editor\u2019s Picks](https://towardsdatascience.com/tag/editors-pick/)\n  * [Deep Dives](https://towardsdatascience.com/tag/deep-dives/)\n  * [Contribute](https://towardsdatascience.com/questions-96667b06af5/)\n  * [Newsletter](https://newsletter.towardsdatascience.com/subscription-to-the-newsletter)\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\n\n\nToggle Mobile Navigation\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [X](https://x.com/TDataScience)\n\n\nToggle Search\nSearch\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\n# Fine-tune Llama 3 with ORPO\nA cheaper and faster unified fine-tuning technique \n[Maxime Labonne](https://towardsdatascience.com/author/mlabonne/)\nApr 19, 2024\n8 min read\nShare \n![Image generated with DALL-E 3 by author](https://towardsdatascience.com/wp-content/uploads/2024/04/0DSTQkcyX56nl4qYu.png)Image generated with DALL-E 3 by author\nORPO is a **new exciting fine-tuning technique** that combines the traditional supervised fine-tuning and preference alignment stages into a single process. This reduces the computational resources and time required for training. Moreover, empirical results demonstrate that ORPO outperforms other alignment methods on various model sizes and benchmarks.\nIn this article, we will fine-tune the new Llama 3 8B model using ORPO with the TRL library. The code is available on [Google Colab](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing) and in the [LLM Course](https://github.com/mlabonne/llm-course) on GitHub.\n## \u2696\ufe0f ORPO\nInstruction tuning and preference alignment are essential techniques for adapting [Large Language Models](https://towardsdatascience.com/tag/large-language-models/ \"Large Language Models\") (LLMs) to specific tasks. Traditionally, this involves a multi-stage process: 1/ **Supervised Fine-Tuning** (SFT) on instructions to adapt the model to the target domain, followed by 2/ **preference alignment methods** like Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO) to increase the likelihood of generating preferred responses over rejected ones.\n![Image by author](https://towardsdatascience.com/wp-content/uploads/2024/04/0LlRjrJYf7rWtVxGj.png)Image by author\nHowever, researchers have identified a limitation in this approach. While SFT effectively adapts the model to the desired domain, it inadvertently **increases the probability of generating undesirable answers** alongside preferred ones. This is why the preference alignment stage is necessary to widen the gap between the likelihoods of preferred and rejected outputs.\n![Note how the probability of rejected responses increases during supervised fine-tuning \\(image from the ORPO paper\\).](https://towardsdatascience.com/wp-content/uploads/2024/04/0rfs4IexRUX7T6-5y.png)Note how the probability of rejected responses increases during supervised fine-tuning (image from the ORPO paper).\nIntroduced by [Hong and Lee (2024)](https://arxiv.org/abs/2403.07691), ORPO offers an elegant solution to this problem by combining instruction tuning and preference alignment into a single, monolithic training process. ORPO modifies the standard language modeling objective, combining the negative log-likelihood loss with an odds ratio (OR) term. This OR loss weakly penalizes rejected responses while strongly rewarding preferred ones, allowing the model to simultaneously learn the target task and align with human preferences.\n![](https://towardsdatascience.com/wp-content/uploads/2024/04/1r3V1OdKtcWJJKS6cGT8chQ.png)\nORPO has been implemented in the major fine-tuning libraries, like [TRL](https://github.com/huggingface/trl), [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl), and [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory). In the next section, we will see how to use with TRL.\n## \ud83d\udcbb Fine-tuning Llama 3 with ORPO\n[Llama 3](https://github.com/meta-llama/llama3/tree/main) is the latest family of LLMs developed by Meta. The models were trained on an extensive dataset of **15 trillion tokens** (compared to 2T tokens for Llama 2). Two model sizes have been released: a 70 billion parameter model and a smaller 8 billion parameter model. The 70B model has already demonstrated impressive performance, scoring 82 on the MMLU benchmark and 81.7 on the HumanEval benchmark.\nLlama 3 models also increased the context length up to 8,192 tokens (4,096 tokens for Llama 2), and potentially scale up to 32k with RoPE. Additionally, the models use a new tokenizer with a 128K-token vocabulary, reducing the number of tokens required to encode text by 15%. This vocabulary also explains the bump from 7B to 8B parameters.\n![Samples from ORPO-DPO-mix-40k \\(image by author\\).](https://towardsdatascience.com/wp-content/uploads/2024/04/0G8pGN8e3ppGj0TCa.png)_Samples from ORPO-DPO-mix-40k (image by author)._\nORPO requires a preference dataset, including a prompt, a chosen answer, and a rejected answer. In this example, we will use `[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)`, a combination of the following high-quality DPO datasets:\n  * `[argilla/distilabel-capybara-dpo-7k-binarized](https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized)`: highly scored chosen answers >=5 (2,882 samples)\n  * `[argilla/distilabel-intel-orca-dpo-pairs](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs)`: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\n  * `[argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)`: highly scored chosen answers >=5 (22,799 samples)\n  * `[argilla/distilabel-math-preference-dpo](https://huggingface.co/datasets/argilla/distilabel-math-preference-dpo)`: highly scored chosen answers >=9 (2,181 samples)\n  * `[unalignment/toxic-dpo-v0.2](https://huggingface.co/datasets/unalignment/toxic-dpo-v0.2)` (541 samples)\n  * `[M4-ai/prm_dpo_pairs_cleaned](https://huggingface.co/datasets/M4-ai/prm_dpo_pairs_cleaned)` (7,958 samples)\n  * `[jondurbin/truthy-dpo-v0.1](https://huggingface.co/datasets/jondurbin/truthy-dpo-v0.1)` (1,016 samples)\n\n\nThanks to [argilla](https://huggingface.co/argilla), [unalignment](https://huggingface.co/unalignment), [M4-ai](https://huggingface.co/M4-ai), and [jondurbin](https://huggingface.co/jondurbin) for providing the source datasets.\nAs per usual, let\u2019s start by installing the required libraries:\n```\npip install -U transformers datasets accelerate peft trl bitsandbytes wandb\n```\n\nOnce it\u2019s installed, we can import the necessary libraries and log in to W&B (optional):\n```\nimport gc\nimport os\nimport torch\nimport wandb\nfrom datasets import load_dataset\nfrom google.colab import userdata\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (\n  AutoModelForCausalLM,\n  AutoTokenizer,\n  BitsAndBytesConfig,\n  TrainingArguments,\n  pipeline,\n)\nfrom trl import ORPOConfig, ORPOTrainer, setup_chat_format\nwb_token = userdata.get('wandb')\nwandb.login(key=wb_token)\n```\n\nIf you have a recent GPU, you should also be able to use the [Flash Attention library](https://github.com/Dao-AILab/flash-attention) to replace the default eager attention implementation with a more efficient one.\n```\nif torch.cuda.get_device_capability()[0] >= 8:\n  !pip install -qqq flash-attn\n  attn_implementation = \"flash_attention_2\"\n  torch_dtype = torch.bfloat16\nelse:\n  attn_implementation = \"eager\"\n  torch_dtype = torch.float16\n```\n\nIn the following, we will load the Llama 3 8B model in 4-bit precision thanks to [bitsandbytes](https://github.com/TimDettmers/bitsandbytes). We then set the LoRA configuration using [PEFT](https://github.com/huggingface/peft) for QLoRA. I\u2019m also using the convenient `setup_chat_format()` function to modify the model and tokenizer for [ChatML](https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use) support. It automatically applies this chat template, adds special tokens, and resizes the model\u2019s embedding layer to match the new vocabulary size.\nNote that you need to submit a request to access [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) and be logged in to your Hugging Face account. Alternatively, you can load ungated copies of the model, like [NousResearch/Meta-Llama-3-8B](https://huggingface.co/NousResearch/Meta-Llama-3-8B).\n```\n# Model\nbase_model = \"meta-llama/Meta-Llama-3-8B\"\nnew_model = \"OrpoLlama-3-8B\"\n# QLoRA config\nbnb_config = BitsAndBytesConfig(\n  load_in_4bit=True,\n  bnb_4bit_quant_type=\"nf4\",\n  bnb_4bit_compute_dtype=torch_dtype,\n  bnb_4bit_use_double_quant=True,\n)\n# LoRA config\npeft_config = LoraConfig(\n  r=16,\n  lora_alpha=32,\n  lora_dropout=0.05,\n  bias=\"none\",\n  task_type=\"CAUSAL_LM\",\n  target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n  base_model,\n  quantization_config=bnb_config,\n  device_map=\"auto\",\n  attn_implementation=attn_implementation\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = prepare_model_for_kbit_training(model)\n```\n\nNow that the model is ready for training, we can take care of the dataset. We load `[mlabonne/orpo-dpo-mix-40k](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)` and use the `apply_chat_template()` function to convert the \"chosen\" and \"rejected\" columns into the ChatML format. Note that I\u2019m only using 1,000 samples and not the entire dataset, as it would take too long to run.\n```\ndataset_name = \"mlabonne/orpo-dpo-mix-40k\"\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=42).select(range(1000))\ndef format_chat_template(row):\n  row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n  row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n  return row\ndataset = dataset.map(\n  format_chat_template,\n  num_proc= os.cpu_count(),\n)\ndataset = dataset.train_test_split(test_size=0.01)\n```\n\nFirst, we need to set a few hyperparameters:\n  * `learning_rate`: ORPO uses very low learning rates compared to traditional SFT or even DPO. This value of 8e-6 comes from the original paper, and roughly corresponds to an SFT learning rate of 1e-5 and a DPO learning rate of 5e-6. I would recommend increasing it around 1e-6 for a real fine-tune.\n  * `beta`: It is the $lambda$ parameter in the paper, with a default value of 0.1. An appendix from the original paper shows how it\u2019s been selected with an ablation study.\n  * Other parameters, like `max_length` and batch size are set to use as much VRAM as available (~20 GB in this configuration). Ideally, we would train the model for 3-5 epochs, but we\u2019ll stick to 1 here.\n\n\nFinally, we can train the model using the ORPOTrainer, which acts as a wrapper.\n```\norpo_args = ORPOConfig(\n  learning_rate=8e-6,\n  beta=0.1,\n  lr_scheduler_type=\"linear\",\n  max_length=1024,\n  max_prompt_length=512,\n  per_device_train_batch_size=2,\n  per_device_eval_batch_size=2,\n  gradient_accumulation_steps=4,\n  optim=\"paged_adamw_8bit\",\n  num_train_epochs=1,\n  evaluation_strategy=\"steps\",\n  eval_steps=0.2,\n  logging_steps=1,\n  warmup_steps=10,\n  report_to=\"wandb\",\n  output_dir=\"./results/\",\n)\ntrainer = ORPOTrainer(\n  model=model,\n  args=orpo_args,\n  train_dataset=dataset[\"train\"],\n  eval_dataset=dataset[\"test\"],\n  peft_config=peft_config,\n  tokenizer=tokenizer,\n)\ntrainer.train()\ntrainer.save_model(new_model)\n```\n\nTraining the model on these 1,000 samples took about 2 hours on an L4 GPU. Let\u2019s check the W&B plots:\n![](https://towardsdatascience.com/wp-content/uploads/2024/04/0HDi6G4O5z9rpjeEG.png)\nWhile the loss goes down, the difference between the chosen and rejects answers is not clear: the average margin and accuracy are only slightly above zero and 0.5, respectively.\nIn the original paper, the authors trained models on the `[Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)` dataset (161k samples) for 10 epochs, which is a lot longer than our quick run. They also experimented with Llama 3 and kindly [shared their logs](https://huggingface.co/orpo-explorers/hf-llama3-8b-orpo-v0.0/tensorboard) with me (thanks [Jiwoo Hong](https://twitter.com/jiwoohong98)).\nTo end this tutorial, let\u2019s merge the QLoRA adapter with the base model and push it to the Hugging Face Hub.\n```\n# Flush memory\ndel trainer, model\ngc.collect()\ntorch.cuda.empty_cache()\n# Reload tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(\n  base_model,\n  low_cpu_mem_usage=True,\n  return_dict=True,\n  torch_dtype=torch.float16,\n  device_map=\"auto\",\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n# Merge adapter with base model\nmodel = PeftModel.from_pretrained(model, new_model)\nmodel = model.merge_and_unload()\nmodel.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)\n```\n\nCongrats, we finished this quick fine-tune of Llama 3: [mlabonne/OrpoLlama-3\u20138B](https://huggingface.co/mlabonne/OrpoLlama-3-8B). You can play with it using this [Hugging Face Space](https://huggingface.co/spaces/mlabonne/OrpoLlama-3-8B) (here\u2019s a [notebook](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC?usp=sharing) to make your own). Although the model is undertrained, as highlighted by the W&B curves, I ran some evaluations on Nous\u2019 benchmark suite using [LLM AutoEval](https://github.com/mlabonne/llm-autoeval).\n![](https://towardsdatascience.com/wp-content/uploads/2024/04/1XLNStboeDllWwCD-XyCTXw.png)\nOur ORPO fine-tune is actually pretty decent and improves the base model\u2019s performance on every benchmark. This is encouraging and likely means that a fine-tune on the entire 40k samples would yield great results.\nThis is an exciting time for the open-source community, with more and more high-quality open-weight models being released. The gap between closed-source and open-weight models is slowly closing, and fine-tuning is an essential tool to get the best performance for your use cases.\n![Image by author](https://towardsdatascience.com/wp-content/uploads/2024/04/16MeN5SXi4yrnNyf2O_-5zQ.png)Image by author\n## Conclusion\nIn this article, we introduced the ORPO algorithm and explained how it unifies the SFT and preference alignment stages into a single process. Then, we used TRL to fine-tune a Llama 3 8B model on a custom preference dataset. The final model shows encouraging results and highlights ORPO\u2019s potential as a new fine-tuning paradigm.\nI hope it was useful, and I recommend running the [Colab notebook](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing) to fine-tune your own Llama 3 models. In future articles, we will see how to create high-quality datasets \u2013 a point that is often overlooked. If you liked this article, please follow me on [Hugging Face](https://huggingface.co/mlabonne/) and Twitter [@maximelabonne](https://twitter.com/maximelabonne).\n## References\n  * J. Hong, N. Lee, and J. Thorne, [ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691). 2024.\n  * L. von Werra et al., TRL: Transformer Reinforcement Learning. GitHub, 2020. [Online]. Available: <https://github.com/huggingface/trl>\n  * Bartolome, A., Martin, G., & Vila, D. (2023). Notus. In GitHub Repository. GitHub. <https://github.com/argilla-io/notus>\n  * AI at Meta, [Introducing Meta Llama 3](https://ai.meta.com/blog/meta-llama-3/), 2024.\n\n\nWritten By\nMaxime Labonne\n[See all from Maxime Labonne](https://towardsdatascience.com/author/mlabonne/)\nTopics:\n[Artificial Intelligence](https://towardsdatascience.com/tag/artificial-intelligence/), [Editors Pick](https://towardsdatascience.com/tag/editors-pick/), [Hands On Tutorials](https://towardsdatascience.com/tag/hands-on-tutorials/), [Large Language Models](https://towardsdatascience.com/tag/large-language-models/), [Machine Learning](https://towardsdatascience.com/tag/machine-learning/)\nShare this article:\n  * [ Share on Facebook  ](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-llama-3-with-orpo-56cfab2f9ada%2F&title=Fine-tune%20Llama%203%20with%20ORPO)\n  * [ Share on LinkedIn  ](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-llama-3-with-orpo-56cfab2f9ada%2F&title=Fine-tune%20Llama%203%20with%20ORPO)\n  * [ Share on X  ](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-llama-3-with-orpo-56cfab2f9ada%2F&text=Fine-tune%20Llama%203%20with%20ORPO)\n\n\n## Related Articles\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/0c09RmbCCpfjAbSMq.png)\n## [Implementing Convolutional Neural Networks in TensorFlow](https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nStep-by-step code guide to building a Convolutional Neural Network \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nAugust 20, 2024\n6 min read\n  * ## [What Do Large Language Models \u201cUnderstand\u201d?](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA deep dive on the meaning of understanding and how it applies to LLMs \n[Tarik Dzekman](https://towardsdatascience.com/author/tarikdzekman/)\nAugust 21, 2024\n31 min read\n  * ![Photo by Krista Mangulsone on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/0GyVVTbgotH-DhGPH-scaled.jpg)\n## [How to Forecast Hierarchical Time Series](https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/)\n[ Artificial Intelligence ](https://towardsdatascience.com/category/artificial-intelligence/)\nA beginner\u2019s guide to forecast reconciliation \n[Dr. Robert K\u00fcbler](https://towardsdatascience.com/author/dr-robert-kuebler/)\nAugust 20, 2024\n13 min read\n  * ![Photo by davisuko on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/1bAABgtZtAIG5YW1oEjW3pA-scaled.jpeg)\n## [Hands-on Time Series Anomaly Detection using Autoencoders, with Python](https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nHere\u2019s how to use Autoencoders to detect signals with anomalies in a few lines of\u2026 \n[Piero Paialunga](https://towardsdatascience.com/author/piero-paialunga/)\nAugust 21, 2024\n12 min read\n  * ![Image from Canva.](https://towardsdatascience.com/wp-content/uploads/2024/08/1UAA9jQVdqMXnwzYiz8Q53Q.png)\n## [3 AI Use Cases (That Are Not a Chatbot)](https://towardsdatascience.com/3-ai-use-cases-that-are-not-a-chatbot-f4f328a2707a/)\n[ Machine Learning ](https://towardsdatascience.com/category/artificial-intelligence/machine-learning/)\nFeature engineering, structuring unstructured data, and lead scoring \n[Shaw Talebi](https://towardsdatascience.com/author/shawhin/)\nAugust 21, 2024\n7 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2023/02/1VEUgT5T4absnTqBMOEuNig.png)\n## [Back To Basics, Part Uno: Linear Regression and Cost Function](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nAn illustrated guide on essential machine learning concepts \n[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)\nFebruary 3, 2023\n6 min read\n  * ![](https://towardsdatascience.com/wp-content/uploads/2024/08/1kM8tfYcdaoccB1HX71YDig.png)\n## [Must-Know in Statistics: The Bivariate Normal Projection Explained](https://towardsdatascience.com/must-know-in-statistics-the-bivariate-normal-projection-explained-ace7b2f70b5b/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nDerivation and practical examples of this powerful concept \n[Luigi Battistoni](https://towardsdatascience.com/author/lu-battistoni/)\nAugust 14, 2024\n7 min read\n  * ![Photo by Alex Geerts on Unsplash](https://towardsdatascience.com/wp-content/uploads/2020/11/0BF38u2sw4WQdaMLS-scaled.jpg)\n## [Our Columns](https://towardsdatascience.com/our-columns-53501f74c86d/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nColumns on TDS are carefully curated collections of posts on a particular idea or category\u2026 \n[TDS Editors](https://towardsdatascience.com/author/towardsdatascience/)\nNovember 14, 2020\n4 min read\n  * ![Image created by authors with GPT-4o](https://towardsdatascience.com/wp-content/uploads/2024/08/1vilI3Q4nlwqsAQLq3TOzSA.jpg)\n## [Optimizing Marketing Campaigns with Budgeted Multi-Armed Bandits](https://towardsdatascience.com/optimizing-marketing-campaigns-with-budgeted-multi-armed-bandits-a65fccd61878/)\n[ Data Science ](https://towardsdatascience.com/category/data-science/)\nWith demos, our new solution, and a video \n[Vadim Arzamasov](https://towardsdatascience.com/author/vadim-arzamasov/)\nAugust 16, 2024\n10 min read\n\n\n  * [YouTube](https://www.youtube.com/c/TowardsDataScience)\n  * [X](https://x.com/TDataScience)\n  * [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n  * [Threads](https://www.threads.net/@towardsdatascience)\n  * [Bluesky](https://bsky.app/profile/towardsdatascience.com)\n\n\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nYour home for data science and Al. The world\u2019s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\n\u00a9  Insight Media Group, LLC 2025 \n  * [About](https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/)\n  * [Privacy Policy](https://towardsdatascience.com/privacy-policy/)\n  * [Terms of Use](https://towardsdatascience.com/website-terms-of-use/)\n\n\n[Towards Data Science is now independent!](https://towardsdatascience.com/towards-data-science-is-launching-as-an-independent-publication/)\nCookies Settings\n## Sign up to our newsletter\n![Company Logo](https://cdn.cookielaw.org/logos/static/ot_company_logo.png)\n## Privacy Preference Center\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer. [More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nAllow All\n###  Manage Consent Preferences\n#### Functional Cookies\nFunctional Cookies Active\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\n#### Strictly Necessary Cookies\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n#### Performance Cookies\nPerformance Cookies Active\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n#### Targeting Cookies\nTargeting Cookies Active\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nBack Button\n### Cookie List\nSearch Icon\nFilter Icon\nClear\ncheckbox label label\nApply Cancel\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nReject All Confirm My Choices\n[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-consent/)\nSome areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.\n",
    "answer": "```markdown\n# TL;DR Summary\n\nThe article introduces ORPO, a unified fine-tuning technique for Llama 3 models that merges supervised fine-tuning and preference alignment, enhancing efficiency and performance. ORPO outperforms traditional methods, requiring fewer resources. The fine-tuned Llama 3 8B model shows promising results, indicating ORPO's potential in LLM adaptation.\n\n**Tags:** Generative AI, LLMs, Fine-tuning, ORPO, Llama 3\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\n# \n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/646cf8084eefb026fb8fd8bc/oCTqufkdTkjyGodsx1vo1.png)](https://huggingface.co/meta-llama)\n[meta-llama](https://huggingface.co/meta-llama)\n/\n[Llama-2-70b-hf](https://huggingface.co/meta-llama/Llama-2-70b-hf)\nlike 849\nFollow\n![](https://cdn-avatars.huggingface.co/v1/production/uploads/646cf8084eefb026fb8fd8bc/oCTqufkdTkjyGodsx1vo1.png) Meta Llama 34.4k\n[ Text Generation ](https://huggingface.co/models?pipeline_tag=text-generation)[ Transformers ](https://huggingface.co/models?library=transformers)[ PyTorch ](https://huggingface.co/models?library=pytorch)[ Safetensors ](https://huggingface.co/models?library=safetensors)[ English ](https://huggingface.co/models?language=en)[ llama ](https://huggingface.co/models?other=llama)[ facebook ](https://huggingface.co/models?other=facebook)[ meta ](https://huggingface.co/models?other=meta)[ llama-2 ](https://huggingface.co/models?other=llama-2)[ text-generation-inference ](https://huggingface.co/models?other=text-generation-inference)\narxiv: 2307.09288\nLicense: llama2\n[ Model card ](https://huggingface.co/meta-llama/Llama-2-70b-hf)[ Files Files and versions ](https://huggingface.co/meta-llama/Llama-2-70b-hf/tree/main)\nTrain \nDeploy \nUse this model \n##  You need to share contact information with Meta to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\n### LLAMA 2 COMMUNITY LICENSE AGREEMENT\n\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Llama 2 distributed by Meta at <https://ai.meta.com/resources/models-and-libraries/llama-downloads/>.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity's behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Llama 2\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at ai.meta.com/resources/models-and-libraries/llama-downloads/.\"Llama Materials\" means, collectively, Meta's proprietary Llama 2 and documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). \nBy clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n  1. License Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non- transferable and royalty-free limited license under Meta's intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make the Llama Materials, or any derivative works thereof, available to a third party, you shall provide a copy of this Agreement to such third party.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a \"Notice\" text file distributed as a part of such copies: \"Llama 2 is licensed under the LLAMA 2 Community License, Copyright (c) Meta Platforms, Inc. All Rights Reserved.\"iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at <https://ai.meta.com/llama/use-policy>), which is hereby incorporated by reference into this Agreement.v. You will not use the Llama Materials or any output or results of the Llama Materials to improve any other large language model (excluding Llama 2 or derivative works thereof). \n  2. Additional Commercial Terms. If, on the Llama 2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee's affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n  3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n  4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n  5. Intellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials.b. Subject to Meta's ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 2 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n  6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement. \n  7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\n\n\n### Llama 2 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 2. If you access or use Llama 2, you agree to this Acceptable Use Policy (\u201cPolicy\u201d). The most recent copy of this policy can be found at [ai.meta.com/llama/use-policy](http://ai.meta.com/llama/use-policy).\n#### Prohibited Uses\nWe want everyone to use Llama 2 safely and responsibly. You agree you will not use, or allow others to use, Llama 2 to:\n  1. Violate the law or others\u2019 rights, including to:\n    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as: \n      1. Violence or terrorism \n      2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n      3. Human trafficking, exploitation, and sexual violence\n      4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n      5. Sexual solicitation\n      6. Any other criminal activity\n    2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n    3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n    4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices \n    5. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\n    6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama 2 Materials\n    7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n  2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 2 related to the following:\n    1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\n    2. Guns and illegal weapons (including weapon development)\n    3. Illegal drugs and regulated/controlled substances\n    4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n    5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n    6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n  3. Intentionally deceive or mislead others, including use of Llama 2 related to the following:\n    1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n    2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n    3. Generating, promoting, or further distributing spam\n    4. Impersonating another individual without consent, authorization, or legal right\n    5. Representing that the use of Llama 2 or outputs are human-generated\n    6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement \n    7. Fail to appropriately disclose to end users any known dangers of your AI systemPlease report any violation of this Policy, software \u201cbug,\u201d or other problems that could lead to a violation of this Policy through one of the following means:\n     * Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n     * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n     * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n     * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama: LlamaUseReport@meta.com\n\n\n[Log in](https://huggingface.co/login?next=%2Fmeta-llama%2FLlama-2-70b-hf) or [Sign Up](https://huggingface.co/join?next=%2Fmeta-llama%2FLlama-2-70b-hf) to review the conditions and access this model content.\n  * [**Llama 2**](https://huggingface.co/meta-llama/Llama-2-70b-hf/#llama-2 \"Llama 2\")\n    * [Model Details](https://huggingface.co/meta-llama/Llama-2-70b-hf/#model-details \"Model Details\")\n    * [Intended Use](https://huggingface.co/meta-llama/Llama-2-70b-hf/#intended-use \"Intended Use\")\n    * [Hardware and Software](https://huggingface.co/meta-llama/Llama-2-70b-hf/#hardware-and-software \"Hardware and Software\")\n    * [Training Data](https://huggingface.co/meta-llama/Llama-2-70b-hf/#training-data \"Training Data\")\n    * [Evaluation Results](https://huggingface.co/meta-llama/Llama-2-70b-hf/#evaluation-results \"Evaluation Results\")\n    * [Ethical Considerations and Limitations](https://huggingface.co/meta-llama/Llama-2-70b-hf/#ethical-considerations-and-limitations \"Ethical Considerations and Limitations\")\n    * [Reporting Issues](https://huggingface.co/meta-llama/Llama-2-70b-hf/#reporting-issues \"Reporting Issues\")\n    * [Llama Model Index](https://huggingface.co/meta-llama/Llama-2-70b-hf/#llama-model-index \"Llama Model Index\")\n\n\n#  [ ](https://huggingface.co/meta-llama/Llama-2-70b-hf/#llama-2) **Llama 2**\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.\n##  [ ](https://huggingface.co/meta-llama/Llama-2-70b-hf/#model-details) Model Details \n_Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the[website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License before requesting access here._\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\n**Model Developers** Meta\n**Variations** Llama 2 comes in a range of parameter sizes \u2014 7B, 13B, and 70B \u2014 as well as pretrained and fine-tuned variations.\n**Input** Models input text only.\n**Output** Models generate text only.\n**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\nTraining Data | Params | Content Length | GQA | Tokens | LR  \n---|---|---|---|---|---  \nLlama 2 | _A new mix of publicly available online data_ | 7B | 4k | \u2717 | 2.0T | 3.0 x 10-4  \nLlama 2 | _A new mix of publicly available online data_ | 13B | 4k | \u2717 | 2.0T | 3.0 x 10-4  \nLlama 2 | _A new mix of publicly available online data_ | 70B | 4k | \u2714 | 2.0T | 1.5 x 10-4  \n_Llama 2 family of models._ Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models - 70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\n**Model Dates** Llama 2 was trained between January 2023 and July 2023.\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n**License** A custom commercial license is available at: <https://ai.meta.com/resources/models-and-libraries/llama-downloads/>\n**Research Paper** [\"Llama-2: Open Foundation and Fine-tuned Chat Models\"](https://huggingface.co/meta-llama/Llama-2-70b-hf/blob/main/arxiv.org/abs/2307.09288)\n##  [ ](https://huggingface.co/meta-llama/Llama-2-70b-hf/#intended-use) Intended Use \n**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).\n**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\n##  [ ](https://huggingface.co/meta-llama/Llama-2-70b-hf/#hardware-and-software) Hardware and Software \n**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta\u2019s sustainability program.\nTime (GPU hours) | Power Consumption (W) | Carbon Emitted(tCO2eq)  \n---|---|---  \nLlama 2 7B | 184320 | 400 | 31.22  \nLlama 2 13B | 368640 | 400 | 62.44  \nLlama 2 70B | 1720320 | 400 | 291.42  \nTotal | 3311616 | 539.00  \n**CO 2 emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n##  [ ](https://huggingface.co/meta-llama/Llama-2-70b-hf/#training-data) Training Data \n**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\n##  [ ](https://huggingface.co/meta-llama/Llama-2-70b-hf/#evaluation-results) Evaluation Results \nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\nModel | Size | Code | Commonsense Reasoning | World Knowledge | Reading Comprehension | Math | MMLU | BBH | AGI Eval  \n---|---|---|---|---|---|---|---|---|---  \nLlama 1 | 7B | 14.1 | 60.8 | 46.2 | 58.5 | 6.95 | 35.1 | 30.3 | 23.9  \nLlama 1 | 13B | 18.9 | 66.1 | 52.6 | 62.3 | 10.9 | 46.9 | 37.0 | 33.9  \nLlama 1 | 33B | 26.0 | 70.0 | 58.4 | 67.6 | 21.4 | 57.8 | 39.8 | 41.7  \nLlama 1 | 65B | 30.7 | 70.7 | 60.5 | 68.6 | 30.8 | 63.4 | 43.5 | 47.6  \nLlama 2 | 7B | 16.8 | 63.9 | 48.9 | 61.3 | 14.6 | 45.3 | 32.6 | 29.3  \nLlama 2 | 13B | 24.5 | 66.9 | 55.4 | 65.8 | 28.7 | 54.8 | 39.4 | 39.1  \nLlama 2 | 70B | **37.5** | **71.9** | **63.6** | **69.4** | **35.2** | **68.9** | **51.2** | **54.2**  \n**Overall performance on grouped academic benchmarks.** _Code:_ We report the average pass@1 scores of our models on HumanEval and MBPP. _Commonsense Reasoning:_ We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. _World Knowledge:_ We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. _Reading Comprehension:_ For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. _MATH:_ We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\nTruthfulQA | Toxigen  \n---|---  \nLlama 1 | 7B | 27.42 | 23.00  \nLlama 1 | 13B | 41.74 | 23.08  \nLlama 1 | 33B | 44.19 | 22.57  \nLlama 1 | 65B | 48.71 | 21.77  \nLlama 2 | 7B | 33.29 | **21.25**  \nLlama 2 | 13B | 41.86 | 26.10  \nLlama 2 | 70B | **50.18** | 24.60  \n**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\nTruthfulQA | Toxigen  \n---|---  \nLlama-2-Chat | 7B | 57.04 | **0.00**  \nLlama-2-Chat | 13B | 62.18 | **0.00**  \nLlama-2-Chat | 70B | **64.14** | 0.01  \n**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.\n##  [ ](https://huggingface.co/meta-llama/Llama-2-70b-hf/#ethical-considerations-and-limitations) Ethical Considerations and Limitations \nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\nPlease see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)\n##  [ ](https://huggingface.co/meta-llama/Llama-2-70b-hf/#reporting-issues) Reporting Issues \nPlease report any software \u201cbug,\u201d or other problems with the models through one of the following means:\n  * Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n  * Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n  * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n\n##  [ ](https://huggingface.co/meta-llama/Llama-2-70b-hf/#llama-model-index) Llama Model Index \nModel | Llama2 | Llama2-hf | Llama2-chat | Llama2-chat-hf  \n---|---|---|---|---  \n7B | [Link](https://huggingface.co/meta-llama/Llama-2-7b) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)  \n13B | [Link](https://huggingface.co/meta-llama/Llama-2-13b) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)  \n70B | [Link](https://huggingface.co/meta-llama/Llama-2-70b) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) \n\nDownloads last month\n    28,867   \nSafetensors[](https://huggingface.co/docs/safetensors)\nModel size\n69B params\nTensor type\nFP16 \n\u00b7\nF32 \n\u00b7\nInference Providers [NEW](https://huggingface.co/meta-llama/Llama-2-70b-hf/?inference_api=true)\nTogether AI\n[ Text Generation](https://huggingface.co/tasks/text-generation \"Learn more about text-generation\")\nExamples\nCompute `\u2318+Enter`\nView Code\nMaximize\n##  Model tree for meta-llama/Llama-2-70b-hf [](https://huggingface.co/docs/hub/model-cards#specifying-a-base-model)\nAdapters\n[21 models](https://huggingface.co/models?other=base_model:adapter:meta-llama/Llama-2-70b-hf)\nFinetunes\n[27 models](https://huggingface.co/models?other=base_model:finetune:meta-llama/Llama-2-70b-hf)\nMerges\n[8 models](https://huggingface.co/models?other=base_model:merge:meta-llama/Llama-2-70b-hf)\nQuantizations\n[5 models](https://huggingface.co/models?other=base_model:quantized:meta-llama/Llama-2-70b-hf)\n##  Spaces using meta-llama/Llama-2-70b-hf 100\n[\ud83c\udfc6 eduagarcia/open_pt_llm_leaderboard](https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard)[\ud83c\udfc6 BAAI/open_cn_llm_leaderboard](https://huggingface.co/spaces/BAAI/open_cn_llm_leaderboard)[\ud83d\udcbd Illia56/Ask-AI-Youtube](https://huggingface.co/spaces/Illia56/Ask-AI-Youtube)[\ud83e\udd47 logikon/open_cot_leaderboard](https://huggingface.co/spaces/logikon/open_cot_leaderboard)[\ud83d\udc83 EvanTHU/MotionLLM](https://huggingface.co/spaces/EvanTHU/MotionLLM)[\ud83d\udcbb KBaba7/Quant](https://huggingface.co/spaces/KBaba7/Quant)[\ud83d\udc11 allenai/URIAL-Bench](https://huggingface.co/spaces/allenai/URIAL-Bench)[\ud83d\udcac allenai/BaseChat](https://huggingface.co/spaces/allenai/BaseChat)[\ud83d\udc8a\ud83d\udc30\u2695\ufe0f AIM-Harvard/rabbits-leaderboard](https://huggingface.co/spaces/AIM-Harvard/rabbits-leaderboard)[\ud83d\ude3b GTBench/GTBench](https://huggingface.co/spaces/GTBench/GTBench)[\ud83c\udfc6 Vikhrmodels/small-shlepa-lb](https://huggingface.co/spaces/Vikhrmodels/small-shlepa-lb)[\ud83d\ude3b prometheus-eval/BiGGen-Bench-Leaderboard](https://huggingface.co/spaces/prometheus-eval/BiGGen-Bench-Leaderboard) + 95 Spaces + 88 Spaces\n##  Collections including meta-llama/Llama-2-70b-hf\n#### [Llama 2 Family Collection  This collection hosts the transformers and original repos of the Llama 2 and Llama Guard releases \u2022  13 items \u2022  Updated Dec 6, 2024 \u2022 79](https://huggingface.co/collections/meta-llama/llama-2-family-661da1f90a9d678b6f55773b)\n#### [Meta's Llama2 models Collection  12 items \u2022  Updated Dec 13, 2024 \u2022 116](https://huggingface.co/collections/meta-llama/metas-llama2-models-675bfd70e574a62dd0e40541)\nSystem theme \nCompany\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/) [](https://huggingface.co/)\nWebsite\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)\n",
    "answer": "```markdown\n# TL;DR Summary\n\nLlama 2, developed by Meta, is a series of generative text models with 7B to 70B parameters, optimized for dialogue and outperforming many open-source models. It requires a license for access and has strict usage guidelines to prevent misuse. Ethical considerations and safety testing are emphasized.\n\n**Tags:** Generative AI, LLMs, Meta, Llama 2\n```"
  },
  {
    "instruction": "# Notes\n\n\n\n<child_page>\n# Visualize GPU Memory Usage for Better Optimization\n\nTracking GPU memory usage can reveal inefficiencies and potential bottlenecks. PyTorch offers a built-in tool for detailed memory profiling.\n# Why This Works\n\nTracking memory usage helps identify inefficiencies, spikes, and fragmentation in GPU memory. By recording and visualizing these patterns, you can optimize model performance, debug memory leaks, and improve memory management, especially for large-scale or resource-limited applications.\nBenefits\n- Detects memory spikes and fragmentation.\n- Optimizes model scaling and deployment.\n- Enables debugging of memory leaks in complex pipelines.\nApplications\nUse this when developing memory-intensive models, deploying on limited-resource hardware, or scaling across multiple GPUs.\nUsage\nUse the code below. This generates a profile.pkl file, storing detailed memory usage data. Visualize it using\u00a0[PyTorch's memory visualizer](https://pytorch.org/memory_viz).\n```\nimport torch\nfrom torch import nn\n\n# Start recording memory snapshot history\ntorch.cuda.memory._record_memory_history(max_entries=100000)\n\n# Example model and computation\nmodel = nn.Linear(10_000, 50_000, device=\"cuda\")\nfor _ in range(3):\n    inputs = torch.randn(5_000, 10_000, device=\"cuda\")\n    outputs = model(inputs)\n\n# Dump memory history to a file and stop recording\ntorch.cuda.memory._dump_snapshot(\"profile.pkl\")\ntorch.cuda.memory._record_memory_history(enabled=None)\n````\n</child_page>\n\n\n# Resources\n\n# Articles\n\n\t- [A guide on good usage of\u00a0non_blocking\u00a0and\u00a0\n\tpin_memory()\u00a0in PyTorch](https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html), on PyTorch tutorials\n\t-",
    "answer": "```markdown\n# TL;DR Summary\n\n**Visualize GPU Memory Usage for Better Optimization**  \nTracking GPU memory with PyTorch helps identify inefficiencies, optimize model performance, and debug memory leaks. Use the provided code to record and visualize memory usage, especially for large-scale applications.\n\n**Benefits**: Detects memory spikes, optimizes deployment, and aids in debugging.  \n**Applications**: Ideal for memory-intensive models and limited-resource hardware.\n```"
  },
  {
    "instruction": "[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n  * [ Models](https://huggingface.co/models)\n  * [ Datasets](https://huggingface.co/datasets)\n  * [ Spaces](https://huggingface.co/spaces)\n  * [ Posts](https://huggingface.co/posts)\n  * [ Docs](https://huggingface.co/docs)\n  * [ Enterprise](https://huggingface.co/enterprise)\n  * [Pricing](https://huggingface.co/pricing)\n  * [Log In](https://huggingface.co/login)\n  * [Sign Up](https://huggingface.co/join)\n\n\n# \n[![](https://cdn-avatars.huggingface.co/v1/production/uploads/1676334017493-63ead3a0d0b894bbc77b199c.png)](https://huggingface.co/mosaicml)\n[mosaicml](https://huggingface.co/mosaicml)\n/\n[mpt-30b](https://huggingface.co/mosaicml/mpt-30b)\nlike 342\nFollow\n![](https://cdn-avatars.huggingface.co/v1/production/uploads/1676334017493-63ead3a0d0b894bbc77b199c.png) Mosaic ML, Inc. 380\n[ Text Generation ](https://huggingface.co/models?pipeline_tag=text-generation)[ Transformers ](https://huggingface.co/models?library=transformers)[ PyTorch ](https://huggingface.co/models?library=pytorch)\n5 datasets\n[ mpt ](https://huggingface.co/models?other=mpt)[ Composer ](https://huggingface.co/models?other=Composer)[ MosaicML ](https://huggingface.co/models?other=MosaicML)[ llm-foundry ](https://huggingface.co/models?other=llm-foundry)[ StreamingDatasets ](https://huggingface.co/models?other=StreamingDatasets)[ custom_code ](https://huggingface.co/models?other=custom_code)[ text-generation-inference ](https://huggingface.co/models?other=text-generation-inference)\narxiv: 6 papers\nLicense: apache-2.0\n[ Model card ](https://huggingface.co/mosaicml/mpt-30b)[ Files Files and versions ](https://huggingface.co/mosaicml/mpt-30b/tree/main)[ Community 22 ](https://huggingface.co/mosaicml/mpt-30b/discussions)\nTrain \nDeploy \nUse this model \n  * [MPT-30B](https://huggingface.co/mosaicml/mpt-30b/#mpt-30b \"MPT-30B\")\n    * [Model Date](https://huggingface.co/mosaicml/mpt-30b/#model-date \"Model Date\")\n    * [Model License](https://huggingface.co/mosaicml/mpt-30b/#model-license \"Model License\")\n    * [Documentation](https://huggingface.co/mosaicml/mpt-30b/#documentation \"Documentation\")\n    * [How to Use](https://huggingface.co/mosaicml/mpt-30b/#how-to-use \"How to Use\")\n    * [Model Description](https://huggingface.co/mosaicml/mpt-30b/#model-description \"Model Description\")\n    * [Training Data](https://huggingface.co/mosaicml/mpt-30b/#training-data \"Training Data\")\n      * [Streaming Datasets](https://huggingface.co/mosaicml/mpt-30b/#streaming-datasets \"Streaming Datasets\")\n      * [Data Mix](https://huggingface.co/mosaicml/mpt-30b/#data-mix \"Data Mix\")\n      * [Training Configuration](https://huggingface.co/mosaicml/mpt-30b/#training-configuration \"Training Configuration\")\n    * [Limitations and Biases](https://huggingface.co/mosaicml/mpt-30b/#limitations-and-biases \"Limitations and Biases\")\n    * [MosaicML Platform](https://huggingface.co/mosaicml/mpt-30b/#mosaicml-platform \"MosaicML Platform\")\n    * [Disclaimer](https://huggingface.co/mosaicml/mpt-30b/#disclaimer \"Disclaimer\")\n    * [Citation](https://huggingface.co/mosaicml/mpt-30b/#citation \"Citation\")\n\n\n#  [ ](https://huggingface.co/mosaicml/mpt-30b/#mpt-30b) MPT-30B \nMPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by [MosaicML](https://www.mosaicml.com).\nMPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.\nMPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see [MPT-7B-StoryWriter](https://huggingface.co/mosaicml/mpt-7b-storywriter)), support for context-length extrapolation via [ALiBi](https://arxiv.org/abs/2108.12409), and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's [FasterTransformer](https://github.com/NVIDIA/FasterTransformer). The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPU\u2014either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.\nThis model uses the MosaicML LLM codebase, which can be found in the [llm-foundry repository](https://github.com/mosaicml/llm-foundry). It was trained by MosaicML\u2019s NLP team on the [MosaicML platform](https://www.mosaicml.com/training) for LLM pretraining, finetuning, and inference.\n###  [ ](https://huggingface.co/mosaicml/mpt-30b/#how-is-this-model-different) How is this model different? \nMPT-30B is:\n  * **Licensed for the possibility of commercial use** (unlike [LLaMA](https://arxiv.org/abs/2302.13971)).\n  * **Trained on a large amount of data** (1T tokens like [LLaMA](https://arxiv.org/abs/2302.13971) vs. 300B for [Pythia](https://github.com/EleutherAI/pythia), 300B for [OpenLLaMA](https://github.com/openlm-research/open_llama), and 800B for [StableLM](https://github.com/Stability-AI/StableLM)).\n  * **Prepared to handle extremely long inputs** thanks to [ALiBi](https://arxiv.org/abs/2108.12409).\n  * **Capable of fast training and inference** (via [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf) and [FasterTransformer](https://github.com/NVIDIA/FasterTransformer))\n  * **Equipped with highly efficient open-source training code** via the [llm-foundry repository](https://github.com/mosaicml/llm-foundry)\n\n\n###  [ ](https://huggingface.co/mosaicml/mpt-30b/#models-finetuned-off-mpt-30b) Models finetuned off MPT-30B: \nThe following models are finetuned on MPT-30B:\n  * [MPT-30B-Instruct](https://huggingface.co/mosaicml/mpt-30b-instruct): a model for long-form instruction following (especially summarization and question-answering). Built by finetuning MPT-30B on several carefully curated datasets.\n    * License: _CC-BY-SA-3.0_\n  * [MPT-30B-Chat](https://huggingface.co/mosaicml/mpt-30b-chat): a chatbot-like model for dialogue generation. Built by finetuning MPT-30B on [ShareGPT-Vicuna](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered), [Camel-AI](https://huggingface.co/camel-ai), [GPTeacher](https://github.com/teknium1/GPTeacher), [Guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco), [Baize](https://github.com/project-baize/baize-chatbot) and some generated datasets.\n    * License: _CC-By-NC-SA-4.0_\n    * [Demo on Hugging Face Spaces](https://huggingface.co/spaces/mosaicml/mpt-30b-chat)\n\n\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#model-date) Model Date \nJune 22, 2023\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#model-license) Model License \nApache-2.0\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#documentation) Documentation \n  * [Blog post: MPT-30B: Raising the bar for open-source foundation models](https://www.mosaicml.com/blog/mpt-30b)\n  * [Codebase (mosaicml/llm-foundry repo)](https://github.com/mosaicml/llm-foundry/)\n  * Questions: Feel free to contact us via the [MosaicML Community Slack](https://mosaicml.me/slack)!\n\n\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#how-to-use) How to Use \nThis model is best used with the MosaicML [llm-foundry repository](https://github.com/mosaicml/llm-foundry) for training and finetuning.\n```\nimport transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n 'mosaicml/mpt-30b',\n trust_remote_code=True\n)\n\n```\n\nNote: This model requires that `trust_remote_code=True` be passed to the `from_pretrained` method. This is because we use a custom `MPT` model architecture that is not yet part of the Hugging Face `transformers` package. `MPT` includes options for many training efficiency features such as [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf), [ALiBi](https://arxiv.org/abs/2108.12409), [QK LayerNorm](https://arxiv.org/abs/2010.04245), and more.\nTo use the optimized [triton implementation](https://github.com/openai/triton) of FlashAttention, you can load the model on GPU (`cuda:0`) with `attn_impl='triton'` and with `bfloat16` precision:\n```\nimport torch\nimport transformers\nname = 'mosaicml/mpt-30b'\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton' # change this to use triton-based FlashAttention\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n name,\n config=config,\n torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n trust_remote_code=True\n)\n\n```\n\nThe model was trained initially with a sequence length of 2048 with an additional pretraining stage for sequence length adapation up to 8192. However, ALiBi enables users to increase the maximum sequence length even further during finetuning and/or inference. For example:\n```\nimport transformers\nname = 'mosaicml/mpt-30b'\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 16384 # (input + output) tokens can now be up to 16384\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n name,\n config=config,\n trust_remote_code=True\n)\n\n```\n\nThis model was trained with the MPT-30B tokenizer which is identical to the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer.\n```\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('mosaicml/mpt-30b')\n\n```\n\nThe model can then be used, for example, within a text-generation pipeline.Note: when running Torch modules in lower precision, it is best practice to use the [torch.autocast context manager](https://pytorch.org/docs/stable/amp.html).\n```\nfrom transformers import pipeline\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n  inputs = tokenizer('Here is a recipe for vegan banana bread:\\n', return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(**inputs, max_new_tokens=100)\n  print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n# or using the HF pipeline\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n  print(\n    pipe('Here is a recipe for vegan banana bread:\\n',\n      max_new_tokens=100,\n      do_sample=True,\n      use_cache=True))\n\n```\n\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#model-description) Model Description \nThe architecture is a modification of a standard decoder-only transformer.\nThe model has been modified from a standard transformer in the following ways:\n  * It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)\n  * It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings\n  * It does not use biases\n\n\nHyperparameter | Value  \n---|---  \nn_parameters | 29.95B  \nn_layers | 48  \nn_heads | 64  \nd_model | 7168  \nvocab size | 50432  \nsequence length | 8192  \n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#training-data) Training Data \n###  [ ](https://huggingface.co/mosaicml/mpt-30b/#streaming-datasets) Streaming Datasets \nData was formatted using the MosaicML [StreamingDataset](https://github.com/mosaicml/streaming) library to host our data in object storage and efficiently stream it to our compute cluster during training. StreamingDataset obviates the need to download the whole dataset before starting training, and allows instant resumption of training from any point in the dataset.\n###  [ ](https://huggingface.co/mosaicml/mpt-30b/#data-mix) Data Mix \nThe model was trained for 1T tokens on the following data mix:\nData Source | Number of Tokens in Source | Proportion | Effective Number of Tokens | Epochs  \n---|---|---|---|---  \nmC4 3.1.0 - English (200+ words) | 2417.99 B | 33.50% | 335 B | 0.14  \nc4 - English - SemDedup 80% | 100.42 B | 29.90% | 299 B | 2.98  \nRedPajama - CommonCrawl | 878.45 B | 8.50% | 85 B | 0.097  \nThe Stack - Selected Languages | 463.78 B | 10.00% | 100 B | 0.22  \nRedPajama - Wikipedia | 4.87 B | 4.00% | 40 B | 8.21  \nThe Stack - Markdown | 107.07 B | 4.50% | 45 B | 0.42  \nSemantic Scholar ORC | 48.95 B | 3.30% | 33 B | 0.67  \nRedPajama - Books | 26.02 B | 3.00% | 30 B | 1.15  \nRedPajama - arXiv | 28.10 B | 1.90% | 19 B | 0.68  \nRedPajama - StackExchange | 20.54 B | 1.40% | 14 B | 0.68  \nSamples for each batch were selected from one of the datasets with the probability specified above. The examples were shuffled within each dataset, and each example was constructed from as many sequences from that dataset as were necessary to fill the sequence length. To build 8k support into MPT-30B efficiently, we first pre-trained on 1T tokens using sequences that were 2k tokens long, and then trained for an additional 50B tokens using sequences that were 8k tokens long.\nThe data was tokenized using the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer. This BPE tokenizer has a number of desirable characteristics, most of which are relevant for tokenizing code: (1) It was trained on a diverse mix of data that includes code (The Pile) (2) It applies consistent space delimitation, unlike the GPT2 tokenizer which tokenizes inconsistently depending on the presence of prefix spaces (3) It contains tokens for repeated space characters, which allows superior compression of text with large amounts of repeated space characters.\nThe model vocabulary size of 50432 was set to be a multiple of 128 (as in [MEGATRON-LM](https://arxiv.org/abs/1909.08053)).\n###  [ ](https://huggingface.co/mosaicml/mpt-30b/#training-configuration) Training Configuration \nThe model was trained in three stages using the [MosaicML Platform](https://www.mosaicml.com/platform): (i) First it was trained on 440 A100-40GBs with a batch size of 1760. (ii) Then, on 216 A100-40GBs with a batch size of 1728. (iii) Training was completed on 256 H100-80GBs with a batch size of 512 with 8k context length and 50B tokens. The model was trained with sharded data parallelism using [FSDP](https://pytorch.org/docs/stable/fsdp.html) and used the [LION](https://arxiv.org/abs/2302.06675) optimizer.\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#limitations-and-biases) Limitations and Biases \n_The following language is modified from[EleutherAI's GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)_\nMPT-30B (Base) is **not** intended for deployment without finetuning. It should not be used for human-facing interactions without further guardrails and user consent.\nMPT-30B can produce factually incorrect output, and should not be relied on to produce factually accurate information. MPT-30B was trained on various public datasets. While great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#mosaicml-platform) MosaicML Platform \nIf you're interested in [training](https://www.mosaicml.com/training) and [deploying](https://www.mosaicml.com/inference) your own MPT or LLMs on the MosaicML Platform, [sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-30b).\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#disclaimer) Disclaimer \nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please consult an attorney before using this model for commercial purposes.\n##  [ ](https://huggingface.co/mosaicml/mpt-30b/#citation) Citation \nPlease cite this model using the following format:\n```\n@online{MosaicML2023Introducing,\n  author  = {MosaicML NLP Team},\n  title   = {Introducing MPT-30B: Raising the bar\nfor open-source foundation models},\n  year   = {2023},\n  url    = {www.mosaicml.com/blog/mpt-30b},\n  note   = {Accessed: 2023-06-22},\n  urldate  = {2023-06-22}\n}\n\n```\n\n\nDownloads last month\n    4,777 \nInference Providers [NEW](https://huggingface.co/blog/inference-providers)\n[ Text Generation](https://huggingface.co/tasks/text-generation \"Learn more about text-generation\")\nThis model isn't deployed by any Inference Provider. [\ud83d\ude4b Ask for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=mosaicml/mpt-30b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmosaicml%2Fmpt-30b%5D\\(%2Fmosaicml%2Fmpt-30b\\)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A\\(optional\\)%20Which%20providers%20are%20you%20interested%20in%3F%20\\(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6\\)%0A)\nHF Inference deployability: The model authors have turned it off explicitly.\n##  Model tree for mosaicml/mpt-30b [](https://huggingface.co/docs/hub/model-cards#specifying-a-base-model)\nQuantizations\n[2 models](https://huggingface.co/models?other=base_model:quantized:mosaicml/mpt-30b)\n##  Datasets used to train mosaicml/mpt-30b\n#### [allenai/c4 Viewer \u2022  Updated Jan 9, 2024 \u2022  10.4B \u2022  452k \u2022  394  ](https://huggingface.co/datasets/allenai/c4)#### [legacy-datasets/mc4 Updated Mar 5, 2024 \u2022  38.8k \u2022  151  ](https://huggingface.co/datasets/legacy-datasets/mc4)#### [bigcode/the-stack-dedup Viewer \u2022  Updated Aug 17, 2023 \u2022  237M \u2022  12.5k \u2022  345  ](https://huggingface.co/datasets/bigcode/the-stack-dedup)\n##  Spaces using mosaicml/mpt-30b 35\n[\ud83c\udfc6 eduagarcia/open_pt_llm_leaderboard](https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard)[\ud83c\udfc6 BAAI/open_cn_llm_leaderboard](https://huggingface.co/spaces/BAAI/open_cn_llm_leaderboard)[\ud83d\ude3b Sharathhebbar24/One-stop-for-Open-source-models](https://huggingface.co/spaces/Sharathhebbar24/One-stop-for-Open-source-models)[\ud83d\ude3b GTBench/GTBench](https://huggingface.co/spaces/GTBench/GTBench)[\ud83c\udfc6 Vikhrmodels/small-shlepa-lb](https://huggingface.co/spaces/Vikhrmodels/small-shlepa-lb)[\ud83c\udfc6 kz-transformers/kaz-llm-lb](https://huggingface.co/spaces/kz-transformers/kaz-llm-lb)[\ud83c\udfa8 OPTML-Group/UnlearnCanvas-Benchmark](https://huggingface.co/spaces/OPTML-Group/UnlearnCanvas-Benchmark)[\ud83e\udd47 BAAI/open_flageval_vlm_leaderboard](https://huggingface.co/spaces/BAAI/open_flageval_vlm_leaderboard)[\ud83d\udc28 Zulelee/langchain-chatchat](https://huggingface.co/spaces/Zulelee/langchain-chatchat)[\ud83c\udfc6 wissamantoun/LLM_Detection_Attribution](https://huggingface.co/spaces/wissamantoun/LLM_Detection_Attribution)[\ud83c\udfc6 Adeco/open_llm_leaderboard](https://huggingface.co/spaces/Adeco/open_llm_leaderboard)[\ud83c\udfc6 Intel/low_bit_open_llm_leaderboard](https://huggingface.co/spaces/Intel/low_bit_open_llm_leaderboard) + 30 Spaces + 23 Spaces\n##  Collection including mosaicml/mpt-30b\n#### [MPT Collection  The MPT collections is a series of decoder-style transformer models trained from scratch by MosaicML. Details: https://www.mosaicml.com/mpt \u2022  9 items \u2022  Updated Nov 27, 2023 \u2022 7](https://huggingface.co/collections/mosaicml/mpt-6564f3d9e5aac326bfa22def)\nSystem theme \nCompany\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/) [](https://huggingface.co/)\nWebsite\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)\n",
    "answer": "```markdown\n# TL;DR Summary of MPT-30B Model\n\nMPT-30B, developed by MosaicML, is a decoder-style transformer trained on 1 trillion tokens, featuring an 8k token context window and efficient training/inference via FlashAttention. It supports commercial use and excels in coding tasks. Key finetuned models include MPT-30B-Instruct and MPT-30B-Chat.\n```"
  },
  {
    "instruction": "![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61f958575ff62d320d46701e_closeicon.png)\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\nKindly note that during the maintenance window, app.hopsworks.ai will not be accessible.\n5\n[View the Changes](https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes)\n![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61f958575ff62d320d46701e_closeicon.png)\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\nKindly note that during the maintenance window, app.hopsworks.ai will not be accessible.\n5\n[View the Changes](https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes)\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\n[Contact](https://www.hopsworks.ai/contact/main)[Login](https://app.hopsworks.ai)[![Github Mark](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6418216a570b0da3d471661a_icons8-slack-new.svg)](https://join.slack.com/t/public-hopsworks/shared_invite/zt-1uf21vitz-rhHKNdIf8GEiOf1EJ6Wzsw)[![Github Mark](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/62261cde4669f63d3880938d_github.svg)](https://github.com/logicalclocks/hopsworks)[![linkedin logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/company/hopsworks/)[![Twitter icon](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/66a346ea27ec6d7c0e354747_icons8-twitter%20\\(1\\).svg)](https://twitter.com/hopsworks)\n[![Untitled UI logotext](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6202a13e7cafec5553703f6b_logo.svg)![Logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/666c3cc1cfc4741e6b2d9fe6_untitled-ui-logo.png)](https://www.hopsworks.ai/)\nProduct\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/5fad49f715203ed9d66fc1b0_Hops%20Icon%20Green.png)Hopsworks EnterpriseFull edition of Hopsworks, high levels of SLAs and support.](https://www.hopsworks.ai/try)[IntegrationsLeverage your existing data sources and tools.](https://www.hopsworks.ai/integrations)[ExamplesGet up and running on new features and techniques.](https://www.hopsworks.ai/hopsworks-examples)[FAQAll you need to know about Hopsworks.](https://www.hopsworks.ai/frequently-asked-questions)\n[Hopsworks On-PremisesManage everything securely within your data center.](https://www.hopsworks.ai/product-capabilities/feature-store-on-premises)[Performance & High AvailabilityHighest performance requirements in the industry.](https://www.hopsworks.ai/product-capabilities/operational-performance-and-high-availability)[Feature Engineering in PythonPython-first collaborative environment.](https://www.hopsworks.ai/product-capabilities/feature-engineering-in-python)[Other capabilitiesRead about our extended platform capabilities.](https://www.hopsworks.ai/product-capabilities)\nSolutions\nFor your Team\n[Technical Stakeholders](https://www.hopsworks.ai/solutions/teams)[Machine Learning Engineers](https://www.hopsworks.ai/solutions/teams?tab=ml-engineers)[Data Engineers](https://www.hopsworks.ai/solutions/teams?tab=data-engineers)[Data Scientists](https://www.hopsworks.ai/solutions/teams?tab=data-scientists)[DevOps](https://www.hopsworks.ai/solutions/teams?tab=devops)[Architects](https://www.hopsworks.ai/solutions/teams?tab=architects)[Non-Technical Stakeholders](https://www.hopsworks.ai/solutions/teams?tab=non-technical)[Project Managers ](https://www.hopsworks.ai/solutions/teams?tab=non-technical)\nFor your Industry\n[Defense and Law Enforcement](https://www.hopsworks.ai/solutions/industry/defense-and-law-enforcement)\n[FSI](https://www.hopsworks.ai/solutions/industry/fsi)\n[Online Retail & E-commerce](https://www.hopsworks.ai/solutions/industry/online-retail-and-e-commerce)\n[Public Sector](https://www.hopsworks.ai/solutions/industry/public-sector)\n[Research and Healthcare](https://www.hopsworks.ai/solutions/industry/research-and-healthcare)\n[iGaming](https://www.hopsworks.ai/solutions/industry/i-gaming)\n[All Solutions](https://www.hopsworks.ai/solutions/all)\nUse Cases\n[Generative AI](https://www.hopsworks.ai/use-case/fine-tuning-llms-rag-for-genai)[Real-time Fraud Detection](https://www.hopsworks.ai/use-case/realtime-fraud-detection)[Hopsworks Medical Copilot](https://www.hopscopilot.com/)\n[CustomersExplore how our customers leverage Hopsworks.](https://www.hopsworks.ai/customers)\n[Pricing](https://www.hopsworks.ai/pricing)[Blog](https://www.hopsworks.ai/blog)[Pricing](https://www.hopsworks.ai/pricing)[Blog](https://www.hopsworks.ai/blog)\nResources\n[MLOps DictionaryComprehensive terminology guide for ML solutions.](https://www.hopsworks.ai/mlops-dictionary)[DocumentationDetailed information to help you effectively utilize Hopsworks.](https://docs.hopsworks.ai/latest/)[Research PapersDiscover how our research is driving innovation.](https://www.hopsworks.ai/research-papers)[CommunityJoin our community and get all your questions answered. ](https://community.hopsworks.ai/)\n[EventsOnline & Offline sessions and workshops. ](https://www.hopsworks.ai/events)[AcademyEverything about ML Systems, and the Hopsworks platform.](https://www.hopsworks.ai/academy)[Feature Store ComparisonIn-depth comparisons of feature stores highlighting key features.](https://www.hopsworks.ai/product-comparison/sagemaker)[FAQ: EU AI ActA complete guide to The EU AI Act.](https://www.hopsworks.ai/faq-eu-ai-act)\nCompany\n[About usLearn more about our team. ](https://www.hopsworks.ai/about-us)[NewsThe latest industry news, updates and info.](https://www.hopsworks.ai/news)[Security & ComplianceRobust security and compliance with industry standards.](https://www.hopsworks.ai/security-compliance)\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/66a0b13c473a71304470c35a_oreilly_logo_mark_red.svg)Book](https://www.hopsworks.ai/lp/oreilly-book-building-ml-systems-with-a-feature-store)[Benchmarks](https://www.hopsworks.ai/index#performance)\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/63e4e90bd6c2ad05ecd89669_icons8-great-britain-96.png)EN](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/)[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/63e4e90b88b00c69a52f92cc_icons8-germany-96.png)DE](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/)\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/63e4e90bd6c2ad05ecd89669_icons8-great-britain-96.png)EN](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/)[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/63e4e90b88b00c69a52f92cc_icons8-germany-96.png)DE](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/)\n[![arrow back](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6183f249d69379869e0b3524_icons8-chevron-left-30.png)Back to Blog](https://www.hopsworks.ai/blog)\n[![arrow back](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6183f249d69379869e0b3524_icons8-chevron-left-30.png)Back to Blog](https://www.hopsworks.ai/blog)\nManu Joseph\n[![link to linkedin](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/)\nData Scientist\nJim Dowling\n[![link to linkedin](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/in/jim-dowling-206a98/)\nCEO and Co-Founder\n**Let's keep in touch!**\n**Subscribe to our newsletter and receive the latest product updates, upcoming events, and industry news.**\n**More Blogs**\n[ Migrating from AWS to a European Cloud - How We Cut Costs by 62%](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)\n[The 10 Fallacies of MLOps](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)\n[Hopsworks AI Lakehouse: The Power of Integrated MLOps Components](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)\n[Unlocking the Power of AI in Government](https://www.hopsworks.ai/post/the-power-of-ai-in-government)\n[Optimizing AI Costs](https://www.hopsworks.ai/post/optimizing-ai-costs)\nArticle updated on\n# The Taxonomy for Data Transformations in AI Systems\nIf you plan to reuse features, you need to understand the Taxonomy\n[![link to github](https://cdn.prod.website-files.com/5e6f7cd3ee7f51d539a4da0b/605b3c459e87eff3298d0e25_github%20\\(1\\).svg)](https://github.com/logicalclocks/hopsworks)\n[![Share on Twitter](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61b0ae07358bb3d1224410c1_Twitter%20icon.svg)](https://twitter.com/share?url=https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)\n[![share on linkedin](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/shareArticle?mini=true&url=https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems&title=The Taxonomy for Data Transformations in AI Systems - Hopsworks)\nJuly 1, 2024\n30 min\nRead\nManu Joseph\n[Manu Joseph](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/)[![link to linkedin](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/)\nData Scientist\nHopsworks\nJim Dowling\n[Jim Dowling](https://www.linkedin.com/in/jim-dowling-206a98/)[![link to linkedin](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/in/jim-dowling-206a98/)\nCEO and Co-Founder\nHopsworks\n[MLOps](https://www.hopsworks.ai/blog-categories/mlops)\n[Data Engineering](https://www.hopsworks.ai/blog-categories/data-engineering)\n[SIGMOD 2024](https://www.hopsworks.ai/blog-categories/sigmod-2024)\n## TL;DR\nThis article introduces a taxonomy for data transformations in AI applications that is fundamental for any AI system that wants to reuse feature data in more than one model. The taxonomy consists of model-independent data transformations that produce reusable features (for example, total customer spend in the last week), model-dependent transformations that produce features that are specific to one model (for example, normalizing that customer spend value using the mean and standard deviation of all customer spends in the model\u2019s training dataset), and on-demand transformations, found in real-time AI systems, that require data only available at request-time (for example, transforming longitude/latitude to a zipcode). The data transformation taxonomy shows you what data transformations to apply in which AI pipelines, and potential sources of offline-online skew. We finish the article comparing how existing feature stores fare in addressing the taxonomy, and the event-sourcing pattern requires full support for the taxonomy. \n[Introduction](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/#introduction)  \n---  \n[1 - Data Transformation Taxonomy for AI](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/#data-transformation)  \n[a. Model-Dependent Transformations](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/#dependent-transformations)  \n[b. Model-Independent Transformations](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/#independent-transformations)  \n[c. On-Demand Transformations](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/#ondemand-transformations)  \n[i. On-Demand Transformations without a Feature Store](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/#with-feature)  \n[ii. On-Demand Transformations as Functions, not Microservices](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/#as-functions)  \n[2 - Existing Feature Store support for the Data Transformation Taxonomy](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/#existing-support)  \n[a. Preprocessing Pipelines and Model-Dependent Transformations](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/#pre-processed)  \n[3 - Hopsworks and the Data Transformation Taxonomy](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/#hops-taxonomy)  \n[4 - Summary](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/#summary)  \nThis article is part 2 in a 7 part series describing in lay terms concepts and results from a [SIGMOD 2024 research paper on the Hopsworks Feature Store](https://www.hopsworks.ai/research-papers/the-hopsworks-feature-store-for-machine-learning). \nOther Parts: 1 ([Modularity and Composability for AI Systems](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)), 3 ([Use all features: Snowflake Schema)](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store), 4 [(Lakehouse for AI)](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai), 5 ([From Lakehouse to AI Lakehouse](https://www.hopsworks.ai/post/from-lakehouse-to-ai-lakehouse-with-a-python-native-query-engine)), 6 [(Real-Time AI Database)](https://www.hopsworks.ai/post/rondb-a-real-time-database-for-real-time-ai-systems), 7 ([Reproducible Data](http://www.hopsworks.ai/post/reproducible-data-for-the-ai-lakehouse)). \n## Introduction\nMany [machine learning](https://www.hopsworks.ai/dictionary/ml) models require \"encoded feature data\". That is, a categorical variable transformed into a numerical representation or a numerical variable transformed or scaled to improve model performance. Many data scientists ask an obvious question - should I store encoded [feature data](https://www.hopsworks.ai/dictionary/feature-data) in a feature store?\nMost existing feature stores do not answer this question in their documentation. ChatGPT confidently gives you the wrong answer. It says you should store encoded feature data in a feature store, hallucinating a number of imagined benefits. One of the few answers you will find on Google is [an answer to this Stack Overflow question](https://stackoverflow.com/questions/75079929/storing-encoded-features-in-the-feature-store):\n> \u201cA consultant from one of the big cloud providers opted for storing encoded [features](https://www.hopsworks.ai/dictionary/feature) in the store which I find strange because different models will use the same type of feature but will need different encoding procedures and storing encoded value complicates [model monitoring](https://www.hopsworks.ai/dictionary/ml) and debugging in production.\u201d\nI answered that question as I have been [teaching about model-independent vs model-dependent transformations since 2022](https://www.youtube.com/watch?v=BD1UOJs1Bvo&t=2426s), and given the continued confusion on this topic, I felt compelled to write something more accessible than our research paper linked above.\nData reuse is fundamental to operational, analytical and AI data processing systems. Operational databases organize their data in normalized (2nd/3rd normal form) data models, removing redundant data and reusing tables in views. In [data warehousing](https://www.hopsworks.ai/dictionary/data-warehouse), the medallion architecture organizes data into bronze, silver, and gold tables, where silver tables reuse bronze tables, gold tables reuse silver tables, and dashboards and AI systems reuse gold tables. Data reuse is less well understood for AI systems, but is still practiced by all hyperscalers. For example, [Meta reported](https://dl.acm.org/doi/10.1145/3534678.3539059) that \u201cmost features are used by many models\u201d. They specifically said that the most popular 100 features are reused in over 100 different models. They claim [feature reuse](https://www.hopsworks.ai/dictionary/feature-reuse) results in higher quality features through increased usage, reduced storage costs, reduced feature development and operational costs. Like other Enterprises, Meta reuses feature data using [a feature store](https://www.hopsworks.ai/dictionary/feature-store).\nAlthough Meta showed the benefit of feature stores in enabling features to be computed once and then reused in many different models, it is important to note that not all necessary [data transformations](https://www.hopsworks.ai/dictionary/data-transformation) in an AI system can happen before the feature store. If you store an encoded feature in the feature store, it will prevent a model from reusing that feature if it wants to encode the feature differently. For example, gradient-descent models often work better when numerical features have been normalized, but decision trees do not require normalized numerical features. But, if you do not encode feature data before the feature store, this means that not all data transformations can be performed in your [feature pipelines](https://www.hopsworks.ai/dictionary/feature-pipeline) (the [data pipelines](https://www.hopsworks.ai/dictionary/data-pipelines) that transform raw data to features before they are stored in the feature store). Where should you perform the model-specific data transformations for [feature encoding](https://www.hopsworks.ai/dictionary/encoding-for-features)? The answer is that they should be performed consistently in training/inference pipelines - without introducing training/inference skew. As we will see later with Hopsworks, a feature store can add support for model-specific data transformations to ensure there is no skew between the training and inference pipelines.\nThe data transformations familiar to data engineers, such as data cleaning/[filtering](https://www.hopsworks.ai/dictionary/filtering)/aggregations/binning, produce reusable feature data that can be stored in a feature store and used by many models. This feature data is sample-independent - there is no relationship between the feature data and any training datasets created from it.\nIn contrast, many of the data transformations performed by data scientists transform input data into a numerical format to improve [model performance](http://www.hopsworks.ai/dictionary/model-performance). These [model-dependent transformations](https://www.hopsworks.ai/dictionary/model-dependent-transformations) are typically sample dependent, as they are parameterized by the model\u2019s training dataset. That is, the output of these model-dependent transformations is [feature data](https://www.hopsworks.ai/dictionary/feature-data) that is specific to one model - it cannot be reused by other models. Another class of data transformation for AI systems are on-demand transformations that need request-time parameters to be computed. On-demand transformations cannot be precomputed for online inference as they require request-time parameters and functions to compute them can be combined with precomputed data from the feature store.\n[ ![image \\(15\\)](https://no-cache.hubspot.com/cta/default/5524414/interactive-176024224097.png) ](https://cta-service-cms2.hubspot.com/web-interactives/public/v1/track/redirect?encryptedPayload=AVxigLL8aIV1Q%2B5ZSUfYLKLxVSlUlPku38D2BJ0CuYkCy8h9lgh0Gpu2ooFCOgz6zRefuhkCC2mLJaJwbuiEs%2FTQ9njgFDF%2BXLvsMd8DRQ2UekbP5wohn10%2FrMWjt8DWViwIwazinBqy%2BcAZ0lv%2BfqgKTFoSbOJh4rMKpeEhYyeL%2Bjb%2BuZMeqcWmPtJIy0raFSsndm1lSEE%3D&webInteractiveContentId=176024224097&portalId=5524414)\n## Data Transformation Taxonomy for AI\nData transformations in AI systems are not equivalent, when it comes to feature reuse, as shown in figure 1. Different types of data transformations produce features that can be classified according to a taxonomy.\n[![Data transformations in AI systems are not equivalent, when it comes to feature reuse](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b21b_6682a338ded3a8e40a72b19b_1_data%2520transformation%2520taxonomy_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b21b_6682a338ded3a8e40a72b19b_1_data%2520transformation%2520taxonomy_lightbox.png)\n**_Figure 1:_**_When you want to apply a_[ _data transformation_](https://www.hopsworks.ai/dictionary/transformation) _in an AI system, you need to consider the type of feature created, according to this taxonomy. Reusable features are stored in a feature store, but model-specific features are typically not (they are applied consistently in training and inference pipelines). Real-time AI systems may have on-demand features._\nThe data [transformation](https://www.hopsworks.ai/dictionary/transformation) taxonomy for AI is based on the following observations:\n  * There are **data transformations** that produce **feature data that is reusable across many models**. These transformations produce feature data that is _model independent_. \n  * There are **data transformations** that produce feature data **specific to one model** (and not usable by other models). These can be data transformations that are parameterized a training dataset (a training dataset is typically specific to a single model) or a transformation specific to a model (such as encoding text for a [LLM](https://www.hopsworks.ai/dictionary/llms-large-language-models) using its encoder). \n  * There are **data transformations** that create features for inference in real-time AI systems that **have input parameter(s) only available at request time**. On-demand transformations can also be applied on historical data to produce model-independent feature data for training models.\n\n\n[![Data Transformation comparison](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1df_6682a394decfc5b65e08b199_Table%25201_transformation%2520comparison_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1df_6682a394decfc5b65e08b199_Table%25201_transformation%2520comparison_lightbox.png)\n**_Table 1:_**_A comparison of the different types of Data Transformation based on whether they produce reusable feature data or not, whether they can be precomputed for predictions by AI applications or not, and whether they need request-time data for online inference._\nA taxonomy is a (often hierarchical) classification, where things are organized into groups or types. From the above observations, we can see that data transformations can be classified as model-independent, model-dependent, or on-demand, see figure 2.\n[![Transformations grouped in hierarchical classification](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1fc_6682a382967f6efdcc6da5e9_2_3%2520groups%2520of%2520data%2520transformations_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1fc_6682a382967f6efdcc6da5e9_2_3%2520groups%2520of%2520data%2520transformations_lightbox.png)\n**_Figure 2:_**_Data transformations in AI systems can be classified into one of three groups: model-independent transformations that are performed on raw data to create reusable feature data. Model-dependent transformations are performed on the_[ _training data_](https://www.hopsworks.ai/dictionary/training-data) _for a model as well as on feature data at inference time. On-demand transformations require data that is only available at request time and can either output reusable feature data (model-independent) or be chained with model-dependent transformations to produce model-specific feature data._\nThis data transformation taxonomy for AI systems generalizes to any AI system architecture that has separate training and inference programs - which is pretty much all existing AI systems - and reuses features, which is all AI systems that use a feature store. \nFigure 3 includes our three different types of data transformations and the AI artifacts created by them. The data that arrives over time in the diagram is typically stored in a feature store, but could be any AI system that manages newly arriving data, where the data is used both for [model training](http://www.hopsworks.ai/dictionary/model-training) and inference.\n[![three different types of data transformations and the AI artifacts created by them](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1f0_6682a5285f675e07f640d48b_3_3%2520types%2520of%2520transformations_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1f0_6682a5285f675e07f640d48b_3_3%2520types%2520of%2520transformations_lightbox.png)\n**_Figure 3:_**_Model-independent transformations are performed on new input data and create reusable feature data for training and inference. Model-dependent transformations are performed in training and inference on feature data and need to be consistent across both training/inference to prevent skew. On-Demand transformations are performed on request data (possibly combined with precomputed feature data) and the output can have further model-dependent transformations applied to it. On-demand transformations can also be performed on historical data (backfilling feature data), creating reusable feature data._\nWe note here that [model-independent transformations](https://www.hopsworks.ai/dictionary/model-independent-transformations) are only found at one place in the diagram, where new and historical data arrives. In contrast, model-dependent transformations and on-demand transformations can be found in two places. Model-dependent transformations need to be applied when creating training data, in training pipelines, but also in inference pipelines, before making predictions. These training and inference pipelines are typically separate programs, which introduces the possibility of skew between the implementations (see [online-offline skew](https://www.hopsworks.ai/dictionary/skew)). Similarly, on-demand transformations are found in [online inference pipelines](https://www.hopsworks.ai/dictionary/online-inference-pipeline), but may also be applied to backfill feature data from historical data in a feature pipeline. Again, online inference programs and feature pipelines are typically separate programs, meaning there is potential for skew between the two on-demand transformations (for example, if there are two different implementations of the on-demand transformation). Online-offline skew is a general challenge for AI systems, and the data transformation taxonomy helps provide an understanding of its cause and alerts you where it can occur, so you can prevent it occurring. In general, the taxonomy also applies to any AI system that aims to create [reusable feature data](https://www.hopsworks.ai/dictionary/feature-reuse), where solutions to online-offline skew will always need to be provided.\n### Model-Dependent Transformations\nModel-dependent transformations produce feature data that is specific to one model. When data scientists talk about transformations, they typically refer to model-dependent transformations, such as transforming a categorical variable into a numerical format or scaling/normalizing/standardizing a numerical variable to improve the performance of your gradient-based ML model. Feature encoding is the most common example of model-dependent transformations in AI systems. Feature encoding transformations are parameterized by the model\u2019s training dataset (they are sample dependent), and as most models typically have their own training dataset, the output data from feature encoding is specific to only that one model. Similarly, transformations that impute missing data are typically parameterized by the training dataset, and are, therefore, model-dependent transformations.\nA well known example of a model-dependent transformation for LLMs, is text tokenization. Each LLM typically has their own tokenizer, and, therefore, tokenization is a model-dependent transformation. For example, if you were to tokenize text when creating [instruction datasets for LLM fine-tuning](https://www.hopsworks.ai/dictionary/instruction-datasets-for-fine-tuning-llms), the tokenized text would not be reusable for [fine-tuning a different LLM.](http://www.hopsworks.ai/dictionary/fine-tuning-llms) This means you should not tokenize text in feature pipelines, where model-independent transformations are performed. Instead, your feature pipeline should create clear-text instruction datasets for fine-tuning, and tokenization should be performed in both the training and inference pipelines, where model-dependent transformations are performed, see figure 4.\n[![Model-dependent transformations are applied in both training and inference pipelines ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1e9_6682a59661617e47e531a5ff_4_on-demand%2520transformations_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1e9_6682a59661617e47e531a5ff_4_on-demand%2520transformations_lightbox.png)\n**_Figure 4_** _: Model-dependent transformations are applied in both training and inference pipelines and the transformations need to be consistent to prevent skew. For example, normalizing a numerical feature should use the same values for the mean and standard deviation in both training and inference pipelines._\n### Model-Independent Transformations\nModel-independent transformations produce features that can be reused in many models. They cover the types of transformations that data engineers are very familiar with that are widely used in [feature engineering](https://www.hopsworks.ai/dictionary/feature-engineering), such as:\n  * grouped aggregations (such as the avg/max/min of some numerical variable),\n  * windowed counts (for example, number of clicks per day), \n  * transformations that create RFM (recency, frequency, monetary) features,\n  * binning _to create categorical variables that_ are easier for models to identify signals in sparse data,\n  * stream processing windowed features that are computed in sliding/tumbling/session windows in frameworks like Flink/Beam/Spark-Streaming,\n  * joining data from different sources together using a common join key,\n  * time series transformations that aggregate/extract data over time to produce features that identify trends, patterns, or anomalies.\n\n\nModel-independent transformations are applied once in batch or [streaming feature pipelines](https://www.hopsworks.ai/dictionary/streaming-feature-pipeline), see figure 5. \n[![Model-independent transformations are applied once in batch or streaming feature pipelines](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1ec_6683b87de8bcaaf0a96dc544_v2_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1ec_6683b87de8bcaaf0a96dc544_v2_lightbox.png)\n** _Figure 5:_**_Model-independent transformations are performed in feature pipelines and produce reusable feature data stored in a feature store._\nThe reusable feature data output by feature pipelines is stored in a feature store, to be later used by [downstream](https://www.hopsworks.ai/dictionary/downstream) training and inference pipelines.\n### On-Demand Transformations\nOn-demand transformations are found in real-time AI systems, see figure 6. In an online inference pipeline, an on-demand transformation requires input data that is only available at prediction request time. If an on-demand transformation is implemented as a function, that function has at least one parameter that originates from the prediction request. The function may also include (precomputed feature) parameters that originate from a feature store. \n[![On-demand transformations are found in real-time AI systems](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1f3_6682a64bf17f7395ff901934_6_real%2520time%2520ai%2520systems_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1f3_6682a64bf17f7395ff901934_6_real%2520time%2520ai%2520systems_lightbox.png)\n** _Figure 6:_**_On-demand transformations are found in online inference pipelines, where they compute features using prediction request data, and they can be combined with \u201cother\u201d precomputed features. On-demand transformations can also be used to compute reusable feature data from backfill data sources._\nWhen historical data used to compute [on-demand features](https://www.hopsworks.ai/dictionary/on-demand-features) is available, you can use the on-demand [feature function](https://www.hopsworks.ai/dictionary/feature-function) to [backfill feature data](https://www.hopsworks.ai/dictionary/backfill-training-data) to a feature store. For example, in figure 7, you can see an event-sourcing pattern, where event data in Kafka is stored in a S3 data lake. You can use that historical data to create on-demand features by applying the same [on-demand transformation](https://www.hopsworks.ai/dictionary/on-demand-transformation) function on the event data in a feature pipeline. \n[![An event-sourcing pattern, where event data in Kafka is stored in a S3 data lake](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1e3_6682a6838dd9a33c10e159c3_7_event%2520sourcing%2520pattern_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1e3_6682a6838dd9a33c10e159c3_7_event%2520sourcing%2520pattern_lightbox.png)\n** _Figure 7:_**_On-demand transformations can be used in batch feature pipelines to backfill event-sourced feature data from data lakes, like S3._\nThe features created can then be used as [training data](https://www.hopsworks.ai/dictionary/training-data) for models. The feature data created should be model-independent as the transformation should not have model-specific output. If the transformation is model-specific, it should be performed in a model-dependent transformation that can be performed consistently after reading the feature data from the feature store (for historical data) and after any on-demand transformations functions in an online inference pipeline. Note that the model-independent requirement for on-demand transformations means just that the transformation should create sample-independent feature data - it is not parameterized by the training dataset or model itself. An on-demand feature may ultimately only be used by one model, but if its transformations are model-independent, it does not need to be performed in a model-dependent transformation. The benefit of computing on-demand feature data in a feature pipeline over model-dependent transformations in a [training pipeline](https://www.hopsworks.ai/dictionary/training-pipeline) is that you only need to compute them once, not every time you create a training dataset. \nSo, you should factor out those parts of the online transformation that are model-specific (such as feature encoding) from those that are model-independent. The parts that are model-independent go in the on-demand transformation logic, while model-specific transformations still happen in model-dependent transformations. In online inference, the output of an on-demand transformation function is passed through model-dependent transformations before it is received by the model.\n[ ![The AI Lakehouse](https://no-cache.hubspot.com/cta/default/5524414/interactive-177758961194.png) ](https://cta-service-cms2.hubspot.com/web-interactives/public/v1/track/redirect?encryptedPayload=AVxigLKqezz9RftCB8ctXyqqlNDjMtJOTRyq%2Fp99txGDgWWoYNFsozaC3wZyD4Oo%2B5cjVs9eqtTCAi%2FnFOL4iBrm3V%2BJEmBKxoIKlHSZ%2B%2FImF1ZMW615qSSuiagfvpQHMmN22urTwQreKeJCnmc7aA5vmtWs37EceSIAudMZH3l%2FwlbReFkxxwl5urF4MsgbQwpOoLY%3D&webInteractiveContentId=177758961194&portalId=5524414)\n#### On-Demand Transformations without a Feature Store\nIn Figure 8, you can see an architecture diagram from an unnamed payments company that did not include a feature store to store the output of their on-demand transformation functions on historical data. The result is massively decreased productivity for data scientists, as any changes to training datasets require many hours of massive PySpark jobs to recompute the training data. A feature store that supports model-independent on-demand transformation functions (like Hopsworks) would have enabled them to compute the feature data incrementally as new events arrive, and create new training datasets in seconds (for EDA) or minutes (for full training). Feature stores that treat on-demand transformation functions as model-dependent transformations (Databricks, Tecton) have the same problem as the payments company below - they have to recompute all on-demand features every time they want to create new training data with them.\n[![Architectural diagram without a feature store to store the output of their on-demand transformation functions on historical data. ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b203_6682a73911329cf4cf20a985_8_architecture%2520without%2520feature%2520store_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b203_6682a73911329cf4cf20a985_8_architecture%2520without%2520feature%2520store_lightbox.png)\n**_Figure 8:_**_In this event-sourcing pattern, event data from Kafka is stored to S3 as historical data used to create training data. The on-demand transformation functions in Python in the ingestion, projection,and features packages are applied to TBs of event data using two PySpark jobs that take many hours to create training data. This makes iterating on training data very slow for data scientists - every time you want to change something in the training data, you have to recompute all the training data. If, instead, they stored the features in a feature store, Data Scientists could iterate faster on data and improve their time-to-value._\n#### On-Demand Transformations as Functions, not Microservices\nOn-demand transformations are typically implemented as functions in online inference pipelines. A previous generation of feature stores implemented on-demand transformations as microservices (or serverless functions) that are invoked from online inference pipelines. The problem with this approach, however, is that for every new on-demand feature you add to your online model, you get large increases in latency due to straggler effects and the tail-at-scale from network calls. They are also massively inefficient in batch pipelines for backfilling with historical data. On-demand features as functions, in contrast, use the same programming language as the model was trained in (typically Python). In Python-based feature pipelines and online inference pipelines, you can implement the on-demand transformations as Python UDFs (although they do not scale for backfilling) or [Pandas UDFs](https://www.hopsworks.ai/dictionary/pandas-udf) (that scale for backfilling, but add some latency to online inference).\n## Existing Feature Store support for the Data Transformation Taxonomy\nThe data transformation taxonomy for AI systems is most relevant for feature stores that all support the computation of features once, and their subsequent reuse in many models. Table 2 summarizes the current support of well-known feature stores for the taxonomy.\n[![Summary of the current support of well-known feature stores for the taxonomy.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1f9_6682a7c1b7dbdd45e3c42fea_table%25202_support%2520for%2520on-demand%2520features_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b1f9_6682a7c1b7dbdd45e3c42fea_table%25202_support%2520for%2520on-demand%2520features_lightbox.png)\n**_Table 2:_**_Support for the Data Transformation Taxonomy in leading Feature Stores. Only three feature stores support on-demand features. Only one feature supports the full taxonomy._\nFirstly, Hopsworks is the only feature store to support the full taxonomy, with open APIs for feature engineering, enabling support for a wide range of compute engines for model-independent transformations. Model-dependent transformations and on-demand transformations are supported as Pandas or Python user-defined functions (UDFs). \nIn Databricks, model-independent transformations are supported in PySpark and SQL, while Tecton provides a domain-specific language (DSL) for defining model-independent transformations that are transpiled into Spark, SQL, or Rift (their DuckDB engine) for execution. Both Databricks and Tecton support computing features using request-time parameters with Python UDFs. Tecton also supports Pandas UDFs.. These on-demand transformations are model-dependent transformations applied to request-time (and historical) parameters. Neither feature store supports model-independent on-demand transformations in feature pipelines to precompute feature data for the feature store. Model-dependent transformations are not explicitly supported, except as on-demand transformations. Neither of these feature stores has support for declarative model-dependent transformations for encoding a categorical/numerical feature (found in Hopsworks). \nThe other feature stores - AWS Sagemaker, GCP Vertex, and Snowflake - only support model-independent transformations, mostly with SQL and some Python support. This makes it challenging to build real-time AI systems that require on-demand transformations.\n### Preprocessing Pipelines and Model-Dependent Transformations\nIt is possible to implement your own model-dependent transformations by tightly coupling them with your model, for example, using Scikit-Learn pipelines ([as shown here in Hopsworks lending club tutorial](https://github.com/logicalclocks/hopsworks-tutorials/tree/master/loan_approval)). A Scikit-Learn pipeline can be designed to contain a series of model-dependent transformation steps before either model.fit() or model.predict() is invoked as part of the pipeline. If you save and package your Scikit-Learn pipeline along with your versioned model in your [model registry](https://www.hopsworks.ai/dictionary/model-registry) ([see example training pipeline](https://github.com/logicalclocks/hopsworks-tutorials/blob/master/loan_approval/2-loan-approval-training-pipeline.ipynb)), then in your [inference pipeline](https://github.com/logicalclocks/hopsworks-tutorials/blob/master/loan_approval/3-loan-approval-batch-inference.ipynb) when you download the versioned model, you get exactly the same pipeline object (and model-dependent transformations). Similarly, TensorFlow enables you to perform a limited number of model-dependent transformations in pre-processing layers for the model. In PyTorch, the [transforms API](https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html) is often used for model-dependent transformations.\nOne disadvantage of preprocessing pipelines is that they consume CPU cycles and may cause under utilization of GPUs in model training or inference. If your model-dependent transformations are too CPU-intensive, it may not be possible to run your GPUs at maximum capacity. An alternative is to introduce a training dataset creation pipeline that creates training data as files, after applying model-dependent transformation functions. This is possible in Hopsworks with [feature views](https://www.hopsworks.ai/dictionary/feature-view).\n## Hopsworks and the Data Transformation Taxonomy\nIn Hopsworks, an AI system is typically decomposed into different AI pipelines that fall into one of the following categories:\n  * feature pipelines that perform model-independent transformations (and on-demand transformations) to transform raw data into features and labels/observations,\n  * training pipelines that can perform model-dependent transformations on features (and labels) before they are used to train one or more ML models,\n  * Inference pipelines that can perform on-demand transformations using request-time parameters, read [precomputed features](http://www.hopsworks.ai/dictionary/precomputed-features) from the feature store and perform model-dependent transformations on them, and then output predictions using the encoded feature data and trained model.\n\n\nEach of these AI pipelines has a different role in transforming data so that it gets progressively closer to the input data used for model training and inference. Another sub-pipeline of the training pipeline is the training dataset pipeline that reads features (and labels) from the feature store, applies model-dependent transformations and outputs training data as files. Training dataset pipelines are important when training data is large, and needs to be streamed into workers, and when model-dependent transformations are expensive and can be applied before the model training pipeline.\nHopsworks stores feature data in [feature groups](https://www.hopsworks.ai/dictionary/feature-groups) (tables containing feature data in an [offline store](https://www.hopsworks.ai/dictionary/offline-store) and an [online store](https://www.hopsworks.ai/dictionary/online-store)). Feature data is read for both training and inference, however, using a [feature view](https://www.hopsworks.ai/dictionary/feature-view), not using feature groups directly. A feature view is a meta-data only selection of features (from potentially different feature groups) that include the input and output [schema](https://www.hopsworks.ai/dictionary/schema) for a model. That is, the feature view describes the input features, the output target(s) as well as any helper columns used for training or inference. But a feature view is not only a schema, it can also include model-dependent transformations. Transformations are included in feature views as their primary goal is to prevent skew between training and inference pipelines - whether that is the model schema or model-dependent transformations. \nFeature views are used to create consistent snapshots of data for both training and inference. The feature view also computes and saves statistics for the training datasets it creates, and these statistics are made available as parameters for many model-dependent transformations (like categorical encoding, normalization, scaling) in both training and inference, see Figure 9. \n[![Feature views are used to create consistent snapshots of data for both training and inference. ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b20e_6682a8f6264c5acc4e566595_9_full%2520overview%2520transformations_lightbox.png)](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b08da3ff1e129fac59b20e_6682a8f6264c5acc4e566595_9_full%2520overview%2520transformations_lightbox.png)\n**_Figure 9:_**_In Hopsworks, feature pipelines execute model-independent transformations, and can execute on-demand transformations on backfill data. Training pipelines and inference pipelines execute model-dependent transformations, and they must be consistent, to avoid training-serving skew. Online inference pipelines execute on-demand transformations, and they should be consistent with on-demand transformations in feature pipelines. On-demand and model-dependent transformations can be registered with feature groups and feature views in the feature store, respectively, to ensure the same transformations are performed in both AI pipelines._\nIn Figure 9, we can also see that on-demand transformations may need to be kept consistent between online inference pipelines and feature pipelines. We use on-demand transformations in feature pipelines to [backfill feature](https://www.hopsworks.ai/dictionary/backfill-features) data for training using historical data. In Hopsworks, you can register those on-demand features with [feature groups](https://www.hopsworks.ai/dictionary/feature-groups) and, through lineage, use exactly the same on-demand transformation in online inference.\nYou can define model-dependent transformations and on-demand transformations in Hopsworks as either Pandas or Python user-defined functions (UDFs). [Pandas UDFs](https://www.hopsworks.ai/dictionary/pandas-udf) enable the vectorized execution of the transformation functions, providing orders of magnitude higher throughput compared to [Python UDFs](https://www.hopsworks.ai/dictionary/python-udf). Pandas UDFs can also be scaled out in workers in a Spark program, enabling scaling from GBs to TBs or more. Here is an example of declarative model-dependent transformation function - one-hot encoding is applied to the _city_ feature in the _my_fv_ feature view:\n```\n#### Training Pipeline ########################\none_hot_encoder = fs.get_transformation_function(name=\"one_hot_encoder\",\n                         version=1)\nfv = fs.create_feature_view(name=\"my_fv\",\n         version=1,\n         query=fg.select(\"id\", \"amount\", \"age\", \"city\"),\n         transformation_functions=[one_hot_encoder(\"city\")],\n         labels=[\"amount\"])\n# one_hot_encoder is applied creating training data \nX_train, X_test, y_train, y_test = fv.train_test_split(test_size=0.2)\n#### Batch Inference Pipeline #################\n# one_hot_encoder is applied on-reading batch inference data \ndf = fv.get_batch_data(start_time=\"2024-06-28 00:00\")\n#### Online Inference Pipeline ############\n\n# one_hot_encoder is applied on-reading online inference data \nrow = fv.get_feature_vector(entity={'id':1234})\n\n```\n\nFor on-demand features, you implement the same Pandas/Python UDF, one for each on-demand feature:\n```\n# Define on-demand transformation function\n@hopsworks.udf(return_type=float)\ndef on_demand_feature(feature, request_parameter):\n   import packages\n   ....\n   return df\n...\n##### Feature Pipeline ########################\nfg = fs.create_feature_group(name=\"my_fg\", \n            version=1,\n            primary_key=[\"id\"],\n            event_time=\"evt_ts\",\n            transformation_functions=[on_demand_feature],\n            online_enabled = True)\n# `on_demand_feature` is computed with columns in `df` before insertion \nfg.insert(df)\n\n##### Online Inference Pipeline ###############\ndef prediction_request(id, request_parameter):\n   # Get feature vector without any transformations\n   df = fv.get_feature_vector(entity={\"id\":id}, transform=False, return_type=\"pandas\")\n   # Apply on-demand transformation function \n   df['on_demand_feature'] = fv.on_demand_transformations[\"on_demand_feature\"](df['feature'], request_parameter)\n   # Apply model-dependent transformation functions\n   df_encoded = fv.transform(df\n   df['prediction'] = model.predict(df_encoded)\n   fv.log(df)\n   return df['prediction']\n\n```\n\n## Summary\nIf you intend to reuse feature data in many models, you need to first understand the data transformation taxonomy, so that you know what data transformations to put in your different AI pipelines. Feature stores can help by providing infrastructure support for the data transformation taxonomy, but currently only Hopsworks Feature Store supports the complete taxonomy.\n## References\n![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/663b8c6a964c76be96237444_written%20by%20human_hops.png)![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/663b8ae5a0a33abc94baf561_written%20by%20ai_hops.png)\n### Interested for more?\n  * \ud83e\udd16 Register for free on [Hopsworks Serverless](https://app.hopsworks.ai/app?utm_source=blog&utm_medium=list&utm_id=backlink)\n  * \ud83c\udf10 Read about the open, disaggregated [AI Lakehouse stack](https://www.hopsworks.ai/post/the-ai-lakehouse)\n  * \ud83d\udcda Get your early copy: O'Reilly's ['**Building Machine Learning Systems'**](https://www.hopsworks.ai/lp/oreilly-book-building-ml-systems-with-a-feature-store)book\n  * \ud83d\udee0\ufe0f Explore all [Hopsworks Integrations](https://www.hopsworks.ai/integrations)\n  * \ud83e\udde9 Get started with [codes and examples](https://www.hopsworks.ai/hopsworks-examples)\n  * \u2696\ufe0f [Compare other Feature Stores](https://www.hopsworks.ai/product-comparison/sagemaker) with Hopsworks\n\n\n### More blogs\n[![Hopsworks new feature service, ArrowFlight with DuckDB, introduces significant performance improvements for Python-clients working with feature data.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65113a725a93dc40f6d74c85_duckdb_thumbnail.png)](https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb)[Feature StoreAugust 9, 202313 minReadFaster reading from the Lakehouse to Python with DuckDB/ArrowFlightIn this article, we outline how we leveraged ArrowFlight with DuckDB to build a new service that massively improves the performance of Python clients reading from lakehouse data in the Feature Store](https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb)\nTill D\u00f6hmen\n[![ML and AI applications are becoming increasingly demanding in terms of performance, we compare Redis to RonDB in terms of Scalability, Throughput and H.A.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61eace23ecf46e36f195d5d1_REDIS.png)](https://www.hopsworks.ai/post/ai-ml-needs-a-key-value-store-and-redis-is-not-up-to-it)[BenchmarkFebruary 26, 202112 minReadAI/ML needs a Key-Value store, and Redis is not up to itSeeing how Redis is a popular open-source feature store with features significantly similar to RonDB, we compared the innards of RonDB\u2019s multithreading architecture to the commercial Redis products. ](https://www.hopsworks.ai/post/ai-ml-needs-a-key-value-store-and-redis-is-not-up-to-it)\nMikael Ronstr\u00f6m\n[![This article explores the limitations of existing feature stores that only support Star Schema data models and introduces the Snowflake Schema data model.](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66869f1f4f75a7d91c67775f_PR%20Blog%203_snowflake%20schema.png)](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store)[Feature StoreJuly 8, 202414 minReadThe Journey from Star Schema to Snowflake Schema in the Feature StoreIn this article we introduce the snowflake schema data model for feature stores, and show how it helps you include more features to make better predictions ](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store)\nDavit Bzhalava\n[![Untitled UI logotext](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/630e3413d3fafa0f79c52da2_hopsworks-logo%202022_white.svg)![Logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/666c3cc1cfc4741e6b2d9fe6_untitled-ui-logo.png)The AI Lakehouse](https://www.hopsworks.ai/)\nProduct\n[Hopsworks Enterprise](https://www.hopsworks.ai/try)[Capabilities](https://www.hopsworks.ai/product-capabilities)[Integrations](https://www.hopsworks.ai/integrations)[Examples](https://www.hopsworks.ai/hopsworks-examples)[Pricing](https://www.hopsworks.ai/pricing)[App Status](https://hopsworks.statuspage.io/)[FAQ](https://www.hopsworks.ai/frequently-asked-questions)\nSolutions\n[Industry & Team Solutions](https://www.hopsworks.ai/solutions/all)[Generative AI](https://www.hopsworks.ai/use-case/fine-tuning-llms-rag-for-genai)[Real-time Fraud Detection ](https://www.hopsworks.ai/use-case/realtime-fraud-detection)[Hopsworks Medical Copilot](https://www.hopscopilot.com/)[Customers](https://www.hopsworks.ai/customers)\nResources\n[Blog](https://www.hopsworks.ai/blog)[MLOps Dictionary](https://www.hopsworks.ai/mlops-dictionary)[Events](https://www.hopsworks.ai/events)[Documentation](https://docs.hopsworks.ai/latest/)[Academy](https://www.hopsworks.ai/academy)[Research Papers](https://www.hopsworks.ai/research-papers)[Feature Store Comparison](https://www.hopsworks.ai/product-comparison/sagemaker)[Community](https://community.hopsworks.ai/)[FAQ: EU AI Act](https://www.hopsworks.ai/faq-eu-ai-act)\nCompany\n[About Us](https://www.hopsworks.ai/about-us)[News](https://www.hopsworks.ai/news)[Security & Compliance](https://www.hopsworks.ai/security-compliance)[Contact Us](https://www.hopsworks.ai/contact/main)\nJoin our newsletter\n**Receive the latest product updates, upcoming events, and industry news.**\n\u00a9 Hopsworks 2025. All rights reserved. Various trademarks held by their respective owners.\n[](https://join.slack.com/t/public-hopsworks/shared_invite/zt-1uf21vitz-rhHKNdIf8GEiOf1EJ6Wzsw)[](https://github.com/logicalclocks/hopsworks)[](https://www.linkedin.com/company/hopsworks/)[](https://twitter.com/hopsworks)[](https://www.youtube.com/@hopsworks)\n[Privacy Policy](https://www.iubenda.com/privacy-policy/90800199 \"Privacy Policy \")\n[Cookie Policy](https://www.iubenda.com/privacy-policy/90800199/cookie-policy \"Cookie Policy \")\n[Terms and Conditions](https://www.iubenda.com/terms-and-conditions/90800199 \"Terms and Conditions \")\n",
    "answer": "```markdown\n## TL;DR\nThe article presents a taxonomy for data transformations in AI systems, categorizing them into model-independent, model-dependent, and on-demand transformations. It emphasizes the importance of understanding these transformations for feature reuse across models and highlights that only Hopsworks supports the complete taxonomy.\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/Netflix/metaflow/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FNetflix%2Fmetaflow%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FNetflix%2Fmetaflow%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=Netflix%2Fmetaflow) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/Netflix/metaflow/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/Netflix/metaflow/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/Netflix/metaflow/) to refresh your session. Dismiss alert\n{{ message }}\n[ Netflix ](https://github.com/Netflix) / **[metaflow](https://github.com/Netflix/metaflow) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2FNetflix%2Fmetaflow) You must be signed in to change notification settings\n  * [ Fork 822 ](https://github.com/login?return_to=%2FNetflix%2Fmetaflow)\n  * [ Star  8.7k ](https://github.com/login?return_to=%2FNetflix%2Fmetaflow)\n\n\nBuild, Manage and Deploy AI/ML Systems \n[metaflow.org](https://metaflow.org \"https://metaflow.org\")\n### License\n[ Apache-2.0 license ](https://github.com/Netflix/metaflow/blob/master/LICENSE)\n[ 8.7k stars ](https://github.com/Netflix/metaflow/stargazers) [ 822 forks ](https://github.com/Netflix/metaflow/forks) [ Branches ](https://github.com/Netflix/metaflow/branches) [ Tags ](https://github.com/Netflix/metaflow/tags) [ Activity ](https://github.com/Netflix/metaflow/activity)\n[ Star  ](https://github.com/login?return_to=%2FNetflix%2Fmetaflow)\n[ Notifications ](https://github.com/login?return_to=%2FNetflix%2Fmetaflow) You must be signed in to change notification settings\n  * [ Code ](https://github.com/Netflix/metaflow)\n  * [ Issues 242 ](https://github.com/Netflix/metaflow/issues)\n  * [ Pull requests 91 ](https://github.com/Netflix/metaflow/pulls)\n  * [ Actions ](https://github.com/Netflix/metaflow/actions)\n  * [ Security ](https://github.com/Netflix/metaflow/security)\n  * [ Insights ](https://github.com/Netflix/metaflow/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/Netflix/metaflow)\n  * [ Issues  ](https://github.com/Netflix/metaflow/issues)\n  * [ Pull requests  ](https://github.com/Netflix/metaflow/pulls)\n  * [ Actions  ](https://github.com/Netflix/metaflow/actions)\n  * [ Security  ](https://github.com/Netflix/metaflow/security)\n  * [ Insights  ](https://github.com/Netflix/metaflow/pulse)\n\n\n# Netflix/metaflow\nmaster\n[Branches](https://github.com/Netflix/metaflow/branches)[Tags](https://github.com/Netflix/metaflow/tags)\n[](https://github.com/Netflix/metaflow/branches)[](https://github.com/Netflix/metaflow/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[1,250 Commits](https://github.com/Netflix/metaflow/commits/master/)[](https://github.com/Netflix/metaflow/commits/master/)  \n[.github/workflows](https://github.com/Netflix/metaflow/tree/master/.github/workflows \"This path skips through empty directories\")| [.github/workflows](https://github.com/Netflix/metaflow/tree/master/.github/workflows \"This path skips through empty directories\")  \n[R](https://github.com/Netflix/metaflow/tree/master/R \"R\")| [R](https://github.com/Netflix/metaflow/tree/master/R \"R\")  \n[devtools](https://github.com/Netflix/metaflow/tree/master/devtools \"devtools\")| [devtools](https://github.com/Netflix/metaflow/tree/master/devtools \"devtools\")  \n[docs](https://github.com/Netflix/metaflow/tree/master/docs \"docs\")| [docs](https://github.com/Netflix/metaflow/tree/master/docs \"docs\")  \n[metaflow](https://github.com/Netflix/metaflow/tree/master/metaflow \"metaflow\")| [metaflow](https://github.com/Netflix/metaflow/tree/master/metaflow \"metaflow\")  \n[stubs](https://github.com/Netflix/metaflow/tree/master/stubs \"stubs\")| [stubs](https://github.com/Netflix/metaflow/tree/master/stubs \"stubs\")  \n[test](https://github.com/Netflix/metaflow/tree/master/test \"test\")| [test](https://github.com/Netflix/metaflow/tree/master/test \"test\")  \n[.gitignore](https://github.com/Netflix/metaflow/blob/master/.gitignore \".gitignore\")| [.gitignore](https://github.com/Netflix/metaflow/blob/master/.gitignore \".gitignore\")  \n[.pre-commit-config.yaml](https://github.com/Netflix/metaflow/blob/master/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/Netflix/metaflow/blob/master/.pre-commit-config.yaml \".pre-commit-config.yaml\")  \n[ADOPTERS.md](https://github.com/Netflix/metaflow/blob/master/ADOPTERS.md \"ADOPTERS.md\")| [ADOPTERS.md](https://github.com/Netflix/metaflow/blob/master/ADOPTERS.md \"ADOPTERS.md\")  \n[CONTRIBUTING.md](https://github.com/Netflix/metaflow/blob/master/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/Netflix/metaflow/blob/master/CONTRIBUTING.md \"CONTRIBUTING.md\")  \n[LICENSE](https://github.com/Netflix/metaflow/blob/master/LICENSE \"LICENSE\")| [LICENSE](https://github.com/Netflix/metaflow/blob/master/LICENSE \"LICENSE\")  \n[MANIFEST.in](https://github.com/Netflix/metaflow/blob/master/MANIFEST.in \"MANIFEST.in\")| [MANIFEST.in](https://github.com/Netflix/metaflow/blob/master/MANIFEST.in \"MANIFEST.in\")  \n[README.md](https://github.com/Netflix/metaflow/blob/master/README.md \"README.md\")| [README.md](https://github.com/Netflix/metaflow/blob/master/README.md \"README.md\")  \n[SECURITY.md](https://github.com/Netflix/metaflow/blob/master/SECURITY.md \"SECURITY.md\")| [SECURITY.md](https://github.com/Netflix/metaflow/blob/master/SECURITY.md \"SECURITY.md\")  \n[metaflow-complete.sh](https://github.com/Netflix/metaflow/blob/master/metaflow-complete.sh \"metaflow-complete.sh\")| [metaflow-complete.sh](https://github.com/Netflix/metaflow/blob/master/metaflow-complete.sh \"metaflow-complete.sh\")  \n[setup.cfg](https://github.com/Netflix/metaflow/blob/master/setup.cfg \"setup.cfg\")| [setup.cfg](https://github.com/Netflix/metaflow/blob/master/setup.cfg \"setup.cfg\")  \n[setup.py](https://github.com/Netflix/metaflow/blob/master/setup.py \"setup.py\")| [setup.py](https://github.com/Netflix/metaflow/blob/master/setup.py \"setup.py\")  \n[test_runner](https://github.com/Netflix/metaflow/blob/master/test_runner \"test_runner\")| [test_runner](https://github.com/Netflix/metaflow/blob/master/test_runner \"test_runner\")  \n[tox.ini](https://github.com/Netflix/metaflow/blob/master/tox.ini \"tox.ini\")| [tox.ini](https://github.com/Netflix/metaflow/blob/master/tox.ini \"tox.ini\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/Netflix/metaflow/)\n  * [Apache-2.0 license](https://github.com/Netflix/metaflow/)\n  * [Security](https://github.com/Netflix/metaflow/)\n\n\n[![Metaflow_Logo_Horizontal_FullColor_Ribbon_Dark_RGB](https://user-images.githubusercontent.com/763451/89453116-96a57e00-d713-11ea-9fa6-82b29d4d6eff.png)](https://user-images.githubusercontent.com/763451/89453116-96a57e00-d713-11ea-9fa6-82b29d4d6eff.png)\n# Metaflow\n[](https://github.com/Netflix/metaflow/#metaflow)\n[Metaflow](https://metaflow.org) is a human-centric framework designed to help scientists and engineers **build and manage real-life AI and ML systems**. Serving teams of all sizes and scale, Metaflow streamlines the entire development lifecycle\u2014from rapid prototyping in notebooks to reliable, maintainable production deployments\u2014enabling teams to iterate quickly and deliver robust systems efficiently.\nOriginally developed at [Netflix](https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9) and now supported by [Outerbounds](https://outerbounds.com), Metaflow is designed to boost the productivity for research and engineering teams working on [a wide variety of projects](https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d), from classical statistics to state-of-the-art deep learning and foundation models. By unifying code, data, and compute at every stage, Metaflow ensures seamless, end-to-end management of real-world AI and ML systems.\nToday, Metaflow powers thousands of AI and ML experiences across a diverse array of companies, large and small, including Amazon, Doordash, Dyson, Goldman Sachs, Ramp, and [many others](https://github.com/Netflix/metaflow/blob/master/ADOPTERS.md). At Netflix alone, Metaflow supports over 3000 AI and ML projects, executes hundreds of millions of data-intensive high-performance compute jobs processing petabytes of data and manages tens of petabytes of models and artifacts for hundreds of users across its AI, ML, data science, and engineering teams.\n## From prototype to production (and back)\n[](https://github.com/Netflix/metaflow/#from-prototype-to-production-and-back)\nMetaflow provides a simple and friendly pythonic [API](https://docs.metaflow.org) that covers foundational needs of AI and ML systems: [![](https://github.com/Netflix/metaflow/raw/master/docs/prototype-to-prod.png)](https://github.com/Netflix/metaflow/blob/master/docs/prototype-to-prod.png)\n  1. [Rapid local prototyping](https://docs.metaflow.org/metaflow/basics), [support for notebooks](https://docs.metaflow.org/metaflow/managing-flows/notebook-runs), and built-in support for [experiment tracking, versioning](https://docs.metaflow.org/metaflow/client) and [visualization](https://docs.metaflow.org/metaflow/visualizing-results).\n  2. [Effortlessly scale horizontally and vertically in your cloud](https://docs.metaflow.org/scaling/remote-tasks/introduction), utilizing both CPUs and GPUs, with [fast data access](https://docs.metaflow.org/scaling/data) for running [massive embarrassingly parallel](https://docs.metaflow.org/metaflow/basics#foreach) as well as [gang-scheduled](https://docs.metaflow.org/scaling/remote-tasks/distributed-computing) compute workloads [reliably](https://docs.metaflow.org/scaling/failures) and [efficiently](https://docs.metaflow.org/scaling/checkpoint/introduction).\n  3. [Easily manage dependencies](https://docs.metaflow.org/scaling/dependencies) and [deploy with one-click](https://docs.metaflow.org/production/introduction) to highly available production orchestrators with built in support for [reactive orchestration](https://docs.metaflow.org/production/event-triggering).\n\n\nFor full documentation, check out our [API Reference](https://docs.metaflow.org/api) or see our [Release Notes](https://github.com/Netflix/metaflow/releases) for the latest features and improvements.\n## Getting started\n[](https://github.com/Netflix/metaflow/#getting-started)\nGetting up and running is easy. If you don't know where to start, [Metaflow sandbox](https://outerbounds.com/sandbox) will have you running and exploring in seconds.\n### Installing Metaflow\n[](https://github.com/Netflix/metaflow/#installing-metaflow)\nTo install Metaflow in your Python environment from [PyPI](https://pypi.org/project/metaflow/):\n```\npip install metaflow\n```\n\nAlternatively, using [conda-forge](https://anaconda.org/conda-forge/metaflow):\n```\nconda install -c conda-forge metaflow\n```\n\nOnce installed, a great way to get started is by following our [tutorial](https://docs.metaflow.org/getting-started/tutorials). It walks you through creating and running your first Metaflow flow step by step.\nFor more details on Metaflow\u2019s features and best practices, check out:\n  * [How Metaflow works](https://docs.metaflow.org/metaflow/basics)\n  * [Additional resources](https://docs.metaflow.org/introduction/metaflow-resources)\n\n\nIf you need help, don\u2019t hesitate to reach out on our [Slack community](http://slack.outerbounds.co/)!\n### Deploying infrastructure for Metaflow in your cloud\n[](https://github.com/Netflix/metaflow/#deploying-infrastructure-for-metaflow-in-your-cloud)\n[![](https://github.com/Netflix/metaflow/raw/master/docs/multicloud.png)](https://github.com/Netflix/metaflow/blob/master/docs/multicloud.png)\nWhile you can get started with Metaflow easily on your laptop, the main benefits of Metaflow lie in its ability to [scale out to external compute clusters](https://docs.metaflow.org/scaling/remote-tasks/introduction) and to [deploy to production-grade workflow orchestrators](https://docs.metaflow.org/production/introduction). To benefit from these features, follow this [guide](https://outerbounds.com/engineering/welcome/) to configure Metaflow and the infrastructure behind it appropriately.\n## Get in touch\n[](https://github.com/Netflix/metaflow/#get-in-touch)\nWe'd love to hear from you. Join our community [Slack workspace](http://slack.outerbounds.co/)!\n## Contributing\n[](https://github.com/Netflix/metaflow/#contributing)\nWe welcome contributions to Metaflow. Please see our [contribution guide](https://docs.metaflow.org/introduction/contributing-to-metaflow) for more details.\n## About\nBuild, Manage and Deploy AI/ML Systems \n[metaflow.org](https://metaflow.org \"https://metaflow.org\")\n### Topics\n[ python ](https://github.com/topics/python \"Topic: python\") [ kubernetes ](https://github.com/topics/kubernetes \"Topic: kubernetes\") [ productivity ](https://github.com/topics/productivity \"Topic: productivity\") [ aws ](https://github.com/topics/aws \"Topic: aws\") [ data-science ](https://github.com/topics/data-science \"Topic: data-science\") [ machine-learning ](https://github.com/topics/machine-learning \"Topic: machine-learning\") [ r ](https://github.com/topics/r \"Topic: r\") [ ai ](https://github.com/topics/ai \"Topic: ai\") [ azure ](https://github.com/topics/azure \"Topic: azure\") [ gcp ](https://github.com/topics/gcp \"Topic: gcp\") [ ml ](https://github.com/topics/ml \"Topic: ml\") [ datascience ](https://github.com/topics/datascience \"Topic: datascience\") [ high-performance-computing ](https://github.com/topics/high-performance-computing \"Topic: high-performance-computing\") [ r-package ](https://github.com/topics/r-package \"Topic: r-package\") [ model-management ](https://github.com/topics/model-management \"Topic: model-management\") [ ml-infrastructure ](https://github.com/topics/ml-infrastructure \"Topic: ml-infrastructure\") [ mlops ](https://github.com/topics/mlops \"Topic: mlops\") [ ml-platform ](https://github.com/topics/ml-platform \"Topic: ml-platform\") [ llm ](https://github.com/topics/llm \"Topic: llm\") [ llmops ](https://github.com/topics/llmops \"Topic: llmops\")\n### Resources\n[ Readme ](https://github.com/Netflix/metaflow/#readme-ov-file)\n### License\n[ Apache-2.0 license ](https://github.com/Netflix/metaflow/#Apache-2.0-1-ov-file)\n### Security policy\n[ Security policy ](https://github.com/Netflix/metaflow/#security-ov-file)\n[ Activity](https://github.com/Netflix/metaflow/activity)\n[ Custom properties](https://github.com/Netflix/metaflow/custom-properties)\n### Stars\n[ **8.7k** stars](https://github.com/Netflix/metaflow/stargazers)\n### Watchers\n[ **293** watching](https://github.com/Netflix/metaflow/watchers)\n### Forks\n[ **822** forks](https://github.com/Netflix/metaflow/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FNetflix%2Fmetaflow&report=Netflix+%28user%29)\n##  [Releases 186](https://github.com/Netflix/metaflow/releases)\n[ 2.15.7 Latest  Mar 28, 2025 ](https://github.com/Netflix/metaflow/releases/tag/2.15.7)\n[+ 185 releases](https://github.com/Netflix/metaflow/releases)\n##  [Used by 790](https://github.com/Netflix/metaflow/network/dependents)\n[\n  * ![@noamzilo](https://avatars.githubusercontent.com/u/4740186?s=64&v=4)\n  * ![@cdc-ap66](https://avatars.githubusercontent.com/u/191184558?s=64&v=4)\n  * ![@augurysys](https://avatars.githubusercontent.com/u/9953662?s=64&v=4)\n  * ![@abdulrahman305](https://avatars.githubusercontent.com/u/131770526?s=64&v=4)\n  * ![@sid5150](https://avatars.githubusercontent.com/u/113247094?s=64&v=4)\n  * ![@RaphaelZH](https://avatars.githubusercontent.com/u/32324353?s=64&v=4)\n  * ![@sraddhanjali](https://avatars.githubusercontent.com/u/3305962?s=64&v=4)\n  * ![@arthrod](https://avatars.githubusercontent.com/u/89408329?s=64&v=4)\n\n+ 782  ](https://github.com/Netflix/metaflow/network/dependents)\n##  [Contributors 109](https://github.com/Netflix/metaflow/graphs/contributors)\n[+ 95 contributors](https://github.com/Netflix/metaflow/graphs/contributors)\n## Languages\n  * [ Python 92.8% ](https://github.com/Netflix/metaflow/search?l=python)\n  * [ R 3.0% ](https://github.com/Netflix/metaflow/search?l=r)\n  * [ Svelte 1.5% ](https://github.com/Netflix/metaflow/search?l=svelte)\n  * [ HTML 0.8% ](https://github.com/Netflix/metaflow/search?l=html)\n  * [ Starlark 0.5% ](https://github.com/Netflix/metaflow/search?l=starlark)\n  * [ TypeScript 0.4% ](https://github.com/Netflix/metaflow/search?l=typescript)\n  * Other 1.0%\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary of Metaflow\n\nMetaflow is a human-centric framework by Netflix for building and managing AI/ML systems, supporting rapid prototyping to production deployment. It enhances productivity for diverse projects, utilized by companies like Amazon and Goldman Sachs. Installation is straightforward via PyPI or conda, and it offers seamless scaling and orchestration capabilities.\n\n**Tags:** #GenerativeAI #ML #AI #DataScience #MLOps #Python #Kubernetes\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/triton-lang/triton/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Ftriton-lang%2Ftriton%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Ftriton-lang%2Ftriton%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=triton-lang%2Ftriton) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/triton-lang/triton/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/triton-lang/triton/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/triton-lang/triton/) to refresh your session. Dismiss alert\n{{ message }}\n[ triton-lang ](https://github.com/triton-lang) / **[triton](https://github.com/triton-lang/triton) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton) You must be signed in to change notification settings\n  * [ Fork 1.9k ](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton)\n  * [ Star  15k ](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton)\n\n\nDevelopment repository for the Triton language and compiler \n[triton-lang.org/](https://triton-lang.org/ \"https://triton-lang.org/\")\n### License\n[ MIT license ](https://github.com/triton-lang/triton/blob/main/LICENSE)\n[ 15k stars ](https://github.com/triton-lang/triton/stargazers) [ 1.9k forks ](https://github.com/triton-lang/triton/forks) [ Branches ](https://github.com/triton-lang/triton/branches) [ Tags ](https://github.com/triton-lang/triton/tags) [ Activity ](https://github.com/triton-lang/triton/activity)\n[ Star  ](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton)\n[ Notifications ](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton) You must be signed in to change notification settings\n  * [ Code ](https://github.com/triton-lang/triton)\n  * [ Issues 732 ](https://github.com/triton-lang/triton/issues)\n  * [ Pull requests 69 ](https://github.com/triton-lang/triton/pulls)\n  * [ Discussions ](https://github.com/triton-lang/triton/discussions)\n  * [ Actions ](https://github.com/triton-lang/triton/actions)\n  * [ Projects 1 ](https://github.com/triton-lang/triton/projects)\n  * [ Wiki ](https://github.com/triton-lang/triton/wiki)\n  * [ Security ](https://github.com/triton-lang/triton/security)\n  * [ Insights ](https://github.com/triton-lang/triton/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/triton-lang/triton)\n  * [ Issues  ](https://github.com/triton-lang/triton/issues)\n  * [ Pull requests  ](https://github.com/triton-lang/triton/pulls)\n  * [ Discussions  ](https://github.com/triton-lang/triton/discussions)\n  * [ Actions  ](https://github.com/triton-lang/triton/actions)\n  * [ Projects  ](https://github.com/triton-lang/triton/projects)\n  * [ Wiki  ](https://github.com/triton-lang/triton/wiki)\n  * [ Security  ](https://github.com/triton-lang/triton/security)\n  * [ Insights  ](https://github.com/triton-lang/triton/pulse)\n\n\n# triton-lang/triton\nmain\n[Branches](https://github.com/triton-lang/triton/branches)[Tags](https://github.com/triton-lang/triton/tags)\n[](https://github.com/triton-lang/triton/branches)[](https://github.com/triton-lang/triton/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[3,666 Commits](https://github.com/triton-lang/triton/commits/main/)[](https://github.com/triton-lang/triton/commits/main/)  \n[.github](https://github.com/triton-lang/triton/tree/main/.github \".github\")| [.github](https://github.com/triton-lang/triton/tree/main/.github \".github\")  \n[bin](https://github.com/triton-lang/triton/tree/main/bin \"bin\")| [bin](https://github.com/triton-lang/triton/tree/main/bin \"bin\")  \n[cmake](https://github.com/triton-lang/triton/tree/main/cmake \"cmake\")| [cmake](https://github.com/triton-lang/triton/tree/main/cmake \"cmake\")  \n[docs](https://github.com/triton-lang/triton/tree/main/docs \"docs\")| [docs](https://github.com/triton-lang/triton/tree/main/docs \"docs\")  \n[include](https://github.com/triton-lang/triton/tree/main/include \"include\")| [include](https://github.com/triton-lang/triton/tree/main/include \"include\")  \n[lib](https://github.com/triton-lang/triton/tree/main/lib \"lib\")| [lib](https://github.com/triton-lang/triton/tree/main/lib \"lib\")  \n[python](https://github.com/triton-lang/triton/tree/main/python \"python\")| [python](https://github.com/triton-lang/triton/tree/main/python \"python\")  \n[test](https://github.com/triton-lang/triton/tree/main/test \"test\")| [test](https://github.com/triton-lang/triton/tree/main/test \"test\")  \n[third_party](https://github.com/triton-lang/triton/tree/main/third_party \"third_party\")| [third_party](https://github.com/triton-lang/triton/tree/main/third_party \"third_party\")  \n[unittest](https://github.com/triton-lang/triton/tree/main/unittest \"unittest\")| [unittest](https://github.com/triton-lang/triton/tree/main/unittest \"unittest\")  \n[utils](https://github.com/triton-lang/triton/tree/main/utils \"utils\")| [utils](https://github.com/triton-lang/triton/tree/main/utils \"utils\")  \n[.clang-format](https://github.com/triton-lang/triton/blob/main/.clang-format \".clang-format\")| [.clang-format](https://github.com/triton-lang/triton/blob/main/.clang-format \".clang-format\")  \n[.editorconfig](https://github.com/triton-lang/triton/blob/main/.editorconfig \".editorconfig\")| [.editorconfig](https://github.com/triton-lang/triton/blob/main/.editorconfig \".editorconfig\")  \n[.git-blame-ignore-revs](https://github.com/triton-lang/triton/blob/main/.git-blame-ignore-revs \".git-blame-ignore-revs\")| [.git-blame-ignore-revs](https://github.com/triton-lang/triton/blob/main/.git-blame-ignore-revs \".git-blame-ignore-revs\")  \n[.gitignore](https://github.com/triton-lang/triton/blob/main/.gitignore \".gitignore\")| [.gitignore](https://github.com/triton-lang/triton/blob/main/.gitignore \".gitignore\")  \n[.pre-commit-config.yaml](https://github.com/triton-lang/triton/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/triton-lang/triton/blob/main/.pre-commit-config.yaml \".pre-commit-config.yaml\")  \n[CMakeLists.txt](https://github.com/triton-lang/triton/blob/main/CMakeLists.txt \"CMakeLists.txt\")| [CMakeLists.txt](https://github.com/triton-lang/triton/blob/main/CMakeLists.txt \"CMakeLists.txt\")  \n[CONTRIBUTING.md](https://github.com/triton-lang/triton/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/triton-lang/triton/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\")  \n[LICENSE](https://github.com/triton-lang/triton/blob/main/LICENSE \"LICENSE\")| [LICENSE](https://github.com/triton-lang/triton/blob/main/LICENSE \"LICENSE\")  \n[Makefile](https://github.com/triton-lang/triton/blob/main/Makefile \"Makefile\")| [Makefile](https://github.com/triton-lang/triton/blob/main/Makefile \"Makefile\")  \n[README.md](https://github.com/triton-lang/triton/blob/main/README.md \"README.md\")| [README.md](https://github.com/triton-lang/triton/blob/main/README.md \"README.md\")  \n[RELEASE.md](https://github.com/triton-lang/triton/blob/main/RELEASE.md \"RELEASE.md\")| [RELEASE.md](https://github.com/triton-lang/triton/blob/main/RELEASE.md \"RELEASE.md\")  \n[pyproject.toml](https://github.com/triton-lang/triton/blob/main/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/triton-lang/triton/blob/main/pyproject.toml \"pyproject.toml\")  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/triton-lang/triton/)\n  * [MIT license](https://github.com/triton-lang/triton/)\n\n\n[![Triton logo](https://camo.githubusercontent.com/94edaa23faf33f9f7e76384487d4480ebd006ce791e63aba1fdbe0cf4adc06df/68747470733a2f2f6c68352e676f6f676c6575736572636f6e74656e742e636f6d2f777a514b457354466b72674e514f394a6a68474835774676736c4a723173614c744c614a5f613646705f674e454e70767433564737426d7a74776e67553968464a615534435077476977316f7051744476546b4c72785752624f5f613132512d70644553574867746d686549486350624f4c355a4d43345453694a566535747931773d7733353137)](https://camo.githubusercontent.com/94edaa23faf33f9f7e76384487d4480ebd006ce791e63aba1fdbe0cf4adc06df/68747470733a2f2f6c68352e676f6f676c6575736572636f6e74656e742e636f6d2f777a514b457354466b72674e514f394a6a68474835774676736c4a723173614c744c614a5f613646705f674e454e70767433564737426d7a74776e67553968464a615534435077476977316f7051744476546b4c72785752624f5f613132512d70644553574867746d686549486350624f4c355a4d43345453694a566535747931773d7733353137)\n**`Documentation`**| **`Nightly Wheels`**  \n---|---  \n[![Documentation](https://github.com/triton-lang/triton/actions/workflows/documentation.yml/badge.svg)](https://triton-lang.org/) | [![Wheels](https://github.com/triton-lang/triton/actions/workflows/wheels.yml/badge.svg)](https://github.com/triton-lang/triton/actions/workflows/wheels.yml)  \n# Triton\n[](https://github.com/triton-lang/triton/#triton)\nThis is the development repository of Triton, a language and compiler for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.\nThe foundations of this project are described in the following MAPL2019 publication: [Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations](http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf). Please consider citing this work if you use Triton!\nThe [official documentation](https://triton-lang.org) contains installation instructions and tutorials. See also these third-party [Triton puzzles](https://github.com/srush/Triton-Puzzles), which can all be run using the Triton interpreter -- no GPU required.\n# Quick Installation\n[](https://github.com/triton-lang/triton/#quick-installation)\nYou can install the latest stable release of Triton from pip:\n```\npip install triton\n```\n\nBinary wheels are available for CPython 3.9-3.13.\n# Enabling Blackwell Support\n[](https://github.com/triton-lang/triton/#enabling-blackwell-support)\nThe main branch now features support for NVIDIA Blackwell GPUs using 5th generation tensor cores. To enable this, you will need two additional steps:\n  1. Build a pre-release PyTorch from source with CUDA 12.8\n  2. Build triton from the latest source\n\n\nFirst, to build pytorch you need to have CUDA 12.8 installed locally. If not, follow the [instructions for your platform](https://developer.nvidia.com/cuda-downloads)\n```\n# Clone and checkout pytorch 2.6 release candidate\ngit clone https://github.com/pytorch/pytorch\ncd pytorch\ngit checkout v2.6.0-rc9\ngit submodule sync\ngit submodule update --init --recursive -j 8\n# Install build dependencies (assumes you already have a system compiler)\npip install -r requirements.txt\npip install mkl-static mkl-include wheel\n# Build PyTorch (will take a long time)\nexport CUDA_HOME=/usr/local/cuda-12.8\nexport CUDA_PATH=$CUDA_HOME\nexport TORCH_CUDA_ARCH_LIST=Blackwell\npython setup.py develop\n# Optional, package build into a wheel to install on other machines.\npython setup.py bdist_wheel\nls dist # Wheel should be output in this directory\n```\n\nNote that if you use the domain libraries (`torchvision`, `torchtext`, `torchaudio`, etc.) these will need to be built from source as well, otherwise their custom PyTorch extensions will not work.\nFinally, follow the instructions below to install triton from source.\n# Install from source\n[](https://github.com/triton-lang/triton/#install-from-source)\n```\ngit clone https://github.com/triton-lang/triton.git\ncd triton\npip install -r python/requirements.txt # build-time dependencies\npip install -e python\n```\n\nOr with a virtualenv:\n```\ngit clone https://github.com/triton-lang/triton.git\ncd triton\npython -m venv .venv --prompt triton\nsource .venv/bin/activate\npip install -r python/requirements.txt # build-time dependencies\npip install -e python\n```\n\n# Building with a custom LLVM\n[](https://github.com/triton-lang/triton/#building-with-a-custom-llvm)\nTriton uses LLVM to generate code for GPUs and CPUs. Normally, the Triton build downloads a prebuilt LLVM, but you can also build LLVM from source and use that.\nLLVM does not have a stable API, so the Triton build will not work at an arbitrary LLVM version.\n  1. Find the version of LLVM that Triton builds against. Check `cmake/llvm-hash.txt` to see the current version. For example, if it says: 49af6502c6dcb4a7f7520178bd14df396f78240c\nThis means that the version of Triton you have builds against [LLVM](https://github.com/llvm/llvm-project) 49af6502.\n  2. `git checkout` LLVM at this revision. Optionally, make additional modifications to LLVM.\n  3. [Build LLVM](https://llvm.org/docs/CMake.html). For example, you might run\n```\n$ cd $HOME/llvm-project # your clone of LLVM.\n$ mkdir build\n$ cd build\n$ cmake -G Ninja -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON ../llvm -DLLVM_ENABLE_PROJECTS=\"mlir;llvm;lld\" -DLLVM_TARGETS_TO_BUILD=\"host;NVPTX;AMDGPU\"\n$ ninja\n\n```\n\n  4. Grab a snack, this will take a while.\n  5. Build Triton as above, but set the following environment variables.\n```\n# Modify as appropriate to point to your LLVM build.\n$ export LLVM_BUILD_DIR=$HOME/llvm-project/build\n$ cd <triton install>\n$ LLVM_INCLUDE_DIRS=$LLVM_BUILD_DIR/include \\\n LLVM_LIBRARY_DIR=$LLVM_BUILD_DIR/lib \\\n LLVM_SYSPATH=$LLVM_BUILD_DIR \\\n pip install -e python\n\n```\n\n\n\n# Tips for building\n[](https://github.com/triton-lang/triton/#tips-for-building)\n  * Set `TRITON_BUILD_WITH_CLANG_LLD=true` as an environment variable to use clang and lld. lld in particular results in faster builds.\n  * Set `TRITON_BUILD_WITH_CCACHE=true` to build with ccache.\n  * Set `TRITON_HOME=/some/path` to change the location of the `.triton` directory where Triton's cache is located and downloads are stored during the build. By default, this is the user's home directory. It can be changed anytime.\n  * If you're running out of memory when building Triton, specify the `MAX_JOBS` environment variable (to the `pip install -e python` command) to limit the number of jobs.\n  * Pass `--no-build-isolation` to `pip install` to make nop builds faster. Without this, every invocation of `pip install` uses a different symlink to cmake, and this forces ninja to rebuild most of the `.a` files.\n  * vscode intellisense has some difficulty figuring out how to build Triton's C++ (probably because, in our build, users don't invoke cmake directly, but instead use setup.py). Teach vscode how to compile Triton as follows.\n    * Do a local build. Run command `pip install -e python`\n    * Get the full path to the `compile_commands.json` file produced by the build: `find python/build -name 'compile_commands.json' | xargs readlink -f`. You might get a full path similar to `/Users/{username}/triton/python/build/cmake.macosx-11.1-arm64-cpython-3.12/compile_commands.json`\n    * In vscode, install the [C/C++ extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools), then open the command palette (`Shift + Command + P` on Mac, or `Shift + Ctrl + P` on Windows/Linux) and open `C/C++: Edit Configurations (UI)`.\n    * Open \"Advanced Settings\" and paste the full path to `compile_commands.json` into the \"Compile Commands\" textbox.\n\n\n# Running tests\n[](https://github.com/triton-lang/triton/#running-tests)\nThere currently isn't a turnkey way to run all the Triton tests, but you can follow the following recipe.\n```\n# One-time setup. Note this will reinstall local Triton because torch\n# overwrites it with the public version.\n$ make dev-install\n# To run all tests (requires a GPU)\n$ make test\n# Or, to run tests without a gpu\n$ make test-nogpu\n```\n\n# Tips for hacking\n[](https://github.com/triton-lang/triton/#tips-for-hacking)\nFor detailed instructions on how to debug Triton's frontend, please refer to this [tutorial](https://triton-lang.org/main/programming-guide/chapter-3/debugging.html). The following includes additional tips for hacking on Triton's backend.\n**Helpful environment variables**\n  * `MLIR_ENABLE_DUMP=1` dumps the IR before every MLIR pass Triton runs, for all kernels. Use `MLIR_ENABLE_DUMP=kernelName` to dump for a specific kernel only.\n    * Triton cache can interfere with the dump. In cases where `MLIR_ENABLE_DUMP=1` does not work, try cleaning your triton cache: `rm -r ~/.triton/cache/*`\n  * `MLIR_DUMP_PATH` specifies where `MLIR_ENABLE_DUMP` will dump to. If unset will dump to stderr.\n  * `LLVM_IR_ENABLE_DUMP=1` dumps the IR before every pass run over the LLVM IR.\n  * `TRITON_REPRODUCER_PATH=<reproducer_path>` will generate an MLIR reproducer file at `<reproducer_path>` before each MLIR compiler stage. If any of the stages fail, `<reproducer_path>` will be a local MLIR reproducer captured right before the failing pass.\n  * `TRITON_INTERPRET=1` uses the Triton interpreter instead of running on the GPU. You can insert Python breakpoints in your kernel code!\n  * `TRITON_ENABLE_LLVM_DEBUG=1` passes `-debug` to LLVM, printing a lot of debugging information to stdout. If this is too noisy, run with just `TRITON_LLVM_DEBUG_ONLY` instead to limit the output.\nAn alternative way to reduce output noisiness is running with `LLVM_IR_ENABLE_DUMP=1`, extract the IR before the LLVM pass of interest, and then run LLVM's `opt` standalone, perhaps passing `-debug-only=foo` on the command line.\n  * `TRITON_LLVM_DEBUG_ONLY=<comma-separated>` is the equivalent of LLVM's `-debug-only` command-line option. This limits the LLVM debug output to specific pass or component names (which are specified using `#define DEBUG_TYPE` throughout LLVM and Triton) in order to allow the debug output to be less noisy. `TRITON_LLVM_DEBUG_ONLY` allows for one or more comma separated values to be specified (eg `TRITON_LLVM_DEBUG_ONLY=\"tritongpu-remove-layout-conversions\"` or `TRITON_LLVM_DEBUG_ONLY=\"tritongpu-remove-layout-conversions,regalloc\"`).\n  * `TRITON_ENABLE_ASAN=1` invokes the LLVM address sanitizer for memory leak and out of bounds access detection. Currently only supported on the AMD backend. This must be run using the ASAN libraries documented [here](https://rocm.docs.amd.com/projects/llvm-project/en/latest/conceptual/using-gpu-sanitizer.html).\nWhen enabling the address sanitizer it is recommended to disable various memory caching strategies both within the ROCm stack and PyTorch. This will give the address sanitizer the best chance at finding the memory fault where it originates. See this [test](https://github.com/triton-lang/triton/blob/main/third_party/amd/python/test/test_address_sanitizer.py) for more details.\n  * `USE_IR_LOC={ttir,ttgir}` reparses the IR such that the location information will be the line number of the IR file with that particular extension, instead of line number of the python file. This can provide a direct mapping from the IR to llir/ptx. When used with performance tools, it can provide a breakdown on IR instructions.\n  * `TRITON_PRINT_AUTOTUNING=1` prints out the best autotuning config and total time spent for each kernel after autotuning is complete.\n  * `DISABLE_LLVM_OPT` will disable llvm optimizations for make_llir and make_ptx if its value is true when parsing as Bool. Otherwise, it will be parsed as a list of flags to disable llvm optimizations. One usage case is `DISABLE_LLVM_OPT=\"disable-lsr\"` Loop strength reduction is known to cause up to 10% performance changes for certain kernels with register pressure.\n  * `TRITON_ALWAYS_COMPILE=1` forces to compile kernels regardless of cache hit.\n  * `MLIR_ENABLE_TIMING` dumps the timing information for each MLIR pass.\n  * `LLVM_ENABLE_TIMING` dumps the timing information for each LLVM pass.\n  * `TRITON_DEFAULT_FP_FUSION` overrides the default behavior of allowing fp fusion (mul+add->fma).\n  * `MLIR_ENABLE_DIAGNOSTICS=<comma-separated>` controls diagnostic emission in MLIR. Options are: `warnings`, `remarks`, `stacktraces`, `operations`. Use comma-separated values to customize output. For example, `MLIR_ENABLE_DIAGNOSTICS=remarks,operations` enables remarks and IR operations, while `MLIR_ENABLE_DIAGNOSTICS=warnings,stacktraces` enables warnings with stacktraces. By default, only errors are shown. Setting `warnings` includes errors and warnings; `remarks` includes errors, warnings, and remarks.\n  * `MLIR_ENABLE_REMARK` is deprecated. Please use `MLIR_ENABLE_DIAGNOSTICS=remarks`.\n  * `TRITON_KERNEL_DUMP` enables the dumping of the IR from each compilation stage and the final ptx/amdgcn.\n  * `TRITON_DUMP_DIR` specifies the directory to save the dumped IR and ptx/amdgcn when `TRITON_KERNEL_DUMP` is set to 1.\n  * `TRITON_KERNEL_OVERRIDE` enables the override of the compiled kernel with a user-specified IR/ptx/amdgcn at the beginning of each compilation stage.\n  * `TRITON_OVERRIDE_DIR` specifies the directory from which to load the IR/ptx/amdgcn files when `TRITON_KERNEL_OVERRIDE` is set to 1.\n  * `TRITON_F32_DEFAULT` sets the default input precision of `tl.dot` when using 32-bit floats, which can be either `ieee`, `tf32`, or `tf32x3`.\n  * `TRITON_FRONT_END_DEBUGGING=1` disables exception wrapping when an error occurs in the compiler frontend, allowing the full stack trace to be seen.\n\n\n**Kernel Override Steps**\n```\nexport TRITON_ALWAYS_COMPILE=1\nexport TRITON_KERNEL_DUMP=1\nexport TRITON_DUMP_DIR=<dump_dir>\nexport TRITON_KERNEL_OVERRIDE=1\nexport TRITON_OVERRIDE_DIR=<override_dir>\n# Step 1: Run the kernel once to dump kernel's IRs and ptx/amdgcn in $TRITON_DUMP_DIR\n# Step 2: Copy $TRITON_DUMP_DIR/<kernel_hash> to $TRITON_OVERRIDE_DIR\n# Step 3: Delete the stages that you do not want to override and modify the stage you do want to override\n# Step 4: Run the kernel again to see the overridden result\n```\n\n# Changelog\n[](https://github.com/triton-lang/triton/#changelog)\nVersion 2.0 is out! New features include:\n  * Many, many bug fixes\n  * Performance improvements\n  * Backend rewritten to use MLIR\n  * Support for kernels that contain back-to-back matmuls (e.g., flash attention)\n\n\n# Contributing\n[](https://github.com/triton-lang/triton/#contributing)\nCommunity contributions are more than welcome, whether it be to fix bugs or to add new features at [github](https://github.com/triton-lang/triton/). For more detailed instructions, please visit our [contributor's guide](https://github.com/triton-lang/triton/blob/main/CONTRIBUTING.md).\n# Compatibility\n[](https://github.com/triton-lang/triton/#compatibility)\nSupported Platforms:\n  * Linux\n\n\nSupported Hardware:\n  * NVIDIA GPUs (Compute Capability 8.0+)\n  * AMD GPUs (ROCm 6.2+)\n  * Under development: CPUs\n\n\n# Development Container (Dev Container)\n[](https://github.com/triton-lang/triton/#development-container-dev-container)\n**Dev Containers** for the Triton project are available from the [triton-dev-containers repository](https://github.com/redhat-et/triton-dev-containers)\n### Key Benefits:\n[](https://github.com/triton-lang/triton/#key-benefits)\n  * **Consistency** : All developers can work with the same development environment, ensuring uniform behavior across different systems.\n  * **Isolation** : The container prevents potential conflicts with software installed on your local machine.\n  * **Portability** : Easily share the development environment with team members, minimizing onboarding time and setup issues.\n\n\n### How to Use the Dev Container:\n[](https://github.com/triton-lang/triton/#how-to-use-the-dev-container)\nFor detailed instructions on how to use the dev containers please see the [dev container user guide](https://github.com/redhat-et/triton-dev-containers/blob/main/.devcontainer/devcontainer.md)\n## About\nDevelopment repository for the Triton language and compiler \n[triton-lang.org/](https://triton-lang.org/ \"https://triton-lang.org/\")\n### Resources\n[ Readme ](https://github.com/triton-lang/triton/#readme-ov-file)\n### License\n[ MIT license ](https://github.com/triton-lang/triton/#MIT-1-ov-file)\n[ Activity](https://github.com/triton-lang/triton/activity)\n[ Custom properties](https://github.com/triton-lang/triton/custom-properties)\n### Stars\n[ **15k** stars](https://github.com/triton-lang/triton/stargazers)\n### Watchers\n[ **196** watching](https://github.com/triton-lang/triton/watchers)\n### Forks\n[ **1.9k** forks](https://github.com/triton-lang/triton/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Ftriton-lang%2Ftriton&report=triton-lang+%28user%29)\n##  [Releases](https://github.com/triton-lang/triton/releases)\n[ 11 tags ](https://github.com/triton-lang/triton/tags)\n##  [Packages 0](https://github.com/orgs/triton-lang/packages?repo_name=triton)\nNo packages published \n##  [Used by 58.8k](https://github.com/triton-lang/triton/network/dependents)\n[\n  * ![@mshmygel](https://avatars.githubusercontent.com/u/177831504?s=64&v=4)\n  * ![@PrithvirajG](https://avatars.githubusercontent.com/u/67669857?s=64&v=4)\n  * ![@FhG-IISB-MKI](https://avatars.githubusercontent.com/u/205148278?s=64&v=4)\n  * ![@mlvc-lab](https://avatars.githubusercontent.com/u/51532443?s=64&v=4)\n  * ![@Bigkatoan](https://avatars.githubusercontent.com/u/63138206?s=64&v=4)\n  * ![@3078139041](https://avatars.githubusercontent.com/u/103644827?s=64&v=4)\n  * ![@Varshaa-08](https://avatars.githubusercontent.com/u/202936892?s=64&v=4)\n  * ![@OpenRobotLab](https://avatars.githubusercontent.com/u/127282590?s=64&v=4)\n\n+ 58,787  ](https://github.com/triton-lang/triton/network/dependents)\n##  [Contributors 397](https://github.com/triton-lang/triton/graphs/contributors)\n  * [ ![@ptillet](https://avatars.githubusercontent.com/u/614788?s=64&v=4) ](https://github.com/ptillet)\n  * [ ![@Jokeren](https://avatars.githubusercontent.com/u/2306281?s=64&v=4) ](https://github.com/Jokeren)\n  * [ ![@ThomasRaoux](https://avatars.githubusercontent.com/u/24322263?s=64&v=4) ](https://github.com/ThomasRaoux)\n  * [ ![@antiagainst](https://avatars.githubusercontent.com/u/487928?s=64&v=4) ](https://github.com/antiagainst)\n  * [ ![@jlebar](https://avatars.githubusercontent.com/u/150663?s=64&v=4) ](https://github.com/jlebar)\n  * [ ![@peterbell10](https://avatars.githubusercontent.com/u/13238737?s=64&v=4) ](https://github.com/peterbell10)\n  * [ ![@zahimoud](https://avatars.githubusercontent.com/u/128723247?s=64&v=4) ](https://github.com/zahimoud)\n  * [ ![@pawelszczerbuk](https://avatars.githubusercontent.com/u/153013546?s=64&v=4) ](https://github.com/pawelszczerbuk)\n  * [ ![@lezcano](https://avatars.githubusercontent.com/u/3291265?s=64&v=4) ](https://github.com/lezcano)\n  * [ ![@Mogball](https://avatars.githubusercontent.com/u/15016832?s=64&v=4) ](https://github.com/Mogball)\n  * [ ![@anmyachev](https://avatars.githubusercontent.com/u/45976948?s=64&v=4) ](https://github.com/anmyachev)\n  * [ ![@zhanglx13](https://avatars.githubusercontent.com/u/3949632?s=64&v=4) ](https://github.com/zhanglx13)\n  * [ ![@daadaada](https://avatars.githubusercontent.com/u/29946339?s=64&v=4) ](https://github.com/daadaada)\n  * [ ![@binarman](https://avatars.githubusercontent.com/u/3631351?s=64&v=4) ](https://github.com/binarman)\n\n\n[+ 383 contributors](https://github.com/triton-lang/triton/graphs/contributors)\n## Languages\n  * [ MLIR 41.9% ](https://github.com/triton-lang/triton/search?l=mlir)\n  * [ C++ 34.5% ](https://github.com/triton-lang/triton/search?l=c%2B%2B)\n  * [ Python 22.8% ](https://github.com/triton-lang/triton/search?l=python)\n  * [ CMake 0.6% ](https://github.com/triton-lang/triton/search?l=cmake)\n  * [ C 0.1% ](https://github.com/triton-lang/triton/search?l=c)\n  * [ Makefile 0.1% ](https://github.com/triton-lang/triton/search?l=makefile)\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n# TL;DR Summary\n\nTriton is an open-source language and compiler designed for efficient custom Deep-Learning primitives, aiming for higher productivity than CUDA. It supports NVIDIA and AMD GPUs, with installation via pip. Key features include Blackwell GPU support, MLIR backend, and community contributions are encouraged. \n\nFor more details, visit the [official documentation](https://triton-lang.org).\n```"
  },
  {
    "instruction": "[Skip to content](https://github.com/scrapy/scrapy/#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2F)\n  * Product \n    * [ GitHub Copilot Write better code with AI  ](https://github.com/features/copilot)\n    * [ Security Find and fix vulnerabilities  ](https://github.com/features/security)\n    * [ Actions Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ All features ](https://github.com/features)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles/ai)\n    * [ DevOps ](https://github.com/resources/articles/devops)\n    * [ Security ](https://github.com/resources/articles/security)\n    * [ Software Development ](https://github.com/resources/articles/software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://resources.github.com)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://partner.github.com)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ Advanced Security Enterprise-grade security features  ](https://github.com/enterprise/advanced-security)\n    * [ Copilot for business Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy%2F)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=scrapy%2Fscrapy) Reseting focus\nYou signed in with another tab or window. [Reload](https://github.com/scrapy/scrapy/) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/scrapy/scrapy/) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/scrapy/scrapy/) to refresh your session. Dismiss alert\n{{ message }}\n[ scrapy ](https://github.com/scrapy) / **[scrapy](https://github.com/scrapy/scrapy) ** Public\n  * [ Notifications ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy) You must be signed in to change notification settings\n  * [ Fork 10.7k ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy)\n  * [ Star  54.7k ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy)\n\n\nScrapy, a fast high-level web crawling & scraping framework for Python. \n[scrapy.org](https://scrapy.org \"https://scrapy.org\")\n### License\n[ BSD-3-Clause license ](https://github.com/scrapy/scrapy/blob/master/LICENSE)\n[ 54.7k stars ](https://github.com/scrapy/scrapy/stargazers) [ 10.7k forks ](https://github.com/scrapy/scrapy/forks) [ Branches ](https://github.com/scrapy/scrapy/branches) [ Tags ](https://github.com/scrapy/scrapy/tags) [ Activity ](https://github.com/scrapy/scrapy/activity)\n[ Star  ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy)\n[ Notifications ](https://github.com/login?return_to=%2Fscrapy%2Fscrapy) You must be signed in to change notification settings\n  * [ Code ](https://github.com/scrapy/scrapy)\n  * [ Issues 437 ](https://github.com/scrapy/scrapy/issues)\n  * [ Pull requests 185 ](https://github.com/scrapy/scrapy/pulls)\n  * [ Discussions ](https://github.com/scrapy/scrapy/discussions)\n  * [ Actions ](https://github.com/scrapy/scrapy/actions)\n  * [ Projects 0 ](https://github.com/scrapy/scrapy/projects)\n  * [ Wiki ](https://github.com/scrapy/scrapy/wiki)\n  * [ Security 10 ](https://github.com/scrapy/scrapy/security)\n  * [ Insights ](https://github.com/scrapy/scrapy/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/scrapy/scrapy)\n  * [ Issues  ](https://github.com/scrapy/scrapy/issues)\n  * [ Pull requests  ](https://github.com/scrapy/scrapy/pulls)\n  * [ Discussions  ](https://github.com/scrapy/scrapy/discussions)\n  * [ Actions  ](https://github.com/scrapy/scrapy/actions)\n  * [ Projects  ](https://github.com/scrapy/scrapy/projects)\n  * [ Wiki  ](https://github.com/scrapy/scrapy/wiki)\n  * [ Security  ](https://github.com/scrapy/scrapy/security)\n  * [ Insights  ](https://github.com/scrapy/scrapy/pulse)\n\n\n# scrapy/scrapy\nmaster\n[**46** Branches](https://github.com/scrapy/scrapy/branches)[**119** Tags](https://github.com/scrapy/scrapy/tags)\n[](https://github.com/scrapy/scrapy/branches)[](https://github.com/scrapy/scrapy/tags)\nGo to file\nCode\n## Folders and files\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n## Latest commit\n[![wRAR](https://avatars.githubusercontent.com/u/241039?v=4&size=40)](https://github.com/wRAR)[wRAR](https://github.com/scrapy/scrapy/commits?author=wRAR)[Convert test_downloadermiddleware_robotstxt.py from callbacks to awai\u2026](https://github.com/scrapy/scrapy/commit/9f99da8f865efff11f6c8736567b8fbd4413091c)Mar 24, 2025[9f99da8](https://github.com/scrapy/scrapy/commit/9f99da8f865efff11f6c8736567b8fbd4413091c) \u00b7 Mar 24, 2025\n## History\n[10,758 Commits](https://github.com/scrapy/scrapy/commits/master/)[](https://github.com/scrapy/scrapy/commits/master/)  \n[.github](https://github.com/scrapy/scrapy/tree/master/.github \".github\")| [.github](https://github.com/scrapy/scrapy/tree/master/.github \".github\")| [Codecov: Add test analytics (](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")[#6741](https://github.com/scrapy/scrapy/pull/6741)[)](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")| Mar 21, 2025  \n[artwork](https://github.com/scrapy/scrapy/tree/master/artwork \"artwork\")| [artwork](https://github.com/scrapy/scrapy/tree/master/artwork \"artwork\")| [artwork/README.rst: add missing articles (](https://github.com/scrapy/scrapy/commit/874a87976830ed519ee49742c5b427618fa362e8 \"artwork/README.rst: add missing articles \\(#5827\\)\")[#5827](https://github.com/scrapy/scrapy/pull/5827)[)](https://github.com/scrapy/scrapy/commit/874a87976830ed519ee49742c5b427618fa362e8 \"artwork/README.rst: add missing articles \\(#5827\\)\")| Feb 14, 2023  \n[docs](https://github.com/scrapy/scrapy/tree/master/docs \"docs\")| [docs](https://github.com/scrapy/scrapy/tree/master/docs \"docs\")| [Syntax Error Fixed (](https://github.com/scrapy/scrapy/commit/3ca882fba86750199c2f41ef24b5495a4afa7988 \"Syntax Error Fixed \\(#6738\\)\n* Syntax error fix issue #6731\n* test case added\n* extra logic removed\n* mock spider fixture\n* Update scrapy/utils/misc.py\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\n* settings.rst updated\n* settings.rst updated\n* settings.rst updated\n---------\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\")[#6738](https://github.com/scrapy/scrapy/pull/6738)[)](https://github.com/scrapy/scrapy/commit/3ca882fba86750199c2f41ef24b5495a4afa7988 \"Syntax Error Fixed \\(#6738\\)\n* Syntax error fix issue #6731\n* test case added\n* extra logic removed\n* mock spider fixture\n* Update scrapy/utils/misc.py\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\n* settings.rst updated\n* settings.rst updated\n* settings.rst updated\n---------\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\")| Mar 20, 2025  \n[extras](https://github.com/scrapy/scrapy/tree/master/extras \"extras\")| [extras](https://github.com/scrapy/scrapy/tree/master/extras \"extras\")| [Run and fix linkcheck. (](https://github.com/scrapy/scrapy/commit/ce5a132f12341a4118edb7c8ae3b7c2a27306057 \"Run and fix linkcheck. \\(#6524\\)\")[#6524](https://github.com/scrapy/scrapy/pull/6524)[)](https://github.com/scrapy/scrapy/commit/ce5a132f12341a4118edb7c8ae3b7c2a27306057 \"Run and fix linkcheck. \\(#6524\\)\")| Nov 4, 2024  \n[scrapy](https://github.com/scrapy/scrapy/tree/master/scrapy \"scrapy\")| [scrapy](https://github.com/scrapy/scrapy/tree/master/scrapy \"scrapy\")| [Syntax Error Fixed (](https://github.com/scrapy/scrapy/commit/3ca882fba86750199c2f41ef24b5495a4afa7988 \"Syntax Error Fixed \\(#6738\\)\n* Syntax error fix issue #6731\n* test case added\n* extra logic removed\n* mock spider fixture\n* Update scrapy/utils/misc.py\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\n* settings.rst updated\n* settings.rst updated\n* settings.rst updated\n---------\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\")[#6738](https://github.com/scrapy/scrapy/pull/6738)[)](https://github.com/scrapy/scrapy/commit/3ca882fba86750199c2f41ef24b5495a4afa7988 \"Syntax Error Fixed \\(#6738\\)\n* Syntax error fix issue #6731\n* test case added\n* extra logic removed\n* mock spider fixture\n* Update scrapy/utils/misc.py\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\n* settings.rst updated\n* settings.rst updated\n* settings.rst updated\n---------\nCo-authored-by: Adri\u00e1n Chaves <adrian@chaves.gal>\")| Mar 20, 2025  \n[sep](https://github.com/scrapy/scrapy/tree/master/sep \"sep\")| [sep](https://github.com/scrapy/scrapy/tree/master/sep \"sep\")| [Remove trailing whitespace](https://github.com/scrapy/scrapy/commit/bee74fb753afa373ecdb1dbf272af9a108b14b22 \"Remove trailing whitespace\")| Mar 11, 2025  \n[tests](https://github.com/scrapy/scrapy/tree/master/tests \"tests\")| [tests](https://github.com/scrapy/scrapy/tree/master/tests \"tests\")| [Convert test_downloadermiddleware_robotstxt.py from callbacks to awai\u2026](https://github.com/scrapy/scrapy/commit/9f99da8f865efff11f6c8736567b8fbd4413091c \"Convert test_downloadermiddleware_robotstxt.py from callbacks to awaits. \\(#6743\\)\")| Mar 24, 2025  \n[tests_typing](https://github.com/scrapy/scrapy/tree/master/tests_typing \"tests_typing\")| [tests_typing](https://github.com/scrapy/scrapy/tree/master/tests_typing \"tests_typing\")| [Drop Python 3.8 Support (](https://github.com/scrapy/scrapy/commit/53916630723a86277836053e1c54fe50655f0bd5 \"Drop Python 3.8 Support \\(#6472\\)\")[#6472](https://github.com/scrapy/scrapy/pull/6472)[)](https://github.com/scrapy/scrapy/commit/53916630723a86277836053e1c54fe50655f0bd5 \"Drop Python 3.8 Support \\(#6472\\)\")| Oct 16, 2024  \n[.git-blame-ignore-revs](https://github.com/scrapy/scrapy/blob/master/.git-blame-ignore-revs \".git-blame-ignore-revs\")| [.git-blame-ignore-revs](https://github.com/scrapy/scrapy/blob/master/.git-blame-ignore-revs \".git-blame-ignore-revs\")| [chore: fix some typos in comments (](https://github.com/scrapy/scrapy/commit/5f67c01d1d2cc8a0104361b0323d755e22ed93dc \"chore: fix some typos in comments \\(#6317\\)\nSigned-off-by: TechVest <techdashen@qq.com>\")[#6317](https://github.com/scrapy/scrapy/pull/6317)[)](https://github.com/scrapy/scrapy/commit/5f67c01d1d2cc8a0104361b0323d755e22ed93dc \"chore: fix some typos in comments \\(#6317\\)\nSigned-off-by: TechVest <techdashen@qq.com>\")| Apr 17, 2024  \n[.gitattributes](https://github.com/scrapy/scrapy/blob/master/.gitattributes \".gitattributes\")| [.gitattributes](https://github.com/scrapy/scrapy/blob/master/.gitattributes \".gitattributes\")| [Maybe the problem is not in the code after all](https://github.com/scrapy/scrapy/commit/24ba5a71aca9b368eee21bd7d9043a7d26dba403 \"Maybe the problem is not in the code after all\")| Aug 12, 2020  \n[.gitignore](https://github.com/scrapy/scrapy/blob/master/.gitignore \".gitignore\")| [.gitignore](https://github.com/scrapy/scrapy/blob/master/.gitignore \".gitignore\")| [Codecov: Add test analytics (](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")[#6741](https://github.com/scrapy/scrapy/pull/6741)[)](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")| Mar 21, 2025  \n[.pre-commit-config.yaml](https://github.com/scrapy/scrapy/blob/master/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [.pre-commit-config.yaml](https://github.com/scrapy/scrapy/blob/master/.pre-commit-config.yaml \".pre-commit-config.yaml\")| [Remove trailing whitespace](https://github.com/scrapy/scrapy/commit/bee74fb753afa373ecdb1dbf272af9a108b14b22 \"Remove trailing whitespace\")| Mar 11, 2025  \n[.readthedocs.yml](https://github.com/scrapy/scrapy/blob/master/.readthedocs.yml \".readthedocs.yml\")| [.readthedocs.yml](https://github.com/scrapy/scrapy/blob/master/.readthedocs.yml \".readthedocs.yml\")| [chore(docs): refactor config (](https://github.com/scrapy/scrapy/commit/e0c828b7f665d8c82e17787996634bc072e416ae \"chore\\(docs\\): refactor config \\(#6623\\)\")[#6623](https://github.com/scrapy/scrapy/pull/6623)[)](https://github.com/scrapy/scrapy/commit/e0c828b7f665d8c82e17787996634bc072e416ae \"chore\\(docs\\): refactor config \\(#6623\\)\")| Jan 20, 2025  \n[AUTHORS](https://github.com/scrapy/scrapy/blob/master/AUTHORS \"AUTHORS\")| [AUTHORS](https://github.com/scrapy/scrapy/blob/master/AUTHORS \"AUTHORS\")| [Scrapinghub \u2192 Zyte](https://github.com/scrapy/scrapy/commit/f30f53b3cc958770406628411db3d93c925db59e \"Scrapinghub \u2192 Zyte\")| Feb 2, 2021  \n[CODE_OF_CONDUCT.md](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| [CODE_OF_CONDUCT.md](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md \"CODE_OF_CONDUCT.md\")| [Update Code of Conduct to Contributor Covenant v2.1](https://github.com/scrapy/scrapy/commit/9f45be439de8a3b9a6d201c33e98b408a73c02bb \"Update Code of Conduct to Contributor Covenant v2.1\")| Oct 27, 2022  \n[CONTRIBUTING.md](https://github.com/scrapy/scrapy/blob/master/CONTRIBUTING.md \"CONTRIBUTING.md\")| [CONTRIBUTING.md](https://github.com/scrapy/scrapy/blob/master/CONTRIBUTING.md \"CONTRIBUTING.md\")| [Be consistent with domain used for links to documentation website](https://github.com/scrapy/scrapy/commit/65d631329a1434ec013f24341e4b8520241aec70 \"Be consistent with domain used for links to documentation website\")| Jan 30, 2019  \n[INSTALL.md](https://github.com/scrapy/scrapy/blob/master/INSTALL.md \"INSTALL.md\")| [INSTALL.md](https://github.com/scrapy/scrapy/blob/master/INSTALL.md \"INSTALL.md\")| [Update and rename INSTALL to INSTALL.md](https://github.com/scrapy/scrapy/commit/e60e8224a23ddd4fd98c54318d128f6744f40a6a \"Update and rename INSTALL to INSTALL.md\")| Oct 6, 2022  \n[LICENSE](https://github.com/scrapy/scrapy/blob/master/LICENSE \"LICENSE\")| [LICENSE](https://github.com/scrapy/scrapy/blob/master/LICENSE \"LICENSE\")| [added oxford commas to LICENSE](https://github.com/scrapy/scrapy/commit/9bd5444a42e82a05d2791fd10a205a9dd99303a6 \"added oxford commas to LICENSE\")| Jun 1, 2018  \n[MANIFEST.in](https://github.com/scrapy/scrapy/blob/master/MANIFEST.in \"MANIFEST.in\")| [MANIFEST.in](https://github.com/scrapy/scrapy/blob/master/MANIFEST.in \"MANIFEST.in\")| [Integrating configs into pyproject.toml (](https://github.com/scrapy/scrapy/commit/8c23da943c5e892515f4fa2eb57229839802010a \"Integrating configs into pyproject.toml \\(#6547\\)\")[#6547](https://github.com/scrapy/scrapy/pull/6547)[)](https://github.com/scrapy/scrapy/commit/8c23da943c5e892515f4fa2eb57229839802010a \"Integrating configs into pyproject.toml \\(#6547\\)\")| Nov 19, 2024  \n[NEWS](https://github.com/scrapy/scrapy/blob/master/NEWS \"NEWS\")| [NEWS](https://github.com/scrapy/scrapy/blob/master/NEWS \"NEWS\")| [added NEWS file pointing to docs/news.rst](https://github.com/scrapy/scrapy/commit/f5b87dbef8f3c657e31eeb026d8a77b0ccc77c29 \"added NEWS file pointing to docs/news.rst\")| Apr 28, 2012  \n[README.rst](https://github.com/scrapy/scrapy/blob/master/README.rst \"README.rst\")| [README.rst](https://github.com/scrapy/scrapy/blob/master/README.rst \"README.rst\")| [Remove trailing whitespace](https://github.com/scrapy/scrapy/commit/bee74fb753afa373ecdb1dbf272af9a108b14b22 \"Remove trailing whitespace\")| Mar 11, 2025  \n[SECURITY.md](https://github.com/scrapy/scrapy/blob/master/SECURITY.md \"SECURITY.md\")| [SECURITY.md](https://github.com/scrapy/scrapy/blob/master/SECURITY.md \"SECURITY.md\")| [Bump version: 2.11.2 \u2192 2.12.0](https://github.com/scrapy/scrapy/commit/b1f9e56693cd2000ddcea922306f726f3e9339af \"Bump version: 2.11.2 \u2192 2.12.0\")| Nov 18, 2024  \n[codecov.yml](https://github.com/scrapy/scrapy/blob/master/codecov.yml \"codecov.yml\")| [codecov.yml](https://github.com/scrapy/scrapy/blob/master/codecov.yml \"codecov.yml\")| [codecov config: disable project check, tweak PR comments](https://github.com/scrapy/scrapy/commit/9ce03d096d7ed569071a1951d9e327d7294e1a83 \"codecov config: disable project check, tweak PR comments\")| May 18, 2017  \n[conftest.py](https://github.com/scrapy/scrapy/blob/master/conftest.py \"conftest.py\")| [conftest.py](https://github.com/scrapy/scrapy/blob/master/conftest.py \"conftest.py\")| [Fix running simple tests with --reactor=default. (](https://github.com/scrapy/scrapy/commit/872924721344502cca92a72ce68d5f78a9b9e5bc \"Fix running simple tests with --reactor=default. \\(#6735\\)\")[#6735](https://github.com/scrapy/scrapy/pull/6735)[)](https://github.com/scrapy/scrapy/commit/872924721344502cca92a72ce68d5f78a9b9e5bc \"Fix running simple tests with --reactor=default. \\(#6735\\)\")| Mar 15, 2025  \n[pyproject.toml](https://github.com/scrapy/scrapy/blob/master/pyproject.toml \"pyproject.toml\")| [pyproject.toml](https://github.com/scrapy/scrapy/blob/master/pyproject.toml \"pyproject.toml\")| [Convert remaining unittest assert* calls, use the tmp_path fixture. (](https://github.com/scrapy/scrapy/commit/eb654aa1a8d2ef6433957fcc1361420b6141094e \"Convert remaining unittest assert* calls, use the tmp_path fixture. \\(#6725\\)\")[#\u2026](https://github.com/scrapy/scrapy/pull/6725)| Mar 11, 2025  \n[tox.ini](https://github.com/scrapy/scrapy/blob/master/tox.ini \"tox.ini\")| [tox.ini](https://github.com/scrapy/scrapy/blob/master/tox.ini \"tox.ini\")| [Codecov: Add test analytics (](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")[#6741](https://github.com/scrapy/scrapy/pull/6741)[)](https://github.com/scrapy/scrapy/commit/e50914e0f5b98ee4c9cb1f182ec2cd684fdf9900 \"Codecov: Add test analytics \\(#6741\\)\")| Mar 21, 2025  \nView all files  \n## Repository files navigation\n  * [README](https://github.com/scrapy/scrapy/)\n  * [Code of conduct](https://github.com/scrapy/scrapy/)\n  * [BSD-3-Clause license](https://github.com/scrapy/scrapy/)\n  * [Security](https://github.com/scrapy/scrapy/)\n\n\n[![https://scrapy.org/img/scrapylogo.png](https://camo.githubusercontent.com/664eb5eae435b24275fd1740cb050d5079478554958dc25f44a063b40ee65551/68747470733a2f2f7363726170792e6f72672f696d672f7363726170796c6f676f2e706e67)](https://scrapy.org/)\n## Scrapy\n[](https://github.com/scrapy/scrapy/#scrapy)\n[![PyPI Version](https://camo.githubusercontent.com/7f23a6a9b0298efa3e2944ff9502a91459d70dd0d5244cb444fc342e1533c8f6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f5363726170792e737667) ](https://pypi.org/pypi/Scrapy) [![Supported Python Versions](https://camo.githubusercontent.com/6bfde535940031459b168bf7591e43cac9d5e0b3d1209f282ca1624cd63e4d14/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f5363726170792e737667) ](https://pypi.org/pypi/Scrapy) [![Ubuntu](https://github.com/scrapy/scrapy/workflows/Ubuntu/badge.svg) ](https://github.com/scrapy/scrapy/actions?query=workflow%3AUbuntu) [![Windows](https://github.com/scrapy/scrapy/workflows/Windows/badge.svg) ](https://github.com/scrapy/scrapy/actions?query=workflow%3AWindows) [![Wheel Status](https://camo.githubusercontent.com/55f433059ee431d5b2ffd26fdea757a2b88923121950c6840987f5563900e40f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f776865656c2d7965732d627269676874677265656e2e737667) ](https://pypi.org/pypi/Scrapy) [![Coverage report](https://camo.githubusercontent.com/cbe200ed5f89e6f0076d98a4d7f2deac81dcf7db4bad2401a83337a7d136ed4e/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f7363726170792f7363726170792f6d61737465722e737667) ](https://codecov.io/github/scrapy/scrapy?branch=master) [![Conda Version](https://camo.githubusercontent.com/32de2f70318d9be0205e95ca2ff5c2946a26b7e3519890747e4d37c0aa709526/68747470733a2f2f616e61636f6e64612e6f72672f636f6e64612d666f7267652f7363726170792f6261646765732f76657273696f6e2e737667) ](https://anaconda.org/conda-forge/scrapy)\n### Overview\n[](https://github.com/scrapy/scrapy/#overview)\nScrapy is a BSD-licensed fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.\nScrapy is maintained by [Zyte](https://www.zyte.com/) (formerly Scrapinghub) and [many other contributors](https://github.com/scrapy/scrapy/graphs/contributors).\nCheck the Scrapy homepage at <https://scrapy.org> for more information, including a list of features.\n### Requirements\n[](https://github.com/scrapy/scrapy/#requirements)\n  * Python 3.9+\n  * Works on Linux, Windows, macOS, BSD\n\n\n### Install\n[](https://github.com/scrapy/scrapy/#install)\nThe quick way:\n```\npip install scrapy\n```\n\nSee the install section in the documentation at <https://docs.scrapy.org/en/latest/intro/install.html> for more details.\n### Documentation\n[](https://github.com/scrapy/scrapy/#documentation)\nDocumentation is available online at <https://docs.scrapy.org/> and in the `docs` directory.\n### Releases\n[](https://github.com/scrapy/scrapy/#releases)\nYou can check <https://docs.scrapy.org/en/latest/news.html> for the release notes.\n### Community (blog, twitter, mail list, IRC)\n[](https://github.com/scrapy/scrapy/#community-blog-twitter-mail-list-irc)\nSee <https://scrapy.org/community/> for details.\n### Contributing\n[](https://github.com/scrapy/scrapy/#contributing)\nSee <https://docs.scrapy.org/en/master/contributing.html> for details.\n#### Code of Conduct\n[](https://github.com/scrapy/scrapy/#code-of-conduct)\nPlease note that this project is released with a Contributor [Code of Conduct](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md).\nBy participating in this project you agree to abide by its terms. Please report unacceptable behavior to opensource@zyte.com.\n### Companies using Scrapy\n[](https://github.com/scrapy/scrapy/#companies-using-scrapy)\nSee <https://scrapy.org/companies/> for a list.\n### Commercial Support\n[](https://github.com/scrapy/scrapy/#commercial-support)\nSee <https://scrapy.org/support/> for details.\n## About\nScrapy, a fast high-level web crawling & scraping framework for Python. \n[scrapy.org](https://scrapy.org \"https://scrapy.org\")\n### Topics\n[ python ](https://github.com/topics/python \"Topic: python\") [ crawler ](https://github.com/topics/crawler \"Topic: crawler\") [ framework ](https://github.com/topics/framework \"Topic: framework\") [ scraping ](https://github.com/topics/scraping \"Topic: scraping\") [ crawling ](https://github.com/topics/crawling \"Topic: crawling\") [ web-scraping ](https://github.com/topics/web-scraping \"Topic: web-scraping\") [ hacktoberfest ](https://github.com/topics/hacktoberfest \"Topic: hacktoberfest\") [ web-scraping-python ](https://github.com/topics/web-scraping-python \"Topic: web-scraping-python\")\n### Resources\n[ Readme ](https://github.com/scrapy/scrapy/#readme-ov-file)\n### License\n[ BSD-3-Clause license ](https://github.com/scrapy/scrapy/#BSD-3-Clause-1-ov-file)\n### Code of conduct\n[ Code of conduct ](https://github.com/scrapy/scrapy/#coc-ov-file)\n### Security policy\n[ Security policy ](https://github.com/scrapy/scrapy/#security-ov-file)\n[ Activity](https://github.com/scrapy/scrapy/activity)\n[ Custom properties](https://github.com/scrapy/scrapy/custom-properties)\n### Stars\n[ **54.7k** stars](https://github.com/scrapy/scrapy/stargazers)\n### Watchers\n[ **1.8k** watching](https://github.com/scrapy/scrapy/watchers)\n### Forks\n[ **10.7k** forks](https://github.com/scrapy/scrapy/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fscrapy%2Fscrapy&report=scrapy+%28user%29)\n##  [Releases 39](https://github.com/scrapy/scrapy/releases)\n[ 2.12.0 Latest  Nov 18, 2024 ](https://github.com/scrapy/scrapy/releases/tag/2.12.0)\n[+ 38 releases](https://github.com/scrapy/scrapy/releases)\n##  [Packages 0](https://github.com/orgs/scrapy/packages?repo_name=scrapy)\nNo packages published \n##  [Used by 52.2k](https://github.com/scrapy/scrapy/network/dependents)\n[\n  * ![@PHFernandes9](https://avatars.githubusercontent.com/u/194980440?s=64&v=4)\n  * ![@Victor-Genius](https://avatars.githubusercontent.com/u/200241745?s=64&v=4)\n  * ![@Ayaaayaaa](https://avatars.githubusercontent.com/u/146444257?s=64&v=4)\n  * ![@maichi98](https://avatars.githubusercontent.com/u/58106854?s=64&v=4)\n  * ![@hiteshsurya17](https://avatars.githubusercontent.com/u/83537863?s=64&v=4)\n  * ![@ximengailun](https://avatars.githubusercontent.com/u/145632297?s=64&v=4)\n  * ![@moizez256](https://avatars.githubusercontent.com/u/28323489?s=64&v=4)\n  * ![@dkffkrsm](https://avatars.githubusercontent.com/u/47800954?s=64&v=4)\n\n+ 52,146  ](https://github.com/scrapy/scrapy/network/dependents)\n##  [Contributors 577](https://github.com/scrapy/scrapy/graphs/contributors)\n  * [ ![@pablohoffman](https://avatars.githubusercontent.com/u/185212?s=64&v=4) ](https://github.com/pablohoffman)\n  * [ ![@dangra](https://avatars.githubusercontent.com/u/37369?s=64&v=4) ](https://github.com/dangra)\n  * [ ![@wRAR](https://avatars.githubusercontent.com/u/241039?s=64&v=4) ](https://github.com/wRAR)\n  * [ ![@kmike](https://avatars.githubusercontent.com/u/107893?s=64&v=4) ](https://github.com/kmike)\n  * [ ![@Gallaecio](https://avatars.githubusercontent.com/u/705211?s=64&v=4) ](https://github.com/Gallaecio)\n  * [ ![@elacuesta](https://avatars.githubusercontent.com/u/1731933?s=64&v=4) ](https://github.com/elacuesta)\n  * [ ![@redapple](https://avatars.githubusercontent.com/u/886296?s=64&v=4) ](https://github.com/redapple)\n  * [ ![@void](https://avatars.githubusercontent.com/u/25639?s=64&v=4) ](https://github.com/void)\n  * [ ![@curita](https://avatars.githubusercontent.com/u/213781?s=64&v=4) ](https://github.com/curita)\n  * [ ![@eliasdorneles](https://avatars.githubusercontent.com/u/37565?s=64&v=4) ](https://github.com/eliasdorneles)\n  * [ ![@lopuhin](https://avatars.githubusercontent.com/u/424613?s=64&v=4) ](https://github.com/lopuhin)\n  * [ ![@Laerte](https://avatars.githubusercontent.com/u/5853172?s=64&v=4) ](https://github.com/Laerte)\n  * [ ![@whalebot-helmsman](https://avatars.githubusercontent.com/u/1234272?s=64&v=4) ](https://github.com/whalebot-helmsman)\n  * [ ![@adityaa30](https://avatars.githubusercontent.com/u/37837542?s=64&v=4) ](https://github.com/adityaa30)\n\n\n[+ 563 contributors](https://github.com/scrapy/scrapy/graphs/contributors)\n## Languages\n  * [ Python 99.8% ](https://github.com/scrapy/scrapy/search?l=python)\n  * Other 0.2%\n\n\n## Footer\n[ ](https://github.com \"GitHub\") \u00a9 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nYou can\u2019t perform that action at this time. \n",
    "answer": "```markdown\n## TL;DR Summary\n\n**Scrapy** is a high-level web crawling and scraping framework for Python, maintained by **Zyte**. It supports Python 3.9+ and is used for data mining, monitoring, and automated testing. Installation is simple via `pip install scrapy`. More info at [scrapy.org](https://scrapy.org).\n```"
  },
  {
    "instruction": "[Skip to main content](https://arxiv.org/abs/2303.11366?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN/#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/) > [cs](https://arxiv.org/list/cs/recent) > arXiv:2303.11366 \n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[ ![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg) ](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n## quick links\n  * [Login](https://arxiv.org/login)\n  * [Help Pages](https://info.arxiv.org/help)\n  * [About](https://info.arxiv.org/about)\n\n\n# Computer Science > Artificial Intelligence\n**arXiv:2303.11366** (cs) \n[Submitted on 20 Mar 2023 ([v1](https://arxiv.org/abs/2303.11366v1)), last revised 10 Oct 2023 (this version, v4)]\n# Title:Reflexion: Language Agents with Verbal Reinforcement Learning\nAuthors:[Noah Shinn](https://arxiv.org/search/cs?searchtype=author&query=Shinn,+N), [Federico Cassano](https://arxiv.org/search/cs?searchtype=author&query=Cassano,+F), [Edward Berman](https://arxiv.org/search/cs?searchtype=author&query=Berman,+E), [Ashwin Gopinath](https://arxiv.org/search/cs?searchtype=author&query=Gopinath,+A), [Karthik Narasimhan](https://arxiv.org/search/cs?searchtype=author&query=Narasimhan,+K), [Shunyu Yao](https://arxiv.org/search/cs?searchtype=author&query=Yao,+S)\nView a PDF of the paper titled Reflexion: Language Agents with Verbal Reinforcement Learning, by Noah Shinn and 5 other authors\n[View PDF](https://arxiv.org/pdf/2303.11366)\n> Abstract:Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance. \nComments: | v4 contains a few additional experiments  \n---|---  \nSubjects: |  Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)  \nCite as: | [arXiv:2303.11366](https://arxiv.org/abs/2303.11366) [cs.AI]  \n(or  [arXiv:2303.11366v4](https://arxiv.org/abs/2303.11366v4) [cs.AI] for this version)   \n<https://doi.org/10.48550/arXiv.2303.11366> Focus to learn more arXiv-issued DOI via DataCite  \n## Submission history\nFrom: Noah Shinn [[view email](https://arxiv.org/show-email/04212f49/2303.11366)] **[[v1]](https://arxiv.org/abs/2303.11366v1)** Mon, 20 Mar 2023 18:08:50 UTC (506 KB) **[[v2]](https://arxiv.org/abs/2303.11366v2)** Sun, 21 May 2023 06:20:36 UTC (404 KB) **[[v3]](https://arxiv.org/abs/2303.11366v3)** Sat, 10 Jun 2023 04:32:30 UTC (396 KB) **[v4]** Tue, 10 Oct 2023 05:21:45 UTC (386 KB) \nFull-text links:\n## Access Paper:\nView a PDF of the paper titled Reflexion: Language Agents with Verbal Reinforcement Learning, by Noah Shinn and 5 other authors\n  * [View PDF](https://arxiv.org/pdf/2303.11366)\n  * [TeX Source](https://arxiv.org/src/2303.11366)\n  * [Other Formats](https://arxiv.org/format/2303.11366)\n\n\n[ ![license icon](https://arxiv.org/icons/licenses/by-4.0.png) view license ](http://creativecommons.org/licenses/by/4.0/ \"Rights to this article\")\nCurrent browse context: \ncs.AI\n[< prev](https://arxiv.org/prevnext?id=2303.11366&function=prev&context=cs.AI \"previous in cs.AI \\(accesskey p\\)\") |  [next >](https://arxiv.org/prevnext?id=2303.11366&function=next&context=cs.AI \"next in cs.AI \\(accesskey n\\)\")\n[new](https://arxiv.org/list/cs.AI/new) |  [recent](https://arxiv.org/list/cs.AI/recent) | [2023-03](https://arxiv.org/list/cs.AI/2023-03)\nChange to browse by: \n[cs](https://arxiv.org/abs/2303.11366?context=cs) [cs.CL](https://arxiv.org/abs/2303.11366?context=cs.CL) [cs.LG](https://arxiv.org/abs/2303.11366?context=cs.LG)\n### References & Citations\n  * [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2303.11366)\n  * [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2303.11366)\n  * [Semantic Scholar](https://api.semanticscholar.org/arXiv:2303.11366)\n\n\n### [ 3 blog links](https://arxiv.org/tb/2303.11366)\n([what is this?](https://info.arxiv.org/help/trackback.html)) \n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citation Loading...\n## BibTeX formatted citation\n\u00d7\nloading...\nData provided by: \n### Bookmark\n[ ![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png) ](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2303.11366&description=Reflexion: Language Agents with Verbal Reinforcement Learning \"Bookmark on BibSonomy\") [ ![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png) ](https://reddit.com/submit?url=https://arxiv.org/abs/2303.11366&title=Reflexion: Language Agents with Verbal Reinforcement Learning \"Bookmark on Reddit\")\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\nConnected Papers Toggle\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\nLitmaps Toggle\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\nscite.ai Toggle\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_\nDagsHub Toggle\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\nGotitPub Toggle\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\nHuggingface Toggle\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\nLinks to Code Toggle\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\nScienceCast Toggle\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\nDemos\n# Demos\nReplicate Toggle\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\nSpaces Toggle\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\nSpaces Toggle\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\nCore recommender toggle\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n\nAbout arXivLabs \n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2303.11366) | [Disable MathJax](javascript:setMathjaxCookie\\(\\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n  * [About](https://info.arxiv.org/about)\n  * [Help](https://info.arxiv.org/help)\n\n\n  * contact arXivClick here to contact arXiv [ Contact](https://info.arxiv.org/help/contact.html)\n  * subscribe to arXiv mailingsClick here to subscribe [ Subscribe](https://info.arxiv.org/help/subscribe)\n\n\n  * [Copyright](https://info.arxiv.org/help/license/index.html)\n  * [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n\n  * [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n  * [arXiv Operational Status ](https://status.arxiv.org) Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n\n\n",
    "answer": "```markdown\n# TL;DR: Reflexion: Language Agents with Verbal Reinforcement Learning\n\n**Authors:** Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao  \n**Key Insight:** Reflexion introduces a framework for language agents that uses verbal feedback instead of traditional reinforcement learning, achieving a 91% pass rate on the HumanEval coding benchmark, outperforming GPT-4.  \n**Tags:** Generative AI, LLMs, Reinforcement Learning, Artificial Intelligence  \n```"
  },
  {
    "instruction": "![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61f958575ff62d320d46701e_closeicon.png)\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\nKindly note that during the maintenance window, app.hopsworks.ai will not be accessible.\n5\n[View the Changes](https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes)\n![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61f958575ff62d320d46701e_closeicon.png)\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\nKindly note that during the maintenance window, app.hopsworks.ai will not be accessible.\n5\n[View the Changes](https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes)\nScheduled upgrade from November 26, 07:00 UTC to November 26, 17:00 UTC\n[Contact](https://www.hopsworks.ai/contact/main)[Login](https://app.hopsworks.ai)[![Github Mark](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6418216a570b0da3d471661a_icons8-slack-new.svg)](https://join.slack.com/t/public-hopsworks/shared_invite/zt-1uf21vitz-rhHKNdIf8GEiOf1EJ6Wzsw)[![Github Mark](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/62261cde4669f63d3880938d_github.svg)](https://github.com/logicalclocks/hopsworks)[![linkedin logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/61926637aaba4d3968d7956d_linkedin.svg)](https://www.linkedin.com/company/hopsworks/)[![Twitter icon](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/66a346ea27ec6d7c0e354747_icons8-twitter%20\\(1\\).svg)](https://twitter.com/hopsworks)\n[![Untitled UI logotext](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/6202a13e7cafec5553703f6b_logo.svg)![Logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/666c3cc1cfc4741e6b2d9fe6_untitled-ui-logo.png)](https://www.hopsworks.ai/)\nProduct\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/5fad49f715203ed9d66fc1b0_Hops%20Icon%20Green.png)Hopsworks EnterpriseFull edition of Hopsworks, high levels of SLAs and support.](https://www.hopsworks.ai/try)[IntegrationsLeverage your existing data sources and tools.](https://www.hopsworks.ai/integrations)[ExamplesGet up and running on new features and techniques.](https://www.hopsworks.ai/hopsworks-examples)[FAQAll you need to know about Hopsworks.](https://www.hopsworks.ai/frequently-asked-questions)\n[Hopsworks On-PremisesManage everything securely within your data center.](https://www.hopsworks.ai/product-capabilities/feature-store-on-premises)[Performance & High AvailabilityHighest performance requirements in the industry.](https://www.hopsworks.ai/product-capabilities/operational-performance-and-high-availability)[Feature Engineering in PythonPython-first collaborative environment.](https://www.hopsworks.ai/product-capabilities/feature-engineering-in-python)[Other capabilitiesRead about our extended platform capabilities.](https://www.hopsworks.ai/product-capabilities)\nSolutions\nFor your Team\n[Technical Stakeholders](https://www.hopsworks.ai/solutions/teams)[Machine Learning Engineers](https://www.hopsworks.ai/solutions/teams?tab=ml-engineers)[Data Engineers](https://www.hopsworks.ai/solutions/teams?tab=data-engineers)[Data Scientists](https://www.hopsworks.ai/solutions/teams?tab=data-scientists)[DevOps](https://www.hopsworks.ai/solutions/teams?tab=devops)[Architects](https://www.hopsworks.ai/solutions/teams?tab=architects)[Non-Technical Stakeholders](https://www.hopsworks.ai/solutions/teams?tab=non-technical)[Project Managers ](https://www.hopsworks.ai/solutions/teams?tab=non-technical)\nFor your Industry\n[Defense and Law Enforcement](https://www.hopsworks.ai/solutions/industry/defense-and-law-enforcement)\n[FSI](https://www.hopsworks.ai/solutions/industry/fsi)\n[Online Retail & E-commerce](https://www.hopsworks.ai/solutions/industry/online-retail-and-e-commerce)\n[Public Sector](https://www.hopsworks.ai/solutions/industry/public-sector)\n[Research and Healthcare](https://www.hopsworks.ai/solutions/industry/research-and-healthcare)\n[iGaming](https://www.hopsworks.ai/solutions/industry/i-gaming)\n[All Solutions](https://www.hopsworks.ai/solutions/all)\nUse Cases\n[Generative AI](https://www.hopsworks.ai/use-case/fine-tuning-llms-rag-for-genai)[Real-time Fraud Detection](https://www.hopsworks.ai/use-case/realtime-fraud-detection)[Hopsworks Medical Copilot](https://www.hopscopilot.com/)\n[CustomersExplore how our customers leverage Hopsworks.](https://www.hopsworks.ai/customers)\n[Pricing](https://www.hopsworks.ai/pricing)[Blog](https://www.hopsworks.ai/blog)[Pricing](https://www.hopsworks.ai/pricing)[Blog](https://www.hopsworks.ai/blog)\nResources\n[MLOps DictionaryComprehensive terminology guide for ML solutions.](https://www.hopsworks.ai/mlops-dictionary)[DocumentationDetailed information to help you effectively utilize Hopsworks.](https://docs.hopsworks.ai/latest/)[Research PapersDiscover how our research is driving innovation.](https://www.hopsworks.ai/research-papers)[CommunityJoin our community and get all your questions answered. ](https://community.hopsworks.ai/)\n[EventsOnline & Offline sessions and workshops. ](https://www.hopsworks.ai/events)[AcademyEverything about ML Systems, and the Hopsworks platform.](https://www.hopsworks.ai/academy)[Feature Store ComparisonIn-depth comparisons of feature stores highlighting key features.](https://www.hopsworks.ai/product-comparison/sagemaker)[FAQ: EU AI ActA complete guide to The EU AI Act.](https://www.hopsworks.ai/faq-eu-ai-act)\nCompany\n[About usLearn more about our team. ](https://www.hopsworks.ai/about-us)[NewsThe latest industry news, updates and info.](https://www.hopsworks.ai/news)[Security & ComplianceRobust security and compliance with industry standards.](https://www.hopsworks.ai/security-compliance)\n[![](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/66a0b13c473a71304470c35a_oreilly_logo_mark_red.svg)Book](https://www.hopsworks.ai/lp/oreilly-book-building-ml-systems-with-a-feature-store)[Benchmarks](https://www.hopsworks.ai/index#performance)\n[// 5-minute Interviews](https://www.hopsworks.ai/blog-categories/5-minute-interviews)\n[// AI](https://www.hopsworks.ai/blog-categories/ai)\n[// Benchmark](https://www.hopsworks.ai/blog-categories/benchmark)\n[// Data Engineering](https://www.hopsworks.ai/blog-categories/data-engineering)\n[// Data Science](https://www.hopsworks.ai/blog-categories/data-science)\n[// Feature Store](https://www.hopsworks.ai/blog-categories/feature-stores)\n[// MLOps](https://www.hopsworks.ai/blog-categories/mlops)\n[// SIGMOD 2024](https://www.hopsworks.ai/blog-categories/sigmod-2024)\n[![The 10 Fallacies of MLOps](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bdbdb52730076471457479_MLOps%20Fallacies%20Blog.png)MLOpsFebruary 25, 202520 minReadThe 10 Fallacies of MLOpsMLOps fallacies can slow AI deployment and add complexity. This blog breaks down 10 common misconceptions, their impact, and how to avoid them to build scalable, production-ready AI systems efficient.Jim Dowling](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)\n[Migrating from AWS to a European Cloud - How We Cut Costs by 62%This post describes how we successfully migrated our serverless offering from AWS US-East to OVHCloud North America, reducing our monthly spend from $8,000 to $3,000 with no loss in service quality. Jim Dowling](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)\n[Migrating Hopsworks to KubernetesNearly a year ago, the Hopsworks team embarked on a journey to migrate its infrastructure to Kubernetes. In this article we describe three main pillars of our Kubernetes migration.Javier Cabrera](https://www.hopsworks.ai/post/migrating-hopsworks-to-kubernetes)\n[Introducing the AI LakehouseWe describe the capabilities that need to be added to Lakehouse to make it an AI Lakehouse that can support building and operating AI-enabled batch and real-time applications as well LLM applications.Jim Dowling](https://www.hopsworks.ai/post/the-ai-lakehouse)\n### // SIGMOD 2024\nA series of articles describing concepts and results from the [**SIGMOD 2024 research paper on the Hopsworks Feature Store**](https://www.hopsworks.ai/research-papers/the-hopsworks-feature-store-for-machine-learning)\n[View all posts](https://www.hopsworks.ai/blog/)\n[![Modularity and Composability for AI Systems with AI Pipelines and Shared Storage](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/667acae0ae637d5bb9a72ea2_modularity%20%26%20composability.png)](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)[Feature StoreJune 25, 202425 minReadModularity and Composability for AI Systems with AI Pipelines and Shared StorageWe present a unified software architecture for batch, real-time, and LLM AI systems that is based on a shared storage layer and a decomposition of machine learning pipelines. ](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)\nJim Dowling\n[![The Taxonomy for Data Transformations in AI Systems](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/668d97122ea716ed1633a7f7_PR%20Blog%202_Taxonomy.png)](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)[Feature StoreJuly 1, 202430 minReadThe Taxonomy for Data Transformations in AI SystemsThis article introduces a taxonomy for data transformations in AI applications that is fundamental for any AI system that wants to reuse feature data in more than one model. ](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)\nManu Joseph\n[![The Journey from Star Schema to Snowflake Schema in the Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66869f1f4f75a7d91c67775f_PR%20Blog%203_snowflake%20schema.png)](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store)[Feature StoreJuly 8, 202414 minReadThe Journey from Star Schema to Snowflake Schema in the Feature StoreIn this article we introduce the snowflake schema data model for feature stores, and show how it helps you include more features to make better predictions ](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store)\nDavit Bzhalava\n[![The Feature Store Makes Your Data Warehouse Easy to Use for AI](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/668bf0ab60826eb58e0d6af1_PR%20Blog%204_data%20warehouse.png)](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai)[Feature StoreJuly 10, 202424 minReadThe Feature Store Makes Your Data Warehouse Easy to Use for AIIn this article, we cover the added value of a feature store over a data warehouse when managing offline data for AI. ](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai)\nJim Dowling\n[![From Lakehouse to AI Lakehouse with a Python-Native Query Engine](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/669fa7eaa10b0dd15c46ab80_Lakehouse%20PQE%20Blog.png)](https://www.hopsworks.ai/post/from-lakehouse-to-ai-lakehouse-with-a-python-native-query-engine)[Data EngineeringJuly 23, 20249 minReadFrom Lakehouse to AI Lakehouse with a Python-Native Query EngineRead how Hopsworks generates temporal queries from Python code, and how a native query engine built on Arrow can massively outperform JDBC/ODBC APIs.](https://www.hopsworks.ai/post/from-lakehouse-to-ai-lakehouse-with-a-python-native-query-engine)\nJim Dowling\n[![RonDB: A Real-Time Database for Real-Time AI Systems](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b207bd4cf6636717910f8b_Real%20time%20Database%20Blog.png)](https://www.hopsworks.ai/post/rondb-a-real-time-database-for-real-time-ai-systems)[SIGMOD 2024July 25, 20248 minReadRonDB: A Real-Time Database for Real-Time AI SystemsLearn more about how Hopsworks (RonDB) outperforms AWS Sagemaker and GCP Vertex in latency for real-time AI databases, based on a peer-reviewed SIGMOD 2024 benchmark.](https://www.hopsworks.ai/post/rondb-a-real-time-database-for-real-time-ai-systems)\nMikael Ronstr\u00f6m\n[![Reproducible Data for the AI Lakehouse](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b1d5bb06a34c527b352841_reproducible%20data.png)](https://www.hopsworks.ai/post/reproducible-data-for-the-ai-lakehouse)[SIGMOD 2024August 6, 202413 minReadReproducible Data for the AI LakehouseWe present how Hopsworks leverages its time-travel capabilities for feature groups to support reproducible creation of training datasets using metadata.](https://www.hopsworks.ai/post/reproducible-data-for-the-ai-lakehouse)\nJim Dowling\n## Explore by Category\n[SIGMOD 2024](https://www.hopsworks.ai/blog-categories/sigmod-2024)\n[AI](https://www.hopsworks.ai/blog-categories/ai)\n[5-minute Interviews](https://www.hopsworks.ai/blog-categories/5-minute-interviews)\n[Benchmark](https://www.hopsworks.ai/blog-categories/benchmark)\n[Data Science](https://www.hopsworks.ai/blog-categories/data-science)\n[Data Engineering](https://www.hopsworks.ai/blog-categories/data-engineering)\n[![The 10 Fallacies of MLOps](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bdbdb52730076471457479_MLOps%20Fallacies%20Blog.png)](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)[MLOpsFebruary 25, 202520 minReadThe 10 Fallacies of MLOpsMLOps fallacies can slow AI deployment and add complexity. This blog breaks down 10 common misconceptions, their impact, and how to avoid them to build scalable, production-ready AI systems efficient.](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)\nJim Dowling\n[![Hopsworks AI Lakehouse: The Power of Integrated MLOps Components](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bc4ae4464caed2e2bff646_AI%20lakehouse%20Blog.png)](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)[AIFebruary 24, 20259 minReadHopsworks AI Lakehouse: The Power of Integrated MLOps ComponentsHopsworks AI Lakehouse simplifies MLOps by eliminating integration overhead with a pre-integrated, modular platform that connects seamlessly to existing AI ecosystems. ](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)\nRik Van Bruggen\n### // Latest Posts\n[![Migrating from AWS to a European Cloud - How We Cut Costs by 62%](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d4012dfee4417ef189aa94_AWS%20to%20OVH%20blog%20v2.png)](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)[Data EngineeringMarch 11, 202510minReadMigrating from AWS to a European Cloud - How We Cut Costs by 62%This post describes how we successfully migrated our serverless offering from AWS US-East to OVHCloud North America, reducing our monthly spend from $8,000 to $3,000 with no loss in service quality. ](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)\nJim Dowling\n[![Hopsworks AI Lakehouse: The Power of Integrated MLOps Components](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bc4ae4464caed2e2bff646_AI%20lakehouse%20Blog.png)](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)[AIFebruary 24, 20259 minReadHopsworks AI Lakehouse: The Power of Integrated MLOps ComponentsHopsworks AI Lakehouse simplifies MLOps by eliminating integration overhead with a pre-integrated, modular platform that connects seamlessly to existing AI ecosystems. ](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)\nRik Van Bruggen\n[![Unlocking the Power of AI in Government](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/679b54ad11fe440d84e8ece6_Government%20ai.png)](https://www.hopsworks.ai/post/the-power-of-ai-in-government)[AIJanuary 31, 20259 minReadUnlocking the Power of AI in GovernmentUnlock the transformative power of AI for government, enabling agencies to navigate the challenges, realize the benefits, and build a future-oriented public sector. ](https://www.hopsworks.ai/post/the-power-of-ai-in-government)\nRik Van Bruggen\n### // Feature Store\n[![](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d9dab74eb40a787c6b7c5e_why%20copy.jpg)](https://www.hopsworks.ai/post/why-do-you-need-a-feature-store)[Feature StoreAugust 11, 202320 minReadWhy Do You Need a Feature Store? Discover the power of feature stores in modern machine learning systems and how they bridge the gap between model development and production. ](https://www.hopsworks.ai/post/why-do-you-need-a-feature-store)\nLex Avstreikh\n[![](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65113a725a93dc40f6d74c85_duckdb_thumbnail.png)](https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb)[Feature StoreAugust 9, 202313 minReadFaster reading from the Lakehouse to Python with DuckDB/ArrowFlightIn this article, we outline how we leveraged ArrowFlight with DuckDB to build a new service that massively improves the performance of Python clients reading from lakehouse data in the Feature Store](https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb)\nTill D\u00f6hmen\n[![](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61bb72f4fbcc74f4c86eb3f8_61b8aed0674af6312e2c05b2_missingdatalayer.jpg)](https://www.hopsworks.ai/post/feature-store-the-missing-data-layer-in-ml-pipelines)[Feature StoreFebruary 2, 202330 minReadFeature Store: The missing data layer for Machine Learning pipelines?In this blog, we discuss the state-of-the-art in data management and machine learning pipelines (within the wider field of MLOps) and present the first open-source feature store, Hopsworks.](https://www.hopsworks.ai/post/feature-store-the-missing-data-layer-in-ml-pipelines)\nJim Dowling\n[View More](https://www.hopsworks.ai/blog-categories/feature-stores)\n### // MLOps\n[![The 10 Fallacies of MLOps](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bdbdb52730076471457479_MLOps%20Fallacies%20Blog.png)](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)[MLOpsFebruary 25, 202520 minReadThe 10 Fallacies of MLOpsMLOps fallacies can slow AI deployment and add complexity. This blog breaks down 10 common misconceptions, their impact, and how to avoid them to build scalable, production-ready AI systems efficient.](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)\nJim Dowling\n[![From MLOps to ML Systems with Feature/Training/Inference Pipelines](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65019f581267ff943c3931a6_mlops%20to%20ml%20systems_thumbnail.png)](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)[MLOpsSeptember 13, 202325 minReadFrom MLOps to ML Systems with Feature/Training/Inference PipelinesWe explain a new framework for ML systems as three independent ML pipelines: feature pipelines, training pipelines, and inference pipelines, creating a unified MLOps architecture.](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)\nJim Dowling\n[![MLOps vs. DevOps: Best Practices, Challenges and Differences](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d355fb6ba1878a9afd9066_Mlops%20Devops%20Blog.png)](https://www.hopsworks.ai/post/mlops-vs-devops-best-practices-challenges-and-differences)[MLOpsJuly 11, 202317ReadMLOps vs. DevOps: Best Practices, Challenges and DifferencesExplore the convergence of MLOps and DevOps. Learn about their purposes, differences, and areas of integration and discover best practices, challenges, and their future potential. ](https://www.hopsworks.ai/post/mlops-vs-devops-best-practices-challenges-and-differences)\nPrithivee Ramalingam\n[View More](https://www.hopsworks.ai/blog-categories/mlops)\n### // Data Engineering\n[![Migrating from AWS to a European Cloud - How We Cut Costs by 62%](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d4012dfee4417ef189aa94_AWS%20to%20OVH%20blog%20v2.png)](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)[Data EngineeringMarch 11, 202510minReadMigrating from AWS to a European Cloud - How We Cut Costs by 62%This post describes how we successfully migrated our serverless offering from AWS US-East to OVHCloud North America, reducing our monthly spend from $8,000 to $3,000 with no loss in service quality. ](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)\nJim Dowling\n[![Pandas2 and Polars for Feature Engineering](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/652709e8e9ee4bd6e7245f2d_pandas%26polars_thumbnail.png)](https://www.hopsworks.ai/post/pandas2-and-polars-for-feature-engineering)[Data EngineeringNovember 6, 202320 minReadPandas2 and Polars for Feature EngineeringWe review Python libraries, such as Pandas, Pandas2 and Polars, for Feature Engineering, evaluate their performance and explore how they power machine learning use cases. ](https://www.hopsworks.ai/post/pandas2-and-polars-for-feature-engineering)\nHaziqa Sajid\n[![Feature Engineering with Apache Airflow](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d3a7213fc689589547090e_airflow.png)](https://www.hopsworks.ai/post/feature-engineering-with-apache-airflow)[Data EngineeringSeptember 4, 202318 minReadFeature Engineering with Apache AirflowUnlock the power of Apache Airflow in the context of feature engineering. We will delve into building a feature pipeline using Airflow, focusing on two tasks: feature binning and aggregations.](https://www.hopsworks.ai/post/feature-engineering-with-apache-airflow)\nPrithivee Ramalingam\n[View More](https://www.hopsworks.ai/blog-categories/data-engineering)\n### // Data Science\n[![Facebook Prophet for Time-Series Machine Learning](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/651eac8c99616d72197da06a_facebook%20prophet_thumbnail.png)](https://www.hopsworks.ai/post/facebook-prophet-for-time-series-machine-learning)[Data ScienceOctober 5, 202317minReadFacebook Prophet for Time-Series Machine LearningTime-series data consists of records in chronological order and is crucial for forecasting trends and patterns. In this blog, we take a look at how to use Facebook Prophet for time-series forecasting.](https://www.hopsworks.ai/post/facebook-prophet-for-time-series-machine-learning)\nKais Laribi \n[![Elasticsearch is dead, long live Open Distro for Elasticsearch](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61e697708af5d9c46a0395a6_DFS.png)](https://www.hopsworks.ai/post/open-distro-for-elasticsearch-dynamic-support-hopsworks)[Data ScienceJanuary 14, 202113 minReadElasticsearch is dead, long live Open Distro for ElasticsearchIn this blog, we describe how we leverage the authentication and authorization support in Open Distro for Elasticsearch to make elasticsearch a project-based multi-tenant service in Hopsworks.](https://www.hopsworks.ai/post/open-distro-for-elasticsearch-dynamic-support-hopsworks)\nMahmoud Ismail\n[![Deep Learning for Anti-Money Laundering with a feature store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61b9d4f49a1b41665ee189ef_005%20\\(1\\).png)](https://www.hopsworks.ai/post/ai-deep-learning-for-fraud-and-aml)[Data ScienceNovember 27, 201918 minReadDeep Learning for Anti-Money Laundering with a feature storeDeep learning is now the state-of-the-art technique for identifying financial transactions suspected of money laundering. It delivers a lower number of false positives and with higher accuracy.](https://www.hopsworks.ai/post/ai-deep-learning-for-fraud-and-aml)\nJim Dowling\n[View More](https://www.hopsworks.ai/blog-categories/data-science)\n### // 5-minute Interviews\n[![5-minute interview Tales Marra](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66ffe16434403e12ef589291_5-min%20interviews%20\\(1\\).png)](https://www.hopsworks.ai/post/5-minute-interview-tales-marra-assessfirst)[5-minute InterviewsOctober 4, 20245 minRead5-minute interview Tales MarraNext up in our lineup is Tales Marra, Machine Learning Engineer at AssessFirst. ](https://www.hopsworks.ai/post/5-minute-interview-tales-marra-assessfirst)\nHopsworks Team\n[![5-minute interview Niels Nagl\u00e9 and Joop Snijder](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66f6a05da8e52f53288eb8d1_5-min%20interviews%20\\(1\\).png)](https://www.hopsworks.ai/post/5-minute-interview-niels-nagle-and-joop-snijder-info-support)[5-minute InterviewsSeptember 27, 20245 min Read5-minute interview Niels Nagl\u00e9 and Joop SnijderThis week Rik met with Niels Nagl\u00e9, Area Lead Data & AI at Info Support, and Joop Snijder, CTO at Aigency, for a quick chat. ](https://www.hopsworks.ai/post/5-minute-interview-niels-nagle-and-joop-snijder-info-support)\nHopsworks Team\n[![5-minute interview David Colls](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66d8618ae791fd129c5d1a30_5-min%20interviews%20\\(1\\).png)](https://www.hopsworks.ai/post/5-minute-interview-with-david-colls-nextdata)[5-minute InterviewsSeptember 4, 20246 minRead5-minute interview David CollsJoin Rik as he chats with David Colls, Founding Data Scientist at Nextdata, about Data & AI, the evolution of machine learning, and effective tech practices.](https://www.hopsworks.ai/post/5-minute-interview-with-david-colls-nextdata)\nHopsworks Team\n[View More](https://www.hopsworks.ai/blog-categories/5-minute-interviews)\n### // All Categories\n[![Migrating from AWS to a European Cloud - How We Cut Costs by 62%](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d4012dfee4417ef189aa94_AWS%20to%20OVH%20blog%20v2.png)](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)[Data EngineeringMarch 11, 202510minReadMigrating from AWS to a European Cloud - How We Cut Costs by 62%This post describes how we successfully migrated our serverless offering from AWS US-East to OVHCloud North America, reducing our monthly spend from $8,000 to $3,000 with no loss in service quality. ](https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62)\nJim Dowling\n[![The 10 Fallacies of MLOps](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bdbdb52730076471457479_MLOps%20Fallacies%20Blog.png)](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)[MLOpsFebruary 25, 202520 minReadThe 10 Fallacies of MLOpsMLOps fallacies can slow AI deployment and add complexity. This blog breaks down 10 common misconceptions, their impact, and how to avoid them to build scalable, production-ready AI systems efficient.](https://www.hopsworks.ai/post/the-10-fallacies-of-mlops)\nJim Dowling\n[![Hopsworks AI Lakehouse: The Power of Integrated MLOps Components](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67bc4ae4464caed2e2bff646_AI%20lakehouse%20Blog.png)](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)[AIFebruary 24, 20259 minReadHopsworks AI Lakehouse: The Power of Integrated MLOps ComponentsHopsworks AI Lakehouse simplifies MLOps by eliminating integration overhead with a pre-integrated, modular platform that connects seamlessly to existing AI ecosystems. ](https://www.hopsworks.ai/post/hopsworks-ai-lakehouse-the-power-of-integrated-mlops-components)\nRik Van Bruggen\n[![Unlocking the Power of AI in Government](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/679b54ad11fe440d84e8ece6_Government%20ai.png)](https://www.hopsworks.ai/post/the-power-of-ai-in-government)[AIJanuary 31, 20259 minReadUnlocking the Power of AI in GovernmentUnlock the transformative power of AI for government, enabling agencies to navigate the challenges, realize the benefits, and build a future-oriented public sector. ](https://www.hopsworks.ai/post/the-power-of-ai-in-government)\nRik Van Bruggen\n[![Optimizing AI Costs](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/679b3d691b2ec03b7e664e84_optimizing%20AI%20costs.png)](https://www.hopsworks.ai/post/optimizing-ai-costs)[AIJanuary 30, 20259 minReadOptimizing AI CostsThis post will explore several key strategies to mitigate AI related costs and highlight how Hopsworks can facilitate these optimizations.](https://www.hopsworks.ai/post/optimizing-ai-costs)\nRik Van Bruggen\n[![Revolutionizing Telecom with AI-powered Customer and Employee Experiences in 2025](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67978dddb5d0422694708b94_telecom%20ai.png)](https://www.hopsworks.ai/post/revolutionizing-telecom-with-ai-powered-customer-and-employee-experiences-in-2025)[AIJanuary 27, 202510 minReadRevolutionizing Telecom with AI-powered Customer and Employee Experiences in 2025The ever changing telecommunications industry is on the verge of another major transformation, driven by a convergence of powerful trends. Here are some of the biggest AI trends in Telecom in 2025. ](https://www.hopsworks.ai/post/revolutionizing-telecom-with-ai-powered-customer-and-employee-experiences-in-2025)\nRik Van Bruggen\n[![How AI Will Redefine Retail in 2025](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/677f9c450ce2d25e39b3485a_retail%20ai.png)](https://www.hopsworks.ai/post/how-ai-will-redefine-retail-in-2025)[AIJanuary 9, 20258 minReadHow AI Will Redefine Retail in 2025In this blog post, we wanted to share a less technical, more business-level perspective on AI in retail and how retailers can adapt to a changing e-commerce landscape in 2025. ](https://www.hopsworks.ai/post/how-ai-will-redefine-retail-in-2025)\nRik Van Bruggen\n[![Amazon FSx for NetApp ONTAP interoperability test in a Hopsworks 4.x Deployment](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6777a9f7b803f755bcb9aec0_netapp.png)](https://www.hopsworks.ai/post/amazon-fsx-for-netapp-ontap-interoperability-test-in-a-hopsworks-4-x-deployment)[BenchmarkJanuary 3, 202523 minReadAmazon FSx for NetApp ONTAP interoperability test in a Hopsworks 4.x DeploymentBy following this tutorial, you can evaluate the interoperability between Hopsworks 4.x and Amazon FSx for NetApp ONTAP](https://www.hopsworks.ai/post/amazon-fsx-for-netapp-ontap-interoperability-test-in-a-hopsworks-4-x-deployment)\nJavier Cabrera\n[![Breaking Down FSI Regulations for AI in 2025](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/675fe310a23998da10110fbb_FSI%20regulations.png)](https://www.hopsworks.ai/post/breaking-down-fsi-regulations-for-ai-in-2025)[AIDecember 13, 202419 minReadBreaking Down FSI Regulations for AI in 2025This blog explores how Hopsworks can support FSIs in addressing key regulations, including DORA, AMLA, PSD3, and more. ](https://www.hopsworks.ai/post/breaking-down-fsi-regulations-for-ai-in-2025)\nRik Van Bruggen\n[![Hopsworks PKI: The Unseen Hero](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6757f688e7e67c915a6abd08_Hopsworks%20PKI.png)](https://www.hopsworks.ai/post/hopsworks-pki-the-unseen-hero)[Feature StoreDecember 9, 202416 minReadHopsworks PKI: The Unseen HeroIn this article we explore how our Public Key Infrastructure has changed over the years coming to its current form, a Kubernetes first-class citizen. ](https://www.hopsworks.ai/post/hopsworks-pki-the-unseen-hero)\nAntonios Kouzoupis\n[![Air-gapped Installations in Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/673c9f546015f6cf7d115de0_air%20gapped.png)](https://www.hopsworks.ai/post/air-gapped-installations-in-hopsworks)[Data EngineeringNovember 19, 202416 minReadAir-gapped Installations in HopsworksIn this tutorial, we walk you through how we install air-gapped environments in Hopsworks and how we extended this process to test air-gapped installations as well.](https://www.hopsworks.ai/post/air-gapped-installations-in-hopsworks)\nJavier Cabrera\n[![Migrating Hopsworks to Kubernetes](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67163b4a1b6ac639d242a23a_Hopsworks%20to%20Kubernetes.png)](https://www.hopsworks.ai/post/migrating-hopsworks-to-kubernetes)[Feature StoreOctober 21, 202412 minReadMigrating Hopsworks to KubernetesNearly a year ago, the Hopsworks team embarked on a journey to migrate its infrastructure to Kubernetes. In this article we describe three main pillars of our Kubernetes migration.](https://www.hopsworks.ai/post/migrating-hopsworks-to-kubernetes)\nJavier Cabrera\n[![A Year of Insights with the MLOps & LLMOps Dictionary](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66f65ada28726b20f5cb3b5f_dictionary%20thumbnail.png)](https://www.hopsworks.ai/post/a-year-of-insights-with-the-mlops-llmops-dictionary)[MLOpsSeptember 26, 202410 minReadA Year of Insights with the MLOps & LLMOps DictionaryIn this blog post, we have selected the 25 most-read dictionary entries from the MLOps and LLMOps Dictionary to highlight key trends and lessons in AI.](https://www.hopsworks.ai/post/a-year-of-insights-with-the-mlops-llmops-dictionary)\nCarolin Svenberg \n[![Introducing the AI Lakehouse](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66d6c9970cfc8086e7e1046b_The%20AI%20Lakehouse.png)](https://www.hopsworks.ai/post/the-ai-lakehouse)[Feature StoreSeptember 2, 202430 minReadIntroducing the AI LakehouseWe describe the capabilities that need to be added to Lakehouse to make it an AI Lakehouse that can support building and operating AI-enabled batch and real-time applications as well LLM applications.](https://www.hopsworks.ai/post/the-ai-lakehouse)\nJim Dowling\n[![Reproducible Data for the AI Lakehouse](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b1d5bb06a34c527b352841_reproducible%20data.png)](https://www.hopsworks.ai/post/reproducible-data-for-the-ai-lakehouse)[SIGMOD 2024August 6, 202413 minReadReproducible Data for the AI LakehouseWe present how Hopsworks leverages its time-travel capabilities for feature groups to support reproducible creation of training datasets using metadata.](https://www.hopsworks.ai/post/reproducible-data-for-the-ai-lakehouse)\nJim Dowling\n[![RonDB: A Real-Time Database for Real-Time AI Systems](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66b207bd4cf6636717910f8b_Real%20time%20Database%20Blog.png)](https://www.hopsworks.ai/post/rondb-a-real-time-database-for-real-time-ai-systems)[SIGMOD 2024July 25, 20248 minReadRonDB: A Real-Time Database for Real-Time AI SystemsLearn more about how Hopsworks (RonDB) outperforms AWS Sagemaker and GCP Vertex in latency for real-time AI databases, based on a peer-reviewed SIGMOD 2024 benchmark.](https://www.hopsworks.ai/post/rondb-a-real-time-database-for-real-time-ai-systems)\nMikael Ronstr\u00f6m\n[![From Lakehouse to AI Lakehouse with a Python-Native Query Engine](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/669fa7eaa10b0dd15c46ab80_Lakehouse%20PQE%20Blog.png)](https://www.hopsworks.ai/post/from-lakehouse-to-ai-lakehouse-with-a-python-native-query-engine)[Data EngineeringJuly 23, 20249 minReadFrom Lakehouse to AI Lakehouse with a Python-Native Query EngineRead how Hopsworks generates temporal queries from Python code, and how a native query engine built on Arrow can massively outperform JDBC/ODBC APIs.](https://www.hopsworks.ai/post/from-lakehouse-to-ai-lakehouse-with-a-python-native-query-engine)\nJim Dowling\n[![The Feature Store Makes Your Data Warehouse Easy to Use for AI](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/668bf0ab60826eb58e0d6af1_PR%20Blog%204_data%20warehouse.png)](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai)[Feature StoreJuly 10, 202424 minReadThe Feature Store Makes Your Data Warehouse Easy to Use for AIIn this article, we cover the added value of a feature store over a data warehouse when managing offline data for AI. ](https://www.hopsworks.ai/post/the-feature-store-makes-your-data-warehouse-easy-to-use-for-ai)\nJim Dowling\n[![The Journey from Star Schema to Snowflake Schema in the Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66869f1f4f75a7d91c67775f_PR%20Blog%203_snowflake%20schema.png)](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store)[Feature StoreJuly 8, 202414 minReadThe Journey from Star Schema to Snowflake Schema in the Feature StoreIn this article we introduce the snowflake schema data model for feature stores, and show how it helps you include more features to make better predictions ](https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store)\nDavit Bzhalava\n[![The Taxonomy for Data Transformations in AI Systems](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/668d97122ea716ed1633a7f7_PR%20Blog%202_Taxonomy.png)](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)[Feature StoreJuly 1, 202430 minReadThe Taxonomy for Data Transformations in AI SystemsThis article introduces a taxonomy for data transformations in AI applications that is fundamental for any AI system that wants to reuse feature data in more than one model. ](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)\nManu Joseph\n[![Modularity and Composability for AI Systems with AI Pipelines and Shared Storage](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/667acae0ae637d5bb9a72ea2_modularity%20%26%20composability.png)](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)[Feature StoreJune 25, 202425 minReadModularity and Composability for AI Systems with AI Pipelines and Shared StorageWe present a unified software architecture for batch, real-time, and LLM AI systems that is based on a shared storage layer and a decomposition of machine learning pipelines. ](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)\nJim Dowling\n[![Building a Cheque Fraud Detection and Explanation AI System using a fine-tuned LLM](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/663dcc9e88ea5f92535220cf_Makerspace%20cheque%20fraud.png)](https://www.hopsworks.ai/post/building-a-cheque-fraud-detection-and-explanation-ai-system-using-a-fine-tuned-llm)[Data EngineeringMay 13, 202410 minReadBuilding a Cheque Fraud Detection and Explanation AI System using a fine-tuned LLMThe third edition of the LLM Makerspace dived into an example of an LLM system for detecting check fraud.](https://www.hopsworks.ai/post/building-a-cheque-fraud-detection-and-explanation-ai-system-using-a-fine-tuned-llm)\nJim Dowling\n[![Feature Pipelines in Production with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6630f43b3472fc184b558f94_feature%20pipelines.png)](https://www.hopsworks.ai/post/feature-pipelines-in-production-with-hopsworks)[MLOpsApril 30, 20243 minReadFeature Pipelines in Production with HopsworksIn this post, we will look at how to put feature pipelines into production using Hopsworks. ](https://www.hopsworks.ai/post/feature-pipelines-in-production-with-hopsworks)\nFabio Buso\n[![Build your Value Case for Feature Store Implementations](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6628b049bd8bd1759cec38c7_value%20case%20v2.png)](https://www.hopsworks.ai/post/build-your-value-case-for-feature-store-implementations)[AIApril 24, 20249 minReadBuild your Value Case for Feature Store ImplementationsWe explore how to develop a compelling value case that quantify the benefits and costs associated with implementing a feature store.](https://www.hopsworks.ai/post/build-your-value-case-for-feature-store-implementations)\nRik Van Bruggen\n[![Job Scheduling & Orchestration using Hopsworks and Airflow](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/661f828bcf5d65c8431cb023_orchestration.png)](https://www.hopsworks.ai/post/job-scheduling-orchestration-using-hopsworks-and-airflow)[Data EngineeringApril 17, 202410 minReadJob Scheduling & Orchestration using Hopsworks and AirflowThis article covers the different aspects of Job Scheduling in Hopsworks including how simple jobs can be scheduled through the Hopsworks UI by non-technical users](https://www.hopsworks.ai/post/job-scheduling-orchestration-using-hopsworks-and-airflow)\nEhsan Heydari\n[![Build Your Own Private PDF Search Tool ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66447bbb6d7e1ff8947d02c1_Makerspace%20blogs%20pdfai.png)](https://www.hopsworks.ai/post/build-your-own-pdf-ai-using-both-rag-and-fine-tuning-in-one-platform)[Data EngineeringApril 11, 20246 minReadBuild Your Own Private PDF Search Tool A summary from our LLM Makerspace event where we built our own PDF Search Tool using RAG and fine-tuning in one platform. Follow along the journey to build a LLM application from scratch. ](https://www.hopsworks.ai/post/build-your-own-pdf-ai-using-both-rag-and-fine-tuning-in-one-platform)\nJim Dowling\n[![Build Vs Buy: For Machine Learning/AI Feature Stores](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66168d151c36e53c9c40b93a_build%20vs%20buy.png)](https://www.hopsworks.ai/post/build-versus-buy-machine-learning)[AIApril 10, 202417 minReadBuild Vs Buy: For Machine Learning/AI Feature StoresOn the decision of building versus buying a feature store there are strategic and technical components to consider as it impacts both cost and technological debt.](https://www.hopsworks.ai/post/build-versus-buy-machine-learning)\nRik Van Bruggen\n[![Unlocking the Power of Function Calling with LLMs](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6617d0fd1c284b69172c8478_Makerspace%20blogs.png)](https://www.hopsworks.ai/post/unlocking-the-power-of-function-calling-with-llms)[Data EngineeringApril 2, 20247 minReadUnlocking the Power of Function Calling with LLMsThis is a summary of our latest LLM Makerspace event where we pulled back the curtain on a exciting paradigm in AI \u2013 function calling with LLMs.](https://www.hopsworks.ai/post/unlocking-the-power-of-function-calling-with-llms)\nJim Dowling\n[![Doubling Down on Open Source: How RonDB Upholds the Principles Redis Left Behind](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/66014ad6f7a779228f92f926_RonDB%20vs%20Redis.png)](https://www.hopsworks.ai/post/doubling-down-on-open-source-how-rondb-upholds-the-principles-redis-left-behind)[Feature StoreMarch 25, 20249 minReadDoubling Down on Open Source: How RonDB Upholds the Principles Redis Left BehindRedis will no longer be open source. Our own project, RonDB, will continue being open source in order to uphold the principles that keeps the technology advancing. ](https://www.hopsworks.ai/post/doubling-down-on-open-source-how-rondb-upholds-the-principles-redis-left-behind)\nMikael Ronstr\u00f6m\n[![The Enterprise Journey to introducing a Software Factory for AI Systems](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6613cabcbb4ee391296e4978_software%20factory.png)](https://www.hopsworks.ai/post/the-enterprise-journey-to-introducing-a-software-factory-for-ai-systems)[AIMarch 21, 202419 minReadThe Enterprise Journey to introducing a Software Factory for AI SystemsIn this article we describe the software factory approach to building and maintaining AI systems.](https://www.hopsworks.ai/post/the-enterprise-journey-to-introducing-a-software-factory-for-ai-systems)\nJim Dowling\n[![GenAI comes to Hopsworks with Vector Similarity Search](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65f838173d7ea02b2b9d8c66_vector%20similairty%20search.png)](https://www.hopsworks.ai/post/genai-comes-to-hopsworks-with-vector-similarity-search)[Feature StoreMarch 20, 20248 minReadGenAI comes to Hopsworks with Vector Similarity SearchHopsworks has added support for approximate nearest neighbor (ANN) indexing and vector similarity search for vector embeddings stored in its feature store. ](https://www.hopsworks.ai/post/genai-comes-to-hopsworks-with-vector-similarity-search)\nKenneth Mak \n[![Delta Lake comes to Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65e7352423a296a8a2cf90a1_delta%20lake.png)](https://www.hopsworks.ai/post/delta-lake-comes-to-hopsworks)[Feature StoreMarch 5, 20248 minReadDelta Lake comes to HopsworksHopsworks has added support for Delta Lake to accelerate our mission to build the Python-Native Data for AI platform. ](https://www.hopsworks.ai/post/delta-lake-comes-to-hopsworks)\nJim Dowling\n[![Federated Data with the Hopsworks Feature Query Service](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65cf5ee3c3dbaa97a035cb49_query%20service.png)](https://www.hopsworks.ai/post/federated-data-with-the-hopsworks-feature-query-service)[Feature StoreFebruary 14, 202414 minReadFederated Data with the Hopsworks Feature Query ServiceA tutorial of the Hopsworks Feature Query Service which efficiently queries and joins features from multiple platforms such as Snowflake, BigQuery and Hopsworks without data any duplication.](https://www.hopsworks.ai/post/federated-data-with-the-hopsworks-feature-query-service)\nSteffen Grohsschmiedt\n[![5 Machine Learning Myths Debunked](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65c348c2c06bb9ba20fe540c_ml%20myth%203.png)](https://www.hopsworks.ai/post/5-machine-learning-myths-debunked)[MLOpsFebruary 6, 20249 minRead5 Machine Learning Myths DebunkedThe rapid development pace in AI is the cause for a lot of misconceptions surrounding ML and MLOps. In this post we debunk a few common myths about MLOps, LLMs and machine learning in production. ](https://www.hopsworks.ai/post/5-machine-learning-myths-debunked)\nCarolin Svenberg \n[![F.A.I.R. Principles in Data for AI](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65bbc47fd2c5f8bdf2a09d5d_fair.png)](https://www.hopsworks.ai/post/fair-principles-in-data-for-ai)[Data ScienceFebruary 1, 20249 minReadF.A.I.R. Principles in Data for AIAt Hopsworks the F.A.I.R principles have been a cornerstone of our approach in designing a platform for managing machine learning data and infrastructure. ](https://www.hopsworks.ai/post/fair-principles-in-data-for-ai)\nLex Avstreikh\n[![Common Error Messages in Pandas](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65ae4ba61b3e0b423831dd3d_errors%20pandas.png)](https://www.hopsworks.ai/post/common-error-messages-in-pandas)[Data ScienceJanuary 18, 202420 minReadCommon Error Messages in PandasWe go through the most common errors messages in Pandas and offer solutions to these errors as well as provide efficiency tips for Pandas code.](https://www.hopsworks.ai/post/common-error-messages-in-pandas)\nHaziqa Sajid\n[![Multi-Region High Availability Comes to Feature Stores](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65f319435c8ec1a9dacb1e15_HA%20part%202.png)](https://www.hopsworks.ai/post/multi-region-high-availability-comes-to-feature-stores-part-2)[Feature StoreJanuary 16, 202413 minReadMulti-Region High Availability Comes to Feature StoresFollowing our previous blog, we expand on the architecture to fit a Tier 1 classification where all components of Hopsworks are replicated in a different geographical region. ](https://www.hopsworks.ai/post/multi-region-high-availability-comes-to-feature-stores-part-2)\nAntonios Kouzoupis\n[![Feature Store Benchmark Comparison: Hopsworks and Feast](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/658599385be1ffd71dcb0e21_feature%20store%20benchmarks.png)](https://www.hopsworks.ai/post/feature-store-benchmark-comparison-hopsworks-and-feast)[BenchmarkDecember 22, 202315minReadFeature Store Benchmark Comparison: Hopsworks and FeastA comparison of the online feature serving performance for Hopsworks and Feast feature stores, contrasting the approaches to building a feature store.](https://www.hopsworks.ai/post/feature-store-benchmark-comparison-hopsworks-and-feast)\nDhananjay Mukhedkar\n[![The Guardrails for High Risk AI Required by the EU AI Act](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65816c47c8bb5c198f11a318_EU%20AI%20part%202.png)](https://www.hopsworks.ai/post/the-guardrails-for-high-risk-ai-required-by-the-eu-ai-act)[AIDecember 19, 202313ReadThe Guardrails for High Risk AI Required by the EU AI ActAs a continuation to our last blog on the EU AI Act, this blog explores the implications of how Hopsworks machine learning systems can address the act's requirements for high risk AI applications.](https://www.hopsworks.ai/post/the-guardrails-for-high-risk-ai-required-by-the-eu-ai-act)\nRik Van Bruggen\n[![Multi-Region High Availability Comes to Feature Stores](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65a658a00f2c2063eba79913_HA%20part%201.png)](https://www.hopsworks.ai/post/multi-region-high-availability-comes-to-feature-stores)[MLOpsDecember 18, 202314 minReadMulti-Region High Availability Comes to Feature StoresLearn how Hopsworks achieves high availability for the Online and Offline Feature Store and a Metadata layer, making it an operational ML system both for on-premise and cloud infrastructure. ](https://www.hopsworks.ai/post/multi-region-high-availability-comes-to-feature-stores)\nAntonios Kouzoupis\n[![High Risk AI in the EU AI Act ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65797453a920f69c57852e7d_EU%20AI%20Act%2023.png)](https://www.hopsworks.ai/post/high-risk-ai-in-the-eu-ai-act)[AIDecember 13, 20239 minReadHigh Risk AI in the EU AI Act What is the corporate and societal significance of the EU AI Act and how does it impact organizations with high risk AI systems? ](https://www.hopsworks.ai/post/high-risk-ai-in-the-eu-ai-act)\nRik Van Bruggen\n[![From BI to AI: A Data-Driven Journey](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6578600892cbaf1d0142e6d0_Bi%20to%20AI%20Blog.png)](https://www.hopsworks.ai/post/from-bi-to-ai-a-data-driven-journey)[AIDecember 8, 20238 minReadFrom BI to AI: A Data-Driven JourneyData is evolving from traditional Business Intelligence to Artificial Intelligence and Machine Learning for predictive analytics, creating new requirements for how businesses operationalises.](https://www.hopsworks.ai/post/from-bi-to-ai-a-data-driven-journey)\nRik Van Bruggen\n[![Feature Engineering with DBT for Data Warehouses](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65703bfc6cc493cb5888a340_feature%20engineering%20with%20dbt_thumbnail.png)](https://www.hopsworks.ai/post/feature-engineering-with-dbt-for-data-warehouses)[Data EngineeringNovember 30, 202320 minReadFeature Engineering with DBT for Data WarehousesRead about the advantages of using DBT for data warehouses and how it's positioned as a preferred solution for many data analytics and engineering teams.](https://www.hopsworks.ai/post/feature-engineering-with-dbt-for-data-warehouses)\nKais Laribi \n[![What is MLOps?](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65648999d04c1ff190699f6e_What%20is%20MLOps%20blog.png)](https://www.hopsworks.ai/post/what-is-mlops)[MLOpsNovember 27, 202320 minReadWhat is MLOps?This blog explores MLOps principles, with a focus on versioning, and provides a practical example using Hopsworks for both data and model versioning. ](https://www.hopsworks.ai/post/what-is-mlops)\nHaziqa Sajid\n[![Pandas2 and Polars for Feature Engineering](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/652709e8e9ee4bd6e7245f2d_pandas%26polars_thumbnail.png)](https://www.hopsworks.ai/post/pandas2-and-polars-for-feature-engineering)[Data EngineeringNovember 6, 202320 minReadPandas2 and Polars for Feature EngineeringWe review Python libraries, such as Pandas, Pandas2 and Polars, for Feature Engineering, evaluate their performance and explore how they power machine learning use cases. ](https://www.hopsworks.ai/post/pandas2-and-polars-for-feature-engineering)\nHaziqa Sajid\n[![How to Build a Python Environment with Custom Docker Commands](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/653902a4e3c0de023af32e87_PythonEnvironment_thumbnail.png)](https://www.hopsworks.ai/post/how-to-build-a-python-environment-with-custom-docker-commands)[Feature StoreOctober 26, 20238 minReadHow to Build a Python Environment with Custom Docker CommandsWith the latest version of Hopsworks we introduce new capabilities such as running custom bash commands and a improved UI which shows you the history of the python environment. ](https://www.hopsworks.ai/post/how-to-build-a-python-environment-with-custom-docker-commands)\nGibson Chikafa\n[![Machine Learning Embeddings as Features for Models](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/652fa0827a2f054fc4f5d726_ML%20embeddings_thumbnail.png)](https://www.hopsworks.ai/post/machine-learning-embeddings-as-features-for-models)[Data EngineeringOctober 9, 202313 minReadMachine Learning Embeddings as Features for ModelsDelve into the profound implications of machine learning embeddings, their diverse applications, and their crucial role in reshaping the way we interact with data.](https://www.hopsworks.ai/post/machine-learning-embeddings-as-features-for-models)\nPrithivee Ramalingam\n[![Facebook Prophet for Time-Series Machine Learning](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/651eac8c99616d72197da06a_facebook%20prophet_thumbnail.png)](https://www.hopsworks.ai/post/facebook-prophet-for-time-series-machine-learning)[Data ScienceOctober 5, 202317minReadFacebook Prophet for Time-Series Machine LearningTime-series data consists of records in chronological order and is crucial for forecasting trends and patterns. In this blog, we take a look at how to use Facebook Prophet for time-series forecasting.](https://www.hopsworks.ai/post/facebook-prophet-for-time-series-machine-learning)\nKais Laribi \n[![Bring Your Own Kafka Cluster to Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6516dbba2c0b331bc39da36b_thumbnail_byok.png)](https://www.hopsworks.ai/post/bring-your-own-kafka-cluster)[Feature StoreSeptember 25, 202310 minReadBring Your Own Kafka Cluster to HopsworksA tutorial of how to use our latest Bring Your Own Kafka (BYOK) capability in Hopsworks. It allows you to connect your existing Kafka clusters to your Hopsworks cluster.](https://www.hopsworks.ai/post/bring-your-own-kafka-cluster)\nRalfs Zangis\n[![From MLOps to ML Systems with Feature/Training/Inference Pipelines](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65019f581267ff943c3931a6_mlops%20to%20ml%20systems_thumbnail.png)](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)[MLOpsSeptember 13, 202325 minReadFrom MLOps to ML Systems with Feature/Training/Inference PipelinesWe explain a new framework for ML systems as three independent ML pipelines: feature pipelines, training pipelines, and inference pipelines, creating a unified MLOps architecture.](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)\nJim Dowling\n[![Feature Engineering with Apache Airflow](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d3a7213fc689589547090e_airflow.png)](https://www.hopsworks.ai/post/feature-engineering-with-apache-airflow)[Data EngineeringSeptember 4, 202318 minReadFeature Engineering with Apache AirflowUnlock the power of Apache Airflow in the context of feature engineering. We will delve into building a feature pipeline using Airflow, focusing on two tasks: feature binning and aggregations.](https://www.hopsworks.ai/post/feature-engineering-with-apache-airflow)\nPrithivee Ramalingam\n[![Automated Feature Engineering with FeatureTools](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d9eb5716ab472d0a7b7bc4_Featuretools%20thumbnail.png)](https://www.hopsworks.ai/post/automated-feature-engineering-with-featuretools)[Data EngineeringAugust 23, 202313 minReadAutomated Feature Engineering with FeatureToolsAn ML model\u2019s ability to learn and read data patterns largely depend on feature quality. With frameworks such as FeatureTools ML practitioners can automate the feature engineering process.](https://www.hopsworks.ai/post/automated-feature-engineering-with-featuretools)\nHaziqa Sajid\n[![Why Do You Need a Feature Store? ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d9dab74eb40a787c6b7c5e_why%20copy.jpg)](https://www.hopsworks.ai/post/why-do-you-need-a-feature-store)[Feature StoreAugust 11, 202320 minReadWhy Do You Need a Feature Store? Discover the power of feature stores in modern machine learning systems and how they bridge the gap between model development and production. ](https://www.hopsworks.ai/post/why-do-you-need-a-feature-store)\nLex Avstreikh\n[![Faster reading from the Lakehouse to Python with DuckDB/ArrowFlight](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/65113a725a93dc40f6d74c85_duckdb_thumbnail.png)](https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb)[Feature StoreAugust 9, 202313 minReadFaster reading from the Lakehouse to Python with DuckDB/ArrowFlightIn this article, we outline how we leveraged ArrowFlight with DuckDB to build a new service that massively improves the performance of Python clients reading from lakehouse data in the Feature Store](https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb)\nTill D\u00f6hmen\n[![MLOps vs. DevOps: Best Practices, Challenges and Differences](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d355fb6ba1878a9afd9066_Mlops%20Devops%20Blog.png)](https://www.hopsworks.ai/post/mlops-vs-devops-best-practices-challenges-and-differences)[MLOpsJuly 11, 202317ReadMLOps vs. DevOps: Best Practices, Challenges and DifferencesExplore the convergence of MLOps and DevOps. Learn about their purposes, differences, and areas of integration and discover best practices, challenges, and their future potential. ](https://www.hopsworks.ai/post/mlops-vs-devops-best-practices-challenges-and-differences)\nPrithivee Ramalingam\n[![Building Feature Pipelines with Apache Flink](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d35592dae4e9246fb7c106_Apache%20Flink%20blog.png)](https://www.hopsworks.ai/post/building-feature-pipelines-with-apache-flink)[Data EngineeringJune 21, 20238 minReadBuilding Feature Pipelines with Apache FlinkFind out how to use Flink to compute real-time features and make them available to online models within seconds using Hopsworks. ](https://www.hopsworks.ai/post/building-feature-pipelines-with-apache-flink)\nFabio Buso\n[![Feature Engineering for Categorical Features with Pandas](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d354fcd29f6c64ab26eb92_Feature%20Engineering%20Blog.png)](https://www.hopsworks.ai/post/feature-engineering-for-categorical-features-with-pandas)[Data EngineeringJune 20, 202310 minReadFeature Engineering for Categorical Features with PandasExplore the power of feature engineering for categorical features using Pandas. Learn essential techniques for handling categorical variables, and creating new features.](https://www.hopsworks.ai/post/feature-engineering-for-categorical-features-with-pandas)\nPrithivee Ramalingam\n[![Hopsworks meets SOC2 Type II standards for data security and privacy](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6447ea626aa4aa2d0c97e786_SOC%202%20Certification%2000.png)](https://www.hopsworks.ai/post/hopsworks-meets-soc2-type-ii-standards-for-data-security)[AIApril 25, 20232 minReadHopsworks meets SOC2 Type II standards for data security and privacyHopsworks has successfully completed the AICPA Service Organization Control (SOC) 2 Type II audit.](https://www.hopsworks.ai/post/hopsworks-meets-soc2-type-ii-standards-for-data-security)\nCarolin Svenberg \n[![Hopsworks receives ISO 27001 Certification for Data Security](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64887cd64eb0d2a378319466_ISO%20Certification.png)](https://www.hopsworks.ai/post/hopsworks-receives-iso-27001-certification-for-data-security)[AIApril 4, 20232 minReadHopsworks receives ISO 27001 Certification for Data SecurityHopsworks has received an ISO 27001 certification, the globally recognized standard for establishing, implementing, maintaining, and continually improving an information security management system.](https://www.hopsworks.ai/post/hopsworks-receives-iso-27001-certification-for-data-security)\nCarolin Svenberg \n[![ROI of Feature Stores](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63aacdf3e93ff87bc5913656_ROI%20Feature%20Stores%20Blog.png)](https://www.hopsworks.ai/post/roi-of-feature-stores)[Feature StoreMarch 1, 20237 minReadROI of Feature StoresThis blog analyses the cost-benefits of Feature Stores for Machine Learning and estimates your return on investment with our Feature Store ROI Calculator.](https://www.hopsworks.ai/post/roi-of-feature-stores)\nJim Dowling\n[![Feature Store: The missing data layer for Machine Learning pipelines?](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61bb72f4fbcc74f4c86eb3f8_61b8aed0674af6312e2c05b2_missingdatalayer.jpg)](https://www.hopsworks.ai/post/feature-store-the-missing-data-layer-in-ml-pipelines)[Feature StoreFebruary 2, 202330 minReadFeature Store: The missing data layer for Machine Learning pipelines?In this blog, we discuss the state-of-the-art in data management and machine learning pipelines (within the wider field of MLOps) and present the first open-source feature store, Hopsworks.](https://www.hopsworks.ai/post/feature-store-the-missing-data-layer-in-ml-pipelines)\nJim Dowling\n[![Hopsworks 3.1 Product Updates: Feature Store & UI Improvements ](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63dbadc63f7ca98a8a34b3bd_Hopsworks%203.1.png)](https://www.hopsworks.ai/post/hopsworks-3-1-product-updates)[February 2, 20234 minReadHopsworks 3.1 Product Updates: Feature Store & UI Improvements Read about Hopsworks 3.1 and the new improvements in the feature store (time-series splits for training data, support for managing thousands of models), stability and user-interface.](https://www.hopsworks.ai/post/hopsworks-3-1-product-updates)\nJim Dowling\n[![Optimize your MLOps Workflow with a Feature Store CI/CD and Github Actions](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/639add1f7195743d82f44ed0_CI_CD%20Blog.png)](https://www.hopsworks.ai/post/optimize-your-mlops-workflow-with-a-feature-store-ci-cd-and-github-actions)[MLOpsDecember 9, 202212 minReadOptimize your MLOps Workflow with a Feature Store CI/CD and Github ActionsIn this blog we present an end to end Git based workflow to test and deploy feature engineering, model training and inference pipelines. ](https://www.hopsworks.ai/post/optimize-your-mlops-workflow-with-a-feature-store-ci-cd-and-github-actions)\nFabio Buso\n[![Data Validation for Enterprise AI: Using Great Expectations with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d3575bfdbcc0984ed69ade_great%20expectations%20blog.png)](https://www.hopsworks.ai/post/data-validation-for-enterprise-ai-using-great-expectations-with-hopsworks)[MLOpsSeptember 21, 202210 minReadData Validation for Enterprise AI: Using Great Expectations with HopsworksLearn more about how Hopsworks stores both data and validation artifacts, enabling easy monitoring on the Feature Group UI page.](https://www.hopsworks.ai/post/data-validation-for-enterprise-ai-using-great-expectations-with-hopsworks)\nVictor Jouffrey\n[![How to use external data stores as an offline feature store in Hopsworks with Connector API](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6322eb50fc669956a18e9c55_Connector%20API_Blog%20Visual.png)](https://www.hopsworks.ai/post/external-data-stores-in-hopsworks-with-connector-api)[Data EngineeringSeptember 15, 20229 minReadHow to use external data stores as an offline feature store in Hopsworks with Connector APIIn this blog, we introduce Hopsworks Connector API that is used to mount a table in an external data source as an external feature group in Hopsworks. ](https://www.hopsworks.ai/post/external-data-stores-in-hopsworks-with-connector-api)\nDhananjay Mukhedkar\n[![Great Models Require Great MLOps: Using Weights & Biases with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63188873e12953561c68a8d3_Weights%20%26%20Biases.png)](https://www.hopsworks.ai/post/using-weights-biases-with-hopsworks)[MLOpsSeptember 7, 202215 minReadGreat Models Require Great MLOps: Using Weights & Biases with HopsworksDiscover how you can easily make the journey from ML models to putting prediction services in production by choosing best-of-breed technologies.](https://www.hopsworks.ai/post/using-weights-biases-with-hopsworks)\nMoritz Meister\n[![From Pandas to Features to Models to Predictions - A deep dive into the Hopsworks APIs](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6304d17d1bc96863f1833bed_pandatomodelstofeatures%20copy.jpg)](https://www.hopsworks.ai/post/a-deep-dive-into-the-hopsworks-apis)[Data EngineeringAugust 23, 202220 minReadFrom Pandas to Features to Models to Predictions - A deep dive into the Hopsworks APIsLearn how the Hopsworks feature store APIs work and what it takes to go from a Pandas DataFrame to features used by models for both training and inference.](https://www.hopsworks.ai/post/a-deep-dive-into-the-hopsworks-apis)\nFabio Buso\n[![Introducing the Serverless Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/62e7aead44765eaa5c619446_Serverless%20Blog.png)](https://www.hopsworks.ai/post/introducing-the-serverless-feature-store)[Feature StoreAugust 3, 20227 minReadIntroducing the Serverless Feature StoreHopsworks Serverless is the first serverless feature store for ML, allowing you to manage features and models seamlessly without worrying about scaling, configuration or management of servers.](https://www.hopsworks.ai/post/introducing-the-serverless-feature-store)\nJim Dowling\n[![Hopsworks 3.0: The Python-Centric Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/64d357d7f496d4ce9b950450_Python%20Centric%20Blog.gif)](https://www.hopsworks.ai/post/the-python-centric-feature-store)[Feature StoreJuly 26, 20229 minReadHopsworks 3.0: The Python-Centric Feature StoreHopsworks is the first feature store to extend its support from the traditional Big Data platforms to the Pandas-sized data realm, where Python reigns supreme. A new Python API is also provided.](https://www.hopsworks.ai/post/the-python-centric-feature-store)\nJim Dowling\n[![Hopsworks 3.0 - Connecting Python to the Modern Data Stack](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/62d9652fd44fb789460094f5_1200-630%20copy.jpg)](https://www.hopsworks.ai/post/hopsworks-3-0-connecting-python-to-the-modern-data-stack)[Feature StoreJuly 20, 20227 minReadHopsworks 3.0 - Connecting Python to the Modern Data StackHopsworks 3.0 is a new release focused on best-in-class Python support, Feature Views unifying Offline and Online read APIs to the Feature Store, Great Expectations support, KServe and a Model serving](https://www.hopsworks.ai/post/hopsworks-3-0-connecting-python-to-the-modern-data-stack)\nJim Dowling\n[![A Spark Join Operator for Point-in-Time Correct Joins](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/62bd5d438fd011e6686e59f5_Spark%20Join%20Blog.png)](https://www.hopsworks.ai/post/a-spark-join-operator-for-point-in-time-correct-joins)[BenchmarkJune 30, 202210 minReadA Spark Join Operator for Point-in-Time Correct JoinsIn this blog post we showcase the results of a study that examined point-in-time join optimization using Apache Spark in Hopsworks. ](https://www.hopsworks.ai/post/a-spark-join-operator-for-point-in-time-correct-joins)\nAxel Pettersson \n[![The EU AI Act: Time to Govern your AI or Turn it off](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/62b445273bb6c3d2a4b67854_EU%20AI%20ActBlog%20Thumbnail.png)](https://www.hopsworks.ai/post/the-eu-ai-act-time-to-govern-your-ai-or-turn-it-off)[AIJune 23, 202210 minReadThe EU AI Act: Time to Govern your AI or Turn it offAn introduction to the EU AI Act and how Feature Stores provide a great solution to the obligations imposed by the regulation.](https://www.hopsworks.ai/post/the-eu-ai-act-time-to-govern-your-ai-or-turn-it-off)\nGeoff Burne\n[![Feature Types for Machine Learning](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6290d72018708f098cfd1abf_featuretypes.png)](https://www.hopsworks.ai/post/feature-types-for-machine-learning)[Feature StoreMay 27, 202215 minReadFeature Types for Machine LearningProgrammers know data types, but what is a feature type to a programmer new to machine learning, given no mainstream programming language has native support for them?](https://www.hopsworks.ai/post/feature-types-for-machine-learning)\nJim Dowling\n[![Testing feature logic, transformations, and feature pipelines with pytest](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/62692b2da3a7260f0d57e36d_testing%20blog2.png)](https://www.hopsworks.ai/post/testing-feature-logic-transformations-and-feature-pipelines-with-pytest)[Data EngineeringApril 26, 202217 minReadTesting feature logic, transformations, and feature pipelines with pytestOperational machine learning requires the offline and online testing of both features and models. In this article, we show you how to design, build, and run test for features.](https://www.hopsworks.ai/post/testing-feature-logic-transformations-and-feature-pipelines-with-pytest)\nJim Dowling\n[![Hopsworks 2.5 Product Updates: Collaboration & Scalability](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6222098a11278365a58434d6_Blog%20Hops%202.5.png)](https://www.hopsworks.ai/post/hopsworks-2-5-product-updates)[March 4, 20223 minReadHopsworks 2.5 Product Updates: Collaboration & ScalabilityWe go through the new features and developments in Hopsworks 2.5 that will benefit open-source users and customers alike.](https://www.hopsworks.ai/post/hopsworks-2-5-product-updates)\nFabio Buso\n[![Model analysis on the What-If framework for TensorFlow on Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61e6717ebeb28447092d1030_WIF.png)](https://www.hopsworks.ai/post/model-analysis-what-if-framework-for-tensorflow)[MLOpsJanuary 17, 20225 minReadModel analysis on the What-If framework for TensorFlow on HopsworksWe introduce how to use the What-If Tool as a Jupyter plugin on Hopsworks to build better machine learning models by making it easier to ask counterfactual questions about your model\u2019s behaviour.](https://www.hopsworks.ai/post/model-analysis-what-if-framework-for-tensorflow)\nAnastasiia Andriievska \n[![How to Transform Snowflake Data into Features with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/637647a53a162c90114c1d6b_Snowflake%20Data%20Into%20Features.png)](https://www.hopsworks.ai/post/how-to-transform-snowflake-data-into-features-with-hopsworks)[Feature StoreDecember 7, 20218 minReadHow to Transform Snowflake Data into Features with HopsworksLearn how to connect Hopsworks to Snowflake and create features and make them available both offline in Snowflake and online in Hopsworks.](https://www.hopsworks.ai/post/how-to-transform-snowflake-data-into-features-with-hopsworks)\nFabio Buso\n[![Show me the code; how we linked notebooks to features](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/62221f7492811211efc66440_Show%20me%20the%20code%20Blog.png)](https://www.hopsworks.ai/post/show-me-the-code-how-we-linked-notebooks-to-features)[Data EngineeringNovember 23, 20213 minReadShow me the code; how we linked notebooks to featuresWe are introducing a new feature in Hopsworks UI - feature code preview - ability to view the notebook used to create a Feature Group or Training Dataset.](https://www.hopsworks.ai/post/show-me-the-code-how-we-linked-notebooks-to-features)\nJim Dowling\n[![Receiving Alerts in Slack/Email/PagerDuty from Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/637f7a009eebc51282fb86ea_Receiving%20Alerts%20Blog_.png)](https://www.hopsworks.ai/post/receiving-alerts-in-slack-email-pagerduty-from-hopsworks-support-for-managing-your-feature-store)[Feature StoreNovember 12, 20216 minReadReceiving Alerts in Slack/Email/PagerDuty from HopsworksLearn how to set up customized alerts in Hopsworks for different events that are triggered as part of the ingestion pipeline.](https://www.hopsworks.ai/post/receiving-alerts-in-slack-email-pagerduty-from-hopsworks-support-for-managing-your-feature-store)\nErmias Gebremeskel\n[![End-to-end Deep Learning Pipelines with Earth Observation Data in Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6380d0bc1484f157d512b6f7_Extreme%20Earth%20Blog.png)](https://www.hopsworks.ai/post/end-to-end-deep-learning-pipelines-with-earth-observation-data-in-hopsworks)[Data EngineeringOctober 8, 202110 minReadEnd-to-end Deep Learning Pipelines with Earth Observation Data in HopsworksIn this blog post we demonstrate how to build such a pipeline with real-world data in order to develop an iceberg classification model.](https://www.hopsworks.ai/post/end-to-end-deep-learning-pipelines-with-earth-observation-data-in-hopsworks)\nTheofilos Kakantousis\n[![Using an External Python Kafka Client to Interact with a Hopsworks Cluster](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63a56c868cfe910550c1c6fc_Guest%20Python%20Kafka%20Blog.png)](https://www.hopsworks.ai/post/using-an-external-python-kafka-client-to-interact-with-a-hopsworks-cluster)[MLOpsSeptember 27, 202118 minReadUsing an External Python Kafka Client to Interact with a Hopsworks ClusterLearn how to publish (write) and subscribe to (read) streams of events and how to interact with the schema registry and use Avro for data serialization.](https://www.hopsworks.ai/post/using-an-external-python-kafka-client-to-interact-with-a-hopsworks-cluster)\nAhmad Al-Shishtawy\n[![MLOps Wars: Versioned Feature Data with a Lakehouse](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/628cc10d7188da434fe09a13_MLOPS%20Wars%20Blog.png)](https://www.hopsworks.ai/post/mlops-wars-versioned-feature-data-with-a-lakehouse)[MLOpsAugust 3, 202112 minReadMLOps Wars: Versioned Feature Data with a LakehouseWith support to Apache Hudi, the Hopsworks Feature Store offers lakehouse capabilities to improve automated feature pipelines and training pipelines (MLOps).](https://www.hopsworks.ai/post/mlops-wars-versioned-feature-data-with-a-lakehouse)\nDavit Bzhalava\n[![AI Software Architecture for Copernicus Data with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63a56d1785e33d7aa02f1fff_Copernicus%20Data%20Blog.png)](https://www.hopsworks.ai/post/ai-software-architecture-for-copernicus-data-with-hopsworks)[Data EngineeringJuly 12, 202110 minReadAI Software Architecture for Copernicus Data with HopsworksHopsworks brings support for scale-out AI with the ExtremeEarth project which focuses on the most concerning issues of food security and sea mapping.](https://www.hopsworks.ai/post/ai-software-architecture-for-copernicus-data-with-hopsworks)\nTheofilos Kakantousis\n[![Hopsworks Online Feature Store: Fast Access to Feature Data for AI Applications](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6286332cf1f4378d630dea76_fast%20access%20features.jpg)](https://www.hopsworks.ai/post/hopsworks-online-feature-store-fast-access-to-feature-data-for-ai-applications)[Feature StoreJune 17, 202112 minReadHopsworks Online Feature Store: Fast Access to Feature Data for AI ApplicationsRead about how the Hopsworks Feature Store abstracts away the complexity of a dual database system, unifying feature access for online and batch applications. ](https://www.hopsworks.ai/post/hopsworks-online-feature-store-fast-access-to-feature-data-for-ai-applications)\nMoritz Meister\n[![How to build ML models with fastai and Jupyter in Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63aaaa8ef3af997b9065b714_How%20to%20build%20ML%20Models%20Blog.png)](https://www.hopsworks.ai/post/how-to-build-ml-models-with-fastai-and-jupyter-in-hopsworks)[Data EngineeringJune 4, 20216 minReadHow to build ML models with fastai and Jupyter in HopsworksThis tutorial gives an overview of how to work with Jupyter on the platform and train a state-of-the-art ML model using the fastai python library.](https://www.hopsworks.ai/post/how-to-build-ml-models-with-fastai-and-jupyter-in-hopsworks)\nRobin Andersson \n[![Scalable metadata: the new breed of file systems \\(em\\)powering big data companies](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63761d7bf7db427f0b5175d9_Scalable%20Metadada.png)](https://www.hopsworks.ai/post/scalable-metadata-the-new-breed-of-file-systems)[BenchmarkMay 31, 202120 minReadScalable metadata: the new breed of file systems (em)powering big data companiesHopsFS is an open-source scaleout metadata file system, but its primary use case is not Exabyte storage, rather customizable consistent metadata.](https://www.hopsworks.ai/post/scalable-metadata-the-new-breed-of-file-systems)\nJim Dowling\n[![Distributed ML Experiments on Databricks with Maggy](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63a57fa35752eb80df04abd0_Distributed%20ML%20Experiments%20Blog.png)](https://www.hopsworks.ai/post/distributed-ml-experiments-on-databricks-with-maggy)[MLOpsMay 13, 202110 minReadDistributed ML Experiments on Databricks with MaggyLearn how to train a ML model in a distributed fashion without reformatting our code on Databricks with Maggy, open source tool available on Hopsworks.](https://www.hopsworks.ai/post/distributed-ml-experiments-on-databricks-with-maggy)\nRiccardo Grigoletto\n[![How to manage Python libraries in Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63a5af136c31f26b2423a7a5_Python%20Libraries%20Blog.png)](https://www.hopsworks.ai/post/how-to-manage-python-libraries-in-hopsworks)[MLOpsMay 12, 20214 minReadHow to manage Python libraries in HopsworksThis tutorial will show an overview of how to install and manage Python libraries in the platform.](https://www.hopsworks.ai/post/how-to-manage-python-libraries-in-hopsworks)\nRobin Andersson \n[![From 100 to ZeRO: PyTorch and DeepSpeed ZeRO on any Spark Cluster with Maggy](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63a5ae6a68385c781aa1cf7e_DeepSpeed%20ZeRO%20Blog.png)](https://www.hopsworks.ai/post/pytorch-and-deepspeed-zero-on-any-spark-cluster-with-maggy)[MLOpsApril 27, 20217 minReadFrom 100 to ZeRO: PyTorch and DeepSpeed ZeRO on any Spark Cluster with MaggyUse open-source Maggy to write and debug PyTorch code on your local machine and run the code at scale without changing a single line in your program.](https://www.hopsworks.ai/post/pytorch-and-deepspeed-zero-on-any-spark-cluster-with-maggy)\nMoritz Meister\n[![Beyond Brainless AI with a Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63a5b57c6ccbdbe2a360ec7d_Brainless%20AI%20Blog.png)](https://www.hopsworks.ai/post/beyond-brainless-ai-with-a-feature-store)[MLOpsApril 27, 202110 minReadBeyond Brainless AI with a Feature StoreEvolve your models from stateless AI to Total Recall AI with the help of a Feature Store. ](https://www.hopsworks.ai/post/beyond-brainless-ai-with-a-feature-store)\nJim Dowling\n[![Detecting Financial Fraud Using GANs at Swedbank with Hopsworks and NVIDIA GPUs](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/638dc7c4ce20b724edd9d773_Nvidia%20Blog.png)](https://www.hopsworks.ai/post/detecting-financial-fraud-using-gans-at-swedbank-with-hopsworks-and-nvidia-gpus)[Feature StoreMarch 26, 202115 minReadDetecting Financial Fraud Using GANs at Swedbank with Hopsworks and NVIDIA GPUsRecently, one of Sweden\u2019s largest banks trained generative adversarial neural networks (GANs) using NVIDIA GPUs as part of its fraud and money-laundering prevention strategy.](https://www.hopsworks.ai/post/detecting-financial-fraud-using-gans-at-swedbank-with-hopsworks-and-nvidia-gpus)\nJim Dowling\n[![AI/ML needs a Key-Value store, and Redis is not up to it](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61eace23ecf46e36f195d5d1_REDIS.png)](https://www.hopsworks.ai/post/ai-ml-needs-a-key-value-store-and-redis-is-not-up-to-it)[BenchmarkFebruary 26, 202112 minReadAI/ML needs a Key-Value store, and Redis is not up to itSeeing how Redis is a popular open-source feature store with features significantly similar to RonDB, we compared the innards of RonDB\u2019s multithreading architecture to the commercial Redis products. ](https://www.hopsworks.ai/post/ai-ml-needs-a-key-value-store-and-redis-is-not-up-to-it)\nMikael Ronstr\u00f6m\n[![How to engineer and use Features in Azure ML Studio with the Hopsworks Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63aab17fc753bd66885fdf6e_Azure%20Ml%20Studio%20Blog.png)](https://www.hopsworks.ai/post/how-to-engineer-and-use-features-in-azure-ml-studio-with-hopsworks)[Feature StoreFebruary 25, 20218 minReadHow to engineer and use Features in Azure ML Studio with the Hopsworks Feature StoreLearn how to design and ingest features, browse existing features, create training datasets as DataFrames or as files on Azure Blob storage.](https://www.hopsworks.ai/post/how-to-engineer-and-use-features-in-azure-ml-studio-with-hopsworks)\nMoritz Meister\n[![How to transform Amazon Redshift data into features with Hopsworks Feature Store](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63aabe5fcefc0ff23bdbe0aa_Amazon%20Redshift%20Blog.png)](https://www.hopsworks.ai/post/how-to-transform-amazon-redshift-data-into-features-with-hopsworks)[Feature StoreFebruary 9, 20219 minReadHow to transform Amazon Redshift data into features with Hopsworks Feature StoreConnect the Hopsworks Feature Store to Amazon Redshift to transform your data into features to train models and make predictions.](https://www.hopsworks.ai/post/how-to-transform-amazon-redshift-data-into-features-with-hopsworks)\nErmias Gebremeskel\n[![Elasticsearch is dead, long live Open Distro for Elasticsearch](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61e697708af5d9c46a0395a6_DFS.png)](https://www.hopsworks.ai/post/open-distro-for-elasticsearch-dynamic-support-hopsworks)[Data ScienceJanuary 14, 202113 minReadElasticsearch is dead, long live Open Distro for ElasticsearchIn this blog, we describe how we leverage the authentication and authorization support in Open Distro for Elasticsearch to make elasticsearch a project-based multi-tenant service in Hopsworks.](https://www.hopsworks.ai/post/open-distro-for-elasticsearch-dynamic-support-hopsworks)\nMahmoud Ismail\n[![HopsFS file system: 100X Times Faster than AWS S3](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61b8ded75115481bbce78769_faster%20file%20system.jpg)](https://www.hopsworks.ai/post/faster-than-aws-s3)[BenchmarkNovember 19, 202013 minReadHopsFS file system: 100X Times Faster than AWS S3Many developers believe S3 is the \"end of file system history\". It is impossible to build a file/object storage system on AWS that can compete with S3 on cost. But what if you could build on top of S3](https://www.hopsworks.ai/post/faster-than-aws-s3)\nMahmoud Ismail\n[![Feature Store for MLOps? Feature reuse means JOINs](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/637614127359609170ae02f8_FS%20for%20MLOps.png)](https://www.hopsworks.ai/post/feature-store-for-mlops-feature-reuse-means-joins)[Feature StoreOctober 23, 20207 minReadFeature Store for MLOps? Feature reuse means JOINsUse JOINs for feature reuse to save on infrastructure and the number of feature pipelines needed to maintain models in production.](https://www.hopsworks.ai/post/feature-store-for-mlops-feature-reuse-means-joins)\nJim Dowling\n[![ML Engineer Guide: Feature Store vs Data Warehouse](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/61b9d4ec9a1b41068de189e7_005%20copy.png)](https://www.hopsworks.ai/post/feature-store-vs-data-warehouse)[Feature StoreOctober 8, 202013 minReadML Engineer Guide: Feature Store vs Data Warehouse The feature store is a data warehouse of features for machine learning (ML). Architecturally, it differs from the traditional data warehouse in that it is a dual-database.](https://www.hopsworks.ai/post/feature-store-vs-data-warehouse)\nJim Dowling\n[![One Function is All you Need: Machine Learning Experiments with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/63aac419fdf77f36ae64b4c9_ML%20Experiments%20Blog.png)](https://www.hopsworks.ai/post/machine-learning-experiments-with-hopsworks)[MLOpsSeptember 30, 202016 minReadOne Function is All you Need: Machine Learning Experiments with HopsworksHopsworks supports machine learning experiments to track and distribute ML for free and with a built-in TensorBoard.](https://www.hopsworks.ai/post/machine-learning-experiments-with-hopsworks)\nRobin Andersson \n[![How we secure your data with Hopsworks](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/627e76114c646d8045325fbc_Secure%20Data.png)](https://www.hopsworks.ai/post/how-we-secure-your-data-with-hopsworks)[Feature StoreJuly 16, 202015 minReadHow we secure your data with HopsworksIntegrate with third-party security standards and take advantage from our project-based multi-tenancy model to host data in one single shared cluster.](https://www.hopsworks.ai/post/how-we-secure-your-data-with-hopsworks)\nJim Dowling\n[![Untitled UI logotext](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/630e3413d3fafa0f79c52da2_hopsworks-logo%202022_white.svg)![Logo](https://cdn.prod.website-files.com/5f6353590bb01cacbcecfbac/666c3cc1cfc4741e6b2d9fe6_untitled-ui-logo.png)The AI Lakehouse](https://www.hopsworks.ai/)\nProduct\n[Hopsworks Enterprise](https://www.hopsworks.ai/try)[Capabilities](https://www.hopsworks.ai/product-capabilities)[Integrations](https://www.hopsworks.ai/integrations)[Examples](https://www.hopsworks.ai/hopsworks-examples)[Pricing](https://www.hopsworks.ai/pricing)[App Status](https://hopsworks.statuspage.io/)[FAQ](https://www.hopsworks.ai/frequently-asked-questions)\nSolutions\n[Industry & Team Solutions](https://www.hopsworks.ai/solutions/all)[Generative AI](https://www.hopsworks.ai/use-case/fine-tuning-llms-rag-for-genai)[Real-time Fraud Detection ](https://www.hopsworks.ai/use-case/realtime-fraud-detection)[Hopsworks Medical Copilot](https://www.hopscopilot.com/)[Customers](https://www.hopsworks.ai/customers)\nResources\n[Blog](https://www.hopsworks.ai/blog)[MLOps Dictionary](https://www.hopsworks.ai/mlops-dictionary)[Events](https://www.hopsworks.ai/events)[Documentation](https://docs.hopsworks.ai/latest/)[Academy](https://www.hopsworks.ai/academy)[Research Papers](https://www.hopsworks.ai/research-papers)[Feature Store Comparison](https://www.hopsworks.ai/product-comparison/sagemaker)[Community](https://community.hopsworks.ai/)[FAQ: EU AI Act](https://www.hopsworks.ai/faq-eu-ai-act)\nCompany\n[About Us](https://www.hopsworks.ai/about-us)[News](https://www.hopsworks.ai/news)[Security & Compliance](https://www.hopsworks.ai/security-compliance)[Contact Us](https://www.hopsworks.ai/contact/main)\nJoin our newsletter\n**Receive the latest product updates, upcoming events, and industry news.**\n\u00a9 Hopsworks 2025. All rights reserved. Various trademarks held by their respective owners.\n[](https://join.slack.com/t/public-hopsworks/shared_invite/zt-1uf21vitz-rhHKNdIf8GEiOf1EJ6Wzsw)[](https://github.com/logicalclocks/hopsworks)[](https://www.linkedin.com/company/hopsworks/)[](https://twitter.com/hopsworks)[](https://www.youtube.com/@hopsworks)\n[Privacy Policy](https://www.iubenda.com/privacy-policy/90800199 \"Privacy Policy \")\n[Cookie Policy](https://www.iubenda.com/privacy-policy/90800199/cookie-policy \"Cookie Policy \")\n[Terms and Conditions](https://www.iubenda.com/terms-and-conditions/90800199 \"Terms and Conditions \")\n",
    "answer": "```markdown\n## TL;DR Summary\n\nScheduled maintenance for app.hopsworks.ai on November 26 from 07:00 to 17:00 UTC. The platform will be inaccessible during this time. For updates, visit [Hopsworks News](https://www.hopsworks.ai/news/hopsworks-4-0-breaking-changes).\n```"
  }
]